/* This file has been generated by the Hex-Rays decompiler.
   Copyright (c) 2007-2014 Hex-Rays <info@hex-rays.com>

   Detected compiler: GNU C++
*/

#include <defs.h>


//-------------------------------------------------------------------------
// Function declarations

__int64 __fastcall ccansikdf_x963(__int64 a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, void *a7);
__int64 __fastcall ccecies_encrypt_gcm_composite(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, unsigned __int64 a8, const void *a9, __int64 a10, __int64 a11);
bool __fastcall ccrsa_pairwise_consistency_check(__int64 *a1);
int __fastcall ccrsa_dump_key(__int64 *a1);
signed __int64 __fastcall ccrsa_crt_makekey(__int64 a1, const void *a2, __int64 a3, __int64 a4, void *a5, void *a6, unsigned __int64 a7, void *a8);
signed __int64 __fastcall ccrsa_generate_key_raw(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4, __int64 a5);
signed __int64 __fastcall ccrsa_generate_key(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4, __int64 a5);
__int64 __fastcall cccmac_final(__int64 a1, signed __int64 a2, unsigned __int64 a3, signed __int64 a4, __int64 a5);
signed __int64 __fastcall ccec_check_pub(__int64 **a1);
void __fastcall ccmode_ccm_reset(__int64 a1, __int64 a2);
// void __usercall ONE(__int64 a1@<rax>);
// void __usercall TWO(__int64 a1@<rax>);
// int __usercall ctr_crypt@<eax>(unsigned __int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __m128i a6@<xmm4>, __m128i a7@<xmm5>, __m128i a8@<xmm6>, __m128i a9@<xmm7>, __m128i a10@<xmm8>, __m128i a11@<xmm9>, __m128i a12@<xmm10>, __m128i a13@<xmm11>, __m128i a14@<xmm12>, __m128i a15@<xmm13>, __m128i a16@<xmm14>, __m128i a17@<xmm15>);
int __fastcall Main_Decrypt_Loop(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD); // weak
unsigned __int64 __fastcall ccecies_decrypt_gcm_plaintext_size(__int64 **a1, __int64 a2, __int64 a3);
// __int64 __usercall ccsha256_vng_intel_avx1_compress@<rax>(__int64 _RDX@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM4@<xmm4>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>);
signed __int64 __fastcall vng_aes_encrypt_opt(__int64 a1, __int64 a2, __int64 a3);
signed __int64 __fastcall vng_aes_decrypt_opt(__int64 a1, __int64 a2, __int64 a3);
signed __int64 __fastcall aes_encrypt_xmm_no_save(__int64 a1, __int64 a2, __int64 a3);
signed __int64 __fastcall aes_decrypt_xmm_no_save(__int64 a1, __int64 a2, __int64 a3);
void __fastcall vng_aes_encrypt_opt_key(__int64 a1, signed int a2);
int vng_aes_encrypt_key128();
int EKeyHas4Words(void); // weak
int vng_aes_encrypt_key192();
int EKeyHas6Words(void); // weak
int vng_aes_encrypt_key256();
int EKeyHas8Words(void); // weak
// __int64 __usercall InvMixColumn@<rax>(unsigned int a1@<eax>);
void __fastcall vng_aes_decrypt_opt_key(__int64 a1, signed int a2);
int vng_aes_decrypt_key128();
int DKeyHas4Words(void); // weak
int vng_aes_decrypt_key192();
int DKeyHas6Words(void); // weak
int vng_aes_decrypt_key256();
int DKeyHas8Words(void); // weak
__int64 __fastcall ccrsa_decrypt_oaep(__int64 *a1, __int64 a2, __int64 a3, void *a4, unsigned __int64 a5, unsigned __int64 a6, unsigned __int64 a7, const void *a8);
signed __int64 __fastcall vng_aes_encrypt_aesni(__m128i *a1, __int64 a2, __int64 _RDX);
signed __int64 __fastcall vng_aes_decrypt_aesni(__m128i *a1, __int64 a2, __int64 _RDX);
signed __int64 __fastcall vng_aes_encrypt_aesni_key(__int64 a1, signed __int64 a2, __int64 a3);
// void __usercall sub_6C90(__int64 a1@<rdx>, __int64 a2@<rcx>, __m128i a3@<xmm1>, __m128i a4@<xmm2>);
// void __usercall sub_6D70(__int64 a1@<rcx>, __m128i a2@<xmm1>, __m128i a3@<xmm2>, __m128i a4@<xmm3>);
// void __usercall sub_6E50(__int64 a1@<rcx>, __m128i a2@<xmm1>, __m128i a3@<xmm2>, __m128i a4@<xmm3>);
// void __usercall sub_6EB0(__int64 a1@<rcx>, __m128i a2@<xmm1>, __m128i a3@<xmm2>);
signed __int64 __fastcall vng_aes_decrypt_aesni_key(__int64 a1, signed __int64 a2, __int64 a3);
__int64 __fastcall vng_aes_encrypt_opt_cbc(__m128i *a1, __int64 a2, signed int a3, __int64 a4, __int64 a5);
void __fastcall vng_aes_decrypt_opt_cbc(__int64 a1, __m128i *a2, int a3, __int64 a4, __int64 a5);
signed __int64 __fastcall vng_aes_encrypt_cbc_hw(__m128i *a1, __m128i *a2, signed int a3, __int64 a4, __int64 a5);
void __fastcall vng_aes_decrypt_cbc_hw(__int64 a1, __m128i *a2, int a3, __int64 a4, __int64 a5);
__int64 __fastcall ccaes_gladman_encrypt(signed __int64 a1, __int64 a2, signed __int64 a3, __int64 a4, signed __int64 a5);
__int64 __fastcall ccaes_gladman_decrypt(__int64 a1, void *a2, signed __int64 a3, __int64 a4, __int64 a5);
signed __int64 __fastcall ccder_encode_rsa_priv(__int64 *a1, __int64 a2, unsigned __int64 a3);
__int64 __fastcall ccecies_encrypt_gcm_setup(__int64 a1, __int64 a2, __int64 a3, __int64 a4, int a5, int a6, unsigned int a7);
__int64 __fastcall ccec_sign_composite(__int64 a1, unsigned __int64 a2, unsigned __int64 a3, void *a4, void *a5, __int64 a6);
signed __int64 __fastcall ccec_der_export_priv_size(__int64 **a1, __int64 a2, int a3);
signed __int64 __fastcall ccec_der_export_priv(__int64 **a1, __int64 a2, int a3, __int64 a4, __int64 a5);
__int64 __fastcall ccaes_gladman_encrypt_key128(__int64 a1, __int64 a2);
__int64 __fastcall ccaes_gladman_encrypt_key192(__int64 a1, __int64 a2);
__int64 __fastcall ccaes_gladman_encrypt_key256(__int64 a1, __int64 a2);
__int64 __fastcall ccaes_gladman_encrypt_key(__int64 a1, signed __int64 a2, __int64 a3);
__int64 __fastcall ccaes_gladman_decrypt_key128(__int64 a1, __int64 a2);
__int64 __fastcall ccaes_gladman_decrypt_key192(__int64 a1, __int64 a2);
__int64 __fastcall ccaes_gladman_decrypt_key256(__int64 a1, __int64 a2);
__int64 __fastcall ccaes_gladman_decrypt_key(__int64 a1, signed __int64 a2, __int64 a3);
void *__fastcall ccrsa_init_pub(__int64 a1, const void *a2, const void *a3);
void gen_tabs();
void __fastcall aesxts_mult_x(__m128i *a1);
signed __int64 __fastcall aesxts_tweak_crypt_opt(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4);
signed __int64 __fastcall aesxts_tweak_crypt_aesni(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4);
signed __int64 __fastcall aesxts_tweak_crypt_group_aesni(__int64 a1, __int64 a2, __m128i *a3, __int64 a4, int a5);
void __fastcall aesxts_tweak_crypt_group_opt(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4, int a5);
signed __int64 __fastcall aesxts_tweak_uncrypt_opt(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4);
signed __int64 __fastcall aesxts_tweak_uncrypt_aesni(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4);
signed __int64 __fastcall aesxts_tweak_uncrypt_group_aesni(__int64 a1, __int64 a2, __m128i *a3, __int64 a4, int a5);
void __fastcall aesxts_tweak_uncrypt_group_opt(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4, int a5);
signed __int64 __fastcall ccrsa_sign(__int64 *a1, __int64 a2, void *a3, __int64 a4, __int64 a5);
__int64 __fastcall CC_CAST_encrypt(__int64 a1, __int64 a2);
__int64 __fastcall CC_CAST_decrypt(__int64 a1, __int64 a2);
__int64 __fastcall CC_CAST_set_key(__int64 a1, unsigned __int64 a2, __int64 a3);
signed __int64 __fastcall ccder_encode_eckey_size(unsigned __int64 a1, __int64 a2, __int64 a3);
signed __int64 __fastcall ccder_encode_eckey(size_t a1, const void *a2, __int64 a3, size_t a4, const void *a5, __int64 a6, size_t a7);
__int64 (*ccaes_cbc_decrypt_mode())[2];
__int64 (*ccaes_cbc_encrypt_mode())[2];
signed __int64 __fastcall ccec_compact_import_pub_size(signed __int64 a1);
__int64 __fastcall ccec_compact_import_pub(__int64 *a1, __int64 a2, unsigned __int64 a3, __int64 a4);
// void __usercall ccmode_ccm_encrypt_x86_64(unsigned __int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __m128i a6@<xmm0>, __m128i a7@<xmm1>, __m128i a8@<xmm2>, __m128i a9@<xmm3>);
__int64 (*ccaes_ecb_decrypt_mode())[2];
__int64 (*ccaes_ecb_encrypt_mode())[2];
__int64 __fastcall ccwrap_auth_encrypt(__int64 a1, __int64 a2, unsigned __int64 a3, void *a4, __int64 a5, void *a6);
__int64 __fastcall ccaes_gladman_cbc_decrypt_init(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4);
__int64 __fastcall ccaes_gladman_cbc_encrypt_init(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4);
// __int64 __usercall ccsha256_vng_intel_ssse3_compress@<rax>(__int64 a1@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm4>, __m128i a9@<xmm5>, __m128i a10@<xmm6>, __m128i a11@<xmm7>);
// void __usercall ONE_0(__int64 a1@<rax>);
// int __usercall ccm128_encrypt@<eax>(__m128i a1@<xmm0>, __m128i a2@<xmm1>, __m128i a3@<xmm2>, __m128i a4@<xmm3>, __m128i a5@<xmm4>, __m128i a6@<xmm5>);
int Main_Loop(void); // weak
void __fastcall init_wrapper_opt(__int64 a1, __int64 a2, signed int a3, __int64 a4);
__int64 __fastcall cbc_wrapper_opt(__int64 a1, __m128i *a2, __int64 a3, __int64 a4, __int64 a5);
signed __int64 __fastcall init_wrapper_aesni(__int64 a1, __int64 a2, unsigned int a3, __int64 a4);
__int64 __fastcall cbc_wrapper_aesni(__int64 a1, __m128i *a2, __int64 a3, __int64 a4, __int64 a5);
void __fastcall init_wrapper_opt_0(__int64 a1, __int64 a2, signed int a3, __int64 a4);
void *__fastcall cbc_wrapper_opt_0(__int64 a1, __int64 a2, __int64 a3, __m128i *a4, __int64 a5);
signed __int64 __fastcall init_wrapper_aesni_0(__int64 a1, __int64 a2, unsigned int a3, __int64 a4);
void *__fastcall cbc_wrapper_aesni_0(__int64 a1, __m128i *a2, __int64 a3, __m128i *a4, __int64 a5);
void __fastcall init_wrapper_opt_1(__int64 a1, __int64 a2, signed int a3, __int64 a4);
signed __int64 __fastcall ecb_wrapper_opt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
signed __int64 __fastcall init_wrapper_aesni_1(__int64 a1, __int64 a2, unsigned int a3, __int64 a4);
__int64 __fastcall ecb_wrapper_aesni(__int64 a1, int a2, __int64 a3, __int64 a4);
int __fastcall ccmode_ccm_init(__int64 a1, __int64 a2);
void __fastcall init_wrapper_opt_2(__int64 a1, __int64 a2, signed int a3, __int64 a4);
signed __int64 __fastcall ecb_wrapper_opt_0(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
signed __int64 __fastcall init_wrapper_aesni_2(__int64 a1, __int64 a2, unsigned int a3, __int64 a4);
__int64 __fastcall ecb_wrapper_aesni_0(__int64 a1, int a2, __int64 a3, __int64 a4);
void __fastcall init_wrapper_opt_3(__int64 a1, __int64 a2, signed int a3, __int64 a4, __int64 a5);
signed __int64 __fastcall set_tweak_wrapper_opt(__int64 a1, __int64 a2, __int64 a3);
__m128i *__fastcall xts_wrapper_opt(__int64 a1, __m128i *a2, __int64 a3, __m128i *a4, __m128i *a5);
signed __int64 __fastcall init_wrapper_aesni_3(__int64 a1, __int64 a2, unsigned int a3, __int64 a4, __int64 a5);
signed __int64 __fastcall set_tweak_wrapper_aesni(__int64 a1, __int64 a2, __m128i *a3);
__m128i *__fastcall xts_wrapper_aesni(__int64 a1, __m128i *a2, __int64 a3, __int64 a4, __int64 a5);
// void __usercall ccmode_gcm_init(__int64 a1@<rdi>, __int64 a2@<rsi>, __m128i a3@<xmm0>, __m128i a4@<xmm1>, __m128i a5@<xmm2>, __m128i a6@<xmm3>);
__int64 __fastcall ccrsa_pub_crypt(__int64 *a1, __int64 a2, __int64 a3);
signed __int64 __fastcall ccec_der_import_priv_keytype(__int64 a1, unsigned __int64 a2, __int64 a3, __int64 a4);
signed __int64 __fastcall ccec_der_import_priv(unsigned __int64 *a1, __int64 a2, unsigned __int64 a3, __int64 a4);
__int64 __fastcall ccrsa_verify(__int64 *a1, size_t *a2, void *a3, unsigned __int64 a4, unsigned __int64 a5, __int64 a6);
void __fastcall init_wrapper_opt_4(__int64 a1, __int64 a2, signed int a3, __int64 a4, __int64 a5);
signed __int64 __fastcall set_tweak_wrapper_opt_0(__int64 a1, __int64 a2, __int64 a3);
// __m128i *__usercall xts_wrapper_opt_0@<rax>(__int64 a1@<rdx>, __m128i *a2@<rcx>, __int64 a3@<rdi>, __m128i *a4@<rsi>, __m128i *a5@<r8>, __m128i *a6@<r15>);
signed __int64 __fastcall init_wrapper_aesni_4(__int64 a1, __int64 a2, unsigned int a3, __int64 a4, __int64 a5);
signed __int64 __fastcall set_tweak_wrapper_aesni_0(__int64 a1, __int64 a2, __m128i *a3);
// __m128i *__usercall xts_wrapper_aesni_0@<rax>(__int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __m128i *a4@<rsi>, __int64 a5@<r8>, __m128i *a6@<r15>);
bool __fastcall ccec_is_point(signed __int64 *a1, __int64 a2);
bool __fastcall ccec_is_point_projective(signed __int64 *a1, __int64 a2);
__int64 __fastcall ccaes_ecb_decrypt_init(__int64 a1, __int64 a2, signed int a3, __int64 a4);
void __fastcall ccaes_ecb_decrypt(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4);
__int64 __fastcall ccaes_ecb_encrypt_init(__int64 a1, __int64 a2, signed int a3, __int64 a4);
void __fastcall ccaes_ecb_encrypt(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4);
__int64 (*ccaes_xts_decrypt_mode())[3];
__int64 __fastcall ccder_decode_rsa_pub(__int64 a1, unsigned __int64 a2, unsigned __int64 a3);
__int64 (*ccaes_xts_encrypt_mode())[3];
__int64 ccblowfish_ecb_decrypt_mode();
__int64 ccblowfish_ecb_encrypt_mode();
__int64 *ccblowfish_cbc_encrypt_mode();
__int64 *ccblowfish_cbc_decrypt_mode();
__int64 *ccblowfish_cfb_encrypt_mode();
__int64 *ccblowfish_cfb_decrypt_mode();
__int64 *ccblowfish_cfb8_encrypt_mode();
__int64 *ccblowfish_cfb8_decrypt_mode();
__int64 *ccblowfish_ctr_crypt_mode();
__int64 *ccblowfish_ofb_crypt_mode();
__int64 __fastcall ccblowfish_ltc_setup(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
void __fastcall ccblowfish_ltc_ecb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
signed __int64 __fastcall ccder_encode_rsa_pub_size(__int64 *a1);
void __fastcall ccblowfish_ltc_ecb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
__int64 cccast_ecb_decrypt_mode();
__int64 cccast_ecb_encrypt_mode();
__int64 *cccast_cbc_encrypt_mode();
__int64 *cccast_cbc_decrypt_mode();
__int64 *cccast_cfb_encrypt_mode();
__int64 *cccast_cfb_decrypt_mode();
__int64 *cccast_cfb8_encrypt_mode();
__int64 *cccast_cfb8_decrypt_mode();
__int64 *cccast_ctr_crypt_mode();
__int64 *cccast_ofb_crypt_mode();
__int64 __fastcall cced25519_make_key_pair(__int64 a1, __int64 a2, __int64 a3, const void *a4);
__int64 __fastcall cccast_ecb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
__int64 __fastcall ccrsa_get_fullkey_components(__int64 *a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9);
__int64 __fastcall cccast_ecb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
__int64 __fastcall cccast_setup(__int64 a1, __int64 a2, unsigned __int64 a3, __int64 a4);
// __int64 __usercall ccder_decode_bitstring@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, unsigned __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>);
// signed __int64 __usercall ccder_decode_constructed_tl@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, unsigned __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>);
signed __int64 __fastcall ccder_decode_len(__int64 a1, unsigned __int64 a2, unsigned __int64 a3);
__int64 __fastcall ccder_decode_oid(__int64 a1, unsigned __int64 a2, unsigned __int64 a3);
__int64 __fastcall ccder_decode_seqii(unsigned __int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4, unsigned __int64 a5);
signed __int64 __fastcall ccder_decode_sequence_tl(__int64 a1, unsigned __int64 a2, unsigned __int64 a3);
__int64 __fastcall ccrsa_get_pubkey_components(__int64 *a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5);
void __fastcall ccecies_decrypt_gcm_setup(__int64 a1, __int64 a2, __int64 a3, int a4, int a5, int a6);
signed __int64 __fastcall ccder_decode_tag(__int64 a1, unsigned __int64 a2, unsigned __int64 a3);
// signed __int64 __usercall ccder_decode_tl@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, unsigned __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>);
// __int64 __usercall ccder_decode_uint@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, unsigned __int64 a3@<rcx>, unsigned __int64 a4@<rdi>, __int64 a5@<rsi>);
unsigned __int64 __fastcall ccder_decode_uint64(__int64 a1, unsigned __int64 a2, unsigned __int64 a3);
__int64 __fastcall ccder_encode_body(size_t a1, const void *a2, __int64 a3, size_t a4);
unsigned __int64 __fastcall ccder_encode_body_nocopy(__int64 a1, __int64 a2, unsigned __int64 a3);
signed __int64 __fastcall ccder_encode_constructed_tl(unsigned __int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4);
signed __int64 __fastcall ccder_encode_implicit_integer(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5);
signed __int64 __fastcall ccder_encode_implicit_octet_string(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5);
signed __int64 __fastcall ccder_encode_implicit_raw_octet_string(unsigned __int64 a1, size_t a2, const void *a3, __int64 a4, size_t a5);
signed __int64 __fastcall ccder_encode_implicit_uint64(unsigned __int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4);
signed __int64 __fastcall ccder_encode_integer(__int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4);
signed __int64 __fastcall ccder_encode_len(unsigned __int64 a1, __int64 a2, unsigned __int64 a3);
__int64 __fastcall ccnistkdf_fb_hmac_fixed(__int64 a1, int a2, unsigned __int64 a3, void *a4, unsigned __int64 a5, const void *a6, unsigned __int8 a7, __int64 a8, unsigned __int64 a9, void *a10);
__int64 __fastcall ccnistkdf_fb_hmac(__int64 a1, int a2, __int64 a3, void *a4, size_t a5, const void *a6, size_t a7, const void *a8, __int64 a9, unsigned __int64 a10, unsigned __int64 a11, void *a12);
__int64 __fastcall ccrsa_eme_pkcs1v15_decode(unsigned __int64 *a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4);
signed __int64 __fastcall ccder_encode_octet_string(__int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4);
int __fastcall ccec_generate_key_internal_fips(__int64 *a1, __int64 a2, __int64 a3);
__int64 __fastcall ccder_encode_oid(__int64 a1, __int64 a2, size_t a3);
signed __int64 __fastcall ccder_encode_raw_octet_string(size_t a1, const void *a2, __int64 a3, size_t a4);
signed __int64 __fastcall ccec_make_priv(signed __int64 a1, unsigned __int64 a2, unsigned __int64 a3, unsigned __int64 a4, unsigned __int64 a5, unsigned __int64 a6, unsigned __int64 a7, __int64 a8);
signed __int64 __fastcall ccder_encode_tag(unsigned __int64 a1, __int64 a2, unsigned __int64 a3);
signed __int64 __fastcall ccder_encode_tl(unsigned __int64 a1, unsigned __int64 a2, __int64 a3, unsigned __int64 a4);
signed __int64 __fastcall ccder_encode_uint64(__int64 a1, __int64 a2, unsigned __int64 a3);
signed __int64 __fastcall ccder_sizeof(__int64 a1, unsigned __int64 a2);
signed __int64 __fastcall ccder_sizeof_implicit_integer(__int64 a1, __int64 a2, __int64 a3);
// int __usercall cc_print@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>);
signed __int64 __fastcall ccder_sizeof_implicit_octet_string(__int64 a1, __int64 a2, __int64 a3);
signed __int64 __fastcall ccder_sizeof_implicit_raw_octet_string(__int64 a1, unsigned __int64 a2);
__int64 *ccaes_gcm_decrypt_mode();
signed __int64 __fastcall ccder_sizeof_implicit_uint64(__int64 a1, __int64 a2);
signed __int64 __fastcall ccder_sizeof_integer(__int64 a1, __int64 a2);
__int64 __fastcall ccecies_decrypt_gcm_composite(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9, unsigned int *a10, __int64 a11);
signed __int64 __fastcall ccder_sizeof_len(unsigned __int64 a1);
signed __int64 __fastcall ccder_sizeof_octet_string(__int64 a1, __int64 a2);
signed __int64 __fastcall ccder_sizeof_oid(__int64 a1);
signed __int64 __fastcall ccder_sizeof_raw_octet_string(unsigned __int64 a1);
// void __usercall ONE_1(__int64 a1@<rax>);
// int __usercall ccm128_decrypt@<eax>(int a1@<ecx>, const __m128i *a2@<rdi>, __m128i *a3@<rsi>, __int64 a4@<r8>, const __m128i *a5@<r9>, __m128i a6@<xmm0>, __m128i a7@<xmm1>, __m128i a8@<xmm2>, __m128i a9@<xmm3>, __m128i a10@<xmm4>, __m128i a11@<xmm5>, int a12);
int __fastcall Main_Loop_0(_QWORD, _QWORD); // weak
signed __int64 __fastcall ccder_sizeof_tag(__int64 a1);
signed __int64 __fastcall ccder_sizeof_uint64(__int64 a1);
__int64 ccdes3_ecb_decrypt_mode();
__int64 ccdes3_ecb_encrypt_mode();
__int64 __fastcall ccdes3_ltc_setup(__int64 a1, void *a2, __int64 a3, __int64 a4);
__int64 __fastcall ltc_des3_setup(__int64 a1, __int64 a2, void *a3);
__int64 __fastcall ccdes168_ltc_setup(__int64 a1, void *a2, __int64 a3, __int64 a4);
__int64 __fastcall ltc_des3_ecb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
__int64 __fastcall cced25519_verify(__int64 a1, unsigned __int64 a2, const void *a3, __int64 a4, __int64 a5);
__int64 __fastcall ltc_des3_ecb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
__int64 *ccdes_cbc_encrypt_mode();
__int64 *ccdes_cbc_decrypt_mode();
__int64 *ccdes_cfb_encrypt_mode();
__int64 *ccdes_cfb_decrypt_mode();
__int64 *ccdes_cfb8_encrypt_mode();
__int64 *ccdes_cfb8_decrypt_mode();
__int64 *ccdes_ctr_crypt_mode();
__int64 *ccdes_ofb_crypt_mode();
__int64 *ccdes3_cbc_encrypt_mode();
__int64 *ccdes3_cbc_decrypt_mode();
__int64 *ccdes3_cfb_encrypt_mode();
__int64 *ccdes3_cfb_decrypt_mode();
__int64 *ccdes3_cfb8_encrypt_mode();
__int64 *ccdes3_cfb8_decrypt_mode();
__int64 *ccdes3_ctr_crypt_mode();
__int64 *ccdes3_ofb_crypt_mode();
char __fastcall ccrsa_test_verify_pkcs1v15_vector(__int64 a1);
__int64 ccdes_ecb_decrypt_mode();
__int64 ccdes_ecb_encrypt_mode();
__int64 __fastcall ccec_verify_internal(const void *a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccdes_key_is_weak(void *a1, __int64 a2);
__int64 __fastcall ccdes_key_set_odd_parity(__int64 a1, __int64 a2);
// void __usercall ONE_2(__int64 a1@<rax>);
// void __usercall TWO_0(__int64 a1@<rax>);
// __int64 __usercall gcmEncrypt_avx1@<rax>(__int64 _RDX@<rdx>, unsigned __int64 a2@<rcx>, __int64 _RDI@<rdi>, __int64 _RSI@<rsi>, __int64 a5@<r8>, __int64 _R9@<r9>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM4@<xmm4>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>, __int128 _XMM8@<xmm8>, __int128 _XMM9@<xmm9>, __int128 _XMM10@<xmm10>, __int128 _XMM11@<xmm11>, __int128 _XMM12@<xmm12>, __int128 _XMM13@<xmm13>, __int128 _XMM14@<xmm14>, __int128 _XMM15@<xmm15>, __int128 a23, __int128 a24, __int128 a25, __int128 a26, __int128 a27, __int64 a28, __int128 a29, __int128 a30, int a31, int a32, int a33, int a34, int a35, int a36, int a37, int a38, int a39, __int128 a40, __int128 a41, __int128 a42, __int128 a43, __int128 a44, __int128 a45, __int128 a46, __int128 a47, __int128 a48, __int128 a49, __int128 a50, __int128 a51, __int128 a52, __int128 a53, __int128 a54, __int128 a55);
// __int64 __usercall Main_Encrypt_Loop@<rax>(int a1@<edx>, __int64 a2@<rcx>, __int64 _RDI@<rdi>, __int64 _RSI@<rsi>, __int64 _R8@<r8>, __int64 _R9@<r9>, unsigned int a7@<r10d>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>, __int128 _XMM8@<xmm8>, __int128 _XMM9@<xmm9>, __int128 _XMM10@<xmm10>, __int128 _XMM11@<xmm11>, __int128 _XMM12@<xmm12>, __int128 _XMM13@<xmm13>, __int128 _XMM14@<xmm14>, __int128 _XMM15@<xmm15>, __int128 a23, __int128 a24, __int128 a25, __int128 a26, __int128 a27, __int64 a28, __int128 a29, __int128 a30, int a31, int a32, int a33, int a34, int a35, int a36, int a37, int a38, int a39, __int128 a40, __int128 a41, __int128 a42, __int128 a43, __int128 a44, __int128 a45, __int128 a46, __int128 a47, __int128 a48, __int128 a49, __int128 a50, __int128 a51, __int128 a52, __int128 a53, __int128 a54, __int128 a55);
__int64 __fastcall End_Main_Encrypt_Loop(_DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128); // weak
// int __usercall gcmDecrypt_avx1@<eax>(__int64 _RDX@<rdx>, unsigned __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r9>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM4@<xmm4>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>, __int128 _XMM8@<xmm8>, __int128 _XMM9@<xmm9>, __int128 _XMM10@<xmm10>, __int128 _XMM11@<xmm11>, __int128 _XMM12@<xmm12>, __int128 _XMM13@<xmm13>, __int128 _XMM14@<xmm14>, __int128 _XMM15@<xmm15>);
int __fastcall Main_Decrypt_Loop_0(_QWORD, _QWORD, _QWORD, _QWORD); // weak
__int64 __fastcall ccdes_ltc_setup(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
__int64 __fastcall ltc_des_ecb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
__int64 __fastcall ccec_sign_internal(__int64 a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ltc_des_ecb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
// __int64 __usercall ccdigest@<rax>(__int64 a1@<rax>, const void *a2@<rdx>, __int64 a3@<rcx>, __int64 a4@<rdi>, unsigned __int64 a5@<rsi>);
unsigned __int64 __fastcall ccec_signature_r_s_size(__int64 **a1);
__int64 __fastcall ccwrap_auth_decrypt(__int64 a1, __int64 a2, unsigned __int64 a3, const void *a4, __int64 a5, signed __int64 a6);
void __fastcall ccmode_ccm_crypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5);
void __fastcall ccmode_ccm_encrypt(__int64 a1, __int64 a2, unsigned __int64 a3, __int64 a4, __int64 a5);
int __fastcall ccdigest_final_64be(__int64 a1, __int64 a2, __int64 a3);
int __fastcall ccdigest_final_64le(__int64 a1, __int64 a2, __int64 a3);
int __fastcall ccdigest_final_common(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccdigest_init(__int64 a1, __int64 a2);
int __fastcall ccdigest_test(size_t *a1, unsigned __int64 a2, const void *a3, const void *a4);
int __fastcall ccdigest_test_chunk(__int64 a1, unsigned __int64 a2, const void *a3, void *a4, unsigned __int64 a5);
int __fastcall ccdigest_test_vector(size_t *a1, __int64 a2);
int __fastcall ccdigest_test_chunk_vector(__int64 a1, __int64 a2, unsigned __int64 a3);
signed __int64 __fastcall ccrsa_oaep_decode_parameter(__int64 a1, __int64 a2, void *a3, __int64 a4, __int64 a5, unsigned __int64 a6, const void *a7);
__int64 __fastcall ccrsa_sign_pkcs1v15(__int64 *a1, __int64 a2, size_t a3, const void *a4, __int64 a5, void *a6);
// int __usercall ccdigest_update@<eax>(signed __int64 a1@<rax>, unsigned __int64 a2@<rdx>, const void *a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>);
unsigned __int64 __fastcall ccder_decode_rsa_pub_n(unsigned __int64 a1, unsigned __int64 a2);
char (__fastcall *__fastcall ccdrbg_factory_nistctr(__int64 a1, __int64 a2))(void *a1);
__int64 __fastcall init(__int64 a1, __int64 a2, unsigned __int64 a3, __int64 a4, int a5, __int64 a6, unsigned __int64 a7, const void *a8);
signed __int64 __fastcall CCADRBGGenerate(__int64 a1, unsigned __int64 a2, __int64 a3, unsigned __int64 a4, __int64 a5);
signed __int64 __fastcall CCADRBGReseed(__int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4, __int64 a5);
char __fastcall CCADRBGDestroy(void *a1);
__int64 __fastcall df(__int64 a1, __int64 a2, __int64 a3, int a4, void *a5, size_t a6);
signed __int64 __fastcall drbg_update(__int64 a1, __int64 a2);
void *__fastcall df_bcc_update(__int64 a1, const void *a2, unsigned __int64 a3, __int64 a4);
__int64 __fastcall bcc_update(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
int __fastcall generate_block(__int64 a1, __int64 a2);
__int64 __fastcall nistctr_init(__int64 a1, __int64 a2, __int64 a3, size_t a4, __int64 a5, __int64 a6, unsigned __int64 a7, __int64 a8, int a9, const void *a10, unsigned __int64 a11, int a12, int a13);
// __int64 __usercall df_initialize@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>);
// __int64 __usercall cccmac@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, signed __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>, __int64 a6@<r8>);
__int64 __fastcall ccec_affinify(signed __int64 *a1, __int64 a2, unsigned __int64 *a3);
__int64 __fastcall ccec_affinify_x_only(signed __int64 *a1, __int64 a2, __int64 a3);
__int64 __fastcall ccmode_gcm_encrypt(); // weak
__int64 __fastcall ccec_compute_key(__int64 **a1, __int64 a2, __int64 a3, void *a4);
__int64 *ccec_cp_192();
__int64 __fastcall ccn_mod_192(__int64 a1, __int64 a2, __int64 a3);
__int64 *ccec_cp_224();
__int64 __fastcall ccn_mod_224(__int64 a1, __int64 a2, __int64 a3);
__int64 *ccec_cp_256();
__int64 __fastcall ccn_mod_256(__int64 a1, __int64 a2, __int64 a3);
__int64 *ccec_cp_384();
__int64 __fastcall ccn_mod_384(__int64 a1, __int64 a2, __int64 a3);
__int64 *ccec_cp_521();
__int64 __fastcall ccn_mod_521(__int64 *a1, __int64 a2, const void *a3);
// unsigned int __usercall ccmode_gcm_set_iv@<eax>(unsigned int result@<eax>, __int64 a2@<rdx>, __int64 a3@<rdi>, unsigned __int64 a4@<rsi>, __m128i a5@<xmm0>, __m128i a6@<xmm1>, __m128i a7@<xmm2>, __m128i a8@<xmm3>);
// int __usercall ccec_alprint@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 *a3@<rdi>);
// int __usercall ccec_plprint@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 *a3@<rdi>);
// int __usercall ccec_print_full_key@<eax>(__int64 a1@<rax>, __int64 a2@<rdi>, __int64 **a3@<rsi>);
int __fastcall ccec_print_public_key(__int64 a1, __int64 **a2);
// int __usercall ccec_print_sig@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>);
__int64 __fastcall ccec_double(signed __int64 *a1, void *a2, const void *a3);
__int64 __fastcall ccder_decode_rsa_priv(__int64 a1, unsigned __int64 a2, unsigned __int64 a3);
unsigned __int64 __fastcall ccec_export_pub(__int64 **a1, __int64 a2);
__int64 __fastcall ccec_full_add(signed __int64 *a1, void *a2, const void *a3, const void *a4);
__int64 __fastcall ccec_add(signed __int64 *a1, void *a2, const void *a3, const void *a4);
signed __int64 __fastcall ccder_encode_rsa_priv_size(__int64 *a1);
__int64 __fastcall ccec_full_sub(__int64 *a1, void *a2, const void *a3, const void *a4);
__int64 __fastcall ccec_generate_key(__int64 *a1, __int64 a2, __int64 a3);
__int64 *__fastcall ccec_get_cp(signed __int64 a1);
signed __int64 __fastcall ccec_keysize_is_supported(signed __int64 a1);
__int64 __fastcall ccec_get_fullkey_components(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8);
char __fastcall cc_clear(size_t a1, void *a2);
__int64 __fastcall ccec_get_pubkey_components(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccrsa_encrypt_oaep(__int64 *a1, __int64 a2, __int64 a3, __int64 a4, void *a5, __int64 a6, void *a7, size_t a8, const void *a9);
__int64 __fastcall ccec_import_pub(unsigned __int64 *a1, __int64 a2, unsigned int *a3, __int64 a4);
// bool __usercall ccec_is_point_affine@<al>(__int64 a1@<rax>, signed __int64 *a2@<rdi>, __int64 a3@<rsi>);
signed __int64 __fastcall ccder_encode_rsa_pub(__int64 *a1, __int64 a2, unsigned __int64 a3);
__int64 __fastcall ccecies_decrypt_gcm(__int64 **a1, __int64 a2, unsigned __int64 a3, unsigned int *a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9, __int64 a10);
signed __int64 __fastcall ccec_make_pub(signed __int64 a1, unsigned __int64 a2, unsigned __int64 a3, unsigned __int64 a4, unsigned __int64 a5, __int64 a6);
signed __int64 __fastcall ccrsa_emsa_pkcs1v15_verify(unsigned __int64 a1, __int64 a2, size_t a3, const void *a4, __int64 a5);
__int64 __fastcall ccec_mult(signed __int64 *a1, __int64 a2, __int64 a3, const void *a4);
__int64 __fastcall cced25519_sign(__int64 a1, __int64 a2, unsigned __int64 a3, const void *a4, const void *a5, const void *a6);
bool __fastcall ccec_pairwise_consistency_check(__int64 a1, __int64 a2);
void __fastcall ccec_projectify(__int64 *a1, void *a2, const void *a3);
__int64 __fastcall ccec_verify_composite(__int64 **a1, unsigned __int64 a2, unsigned __int64 a3, unsigned __int64 a4, unsigned __int64 a5, __int64 a6);
__int64 __fastcall ccec_sign(__int64 a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccec_twin_mult(__int64 a1, __int64 a2, __int64 a3, size_t a4, __int64 a5, const void *a6);
__int64 __fastcall twin_mult_normalize(signed __int64 *a1, __int64 a2, unsigned __int64 *a3, __int64 a4, __int64 a5, unsigned __int64 *a6);
__int64 __fastcall ccec_verify(const void *a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4, unsigned __int64 a5, __int64 a6);
unsigned __int64 __fastcall ccec_x963_export(int a1, __int64 a2, __int64 **a3);
signed __int64 __fastcall ccec_x963_import_priv_size(signed __int64 a1);
__int64 __fastcall ccec_x963_import_priv(unsigned __int64 *a1, __int64 a2, unsigned int *a3, __int64 a4);
__int64 __fastcall ccrsa_verify_oaep(__int64 *a1, __int64 a2, size_t a3, void *a4, unsigned __int64 a5, unsigned __int64 a6, __int64 a7);
__int64 __fastcall ccmode_ccm_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5);
signed __int64 __fastcall ccec_x963_import_pub_size(signed __int64 a1);
__int64 __fastcall ccec_x963_import_pub(unsigned __int64 *a1, __int64 a2, unsigned int *a3, __int64 a4);
// __int64 __usercall cchmac@<rax>(__int64 a1@<rax>, const void *a2@<rdx>, unsigned __int64 a3@<rcx>, __int64 a4@<rdi>, unsigned __int64 a5@<rsi>, const void *a6@<r8>, __int64 a7@<r9>);
int __fastcall cchmac_final(__int64 a1, __int64 a2, __int64 a3);
// void __usercall gcm_init(__int64 a1@<rdi>, const __m128i *a2@<rsi>, __m128i a3@<xmm0>, __m128i a4@<xmm1>, __m128i a5@<xmm2>, __m128i a6@<xmm3>, __m128i a7@<xmm4>);
// void __usercall gcm_gmult(__m128i *a1@<rdx>, const __m128i *a2@<rdi>, const __m128i *a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm4>);
// void __usercall gcm_ghash(__int64 a1@<rdx>, __int64 a2@<rcx>, const __m128i *a3@<rdi>, __int64 _RSI@<rsi>, __m128i a5@<xmm0>, __m128i a6@<xmm1>, __m128i a7@<xmm2>, __m128i a8@<xmm3>, __m128i a9@<xmm4>, __m128i a10@<xmm5>, __m128i a11@<xmm6>, __m128i a12@<xmm7>);
signed __int64 __fastcall cchmac_init(__int64 a1, __int64 a2, unsigned __int64 a3, const void *a4);
__int64 __fastcall ccmgf(__int64 a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5);
// int __usercall cchmac_update@<eax>(signed __int64 a1@<rax>, unsigned __int64 a2@<rdx>, const void *a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>);
unsigned __int64 __fastcall ccecies_encrypt_gcm_ciphertext_size(__int64 **a1, __int64 a2, __int64 a3);
__int64 ccnistkdf_dpi_hmac();
__int64 ccmd5_di();
__int64 __fastcall md5_compress(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccmode_cbc_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5);
unsigned __int64 __fastcall ccder_decode_rsa_priv_n(unsigned __int64 a1, unsigned __int64 a2);
__int64 __fastcall ccrsa_priv_crypt(__int64 *a1, __int64 a2, const void *a3);
__int64 __fastcall cczp_crt_power(__int64 a1, __int64 a2, const void *a3, signed __int64 *a4, __int64 a5, unsigned __int64 *a6, unsigned __int64 a7, __int64 a8);
// int __usercall ccmode_cbc_encrypt@<eax>(int result@<eax>, __int64 a2@<rdx>, __int64 a3@<rcx>, __int64 a4@<rdi>, void *a5@<rsi>, char *a6@<r8>);
int __fastcall ccmode_cbc_init(__int64 a1, __int64 a2);
int __fastcall ccmode_cfb8_decrypt(__int64 a1, __int64 a2, char *a3, __int64 a4);
int __fastcall ccmode_cfb8_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
// __int64 __usercall ccmode_gcm_mult_h@<rax>(__int64 a1@<rdi>, __m128i *a2@<rsi>, __m128i a3@<xmm0>, __m128i a4@<xmm1>, __m128i a5@<xmm2>, __m128i a6@<xmm3>);
int __fastcall ccmode_cfb8_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, const void *a5);
__int64 __fastcall ccmode_cfb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
__int64 __fastcall ccmode_cfb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
int __fastcall ccmode_cfb_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, const void *a5);
signed __int64 __fastcall cczp_sqrt(__int64 a1, __int64 a2, __int64 a3);
// signed __int64 __usercall ccmode_ctr_crypt@<rax>(__int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, unsigned __int64 a4@<rsi>, __m128i a5@<xmm6>, __m128i a6@<xmm7>, __m128i a7@<xmm8>, __m128i a8@<xmm9>, __m128i a9@<xmm10>, __m128i a10@<xmm11>, __m128i a11@<xmm12>, __m128i a12@<xmm13>, __m128i a13@<xmm14>, __m128i a14@<xmm15>);
__int64 __fastcall ccrsa_generate_931_key(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccrsa_make_931_key(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9, __int64 a10, __int64 a11, __int64 a12, __int64 a13, __int64 a14, __int64 a15, __int64 a16, __int64 a17, void *a18, __int64 a19, void *a20, __int64 a21, void *a22, __int64 a23, void *a24);
void *__fastcall ccmode_ctr_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, const void *a5);
char __fastcall ccmode_gcm_reset(__int64 a1);
__int64 __fastcall ccmode_ofb_crypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
int __fastcall ccmode_ofb_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, const void *a5);
signed __int64 __fastcall ccmode_xts_crypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5);
int __fastcall ccmode_xts_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5);
__int64 __fastcall ccmode_xts_mult_alpha(__int64 a1);
int __fastcall ccmode_xts_set_tweak(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccder_decode_eckey(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, unsigned __int64 a7, unsigned __int64 a8);
// int __usercall ccmode_gcm_decrypt@<eax>(__int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, unsigned __int64 a4@<rsi>, __int128 a5@<xmm0>, __int128 a6@<xmm1>, __int128 a7@<xmm2>, __int128 a8@<xmm3>, __int128 a9@<xmm6>, __int128 a10@<xmm7>, __int128 a11@<xmm8>, __int128 a12@<xmm9>, __int128 a13@<xmm10>, __int128 a14@<xmm11>, __int128 a15@<xmm12>, __int128 a16@<xmm13>, __int128 a17@<xmm14>, __int128 a18@<xmm15>);
__int64 __fastcall ccn_add(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
signed __int64 __fastcall ccn_add1(__int64 a1, __int64 a2, __int64 a3, signed __int64 a4);
signed __int64 __fastcall ccec_compact_import_priv_size(signed __int64 a1);
signed __int64 __fastcall ccec_compact_import_priv(unsigned __int64 *a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4);
// __int64 __usercall ccmode_ccm_decrypt_x86_64@<rax>(unsigned __int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __m128i a6@<xmm0>, __m128i a7@<xmm1>, __m128i a8@<xmm2>, __m128i a9@<xmm3>);
__int64 __fastcall ccrsa_export_pub(__int64 *a1, __int64 a2, __int64 a3);
signed __int64 __fastcall ccn_bitlen(__int64 a1, __int64 a2);
// __int64 __usercall ccn_burn_stack@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>);
signed __int64 __fastcall ccn_cmp(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall cccmac_block_update(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
// __int64 __usercall ccsha256_vng_intel_avx2_compress@<rax>(__int64 _RDX@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM4@<xmm4>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>);
__int64 *ccaes_ccm_encrypt_mode();
signed __int64 __fastcall cchkdf(__int64 a1, size_t a2, const void *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, const void *a7, unsigned __int64 a8, void *a9);
// int __usercall ccn_print@<eax>(__int64 a1@<rax>, __int64 a2@<rdi>, __int64 a3@<rsi>);
// int __usercall ccn_lprint@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>);
__int64 __fastcall ccn_divmod(__int64 a1, __int64 a2, void *a3, unsigned __int64 a4, const void *a5);
__int64 __fastcall ccn_trailing_zeros(unsigned __int64 a1, __int64 a2);
__int64 __fastcall ccn_gcdn(unsigned __int64 a1, void *a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccn_gcd(unsigned __int64 a1, void *a2, __int64 a3, __int64 a4);
__int64 __fastcall ccecies_encrypt_gcm(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5, const void *a6, __int64 a7, __int64 a8, __int64 a9, __int64 a10);
__int64 __fastcall ccn_lcm(unsigned __int64 a1, void *a2, __int64 a3, __int64 a4);
void __fastcall ccn_mul_ws(signed __int64 a1, __int64 a2, __int64 a3, unsigned __int64 *a4);
void __fastcall ccn_mul(signed __int64 a1, __int64 a2, __int64 a3, unsigned __int64 *a4);
signed __int64 __fastcall ccn_n(__int64 a1, __int64 a2);
int __fastcall ccn_random_bits(__int64 a1, __int64 a2, __int64 a3);
signed __int64 __fastcall ccn_read_uint(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4);
__int64 __fastcall fe_mul(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall fe_sq(__int64 a1, __int64 a2);
__int64 __fastcall fe_tobytes(__int64 a1, __int64 a2);
__int64 __fastcall ge_double_scalarmult_vartime(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
char __fastcall ge_slide(__int64 a1, __int64 a2);
__int64 __fastcall ge_p3_to_cached(__int64 a1, __int64 a2);
__int64 __fastcall ge_p1p1_to_p3(__int64 a1, __int64 a2);
__int64 __fastcall ge_add(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ge_p2_dbl(__int64 a1, __int64 a2);
__int64 __fastcall ge_madd(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ge_p1p1_to_p2(__int64 a1, __int64 a2);
signed __int64 __fastcall ge_frombytes_negate_vartime(__int64 a1, __int64 a2);
__int64 __fastcall fe_sub(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall fe_add(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall fe_neg(__int64 a1, __int64 a2);
__int64 __fastcall ge_p3_to_p2(__int64 a1, __int64 a2);
__int64 __fastcall ge_p3_tobytes(__int64 a1, __int64 a2);
__int64 __fastcall fe_invert(__int64 a1, __int64 a2);
__int64 __fastcall ge_scalarmult_base(__int64 a1, __int64 a2);
__int64 __fastcall ge_select(__int64 a1, int a2, int a3);
__int64 __fastcall ge_tobytes(__int64 a1, __int64 a2);
unsigned __int64 __fastcall sc_muladd(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
unsigned __int64 __fastcall sc_reduce(__int64 a1);
__int64 __fastcall crypto_verify_32(__int64 a1, __int64 a2);
__int64 __fastcall ge_cmov(__int64 a1, __int64 a2, int a3);
__int64 __fastcall fe_cmov(__int64 a1, __int64 a2, int a3);
void *__fastcall ccn_set(__int64 a1, void *a2, const void *a3);
__int64 __fastcall cc_cmp_safe(__int64 a1, __int64 a2, __int64 a3);
int __fastcall ccrsa_eme_pkcs1v15_encode(__int64 a1, __int64 a2, void *a3, size_t a4, const void *a5);
// int __usercall ccmode_gcm_finalize@<eax>(signed __int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __m128i a5@<xmm0>, __m128i a6@<xmm1>, __m128i a7@<xmm2>, __m128i a8@<xmm3>);
__int64 __fastcall ccrsa_sign_oaep(void *a1, __int64 a2, __int64 a3, size_t a4, __int64 a5, __int64 a6, void *a7);
__int64 __fastcall ccn_shift_left(__int64 a1, __int64 a2, __int64 a3, char a4);
void __fastcall ccn_shift_left_multi(unsigned __int64 a1, void *a2, __int64 a3, unsigned __int64 a4);
__int64 __fastcall ccrsa_decrypt_eme_pkcs1v15(__int64 *a1, unsigned __int64 *a2, __int64 a3, unsigned __int64 a4, unsigned __int64 a5);
__int64 __fastcall ccn_shift_right(__int64 a1, void *a2, const void *a3, __int64 a4);
void __fastcall ccn_shift_right_multi(unsigned __int64 a1, void *a2, void *a3, unsigned __int64 a4);
__int64 __fastcall ccn_sub(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
signed __int64 __fastcall ccn_sub1(__int64 a1, __int64 a2, __int64 a3, signed __int64 a4);
signed __int64 __fastcall ccrsa_emsa_pkcs1v15_encode(unsigned __int64 a1, __int64 a2, size_t a3, const void *a4, __int64 a5);
unsigned __int64 __fastcall ccn_write_int_size(__int64 a1, __int64 a2);
unsigned __int64 __fastcall ccn_write_int(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4);
__int64 __fastcall ccrsa_generate_fips186_key(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccrsa_generate_fips186_prime_factors(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7);
__int64 __fastcall ccrsa_crt_make_fips186_key(__int64 a1, __int64 a2, __int64 a3, const void *a4, __int64 a5, __int64 a6);
__int64 __fastcall ccrsa_make_fips186_key(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9, __int64 a10, __int64 a11, __int64 a12, __int64 a13, __int64 a14, __int64 a15, __int64 a16, __int64 a17, void *a18, __int64 a19, void *a20, __int64 a21, void *a22, __int64 a23, void *a24);
__int64 __fastcall ccrsa_generate_probable_prime(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8);
bool __fastcall cczp_check_delta_100bits(__int64 a1, __int64 *a2, __int64 a3, __int64 a4, __int64 a5);
__int64 __fastcall ccrsa_generate_probable_prime_from_auxilary_primes(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5, __int64 a6, __int64 a7, __int64 a8);
__int64 __fastcall cczp_compute_R(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5);
__int64 __fastcall cczp_seed_X(__int64 a1, __int64 a2, __int64 a3);
signed __int64 __fastcall cczp_find_next_prime(__int64 *a1);
unsigned __int64 __fastcall ccn_write_uint_size(__int64 a1, __int64 a2);
unsigned __int64 __fastcall ccn_write_uint(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4);
__int64 __fastcall ccpad_xts_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccpad_xts_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6);
signed __int64 __fastcall ccpbkdf2_hmac(__int64 a1, unsigned __int64 a2, const void *a3, __int64 a4, const void *a5, unsigned __int64 a6, unsigned __int64 a7, void *a8);
void *__fastcall F(__int64 a1, __int64 a2, const void *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, unsigned int a7, size_t a8, void *a9);
__int64 __fastcall ccnistkdf_ctr_hmac_fixed(void *a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5, size_t a6, void *a7);
__int64 __fastcall ccnistkdf_ctr_hmac(void *a1, unsigned __int64 a2, __int64 a3, size_t a4, const void *a5, size_t a6, const void *a7, size_t a8, const void *a9);
_UNKNOWN *ccrc4();
__int64 __fastcall eay_RC4_set_key(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall eay_RC4(__int64 a1, unsigned __int64 a2, __int64 a3, signed __int64 a4);
__int64 __fastcall ccrng_system_init(__int64 a1);
__int64 __fastcall get_kernel_entropy(__int64 a1, u_int a2, void *a3);
void ccrng_system_done();
__int64 __fastcall ccrng_rsafips_test_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7);
__int64 __fastcall ccrng_rsafips_test_generate(__int64 a1, unsigned __int64 a2, void *a3);
__int64 (*ccsha1_di())[4];
__int64 __fastcall sha1_compress(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccrsa_verify_pkcs1v15(__int64 *a1, __int64 a2, size_t a3, const void *a4, unsigned __int64 a5, unsigned __int64 a6, __int64 a7);
__int64 __fastcall ccmac_generate_subkeys(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
// __int64 __usercall ccecb_one_shot@<rax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>);
__int64 *ccaes_gcm_encrypt_mode();
__int64 __fastcall ccmode_ccm_macdata(__int64 a1, __int64 a2, int a3, unsigned __int64 a4, __int64 a5);
__int64 __fastcall ccmode_ccm_cbcmac(__int64 a1, __int64 a2, unsigned __int64 a3, __int64 a4);
__int64 __fastcall ccrsa_encrypt_eme_pkcs1v15(__int64 *a1, __int64 a2, __int64 a3, void *a4, size_t a5, const void *a6);
int __fastcall ccec_generate_key_fips(__int64 *a1, __int64 a2, __int64 a3);
// __int64 __usercall ccmode_gcm_gf_mult@<rax>(__m128i *a1@<rdx>, const __m128i *a2@<rdi>, const __m128i *a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>);
int __fastcall ccec_compact_generate_key(__int64 *a1, __int64 a2, __int64 a3);
__int64 __fastcall ccsha1_vng_intel_compress_SupplementalSSE3(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall SHA1Transform_nossse3(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccsha256_ltc_compress(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccrsa_oaep_encode_parameter(__int64 a1, __int64 a2, __int64 a3, const void *a4, size_t a5, void *a6, unsigned __int64 a7, const void *a8);
// __int64 __usercall ccsha256_vng_intel_sse3_compress@<rax>(__int64 a1@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm4>, __m128i a9@<xmm5>, __m128i a10@<xmm6>, __m128i a11@<xmm7>);
// __int64 __usercall ccsha256_vng_intel_nossse3_compress@<rax>(__int64 a1@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm4>, __m128i a9@<xmm5>, __m128i a10@<xmm6>, __m128i a11@<xmm7>);
unsigned __int64 __fastcall ccec_compact_export(int a1, void *a2, __int64 **a3);
__int64 __fastcall ccsha512_ltc_compress(__int64 a1, signed __int64 a2, signed __int64 a3);
__int64 __fastcall ccsha512_final(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall cczp_add(__int64 *a1, __int64 a2, __int64 a3, __int64 a4);
unsigned __int64 __fastcall ccecies_pub_key_size(__int64 **a1, __int64 a2);
__int64 __fastcall cczp_div(unsigned __int64 a1, __int64 a2, void *a3, unsigned __int64 a4);
__int64 __fastcall cczp_div2(__int64 *a1, __int64 a2, __int64 a3);
__int64 __fastcall cczp_init(__int64 a1);
__int64 __fastcall ccn_make_recip_shift_sub(__int64 a1, void *a2, __int64 a3);
__int64 __fastcall ccn_make_recip_newtonraphson(__int64 a1, void *a2, __int64 a3);
signed __int64 __fastcall ccec_affine_point_from_x(__int64 *a1, void *a2, const void *a3, __int64 a4);
void __fastcall cczp_mod(signed __int64 *a1, void *a2, __int64 a3, void **a4);
__int64 __fastcall cczp_modn(unsigned __int64 a1, void *a2, unsigned __int64 a3, __int64 a4);
__int64 __fastcall ccrsa_import_pub(__int64 a1, __int64 a2, unsigned __int64 a3);
signed __int64 __fastcall cczp_mod_inv(__int64 a1, void *a2, const void *a3);
__int64 __fastcall cczp_mod_inv_slow(unsigned __int64 a1, __int64 a2, const void *a3);
__int64 __fastcall cczp_mod_inv_slown(unsigned __int64 a1, __int64 a2, __int64 a3, const void *a4);
int __fastcall cczp_mul_ws(signed __int64 *a1, __int64 a2, __int64 a3, unsigned __int64 *a4, __int64 *a5);
__int64 __fastcall cczp_mul(signed __int64 *a1, __int64 a2, __int64 a3, unsigned __int64 *a4);
// char __usercall ccmode_gcm_gmac@<al>(__int64 a1@<rdx>, __int64 a2@<rdi>, unsigned __int64 a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm6>, __m128i a9@<xmm7>);
void __fastcall cczp_powern(signed __int64 *a1, __int64 a2, __int64 a3, unsigned __int64 a4, __int64 a5);
__int64 __fastcall cczp_power(__int64 *a1, __int64 a2, __int64 a3, __int64 a4);
__int64 __fastcall cczp_rabin_miller(__int64 a1, __int64 a2);
char __fastcall ccn_prime_sieve(unsigned __int64 a1, __int64 a2);
__int64 __fastcall ccec_generate_key_internal(__int64 *a1, __int64 a2, __int64 a3);
// void __usercall ONE_3(__int64 a1@<rax>);
// void __usercall TWO_1(__int64 a1@<rax>);
// __int64 __usercall gcmEncrypt_SupplementalSSE3@<rax>(__int64 a1@<rdx>, unsigned __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __int64 a6@<r9>, __m128i a7@<xmm0>, __m128i a8@<xmm1>, __m128i a9@<xmm2>, __m128i a10@<xmm3>, __m128i a11@<xmm4>, __m128i a12@<xmm5>, __m128i a13@<xmm6>, __m128i a14@<xmm7>, __m128i a15@<xmm8>, __m128i a16@<xmm9>, __m128i a17@<xmm10>, __m128i a18@<xmm11>, __m128i a19@<xmm12>, __m128i a20@<xmm13>, __m128i a21@<xmm14>, __m128i a22@<xmm15>, __int128 a23, __int128 a24, __int128 a25, __int128 a26, __int128 a27, int a28, __int128 a29, __int128 a30, int a31, int a32, int a33, int a34, int a35, int a36, int a37, int a38, int a39, __int128 a40, __int128 a41, __int128 a42, __int128 a43, __int128 a44, __int128 a45, __int128 a46, __int128 a47, __int128 a48, __int128 a49, __int128 a50, __int128 a51, __int128 a52, __int128 a53, __int128 a54, __int128 a55);
// __int64 __usercall Main_Encrypt_Loop_0@<rax>(int a1@<edx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 _R8@<r8>, __int64 a6@<r9>, unsigned int a7@<r10d>, __m128i a8@<xmm0>, __m128i a9@<xmm4>, __m128i a10@<xmm7>, __m128i a11@<xmm8>, __m128i a12@<xmm9>, __m128i a13@<xmm10>, __m128i a14@<xmm11>, __m128i a15@<xmm12>, __m128i a16@<xmm13>, __m128i a17@<xmm14>, __m128i a18@<xmm15>, __int128 a19, __int128 a20, __int128 a21, __int128 a22, __int128 a23, int a24, __int128 a25, __int128 a26, int a27, int a28, int a29, int a30, int a31, int a32, int a33, int a34, int a35, __int128 a36, __int128 a37, __int128 a38, __int128 a39, __int128 a40, __int128 a41, __int128 a42, __int128 a43, __int128 a44, __int128 a45, __int128 a46, __int128 a47, __int128 a48, __int128 a49, __int128 a50, __int128 a51);
__int64 __fastcall End_Main_Encrypt_Loop_0(_DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128); // weak
// int __usercall gcmDecrypt_SupplementalSSE3@<eax>(__int64 a1@<rdx>, unsigned __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r9>, __m128i a6@<xmm0>, __m128i a7@<xmm1>, __m128i a8@<xmm2>, __m128i a9@<xmm3>, __m128i a10@<xmm4>, __m128i a11@<xmm5>, __m128i a12@<xmm6>, __m128i a13@<xmm7>, __m128i a14@<xmm8>, __m128i a15@<xmm9>, __m128i a16@<xmm10>, __m128i a17@<xmm11>, __m128i a18@<xmm12>, __m128i a19@<xmm13>, __m128i a20@<xmm14>, __m128i a21@<xmm15>);
int __fastcall Main_Decrypt_Loop_1(_QWORD, _QWORD, _QWORD, _QWORD); // weak
signed __int64 __fastcall cczp_random_prime(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
int __fastcall cczp_sqr_ws(signed __int64 *a1, __int64 a2, __int64 a3, __int64 *a4);
__int64 __fastcall cczp_sqr(signed __int64 *a1, __int64 a2, __int64 a3);
__int64 __fastcall cczp_sub(__int64 *a1, __int64 a2, __int64 a3, __int64 a4);
// __int64 __usercall corecrypto_kext_start@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>, char *a3@<rsi>);
__int64 corecrypto_kext_stop();
__int64 __fastcall KEXT_FIPSPost(__int64 a1, char *a2);
int __fastcall Integrity_POST(__int64 a1, void *a2);
__int64 RSA_POST();
__int64 DRBG_POST();
__int64 ECDSA_POST();
__int64 __fastcall ccxts_one_shot(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
// __int64 __usercall ccecb_one_shot_0@<rax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>);
__int64 __fastcall cccbc_one_shot(__int64 a1, __int64 a2, __int64 a3, const void *a4, __int64 a5, __int64 a6);
void *__fastcall bytesToHexString(__int64 a1, unsigned __int64 a2, void *a3);
__int64 __fastcall deskey(__int64 a1, unsigned __int16 a2, void *a3);
__int64 __fastcall desfunc(__int64 a1, __int64 a2);
__int64 __fastcall desfunc3(__int64 a1, __int64 a2);
__int64 __fastcall cccmac_init(__int64 a1, __int64 a2, __int64 a3);
// void __usercall vng_aes_xts_encrypt_opt(__m128i *a1@<rdx>, __m128i *a2@<rcx>, __m128i *a3@<rdi>, unsigned __int64 a4@<rsi>, __int64 a5@<r8>, __m128i *a6@<r15>);
// signed __int64 __usercall vng_aes_xts_encrypt_aesni@<rax>(__int64 a1@<rdx>, __m128i *a2@<rcx>, __int64 a3@<rdi>, unsigned __int64 a4@<rsi>, __int64 a5@<r8>, __m128i *a6@<r15>);
void __fastcall vng_aes_xts_decrypt_opt(__m128i *a1, unsigned __int64 a2, __m128i *a3, __m128i *a4, __int64 a5);
signed __int64 __fastcall vng_aes_xts_decrypt_aesni(__int64 a1, unsigned __int64 a2, __int64 a3, __m128i *a4, __int64 a5);
__int64 (*ccsha224_di())[4];
__int64 (*ccsha256_di())[4];
__int64 ccsha384_di();
__int64 ccsha512_di();
__int64 __fastcall ccn_zero_multi(__int64 a1, void *a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, char a7);
__int64 *ccsrp_gp_rfc5054_8192();
signed __int64 __fastcall init_0(__int64 a1, __int64 a2, unsigned __int64 a3, const void *a4, unsigned __int64 a5, const void *a6, unsigned __int64 a7, const void *a8);
__int64 __fastcall reseed(__int64 a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5);
__int64 __fastcall generate(__int64 a1, unsigned __int64 a2, void *a3, unsigned __int64 a4, const void *a5);
void __fastcall done(__int64 a1);
void (__fastcall *__fastcall ccdrbg_factory_nisthmac(__int64 a1, __int64 a2))(__int64 a1);
__int64 __fastcall hmac_dbrg_update(__int64 a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, const void *a7);
__int64 *ccsrp_gp_rfc5054_1024();
__int64 *ccsrp_gp_rfc5054_2048();
// int __usercall ccmode_ccm_finalize@<eax>(int result@<eax>, void *a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>);
__int64 *ccsrp_gp_rfc5054_4096();
int __fastcall ccmode_ccm_set_iv(__int64 a1, __int64 a2, size_t a3, const void *a4, unsigned __int64 a5, unsigned __int64 a6, unsigned __int64 a7);
__int64 *ccaes_ccm_decrypt_mode();
__int64 __fastcall ccsrp_generate_salt_and_verification(void *a1, __int64 a2, char *a3, unsigned __int64 a4, const void *a5, __int64 a6, __int64 a7, void *a8);
__int64 __fastcall ccsrp_generate_verifier(void *a1, char *a2, unsigned __int64 a3, const void *a4, __int64 a5, __int64 a6, void *a7);
__int64 __fastcall ccsrp_generate_x(__int64 a1, __int64 a2, char *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, const void *a7);
__int64 __fastcall ccsrp_server_start_authentication(__int64 a1, void *a2, char *a3, unsigned __int64 a4, const void *a5, __int64 a6, unsigned __int64 a7, void *a8);
// __int64 __usercall ccsrp_generate_server_pubkey@<rax>(__int64 a1@<rax>, unsigned __int64 *a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __int64 a6@<r9>);
__int64 __fastcall ccsrp_generate_server_S(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccsrp_digest_ccn(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccsrp_generate_M(__int64 a1, char *a2, unsigned __int64 a3, const void *a4, __int64 a5, __int64 a6);
// __int64 __usercall ccsrp_generate_H_AMK@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>, __int64 a3@<rsi>);
char __fastcall ccsrp_server_verify_session(__int64 a1, const void *a2, void *a3);
__int64 __fastcall ccsrp_digest_update_ccn(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccsrp_digest_ccn_ccn(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
unsigned __int64 __fastcall ccsrp_client_start_authentication(__int64 a1, __int64 a2, void *a3, __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccsrp_client_process_challenge(__int64 a1, void *a2, unsigned __int64 a3, const void *a4, unsigned __int64 a5, const void *a6, unsigned __int64 a7, void *a8);
__int64 __fastcall ccsrp_generate_x_0(__int64 a1, __int64 a2, char *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, const void *a7);
__int64 __fastcall ccsrp_generate_client_S(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6);
__int64 __fastcall ccsrp_digest_ccn_0(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccsrp_generate_M_0(__int64 a1, char *a2, unsigned __int64 a3, const void *a4, __int64 a5, __int64 a6);
// __int64 __usercall ccsrp_generate_H_AMK_0@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>, __int64 a3@<rsi>);
bool __fastcall ccsrp_client_verify_session(__int64 a1, const void *a2);
__int64 __fastcall ccsrp_digest_update_ccn_0(__int64 a1, __int64 a2, __int64 a3);
__int64 __fastcall ccsrp_digest_ccn_ccn_0(__int64 a1, __int64 a2, __int64 a3, __int64 a4);
int _start();
__int64 *OSKextGetCurrentIdentifier();
__int64 *OSKextGetCurrentVersionString();
__int64 OSKextGetCurrentLoadTag();
int _stop();
// int __fastcall PE_parse_boot_argn(_QWORD, _QWORD, _QWORD); weak
// void __cdecl bzero(void *, size_t);
// int cpuid_features(void); weak
// int cpuid_info(void); weak
// int __fastcall kprintf(_QWORD, _QWORD); weak
// int __cdecl memcmp(const void *, const void *, size_t);
// void *__cdecl memcpy(void *, const void *, size_t);
// void *__cdecl memmove(void *, const void *, size_t);
// void *__cdecl memset(void *, int, size_t);
// void panic(const char *, ...);
// int printf(const char *, ...);
// void __cdecl read_random(void *buffer, u_int numBytes);
// int __fastcall register_crypto_functions(_QWORD, _QWORD); weak
// int __cdecl strcmp(const char *, const char *);
// size_t __cdecl strlen(const char *);
// int __cdecl strncmp(const char *, const char *, size_t);
double __cdecl sqrt_2(double);

//-------------------------------------------------------------------------
// Data declarations

int dword_1C = 0; // weak
segment_command_64 stru_20 = { 25u, 392u, "__TEXT", 0uLL, 430080uLL, 0uLL, 430080uLL, 7, 5, 4u, 0u }; // weak
section_64 stru_68 =
{
  {
    '_',
    '_',
    't',
    'e',
    'x',
    't',
    '\0',
    '\0',
    '\0',
    '\0',
    '\0',
    '\0',
    '\0',
    '\0',
    '\0',
    '\0'
  },
  "__TEXT",
  3520uLL,
  305910uLL,
  3520u,
  6u,
  0u,
  0u,
  2147484672u,
  0u,
  0u,
  0u
}; // weak
_UNKNOWN Lbswap_mask; // weak
_UNKNOWN loc_20F7; // weak
_UNKNOWN loc_53D1; // weak
_UNKNOWN loc_5AAF; // weak
_UNKNOWN loc_6FB3; // weak
_UNKNOWN loc_71CB; // weak
_UNKNOWN loc_71D0; // weak
_UNKNOWN Lbswap_mask_0; // weak
_UNKNOWN Lbswap_mask_1; // weak
_UNKNOWN loc_1DB5A; // weak
_UNKNOWN Lbswap_mask_2; // weak
_UNKNOWN loc_1F9AE; // weak
_UNKNOWN loc_2041E; // weak
__int128 xmmword_27A70 = 0x10203040506078090A0B0C0D0E0FLL; // weak
__int128 xmmword_27A80 = 0xC20000000000000000000001LL; // weak
_UNKNOWN Lbswap_mask_3; // weak
_UNKNOWN loc_45D28; // weak
_UNKNOWN loc_468D5; // weak
__int64 ecies_iv_data[2] = { 0LL, 0LL }; // weak
__int64 qword_4B930[2] = { 289644378169868803LL, 868365760874482187LL }; // weak
int AESEncryptTable[] = { 2774754246 }; // weak
int AESDecryptTable[] = { 1353184337 }; // weak
int AESSubBytesWordTable[] = { 99 }; // weak
int AESInvSubBytesWordTable[] = { 82 }; // weak
int dword_5094C = 0; // weak
__int64 t_rc[6] =
{
  8589934593LL,
  34359738372LL,
  137438953488LL,
  549755813952LL,
  231928234011LL,
  0LL
}; // weak
__int64 t_fn[512] =
{
  -8900101356222454842LL,
  -8251865593159387154LL,
  -4797622410931932417LL,
  6108505699135746014LL,
  216455366537392224LL,
  9019350329948858318LL,
  7122398508534922983LL,
  -7316529784762422451LL,
  -7096966991955965297LL,
  -8683645992718186103LL,
  -1488059580461417745LL,
  860452493032114062LL,
  7481838750204276033LL,
  -1535816232767151521LL,
  -602175770556851165LL,
  6611496027456434916LL,
  2089104948246067061LL,
  7648843127923643197LL,
  4701546347910018668LL,
  5750195687307868149LL,
  -818065988440017816LL,
  644562274327324113LL,
  8347660140755907042LL,
  4545562670428926306LL,
  5964955674441155592LL,
  6828516540797887302LL,
  -6803085023456520144LL,
  -5360802879525026550LL,
  3896196573608478478LL,
  4459376031064686619LL,
  7577068114688076749LL,
  -6956524398070025601LL,
  -7024626832736974574LL,
  3321996487728835672LL,
  -5589408195591595210LL,
  -315075657214567756LL,
  5565107146671149732LL,
  -3552298329971435849LL,
  4531716190458423634LL,
  -7528747458018463906LL,
  7553048646692066214LL,
  3237505120333725696LL,
  2304995162807345216LL,
  -1343379223230697095LL,
  5101394812572691156LL,
  5420426791959314023LL,
  -3149057819990668652LL,
  5390755453021477040LL,
  3093955100223262907LL,
  1656194287167777359LL,
  -2932602455218502778LL,
  -7744637674703342746LL,
  1223283556407920010LL,
  -9115426391775378940LL,
  4916871383307276448LL,
  -2042197387805483227LL,
  -98055142962540126LL,
  -8462387919942827904LL,
  -4855552055263980993LL,
  357462162093389936LL,
  -4488199352657068957LL,
  7143027053315152559LL,
  1945554923866296352LL,
  7913619213274838013LL,
  1444542878986390913LL,
  3453395337439548198LL,
  -6730744860411732034LL,
  4113782274389197960LL,
  -961616010433739629LL,
  5133326746656866044LL,
  -1775159621776546616LL,
  -7677665507333826254LL,
  -7457537560883994432LL,
  9213481533842608030LL,
  9091125339779899972LL,
  -8968769074203946949LL,
  3021614941871687308LL,
  4329107307393235051LL,
  -2135165009735524697LL,
  8564680652938218262LL,
  6211082242180047067LL,
  2164553653233007220LL,
  722271442527930770LL,
  -1991614988046556088LL,
  7985959374533345951LL,
  -6457490317721686973LL,
  -6587194839445434055LL,
  -8396545949028784941LL,
  4884374296063371221LL,
  -5229402807740188818LL,
  7265948532376177921LL,
  -2258087603623539044LL,
  -408043416469082920LL,
  2732254298866250995LL,
  -8180090581831817782LL,
  1731642925456993863LL,
  -8613001311572805009LL,
  8227564543789049162LL,
  -1033956171236762568LL,
  5892615517653742707LL,
  8997591314718058699LL,
  2386660681302963432LL,
  -2540666386744915050LL,
  -8824088720592631027LL,
  4773321359779459296LL,
  -6168129609977711247LL,
  361135723053074576LL,
  1300992855811487479LL,
  6860448339492692418LL,
  -3406487778323114066LL,
  6395605811158484503LL,
  -5071442239375467206LL,
  1439173770100203993LL,
  3679741211278088235LL,
  8131769925211089362LL,
  -6371304623750345209LL,
  2458435692701391661LL,
  2371683730743330581LL,
  -48038031146430841LL,
  8854041294212180048LL,
  -530965875020493821LL,
  1660998245046389001LL,
  3595815167264276325LL,
  -5158758127264185724LL,
  -5721373446948961918LL,
  1229217846818319706LL,
  -264493394159423365LL,
  4185557285524454253LL,
  8970317805302040229LL,
  8897976576255717017LL,
  7740516495879503629LL,
  -4195787685992866127LL,
  72341281731272784LL,
  3110675064235216553LL,
  -2893644798772713703LL,
  8536270292776603110LL,
  -9042630339233935547LL,
  9042659087942781248LL,
  6438373868900642581LL,
  -1085091484337205559LL,
  -3110664184729353748LL,
  -5787330122094583811LL,
  -6583044432359316545LL,
  -4557472005834480490LL,
  -144712099548400190LL,
  2748968643182411182LL,
  4557500166469741658LL,
  -3689429615237401342LL,
  -6510707577004464036LL,
  -1012754624569552588LL,
  -2821316654234738029LL,
  1519166900204036691LL,
  -4051105160309241844LL,
  -4340452588213025179LL,
  -7595822556480262104LL,
  -7306474990935406065LL,
  1302143056728231433LL,
  -2097869021159351397LL,
  2821309908407536934LL,
  8463929046331457485LL,
  -8970293468198268389LL,
  1880873167294847092LL,
  7957540249127302701LL,
  -6872391896256563986LL,
  4268135146329384182LL,
  -5497912392768112799LL,
  -2025532148338306437LL,
  -8897965423724634511LL,
  -3327674791924357387LL,
  -1302172322517483520LL,
  -217048959198150560LL,
  6583056412336814536LL,
  -3761757730165304130LL,
  4123452603482662873LL,
  5497937282659423454LL,
  -3472410233323081496LL,
  -1157489791528289429LL,
  -289377069426913307LL,
  5570278563464316613LL,
  -8825628566550780331LL,
  -434059632973608241LL,
  9187341594897024006LL,
  4340476375678886128LL,
  -6293697038242601542LL,
  -6655372477456080141LL,
  -8102251113274638144LL,
  -7089473144614666323LL,
  -723407164697710520LL,
  -5280901839066471457LL,
  2387262274124033909LL,
  -29574875307984LL,
  -3255329101670253298LL,
  868095305806872908LL,
  -1374509179303942603LL,
  -7523485680998039839LL,
  1663849410830043340LL,
  -6366025047093243049LL,
  4412817657376472194LL,
  6727738921345927340LL,
  8319246464729035307LL,
  -9114976028578496352LL,
  -2531969123302269231LL,
  3038333799330104422LL,
  -8608618028024710229LL,
  -1229826668138951478LL,
  1446825621555014611LL,
  6800080169963792249LL,
  -2604297237000677859LL,
  3617063775034727227LL,
  723412810204869710LL,
  434047652406596315LL,
  6655397639345162348LL,
  -3182992228209221795LL,
  7089444983575167983LL,
  -7668168207641790040LL,
  8753294033425847095LL,
  -3978777147012033230LL,
  7885199003035856473LL,
  -3038327328161857140LL,
  -6221360182500352814LL,
  6221350126819793076LL,
  -1519174162353687801LL,
  8825635312209611439LL,
  578730250313025513LL,
  8680952751795630037LL,
  3327698819992603247LL,
  -6438361924209199068LL,
  -4123442031631240249LL,
  -2459632263618835677LL,
  2242579452862458012LL,
  -4774552431158913315LL,
  -8463935483443212922LL,
  4485158903703724176LL,
  7378810073058210244LL,
  217023826801234136LL,
  1012777833983309569LL,
  3834087615664079523LL,
  -5063899962461933831LL,
  -4485135148391196783LL,
  -7017127493385766361LL,
  -506396487452600008LL,
  1229801778247642035LL,
  -2748979797263002949LL,
  -7740505064798812279LL,
  2170238189776350646LL,
  -1591519851748846190LL,
  6149008881907304265LL,
  -2314949739366821768LL,
  -6800055038307990641LL,
  940436585584331136LL,
  -1808521591672314406LL,
  7523492667326629062LL,
  -7378820677484051773LL,
  1085119094340803191LL,
  6076667600479222731LL,
  1591508149091397078LL,
  9005093124712080739LL,
  8932482472368576887LL,
  7770606478410321394LL,
  -4210490969652547217LL,
  72623847632818224LL,
  3122821017006680423LL,
  -2903305704936367618LL,
  8569394025450038955LL,
  -9070357939431193142LL,
  9077716973240533193LL,
  6463487186924672506LL,
  -1082258158831875769LL,
  -3120035953700115283LL,
  -5816985303614751326LL,
  -6605663942056099940LL,
  -4567956514917804430LL,
  -152809036323634505LL,
  2759697385582866067LL,
  4575166011206818358LL,
  -3710033926271204617LL,
  -6534172593951843276LL,
  -1010766806332132123LL,
  -2834044167038790799LL,
  1525100784073724721LL,
  -4065252073912398844LL,
  -4351217472766581469LL,
  -7622446178278299624LL,
  -7336445594849112315LL,
  1307229242899958023LL,
  -2098890858310165632LL,
  2832316835843352299LL,
  8496778974842572210LL,
  -8998862194786297079LL,
  1888184833723102252LL,
  7988456025021099291LL,
  -6891638138625266086LL,
  4284697012674557522LL,
  -5513023097034808874LL,
  -2027395111875085527LL,
  -8929627042159365841LL,
  -3334518800830237357LL,
  -1314720219796602880LL,
  -224300388705083360LL,
  6608730481288136881LL,
  -3779286672809345430LL,
  4139453718886603198LL,
  5519394767476416074LL,
  -3493303681549277096LL,
  -1169481318745281584LL,
  -293553130559248982LL,
  5592018614179251523LL,
  -8858135692247149261LL,
  -438800825892286651LL,
  9222951469396002306LL,
  4357307665322405968LL,
  -6319707338720298337LL,
  -6674899095290449071LL,
  -8140948403535167424LL,
  -7124205750573879918LL,
  -724792613769557960LL,
  -5298553445989228612LL,
  2396586948768527834LL,
  -7570141581660144LL,
  -3260766852388942093LL,
  871468573116157133LL,
  -1386211569319922413LL,
  -7550950429186662049LL,
  1670335283897879620LL,
  -6388933693983729724LL,
  4429931512921358974LL,
  6753964979504327780LL,
  8351526880581659417LL,
  -9144109886957641632LL,
  -2548052379956227761LL,
  3050201567065302562LL,
  -8643670437248914544LL,
  -1240977068954695098LL,
  1452476939518071736LL,
  6826580032277150174LL,
  -2617305125276476149LL,
  3631121976088280032LL,
  726238469192699450LL,
  435734286068538185LL,
  6681341131600981028LL,
  -3189271105326064190LL,
  7116996253879365548LL,
  -7696189325905909615LL,
  8787239178599610340LL,
  -3996025714199153945LL,
  7915840974766364983LL,
  -3048544604396614515LL,
  -6248215990226857394LL,
  6245642032356308076LL,
  -1526960057593165836LL,
  8859863023289544549LL,
  580990777493547438LL,
  8714615331067254202LL,
  3340670566121762597LL,
  -6460429444699380708LL,
  -4136747818843650124LL,
  -2476561027541818392LL,
  2251273282720930932LL,
  -4800313022225130165LL,
  -8498427141739936117LL,
  4502546563412824176LL,
  7407482845225141429LL,
  217867143634540616LL,
  1016707471361114614LL,
  3848993514419561313LL,
  -5086304809679652521LL,
  -4496465164751105658LL,
  -7050462602878900451LL,
  -510292173114492703LL,
  1234605398513398680LL,
  -2762552817329456279LL,
  -7767680675804116594LL,
  2178653834588305051LL,
  -1600712005187693945LL,
  6173026983266961870LL,
  -2331322133221705688LL,
  -6820146787953438836LL,
  944092418788655241LL,
  -1812925457869972801LL,
  7552739333032429122LL,
  -7410197539585866943LL,
  1089326916753323821LL,
  6100403135941692336LL,
  1597715837117912763LL,
  -539169180027100317LL,
  -680752196448848009LL,
  -2973102067409882382LL,
  -7974531586530906257LL,
  144960718428450864LL,
  6232184926501431143LL,
  -5376497682831769858LL,
  -1397674483859936341LL,
  2278120483780414154LL,
  -394208464244520503LL,
  -5554247472633218310LL,
  -356926829467449529LL,
  -5519208810215068243LL,
  5038032300931916450LL,
  6050485649276902556LL,
  -7251990840248733070LL,
  -2225624850783619145LL,
  5506255440099316627LL,
  9097622263898453558LL,
  -8984737550924843017LL,
  5905527141228622900LL,
  -501885337552493083LL,
  -6092287444876299919LL,
  3044175054713860401LL,
  -7686861956320590844LL,
  -7107027918476991709LL,
  4008650715133646872LL,
  3437823876951639301LL,
  2609292902861965063LL,
  -2360481168918609792LL,
  5650090259412085739LL,
  -1540383400021609806LL,
  2134287872387909897LL,
  3759971439356423212LL,
  -2543849419876525285LL,
  6628067888696547930LL,
  8524534797038932562LL,
  9065380684400547542LL,
  -2504313777290532567LL,
  1411742712258965295LL,
  -5086585080735509677LL,
  -4526981923251159040LL,
  -2080666342479880160LL,
  -5265451940882894415LL,
  -8266695989683918230LL,
  8235739262295326398LL,
  -7434233193083024822LL,
  -8842026427455350696LL,
  -4239312286295273264LL,
  -1362624780437181782LL,
  -7289272476506963133LL,
  1266784203191956275LL,
  -1652546213073959611LL,
  -107664732575104510LL,
  8666117814063485008LL,
  5468399812698873759LL,
  6773030812820656465LL,
  399289363478888512LL,
  2430991197576991378LL,
  -1079467576162502600LL,
  8629379255494491324LL,
  4783703657758972634LL,
  -1937955218407813104LL,
  -4652831036373273613LL,
  1735025004292918733LL,
  -4382023413410753773LL,
  3864818108235603807LL,
  3330718789263967300LL,
  6193196775761233092LL,
  8811078532412374654LL,
  -4978908205654711196LL,
  -1831430734856120039LL,
  1844366441690718304LL,
  -6665366083369611441LL,
  6088350107276026402LL,
  829656875064332432LL,
  -4095479677785061818LL,
  2899214338060695736LL,
  -4836199289212772642LL,
  -5947324523880838389LL,
  7230021445797798112LL,
  1449607170077112890LL,
  867512503525656905LL,
  -5123868924618988508LL,
  -4796663647237717310LL,
  -4276622621955609428LL,
  3577148472189030801LL,
  -969547730626550556LL,
  -8411658914102056985LL,
  -2686558335933008073LL,
  -5664167320494502515LL,
  5323441305497521742LL,
  -5982374224005403540LL,
  -3520149238899542796LL,
  -824587013957130907LL,
  1159685735707160238LL,
  -1114508449106511174LL,
  6661437575186556197LL,
  6337029383523671068LL,
  -7543029345220578124LL,
  -6810324591825393432LL,
  4476893726489998452LL,
  7051719738532907851LL,
  1118452408008477579LL,
  8953787448942424176LL,
  -3699031553750026827LL,
  433756252960999496LL,
  2022694640079206134LL,
  7664903595822899553LL,
  7624798376293914455LL,
  -7396949352887777658LL,
  2862493440588324125LL,
  -1507587702559022623LL,
  2464332186547099800LL,
  -6237245954341508759LL,
  3722106980547530382LL,
  4333058906448632731LL,
  -3953903285532719225LL,
  -6125083142370242866LL,
  -6522654960071989208LL,
  6483109376521374860LL,
  1879985719095626121LL,
  -2940315204470521921LL,
  -3406858319992307134LL,
  3004069836059001153LL,
  2166529454358605101LL,
  -6270043859375640400LL,
  3186883971905928123LL
}; // weak
__int64 t_fl[512] =
{
  532575944803LL,
  528280977527LL,
  459561500914LL,
  846108557423LL,
  4294967344LL,
  184683593831LL,
  923417968894LL,
  506806141099LL,
  558345748682LL,
  536870912201LL,
  382252089594LL,
  1030792151111LL,
  910533066925LL,
  751619276962LL,
  704374636700LL,
  824633720946LL,
  1086626726071LL,
  163208757395LL,
  270582939702LL,
  876173328631LL,
  708669603892LL,
  1035087118565LL,
  927712936049LL,
  90194313265LL,
  854698491908LL,
  837518622755LL,
  644245094424LL,
  661424963589LL,
  77309411335LL,
  970662609024LL,
  167503724779LL,
  502511173810LL,
  562640715785LL,
  111669149740LL,
  472446402587LL,
  687194767450LL,
  253403070546LL,
  768799146198LL,
  974957576233LL,
  566935683119LL,
  897648164947LL,
  1017907249152LL,
  1082331758624LL,
  390842024113LL,
  871878361194LL,
  244813136062LL,
  326417514570LL,
  889058230360LL,
  1026497183952LL,
  1078036791466LL,
  330712481859LL,
  571230650419LL,
  1069446856773LL,
  545460846594LL,
  257698037840LL,
  721554505887LL,
  700079669329LL,
  614180323392LL,
  674309865618LL,
  1052266987576LL,
  781684048060LL,
  141733920986LL,
  1095216660496LL,
  901943132403LL,
  51539607757LL,
  1013612281875LL,
  648540061791LL,
  98784247876LL,
  717259538628LL,
  261993005182LL,
  399431958628LL,
  493921239065LL,
  554050781280LL,
  944892805199LL,
  180388626466LL,
  584115552400LL,
  1022202216518LL,
  85899346104LL,
  403726926046LL,
  940597837835LL,
  214748365024LL,
  42949673018LL,
  25769803849LL,
  395136991268LL,
  906238099650LL,
  420906795180LL,
  639950127249LL,
  519691043044LL,
  858993459431LL,
  468151435319LL,
  914828034189LL,
  725849473102LL,
  369367187564LL,
  1005022347508LL,
  523986010213LL,
  34359738542LL,
  515396075706LL,
  197568495653LL,
  712964571164LL,
  850403524788LL,
  949187772648LL,
  133143986292LL,
  811748819019LL,
  592705486987LL,
  266287972464LL,
  438086664373LL,
  12884901960LL,
  60129542390LL,
  227633266785LL,
  794568949847LL,
  828928688262LL,
  678604832797LL,
  1065151889633LL,
  73014444184LL,
  932007903337LL,
  635655159950LL,
  128849019035LL,
  1000727380103LL,
  365072220366LL,
  957777707048LL,
  691489734796LL,
  55834574985LL,
  987842478271LL,
  446676598850LL,
  657129996353LL,
  64424509485LL,
  360777253040LL,
  94489280699LL,
  136339441869568LL,
  135239930246912LL,
  117647744233984LL,
  216603790700288LL,
  1099511640064LL,
  47279000020736LL,
  236395000036864LL,
  129742372121344LL,
  142936511662592LL,
  137438953523456LL,
  97856534936064LL,
  263882790684416LL,
  233096465132800LL,
  192414534902272LL,
  180319906995200LL,
  211106232562176LL,
  278176441874176LL,
  41781441893120LL,
  69269232563712LL,
  224300372129536LL,
  181419418596352LL,
  264982302352640LL,
  237494511628544LL,
  23089744195840LL,
  218802813928448LL,
  214404767425280LL,
  164926744172544LL,
  169324790678784LL,
  19791209301760LL,
  248489627910144LL,
  42880953543424LL,
  128642860495360LL,
  144036023240960LL,
  28587302333440LL,
  120946279062272LL,
  175921860467200LL,
  64871186059776LL,
  196812581426688LL,
  249589139515648LL,
  145135534878464LL,
  229797930226432LL,
  260584255782912LL,
  277076930207744LL,
  100055558172928LL,
  223200860465664LL,
  62672162831872LL,
  83562883729920LL,
  227598906972160LL,
  262783279091712LL,
  275977418615296LL,
  84662395355904LL,
  146235046507264LL,
  273778395333888LL,
  139637976728064LL,
  65970697687040LL,
  184717953507072LL,
  179220395348224LL,
  157230162788352LL,
  172623325598208LL,
  269380348819456LL,
  200111116303360LL,
  36283883772416LL,
  280375465086976LL,
  230897441895168LL,
  13194139585792LL,
  259484744160000LL,
  166026255818496LL,
  25288767456256LL,
  183618441888768LL,
  67070209326592LL,
  102254581408768LL,
  126443837200640LL,
  141837000007680LL,
  241892558130944LL,
  46179488375296LL,
  149533581414400LL,
  261683767428608LL,
  21990232602624LL,
  103354093067776LL,
  240793046485760LL,
  54975581446144LL,
  10995116292608LL,
  6597069785344LL,
  101155069764608LL,
  231996953510400LL,
  107752139566080LL,
  163827232575744LL,
  133040907019264LL,
  219902325614336LL,
  119846767441664LL,
  234195976752384LL,
  185817465114112LL,
  94558000016384LL,
  257285720962048LL,
  134140418614528LL,
  8796093066752LL,
  131941395380736LL,
  50577534887168LL,
  182518930217984LL,
  217703302345728LL,
  242992069797888LL,
  34084860490752LL,
  207807697668864LL,
  151732604668672LL,
  68169720950784LL,
  112150186079488LL,
  3298534901760LL,
  15393162851840LL,
  58274116296960LL,
  203409651160832LL,
  212205744195072LL,
  173722837196032LL,
  272678883746048LL,
  18691697711104LL,
  238594023254272LL,
  162727720947200LL,
  32985348872960LL,
  256186209306368LL,
  93458488413696LL,
  245191093004288LL,
  177021372107776LL,
  14293651196160LL,
  252887674437376LL,
  114349209305600LL,
  168225279066368LL,
  16492674428160LL,
  92358976778240LL,
  24189255858944LL,
  34902897118609408LL,
  34621422143209472LL,
  30117822523899904LL,
  55450570419273728LL,
  281474979856384LL,
  12103424005308416LL,
  60517120009437184LL,
  33214047263064064LL,
  36591746985623552LL,
  35184372102004736LL,
  25051272943632384LL,
  67553994415210496LL,
  59672695073996800LL,
  49258120934981632LL,
  46161896190771200LL,
  54043195535917056LL,
  71213169119789056LL,
  10696049124638720LL,
  17732923536310272LL,
  57420895265161216LL,
  46443371160666112LL,
  67835469402275840LL,
  60798594976907264LL,
  5910974514135040LL,
  56013520365682688LL,
  54887620460871680LL,
  42221246508171264LL,
  43347146413768704LL,
  5066549581250560LL,
  63613344744996864LL,
  10977524107116544LL,
  32932572286812160LL,
  36873221949685760LL,
  7318349397360640LL,
  30962247439941632LL,
  45035996279603200LL,
  16607023631302656LL,
  50384020845232128LL,
  63894819716005888LL,
  37154696928886784LL,
  58828270137966592LL,
  66709569480425472LL,
  70931694133182464LL,
  25614222892269568LL,
  57139420279209984LL,
  16044073684959232LL,
  21392098234859520LL,
  58265320184872960LL,
  67272519447478272LL,
  70650219165515776LL,
  21673573211111424LL,
  37436171905859584LL,
  70087269205475328LL,
  35747322042384384LL,
  16888498607882240LL,
  47287796097810432LL,
  45880421209145344LL,
  40250921673818112LL,
  44191571353141248LL,
  68961369297780736LL,
  51228445773660160LL,
  9288674245738496LL,
  71776119062265856LL,
  59109745125163008LL,
  3377699733962752LL,
  66428094504960000LL,
  42502721489534976LL,
  6473924468801536LL,
  47006321123524608LL,
  17169973587607552LL,
  26177172840644608LL,
  32369622323363840LL,
  36310272001966080LL,
  61924494881521664LL,
  11821949024075776LL,
  38280596842086400LL,
  66991044461723648LL,
  5629499546271744LL,
  26458647825350656LL,
  61643019900354560LL,
  14073748850212864LL,
  2814749770907648LL,
  1688849865048064LL,
  25895697859739648LL,
  59391220098662400LL,
  27584547728916480LL,
  41939771539390464LL,
  34058472196931584LL,
  56294995357270016LL,
  30680772465065984LL,
  59954170048610304LL,
  47569271069212672LL,
  24206848004194304LL,
  65865144566284288LL,
  34339947165319168LL,
  2251799825088512LL,
  33776997217468416LL,
  12947848931115008LL,
  46724846135803904LL,
  55732045400506368LL,
  62205969868259328LL,
  8725724285632512LL,
  53198770603229184LL,
  38843546795180032LL,
  17451448563400704LL,
  28710447636348928LL,
  844424934850560LL,
  3940649690071040LL,
  14918173772021760LL,
  52072870697172992LL,
  54324670513938432LL,
  44473046322184192LL,
  69805794238988288LL,
  4785074614042624LL,
  61080069953093632LL,
  41658296562483200LL,
  8444249311477760LL,
  65583669582430208LL,
  23925373033906176LL,
  62768919809097728LL,
  45317471259590656LL,
  3659174706216960LL,
  64739244655968256LL,
  29273397582233600LL,
  43065671440990208LL,
  4222124653608960LL,
  23643898055229440LL,
  6192449499889664LL,
  8935141662364008448LL,
  8863084068661624832LL,
  7710162566118375424LL,
  -4251398046375477248LL,
  72057594843234304LL,
  3098476545358954496LL,
  -2954361351293632512LL,
  8502796099344400384LL,
  -9079256845389922304LL,
  9007199258113212416LL,
  6413125873569890304LL,
  -1152921503415664640LL,
  -3170534134766370816LL,
  -5836665114354253824LL,
  -6629298648872124416LL,
  -4611686016514785280LL,
  -216172779043553280LL,
  2738188575907512320LL,
  4539628425295429632LL,
  -3746994885828280320LL,
  -6557241056579026944LL,
  -1080863906726936576LL,
  -2882303759621292032LL,
  1513209475618570240LL,
  -4107282860094783488LL,
  -4395513235726401536LL,
  -7638104967617708032LL,
  -7349874591784763392LL,
  1297036692800143360LL,
  -2161727818990354432LL,
  2810246171421835264LL,
  8430738505423912960LL,
  -9007199254589997056LL,
  1873497445724323840LL,
  7926335344625057792LL,
  -6917529026131132416LL,
  4251398049613479936LL,
  -5548434737330126848LL,
  -2089670226412044288LL,
  -8935141659914534912LL,
  -3386706918390104064LL,
  -1369094286720630784LL,
  -288230375614840832LL,
  6557241060421009408LL,
  -3819052482231795712LL,
  4107282863349563392LL,
  5476377148124037120LL,
  -3530822106382073856LL,
  -1224979095155113984LL,
  -360287967337512960LL,
  5548434742044524544LL,
  -8863084065809498112LL,
  -504403157107867648LL,
  9151314442850402304LL,
  4323455643617853440LL,
  -6341068272670081024LL,
  -6701356244168343552LL,
  -8142508125212114944LL,
  -7133701807305392128LL,
  -792633533477683200LL,
  -5332261955652550656LL,
  2377900606909054976LL,
  -72057593769492480LL,
  -3314649321667821568LL,
  864691131894464512LL,
  -1441151880439791616LL,
  -7566047372388597760LL,
  1657324664013193216LL,
  -6413125866087251968LL,
  4395513238427533312LL,
  6701356247205019648LL,
  8286623314781143040LL,
  -9151314441206235136LL,
  -2594073384040005632LL,
  3026418950163398656LL,
  -8646911282135433216LL,
  -1297036691508297728LL,
  1441151883845566464LL,
  6773413843289767936LL,
  -2666130979218784256LL,
  3602879705654493184LL,
  720575941352357888LL,
  432345565452304384LL,
  6629298652093349888LL,
  -3242591728451977216LL,
  7061644218602618880LL,
  -7710162559625592832LL,
  8718968882414485504LL,
  -4035225262248427520LL,
  7854277751056891904LL,
  -3098476541265313792LL,
  -6269010679991107584LL,
  6196953089073741824LL,
  -1585267064740773888LL,
  8791026474321707008LL,
  576460755222659072LL,
  8646911287671914496LL,
  3314649326365442048LL,
  -6485183462943752192LL,
  -4179340451179921408LL,
  -2522015787435163648LL,
  2233785417121923072LL,
  -4827858799282880512LL,
  -8502796094143463424LL,
  4467570832230580224LL,
  7349874594905325568LL,
  216172783321743360LL,
  1008806320658186240LL,
  3819052485637570560LL,
  -5116089175233265664LL,
  -4539628422141313024LL,
  -7061644215230398464LL,
  -576460748528549888LL,
  1224979101194911744LL,
  -2810246165717581824LL,
  -7782220153713852416LL,
  2161727823738306560LL,
  -1657324660607418368LL,
  6124895496679981056LL,
  -2377900602580533248LL,
  -6845471431254343680LL,
  936748724791541760LL,
  -1873497441781678080LL,
  7493989781051801600LL,
  -7421932184816058368LL,
  1080863911323893760LL,
  6052837902138736640LL,
  1585267071971753984LL
}; // weak
__int64 t_in[512] =
{
  6009281288570664017LL,
  -7611603186865662182LL,
  -1061269377293505733LL,
  -7853183411067225428LL,
  -689764681073938400LL,
  2687526331364068488LL,
  -2897174905443850929LL,
  -8096519173401463514LL,
  7429736679585001950LL,
  -2179462541710792123LL,
  1364674804068986819LL,
  -4109020725739305331LL,
  -7666091842917658877LL,
  -2713046496659739201LL,
  -3233175135405752620LL,
  4956433002338574409LL,
  8681126937143919221LL,
  -2490005540005062503LL,
  1706170401275503038LL,
  -5459824560244514615LL,
  -9065435003662704797LL,
  5007812992645157271LL,
  -8886046560086432591LL,
  -7770106866077498882LL,
  1872729502021863536LL,
  -5190262976154902892LL,
  -2161081917585001557LL,
  3074645068654518243LL,
  270978141183863730LL,
  -6554927875668327034LL,
  -5573838802687416272LL,
  6665915608752325378LL,
  -7875536089474347126LL,
  -6781742301095786509LL,
  -3044990771272557979LL,
  -8431118091120659247LL,
  -6893335774870950348LL,
  8497156307053283845LL,
  -6129574738861980917LL,
  5841290474838520158LL,
  4397445726675280190LL,
  5097483574559719133LL,
  386681135680017553LL,
  -66058244403755516LL,
  -7500254976967731175LL,
  8619565776665526409LL,
  -8607635596455515984LL,
  -2598919511984104985LL,
  -1653029436686435167LL,
  3374220536LL,
  5254903836166225929LL,
  5652679905099387166LL,
  6212861989656399613LL,
  2826339951620501053LL,
  2424727052823891722LL,
  4192347834887199643LL,
  1145980922037209612LL,
  -7020659800294363468LL,
  -6764164329802186624LL,
  1592605331463829338LL,
  -1933556343328042014LL,
  2096173916826378812LL,
  -5924612913098847986LL,
  -3987622920696842707LL,
  5478476868432294231LL,
  -189010843141301778LL,
  -4830829283152363017LL,
  3782737108607395396LL,
  -2538301983208946805LL,
  7201788759828655542LL,
  1190466815031783895LL,
  2310846355131766547LL,
  -559084266007016827LL,
  7899641156348344750LL,
  -923041264386990563LL,
  -3394656973297121779LL,
  -7369735442249895125LL,
  2478362190311625745LL,
  1891494941801905320LL,
  -1184390477249741482LL,
  -4480737646254077561LL,
  3894440056364124812LL,
  2944926058916476326LL,
  -6575346444196202534LL,
  977976357921045036LL,
  7081486365942898538LL,
  -1677352744952558090LL,
  -743160448786417362LL,
  8976747634865757599LL,
  -5543326617512848017LL,
  -6377915039288283960LL,
  8915785700945927400LL,
  -857837399873935667LL,
  -6297633704836097300LL,
  9144277200733640166LL,
  -1808171134000055263LL,
  -3587557685884885062LL,
  -2991322358783303702LL,
  3540743184612238385LL,
  -4582797221470886458LL,
  -6428181498339504524LL,
  1574191910217617632LL,
  -586897036249922319LL,
  3456109292263886207LL,
  5598237410243005814LL,
  -2376608795933693236LL,
  1983952514327105950LL,
  9174225270341577921LL,
  6716428355298287261LL,
  3332958373832591354LL,
  5968073877842913203LL,
  1389314774568145129LL,
  8794581432298231706LL,
  -8557943289488803751LL,
  3875736396931508686LL,
  4373355306456259809LL,
  4572264495105168028LL,
  -4668043189967514600LL,
  6605370391895340883LL,
  -8729308170197975585LL,
  4522855071745290186LL,
  6863665886473233464LL,
  875355001600548118LL,
  4725698775199857704LL,
  -2399560946063136711LL,
  -8016030871613819688LL,
  8121734870054783867LL,
  4780492734921993288LL,
  7296251769082106192LL,
  6784455782329424579LL,
  5016200681714564043LL,
  280150698052201643LL,
  7887683073657020501LL,
  5477209638091655313LL,
  -3807013004157562884LL,
  -6673572070880369024LL,
  1997950510525701705LL,
  -4540088153143622248LL,
  -1131387490101378302LL,
  -444893437493867101LL,
  -7164640325895519257LL,
  6436371583055282155LL,
  2410649366021723181LL,
  -3978492371319895767LL,
  8759207123826668906LL,
  8194624828164577643LL,
  -5942235725400260938LL,
  4237462514337630566LL,
  3538392805017608984LL,
  9174784915237082976LL,
  -5878398710988885536LL,
  3101002760187936284LL,
  -196593669434216360LL,
  -541748382408993657LL,
  165352129864903459LL,
  -6100857787227577513LL,
  -4416572168948960761LL,
  592174583722116762LL,
  -6503440087220997902LL,
  -9072803418119929158LL,
  -5442134428710106581LL,
  -2132086491125910544LL,
  -4754386315613936179LL,
  -97174243309727457LL,
  6193472722038568093LL,
  -1443785804323617486LL,
  -1197886398023922887LL,
  1184091910310026758LL,
  494716725135228665LL,
  -4763034592766665298LL,
  6756649609622491573LL,
  1535834124823430255LL,
  -1603889944024573660LL,
  -7000450383843653172LL,
  -8392168148808716099LL,
  -1240607714307348680LL,
  1099578599376986439LL,
  512030921LL,
  -1356935575646500477LL,
  8240017552371162796LL,
  4072678304347258363LL,
  4120008778830462238LL,
  -6459173270971282844LL,
  3329888807429053393LL,
  -1776789833611866959LL,
  -7954733948868840238LL,
  2367874855820361807LL,
  1878594876647889513LL,
  3071667038776975882LL,
  1664944406767221827LL,
  -4067891013337149941LL,
  -6260543579941884487LL,
  537528471928264581LL,
  6953456660423241403LL,
  -760443423910922337LL,
  9150007347826279621LL,
  -4169264683569411210LL,
  -1016484876294179224LL,
  -8835145403789617206LL,
  1280856417587630912LL,
  4448381020161082749LL,
  -6833711687737430511LL,
  3509109996709092683LL,
  -2035213819973333524LL,
  -5084377504325358740LL,
  7271421285938500090LL,
  4607358454694979780LL,
  -8056056897796745512LL,
  -3370704896821459001LL,
  852473589906705662LL,
  -2415436656733215025LL,
  -4634978465492772314LL,
  -7892470327564161820LL,
  5079590213032831643LL,
  -5127188849326426430LL,
  -5781633503687266722LL,
  -7795614971527848002LL,
  1307679635296055209LL,
  9013972966205868091LL,
  -4955125641992411026LL,
  1754555526104403209LL,
  -7327493308050379775LL,
  -1801533865926990235LL,
  -1723207507792617208LL,
  3922436051702364889LL,
  8984727255865354964LL,
  2539795107799118255LL,
  7395532639339660848LL,
  -3854240538021694409LL,
  -2835241271619952464LL,
  -2671687947274358454LL,
  -679736576769687794LL,
  -5697261002013444467LL,
  330703270390189140LL,
  -8616991255121781021LL,
  5865171601517756856LL,
  3858460483625327876LL,
  4687116014151268979LL,
  -3252845424466742438LL,
  5176444751524718899LL,
  910069598472608396LL,
  4329062640828045710LL,
  -3935663160418513170LL,
  -5672430803689348627LL,
  8354833991552113753LL,
  4019308460084828281LL,
  -6125635070251805718LL,
  -2646858162199732460LL,
  -4293978564670272895LL,
  4657780153783695404LL,
  2729951087139034738LL,
  -7706222731494414197LL,
  -5545047291596359311LL,
  -4515311002917218148LL,
  -5317953530730153119LL,
  6321031060451510388LL,
  4719301156886630567LL,
  2826737046987129764LL,
  -7124710803948975253LL,
  -2068397958546805928LL,
  8551762216265733626LL,
  213117569149210998LL,
  3082106790730530007LL,
  7112749117635723332LL,
  -5033503639870748326LL,
  -117689796561364978LL,
  5512708241680958069LL,
  -3212255135265873001LL,
  -7920259870014314657LL,
  5950902560268217210LL,
  8383682848239136131LL,
  -3923122595846084247LL,
  -8145753361657533815LL,
  -5104868173713675458LL,
  -8579331254556641713LL,
  -3567497168056392020LL,
  1938098365365688394LL,
  6008441265997701171LL,
  7762944267251736695LL,
  646710936089795744LL,
  5012253485919393896LL,
  8886367294269458284LL,
  5436656200348017619LL,
  6153652855311128463LL,
  -5391086093226342616LL,
  4022740349264239227LL,
  -4673695565866798457LL,
  1652078351780854378LL,
  8766136504973667100LL,
  7588180414333972722LL,
  362211835734511092LL,
  -6429861543565779102LL,
  -890973488558072493LL,
  -674284389705370911LL,
  6935731371823020524LL,
  7979623245833701023LL,
  -2479727448284006006LL,
  -1851746093161927163LL,
  -4291643071498439283LL,
  5791909586558808020LL,
  -4767456125990132485LL,
  -2781122723306681277LL,
  -8572733231190721214LL,
  -4000924978258692005LL,
  4790960356340090634LL,
  2230896926LL,
  3112630477358990214LL,
  6515669012718726256LL,
  -8858767143244792833LL,
  3257834509138992853LL,
  6658609046167905497LL,
  3901307147989537108LL,
  6310404987856662887LL,
  -7270042823847259498LL,
  -2566592051072184379LL,
  1304942294957975883LL,
  -6863233861080184134LL,
  1950653573800674272LL,
  -8362430487711577331LL,
  2167577951903529384LL,
  8480080220359460121LL,
  9197473455361932253LL,
  8240669178667179814LL,
  -334616029822663365LL,
  2579397955694589481LL,
  -1965711346620798724LL,
  7152297322470689500LL,
  -4142150497352859614LL,
  -4912591301380834012LL,
  3010495418366562610LL,
  -5558300449945334993LL,
  -4505903226535678894LL,
  8118188826165996566LL,
  -1637302124149605816LL,
  -1107856646208961396LL,
  3684770848999659564LL,
  4096518440696137550LL,
  -3127690516938031454LL,
  8837514781853667201LL,
  -5962866238285011314LL,
  8669444204380611741LL,
  9103008801210670028LL,
  -2841515485517397485LL,
  -4358651356413796617LL,
  -3429072668893135232LL,
  2724593193054087469LL,
  1734070014749195161LL,
  4313177113858960995LL,
  6444356501381122424LL,
  5729608532922991031LL,
  -24067319174634130LL,
  1580735881493416143LL,
  8019448821697927579LL,
  -5752831218241842167LL,
  4551504402559643570LL,
  -6758284118699134828LL,
  -9008141766817990724LL,
  -6398746625675579184LL,
  -1422583491514512744LL,
  -7991866276689539504LL,
  -1206034855595962922LL,
  -7573683462191950771LL,
  7659527340696724405LL,
  7297660232820439071LL,
  -8358296949620931350LL,
  863334612560671604LL,
  -2624944569167160803LL,
  -2995716981184187562LL,
  -6829856114308641695LL,
  1435391801205296660LL,
  7041155682670669351LL,
  5150495855243029989LL,
  -984811176823334433LL,
  -4074702994558912050LL,
  -189331869474231603LL,
  4933841667978368111LL,
  7546131665119773171LL,
  -6646645370955224012LL,
  -2108797171974901053LL,
  1008596950769503049LL,
  867187791471669505LL,
  6225259746570050788LL,
  3662957802335986052LL,
  -5129527032328719268LL,
  9102730603057424372LL,
  4221665222752183319LL,
  2301697426178403243LL,
  5445700650810431738LL,
  -5911417106761123280LL,
  -782135386514032948LL,
  -4190657545653921819LL,
  -5363888989226908619LL,
  2695153391411747505LL,
  6764900313609015018LL,
  -9145983680756157137LL,
  7766169294456723270LL,
  1555321397271027599LL,
  -7648702805193295251LL,
  6400496281083282366LL,
  -8195204655066027552LL,
  -830780478487623230LL,
  2872577179289075288LL,
  -1146256777315725343LL,
  9057929410365533216LL,
  -1908909302650221857LL,
  7081205986108650321LL,
  -4934627514122209436LL,
  -462697544089296767LL,
  -8135192777188677560LL,
  5960505841504709854LL,
  8278181589077447539LL,
  7361884926265233183LL,
  3387765430344427755LL,
  -3196139325975004219LL,
  2572300577760511784LL,
  -1343055557816587773LL,
  -6371832088673706801LL,
  5666058750900892167LL,
  492508664659047642LL,
  -4284332103063084492LL,
  -6728283340243381458LL,
  -6596106637332520566LL,
  4659799960252443779LL,
  -4805041261217669263LL,
  -7620927447126930911LL,
  5568346788549494078LL,
  8144018600008977748LL,
  6989328580118303750LL,
  -2983659229729719400LL,
  7455602365292102464LL,
  542837178590118632LL,
  8780874445129014041LL,
  9000742109124692604LL,
  4173930116LL,
  3623406669318555264LL,
  7804300935666888721LL,
  1105132906708991758LL,
  3902150468601566638LL,
  7503461368417540367LL,
  2610449746091922523LL,
  -7849801264422361334LL,
  1990188183283996398LL,
  7035221699323741632LL,
  2023833748678200183LL,
  -4547181378285159789LL,
  1305224872749948962LL,
  -959891745510060791LL,
  1497632823181945014LL,
  -5815264818281244175LL,
  -6630036973837099623LL,
  6682485821240452609LL,
  6572017373896063846LL,
  -3756910133167904445LL,
  -5160014783421285139LL,
  4760451469999266865LL,
  -8926114919024156009LL,
  -3244775655912692662LL,
  -4076424887314337031LL,
  -2525621424470937698LL,
  8633650903971025542LL,
  -6225741118447020365LL,
  5125770278092818580LL,
  -6910140375390450436LL,
  2517389266898136189LL,
  -2755691456247673271LL,
  -7478777123675659574LL,
  -6545737440017022475LL,
  4586001073980214967LL,
  5768427742578384186LL,
  6080499955672009823LL,
  -8004945095658695795LL,
  -9010102226653350087LL,
  7601112795337097309LL,
  -3480418135030813227LL,
  1200065371109562796LL,
  -2631303691290909796LL,
  7995042012460578854LL,
  -8959741790600972390LL,
  -6161233253439803755LL,
  -1159984676555141188LL,
  5390305657513155559LL,
  3014734099866978719LL,
  3040249977775305380LL,
  3873208528834434213LL,
  -241282868373373874LL,
  3681086485986070672LL,
  4753508642908182532LL,
  1670825091540078797LL,
  4849726916280571469LL,
  -1954838667535823446LL,
  5484126962767345105LL,
  5079868402180824876LL,
  98293644918057566LL,
  -347268601558633337LL,
  -7903022555033100953LL,
  7859704763052676624LL,
  3997521507466895831LL,
  -1474581349182597896LL,
  -5245064769082349655LL,
  8808109893521106204LL,
  6142755902599061458LL,
  8340446363162103316LL,
  6871273661495168503LL,
  8684870020277497661LL,
  -5098421773783665745LL,
  -4440759637797751772LL,
  -4896497041889115363LL,
  -53597836804404932LL,
  639145062217875880LL,
  7246504379184637108LL,
  -3066751018304699189LL,
  -3440090716356453268LL
}; // weak
__int64 t_il[512] =
{
  38654705746LL,
  914828034154LL,
  231928234032LL,
  240518168741LL,
  274877907135LL,
  678604832931LL,
  1043677053057LL,
  1078036791511LL,
  974957576316LL,
  558345748537LL,
  201863463067LL,
  579820585215LL,
  609885356084LL,
  292057776195LL,
  953482739908LL,
  871878361321LL,
  528280977492LL,
  214748364948LL,
  833223655590LL,
  261993005091LL,
  326417514734LL,
  47244640405LL,
  1073741824066LL,
  335007449283LL,
  197568495624LL,
  438086664353LL,
  932007903272LL,
  764504178724LL,
  390842024054LL,
  313532612770LL,
  597000454253LL,
  158913790161LL,
  1065151889522LL,
  429496729846LL,
  446676598918LL,
  94489280664LL,
  704374636756LL,
  876173328476LL,
  433791696989LL,
  627065225398LL,
  481036337260LL,
  343597383752LL,
  1017907249405LL,
  936302870713LL,
  90194313310LL,
  373662154822LL,
  605590388903LL,
  566935683229LL,
  927712936080LL,
  171LL,
  807453851788LL,
  42949673171LL,
  979252543735LL,
  21474836568LL,
  768799146168LL,
  25769803845LL,
  188978561232LL,
  614180323358LL,
  270582939850LL,
  8589934607LL,
  751619276993LL,
  12884902077LL,
  81604378625LL,
  459561500810LL,
  622770257978LL,
  279172874257LL,
  442381631567LL,
  1005022347484LL,
  1039382085783LL,
  884763263183LL,
  773094113520LL,
  493921239270LL,
  738734375062LL,
  146028888180LL,
  743029342439LL,
  571230650421LL,
  1069446856930LL,
  996432412727LL,
  502511173660LL,
  472446402783LL,
  1035087118407LL,
  485331304474LL,
  176093659165LL,
  588410519749LL,
  785979015279LL,
  60129542242LL,
  103079215274LL,
  115964117182LL,
  369367187708LL,
  322122547262LL,
  901943132358LL,
  137438953593LL,
  940597837978LL,
  1090921693376LL,
  880468295800LL,
  1047972020314LL,
  949187772447LL,
  219043332264LL,
  30064771208LL,
  210453397703LL,
  77309411505LL,
  382252089360LL,
  549755813927LL,
  408021893356LL,
  347892351072LL,
  725849473151LL,
  777389080601LL,
  55834574922LL,
  983547510829LL,
  682899800186LL,
  863288426643LL,
  1026497183900LL,
  962072674464LL,
  330712481851LL,
  180388626606LL,
  755914244341LL,
  1009317314760LL,
  257698037947LL,
  356482285699LL,
  416611827865LL,
  184683593751LL,
  541165879300LL,
  511101108410LL,
  163208757462LL,
  450971566305LL,
  425201762324LL,
  141733920853LL,
  536870912012LL,
  9895604670976LL,
  234195976743424LL,
  59373627912192LL,
  61572651197696LL,
  70368744226560LL,
  173722837230336LL,
  267181325582592LL,
  275977418626816LL,
  249589139536896LL,
  142936511625472LL,
  51677046545152LL,
  148434069815040LL,
  156130651157504LL,
  74766790705920LL,
  244091581416448LL,
  223200860498176LL,
  135239930237952LL,
  54975581426688LL,
  213305255831040LL,
  67070209303296LL,
  83562883771904LL,
  12094627943680LL,
  274877906960896LL,
  85761907016448LL,
  50577534879744LL,
  112150186074368LL,
  238594023237632LL,
  195713069753344LL,
  100055558157824LL,
  80264348869120LL,
  152832116288768LL,
  40681930281216LL,
  272678883717632LL,
  109951162840576LL,
  114349209323008LL,
  24189255849984LL,
  180319907009536LL,
  224300372089856LL,
  111050674429184LL,
  160528697701888LL,
  123145302338560LL,
  87960930240512LL,
  260584255847680LL,
  239693534902528LL,
  23089744207360LL,
  95657511634432LL,
  155031139559168LL,
  145135534906624LL,
  237494511636480LL,
  43776LL,
  206708186057728LL,
  10995116331776LL,
  250688651196160LL,
  5497558161408LL,
  196812581419008LL,
  6597069784320LL,
  48378511675392LL,
  157230162779648LL,
  69269232601600LL,
  2199023259392LL,
  192414534910208LL,
  3298534931712LL,
  20890720928000LL,
  117647744207360LL,
  159429186042368LL,
  71468255809792LL,
  113249697681152LL,
  257285720955904LL,
  266081813960448LL,
  226499395374848LL,
  197912093061120LL,
  126443837253120LL,
  189116000015872LL,
  37383395374080LL,
  190215511664384LL,
  146235046507776LL,
  273778395374080LL,
  255086697658112LL,
  128642860456960LL,
  120946279112448LL,
  264982302312192LL,
  124244813945344LL,
  45079976746240LL,
  150633093055744LL,
  201210627911424LL,
  15393162813952LL,
  26388279110144LL,
  29686813998592LL,
  94558000053248LL,
  82463372099072LL,
  230897441883648LL,
  35184372119808LL,
  240793046522368LL,
  279275953504256LL,
  225399883724800LL,
  268280837200384LL,
  242992069746432LL,
  56075093059584LL,
  7696581429248LL,
  53876069811968LL,
  19791209345280LL,
  97856534876160LL,
  140737488365312LL,
  104453604699136LL,
  89060441874432LL,
  185817465126656LL,
  199011604633856LL,
  14293651180032LL,
  251788162772224LL,
  174822348847616LL,
  221001837220608LL,
  262783279078400LL,
  246290604662784LL,
  84662395353856LL,
  46179488411136LL,
  193514046551296LL,
  258385232578560LL,
  65970697714432LL,
  91259465138944LL,
  106652627933440LL,
  47279000000256LL,
  138538465100800LL,
  130841883752960LL,
  41781441910272LL,
  115448720974080LL,
  108851651154944LL,
  36283883738368LL,
  137438953475072LL,
  2533274795769856LL,
  59954170046316544LL,
  15199648745521152LL,
  15762598706610176LL,
  18014398521999360LL,
  44473046330966016LL,
  68398419349143552LL,
  70650219168464896LL,
  63894819721445376LL,
  36591746976120832LL,
  13229323915558912LL,
  37999121872650240LL,
  39969446696321024LL,
  19140298420715520LL,
  62487444842610688LL,
  57139420287533056LL,
  34621422140915712LL,
  14073748845232128LL,
  54606145492746240LL,
  17169973581643776LL,
  21392098245607424LL,
  3096224753582080LL,
  70368744181989376LL,
  21955048196210688LL,
  12947848929214464LL,
  28710447635038208LL,
  61080069948833792LL,
  50102545856856064LL,
  25614222888402944LL,
  20547673310494720LL,
  39125021769924608LL,
  10414574151991296LL,
  69805794231713792LL,
  28147497687187456LL,
  29273397586690048LL,
  6192449497595904LL,
  46161896194441216LL,
  57420895255003136LL,
  28428972653871104LL,
  41095346611683328LL,
  31525197398671360LL,
  22517998141571072LL,
  66709569497006080LL,
  61361544935047168LL,
  5910974517084160LL,
  24488322978414592LL,
  39687971727147008LL,
  37154696936095744LL,
  60798594978938880LL,
  11206656LL,
  52917295630778368LL,
  2814749780934656LL,
  64176294706216960LL,
  1407374889320448LL,
  50384020843266048LL,
  1688849864785920LL,
  12384898988900352LL,
  40250921671589888LL,
  17732923546009600LL,
  562949954404352LL,
  49258120937013248LL,
  844424942518272LL,
  5348024557568000LL,
  30117822517084160LL,
  40813871626846208LL,
  18295873487306752LL,
  28991922606374912LL,
  65865144564711424LL,
  68116944373874688LL,
  57983845215961088LL,
  50665495823646720LL,
  32369622336798720LL,
  48413696004063232LL,
  9570149215764480LL,
  48695170986082304LL,
  37436171905990656LL,
  70087269215764480LL,
  65302194600476672LL,
  32932572276981760LL,
  30962247452786688LL,
  67835469391921152LL,
  31806672370008064LL,
  11540474047037440LL,
  38562071822270464LL,
  51509920745324544LL,
  3940649680371712LL,
  6755399452196864LL,
  7599824383639552LL,
  24206848013631488LL,
  21110623257362432LL,
  59109745122213888LL,
  9007199262670848LL,
  61643019909726208LL,
  71494644097089536LL,
  57702370233548800LL,
  68679894323298304LL,
  62205969855086592LL,
  14355223823253504LL,
  1970324845887488LL,
  13792273871863808LL,
  5066549592391680LL,
  25051272928296960LL,
  36028797021519872LL,
  26740122802978816LL,
  22799473119854592LL,
  47569271072423936LL,
  50946970786267136LL,
  3659174702088192LL,
  64457769669689344LL,
  44754521304989696LL,
  56576470328475648LL,
  67272519444070400LL,
  63050394793672704LL,
  21673573210587136LL,
  11821949033250816LL,
  49539595917131776LL,
  66146619540111360LL,
  16888498614894592LL,
  23362423075569664LL,
  27303072750960640LL,
  12103424000065536LL,
  35465847065804800LL,
  33495522240757760LL,
  10696049129029632LL,
  29554872569364480LL,
  27866022695665664LL,
  9288674237022208LL,
  35184372089618432LL,
  648518347717083136LL,
  -3098476541852516352LL,
  3891110078853414912LL,
  4035225268892205056LL,
  4611686021631836160LL,
  -7061644212982251520LL,
  -936748720328802304LL,
  -360287966582538240LL,
  -2089670225019535360LL,
  -9079256847822618624LL,
  3386706922383081472LL,
  -8718968874311090176LL,
  -8214565719451369472LL,
  4899916395703173120LL,
  -2449958194001215488LL,
  -3819052480101089280LL,
  8863084068074422272LL,
  3602879704379424768LL,
  -4467570827566514176LL,
  4395513236900806656LL,
  5476377150875500544LL,
  792633536917012480LL,
  -432345563120271360LL,
  5620492338229936128LL,
  3314649325878902784LL,
  7349874594569781248LL,
  -2810246166808100864LL,
  -5620492334354399232LL,
  6557241059431153664LL,
  5260204367486648320LL,
  -8430738500608851968LL,
  2666130982909771776LL,
  -576460750390820864LL,
  7205759407919988736LL,
  7493989782192652288LL,
  1585267071384551424LL,
  -6629298647932600320LL,
  -3746994888428748800LL,
  7277816999391002624LL,
  -7926335341118619648LL,
  8070450534059868160LL,
  5764607524242194432LL,
  -1369094282475995136LL,
  -2738188570337476608LL,
  1513209476373544960LL,
  6269010682474135552LL,
  -8286623311559917568LL,
  -8935141658069041152LL,
  -2882303759101198336LL,
  2868903936LL,
  -4899916392230289408LL,
  720575943919271936LL,
  -2017612628918009856LL,
  360287971666034688LL,
  -5548434737833443328LL,
  432345565385195520LL,
  3170534141158490112LL,
  -8142508125782540288LL,
  4539628427778457600LL,
  144115188327514112LL,
  -5836665113834160128LL,
  216172785284677632LL,
  1369094286737408000LL,
  7710162564373544960LL,
  -7998392937236922368LL,
  4683743612750528512LL,
  7421932187231977472LL,
  -1585267065143427072LL,
  -1008806313997631488LL,
  -3602879698423513088LL,
  -5476377142855991296LL,
  8286623318220472320LL,
  -6052837896669364224LL,
  2449958199235706880LL,
  -5980780301272481792LL,
  -8863084065775943680LL,
  -504403154473844736LL,
  -1729382255987523584LL,
  8430738502907330560LL,
  7926335347913392128LL,
  -1080863909377736704LL,
  8142508126722064384LL,
  2954361356041584640LL,
  -8574853687208312832LL,
  -5260204362906468352LL,
  1008806318175158272LL,
  1729382259762397184LL,
  1945555042211725312LL,
  6196953091489660928LL,
  5404319553884782592LL,
  -3314649322422796288LL,
  2305843011243737088LL,
  -2666130976819642368LL,
  -144115184854630400LL,
  -3674937293921058816LL,
  -864691126945185792LL,
  -2522015790807384064LL,
  3674937298752897024LL,
  504403160547196928LL,
  3530822111197134848LL,
  1297036695652270080LL,
  6413125869644021760LL,
  -9223372036200464384LL,
  6845471437562576896LL,
  5836665118682775552LL,
  -6269010679169024000LL,
  -5404319552425164800LL,
  936748723734577152LL,
  -1945555038269079552LL,
  -6989586619632189440LL,
  -3963167669619785728LL,
  -1224979096027529216LL,
  -2305843006529339392LL,
  5548434741910306816LL,
  3026418952512208896LL,
  -5764607518923816960LL,
  -1513209471441043456LL,
  4323455645413015552LL,
  5980780307345833984LL,
  6989586624245923840LL,
  3098476544016777216LL,
  9079256848846028800LL,
  8574853693633986560LL,
  2738188577031585792LL,
  7566047377757306880LL,
  7133701810090409984LL,
  2377900604677685248LL,
  9007199254942318592LL
}; // weak
__int64 t_im[512] =
{
  796302664848637952LL,
  2096173916058423836LL,
  2826339951844795448LL,
  3540743182640625188LL,
  6009281288700708976LL,
  5007812992347298412LL,
  9174225269204741192LL,
  7587289227315281492LL,
  -4909598768316247840LL,
  -5924612910485961988LL,
  -7500254974955965224LL,
  -9082722739025959228LL,
  -2038492775875815280LL,
  -743160446107596148LL,
  -3494241739251843928LL,
  -2766362756505997644LL,
  8121734870494559195LL,
  7398324337362676167LL,
  6665915607093813219LL,
  5375051589500603903LL,
  2944926056036135851LL,
  4522855070467449271LL,
  354199549371701139LL,
  1364674804463781263LL,
  -4582797218873103557LL,
  -2991322358625486553LL,
  -1417923538393788669LL,
  -411916561121174241LL,
  -7489791545730407605LL,
  -8208663157420723881LL,
  -5459824558758967437LL,
  -6764164328302193297LL,
  -189010842158795091LL,
  -1478748856765684559LL,
  -3362962296797572459LL,
  -4085246964382089079LL,
  -6554927875340550435LL,
  -5543326617262150463LL,
  -8557943289688286491LL,
  -6978888409709606663LL,
  5598237409050224205LL,
  6605370389309158481LL,
  7026965049533645429LL,
  8619565775328628841LL,
  1574191907768479293LL,
  270978141211573281LL,
  4173855038787127813LL,
  3456109292644177945LL,
  -8729308168161178250LL,
  -8016030871632191638LL,
  -6129574737117812402LL,
  -4830829282735916206LL,
  -2399560944373070586LL,
  -3987622920529373414LL,
  -970763003864932034LL,
  -1973357097046232286LL,
  3894440053115903382LL,
  2310846354779066250LL,
  1891494938792884654LL,
  875354999795280818LL,
  7954373888814519782LL,
  8681126937293522938LL,
  4780492734200459742LL,
  6074699267140789186LL,
  -227461124535423935LL,
  -1527292758966862243LL,
  -3394656973572093831LL,
  -4109020725028529563LL,
  -6575346441895369679LL,
  -5573838803641518547LL,
  -8607635597424164855LL,
  -7020659801317404139LL,
  5478476867391093921LL,
  6493460257594961597LL,
  6931973962234484889LL,
  8514410836899678853LL,
  1472463631686972625LL,
  177100275075000013LL,
  4060867969794052329LL,
  3332958372521313013LL,
  -8688324851512780902LL,
  -7964954003878722170LL,
  -6099850075942030430LL,
  -4809025606407691842LL,
  -2376608793790668822LL,
  -3954577218841899530LL,
  -923041261483607086LL,
  -1933556339512464946LL,
  4016171228849063802LL,
  2424727051847816550LL,
  1983952510783939394LL,
  977976354196648286LL,
  8058072249214242570LL,
  8776974819028835606LL,
  4890946837641243442LL,
  6195317152991885614LL,
  773092884948163308LL,
  2062791557654612208LL,
  2778360167828537044LL,
  3500605081195752648LL,
  5968073877838877340LL,
  4956433002982129792LL,
  9144277201010292388LL,
  7565182841692218552LL,
  -5014191925022488052LL,
  -6021293878437668848LL,
  -7611603187508498892LL,
  -9204173298776589272LL,
  -2161081914275970436LL,
  -857837395753217952LL,
  -3587557686226753980LL,
  -2869781050679004072LL,
  8142974566076817719LL,
  7429736680167748395LL,
  6716428356662363407LL,
  5417722725217244947LL,
  2984162832823937351LL,
  4572264494478064475LL,
  386681132874255743LL,
  1389314774114426723LL,
  -4480737646194445865LL,
  -2897174905981885493LL,
  -1304604760486701585LL,
  -288495367296514061LL,
  -7369735441602020953LL,
  -8096519173327393861LL,
  -5364538596185317985LL,
  -6658775949810970749LL,
  939297437690888192LL,
  1664944403444276246LL,
  4120008776120285228LL,
  2539795105577313338LL,
  7296251768013877336LL,
  9174784916294626382LL,
  5865171602545330292LL,
  5437914483273913442LL,
  -2478687629465296720LL,
  -4067891010158265178LL,
  -1603889944408565604LL,
  -887267941896764278LL,
  -5345316581816102680LL,
  -5781633505103803138LL,
  -9082169663168927548LL,
  -7212590834507664174LL,
  -5317953529816818821LL,
  -6043609257303488659LL,
  -9072803417497279657LL,
  -7492598577407066303LL,
  -2415436655366001885LL,
  -4293978565380032715LL,
  -1558635776636906737LL,
  -1131387487818248423LL,
  7395532638652152779LL,
  8984727257611839453LL,
  5946455666855969767LL,
  5229824833891409905LL,
  1074747980283857811LL,
  1511056141838276485LL,
  4237462512385618879LL,
  2367874853271596969LL,
  6953456658541882870LL,
  8821918263129977312LL,
  6078623445515671002LL,
  5641189298154801612LL,
  592174581733580206LL,
  1307679635050119608LL,
  4329062641300071810LL,
  2738742311411841428LL,
  -5697261002422858426LL,
  -6125635070889537200LL,
  -8877866581577534102LL,
  -7000450382492046980LL,
  -2835241271534142178LL,
  -4416572166150283000LL,
  -1404266315584502478LL,
  -679736573904283356LL,
  -2646858160470657395LL,
  -4515311003325470053LL,
  -1197886398193381727LL,
  -760443420379753801LL,
  -5545047288436160811LL,
  -6260543580019418429LL,
  -8707656061263200519LL,
  -7117326900922211601LL,
  852473587290707517LL,
  1280856417490668075LL,
  4607358453184835089LL,
  2729951084552106503LL,
  7177867467121092197LL,
  8759207123470514803LL,
  6321031060422516297LL,
  5596510149195055711LL,
  -2889815638465035785LL,
  -3605320708961444383LL,
  -2035213817351734821LL,
  -444893435923896883LL,
  -4634978464890080849LL,
  -6503440086658044487LL,
  -8392168147674723965LL,
  -7954733948774247019LL,
  537528471668957511LL,
  2118859349105229137LL,
  3697903208666536299LL,
  2973373518525924733LL,
  8015808190429319455LL,
  8444182241716129033LL,
  6564532161017342259LL,
  4687116013471462693LL,
  7844870910108998284LL,
  8560367150152648346LL,
  6411626871396737696LL,
  4821297728235617974LL,
  330703269248625364LL,
  2199156060563830466LL,
  3509109996945920760LL,
  3071667036312162030LL,
  -4877833563849393604LL,
  -6459173271738423766LL,
  -8616991255934320112LL,
  -7892470327526990330LL,
  -3168840409379042716LL,
  -3597223291118610830LL,
  -2296206597566025144LL,
  -418799211753427362LL,
  -5002406225420555263LL,
  -6880939356521435113LL,
  -8162815940631800787LL,
  -7735558872899991493LL,
  -3252845421204956071LL,
  -3978492369778474929LL,
  -1801533863823498123LL,
  -221320244820133789LL,
  7638952392566377649LL,
  8075269333033947303LL,
  6784455780971984029LL,
  4914876900771113099LL,
  165352126573346025LL,
  1754555524446183679LL,
  3922436050083267781LL,
  3205813996031858899LL,
  119408547831297914LL,
  1997950509384936300LL,
  3858460480641503062LL,
  3431212174642975552LL,
  7629178210901116706LL,
  8354833989927394100LL,
  6756649608607006478LL,
  5176444751336923928LL,
  -3370704894440646710LL,
  -3807013004455457828LL,
  -1937425327758905370LL,
  -67837685824752656LL,
  -5084377501678327918LL,
  -6673572069098407036LL,
  -8262819207589290050LL,
  -7546188391804599384LL,
  652471146477780992LL,
  1950653573530129946LL,
  3257834506823412788LL,
  4551504400713267758LL,
  4719301154002983016LL,
  6008441266145873522LL,
  7297660233886233692LL,
  8600302211380306502LL,
  -7354734581156630320LL,
  -8362430485665896758LL,
  -4767456125507822364LL,
  -5779664288301675778LL,
  -3341983503300368200LL,
  -4358651353974899038LL,
  -745680531086588788LL,
  -1748846241532030314LL,
  3662957799926365115LL,
  2362532369153879457LL,
  1652078352265336719LL,
  356147862469296533LL,
  8837514779672912851LL,
  7546131663809885641LL,
  6817610815446912999LL,
  5512708242046654973LL,
  -6758284118607541397LL,
  -5752831217818411663LL,
  -8787107458599098529LL,
  -7777159891711430331LL,
  -1565678105731034365LL,
  -551253258776640231LL,
  -3567497165260210377LL,
  -2566592050720954067LL,
  9197473454770484845LL,
  7908298381593796727LL,
  6601099838934604377LL,
  5298422625528826947LL,
  4022740347005021701LL,
  2724593190224115743LL,
  1435391797489926705LL,
  141756898993607723LL,
  -1206034853031754051LL,
  -189331869524733785LL,
  -3784323839048717687LL,
  -2781122720892879725LL,
  -6398746625182982443LL,
  -5391086094024374065LL,
  -9004039341906513183LL,
  -7991866277585410821LL,
  4933841668239953366LL,
  6225259745136778188LL,
  6935731371391230434LL,
  8240669180703193080LL,
  867187788733666750LL,
  2167577949234709412LL,
  2896081222591510922LL,
  4191976716994015120LL,
  -3127690516618658554LL,
  -4142150496405542116LL,
  -1107856646258945742LL,
  -2108797168508598488LL,
  -7140335835200199314LL,
  -8145753362638670988LL,
  -5129527031161271974LL,
  -6139439499576189120LL,
  -1923040511551997990LL,
  -624897752817606208LL,
  -4505903225421308946LL,
  -3212255132785455628LL,
  -5962866238614818894LL,
  -4673695563484644952LL,
  -8572733232946510970LL,
  -7270042825401199204LL,
  8480080218401359626LL,
  7472415289196237072LL,
  5879273612201519934LL,
  4867113742019951908LL,
  4458304623980126050LL,
  3441597242426591608LL,
  1866522534345257814LL,
  863334610328954188LL,
  -2408150999192925087LL,
  -3708554353833502085LL,
  -383743469700341675LL,
  -1679634566056331697LL,
  -7573683462374501367LL,
  -8865114733410860525LL,
  -5558300449446889411LL,
  -6863233860712337881LL,
  5653204714049387697LL,
  6658609047348325035LL,
  7659527338845147269LL,
  8669444205306578591LL,
  433594420710499545LL,
  1448041206357848771LL,
  2466921325968640237LL,
  3467866246264807159LL,
  -7790652608818183753LL,
  -9079788322914560083LL,
  -5198800425317028477LL,
  -6501455596950635623LL,
  -2624944567373506081LL,
  -3923122596379340859LL,
  -24067317551565333LL,
  -1317750405580954639LL,
  216940696665108839LL,
  1233612945386153853LL,
  2826737047374699859LL,
  3829889563680607049LL,
  5436656199522904335LL,
  6444356502078684949LL,
  8019448819525928251LL,
  9031643788180247329LL,
  -6324899949851477492LL,
  -5033503639848915946LL,
  -8358296948975905224LL,
  -7053398773622161374LL,
  -2285249801051757980LL,
  -984811176139737986LL,
  -4291643069469618608LL,
  -2995716977720092598LL,
  4096518440298728156LL,
  3082106787483868358LL,
  2081206070993217256LL,
  1080296558407446770LL,
  8118188825201054388LL,
  7112749118551458990LL,
  6093851389574758016LL,
  5083899424640575642LL,
  1011916873580675072LL,
  1305224872210078226LL,
  3902150468518556708LL,
  3040249977549499958LL,
  9102730603578026056LL,
  7081205985386066522LL,
  5079868400146340972LL,
  6514821311857051262LL,
  -1244157219068391280LL,
  -959891741506876798LL,
  -2983659226517412684LL,
  -3854531869810164058LL,
  -6997549484319655720LL,
  -9010102225065827638LL,
  -6426775251135984388LL,
  -4982780093235292434LL,
  -3066751016692368581LL,
  -3934267967676767959LL,
  -1343055554048192737LL,
  -1059867615062551283LL,
  -6545737439944387725LL,
  -5098421774107424415LL,
  -7132318217932904617LL,
  -9145983681627030203LL,
  3873208526512548779LL,
  3014734096596038073LL,
  1003267552541368207LL,
  1295427643850704285LL,
  5014774584530109411LL,
  6453118372921285105LL,
  9057929408928495559LL,
  7035221699044388309LL,
  -6630036974923780746LL,
  -4617462381384009884LL,
  -7218895975365322414LL,
  -8662869280472415424LL,
  -3196139324514106050LL,
  -3480418133654102228LL,
  -1474581346834566886LL,
  -603722035120297208LL,
  4849726915588046310LL,
  6871273661451511796LL,
  8890533148789213634LL,
  7455602364750009296LL,
  3681086483241817518LL,
  3387765427911839676LL,
  808937655561806218LL,
  1670825089830288280LL,
  8684870018523846221LL,
  7246504377339071583LL,
  4659799961383330409LL,
  6682485818473838715LL,
  639145061368839685LL,
  1497632822863832087LL,
  3527030065109502497LL,
  3234883305378648115LL,
  -7478777123520575779LL,
  -8926114917029045041LL,
  -6910140375301541127LL,
  -4896497039278921493LL,
  -1698310483940643179LL,
  -830780476255669113LL,
  -3440090713842689359LL,
  -3723265596127756125LL,
  5763719002781637356LL,
  6046871912014004478LL,
  8633650905781890760LL,
  7766169293788411098LL,
  4586001071153853092LL,
  2572300577706453174LL,
  542837175784539776LL,
  1990188180612422802LL,
  -5815264817043797380LL,
  -5523069661621448594LL,
  -7538969350658900392LL,
  -8397479085206592438LL,
  -2372148792425225676LL,
  -4394821438196320218LL,
  -1785577085407635952LL,
  -347268601647641598LL,
  -7744101330908847657LL,
  -8605966792124630075LL,
  -6004607844674939405LL,
  -5711335185036455967LL,
  -1954838667024135777LL,
  -519850725560151155LL,
  -2525621421422920261LL,
  -4547181378605799511LL,
  8374369077362995527LL,
  7503461369957231445LL,
  5484126961210000739LL,
  5768427743402696561LL,
  319707220975868175LL,
  1763667314763547421LL,
  4342560903192440107LL,
  2330043467077449529LL,
  -2158707926888237158LL,
  -136057408788648568LL,
  -2727335604136344642LL,
  -4165666215567844948LL,
  -7903022555418992686LL,
  -8195204654140766784LL,
  -6161233254546019338LL,
  -5302710463297752604LL,
  196586095074436874LL,
  2210264735728237848LL,
  4221665223185879854LL,
  2774292365564397884LL,
  8278181588301844290LL,
  7995042010647958864LL,
  5390305655532108646LL,
  6257800599104070004LL,
  4139091354527075489LL,
  2695153388410902195LL,
  98293642541021317LL,
  2110833206327517847LL,
  5271861250800903401LL,
  6142755901506092795LL,
  8144018599696027853LL,
  7859704760802757343LL,
  -2755691454905263055LL,
  -4190657543575648733LL,
  -2166823933248608235LL,
  -145242123272130041LL,
  -6225741116364114823LL,
  -5363888986726814101LL,
  -7947290572828540835LL,
  -8240576564045505969LL
}; // weak
__int64 CC_CAST_S_table0[128] =
{
  -6944270201873809196LL,
  4550197455952334127LL,
  -7205673834653860049LL,
  -3485846269580810944LL,
  -8594066875601342681LL,
  -7435277551741484912LL,
  1568204651678114016LL,
  2509911880972592669LL,
  -4575709615036548241LL,
  8601682749174282696LL,
  -2364523574284042029LL,
  -6714096091194940390LL,
  3777473462382747862LL,
  2473618791510535559LL,
  -6173284539683687711LL,
  -6713950257294176710LL,
  -6375908296697888568LL,
  3294896453327912751LL,
  5374977816290148268LL,
  -5335578466744646473LL,
  -2931331699169248529LL,
  6487851013645005037LL,
  -8003752857637787440LL,
  -4859848863136038828LL,
  5817353991892053887LL,
  -4691201403033073392LL,
  -1640693863528848752LL,
  5476153248971678737LL,
  -5966821730201255360LL,
  6195969461451149358LL,
  -6797007508676162899LL,
  -4495986262530541715LL,
  895740480114016498LL,
  5718342765153402839LL,
  -4207396583583769297LL,
  -5744787731913252012LL,
  -4101837126411834486LL,
  1229363252606387525LL,
  7654154072564665045LL,
  3050334100159018976LL,
  7095034963199294815LL,
  8809639461159107807LL,
  -712949104254528299LL,
  -6174292806598514850LL,
  -4128386975590772591LL,
  3087035092284459916LL,
  -6714423479392136153LL,
  -6172514718334155089LL,
  4781825606671745888LL,
  1761334930579889702LL,
  8350442355484046125LL,
  6080483442431015996LL,
  4553855565369286315LL,
  6458952029960720258LL,
  -8503226117108400082LL,
  3629891138836171088LL,
  7594130861628866309LL,
  -3127340057601811754LL,
  4037339445691157967LL,
  -8447885309903217418LL,
  5177611045059403283LL,
  -6349440373683679098LL,
  369001141103911489LL,
  5368393618855246596LL,
  14120187439808720LL,
  -5019360127654507041LL,
  5814097922060859179LL,
  -7973047040596692363LL,
  3101594923616747435LL,
  4944052452049340887LL,
  -6087241669693148571LL,
  -6137030647982204012LL,
  3308637070798667071LL,
  -3981985072208130814LL,
  -3045103393419864653LL,
  3086563802810036658LL,
  -5927224018967778850LL,
  -6173932544255258775LL,
  2472827071344700406LL,
  3085885815500416923LL,
  -3207412112077209104LL,
  4750423018843837701LL,
  225858194096992998LL,
  -3586978093205550388LL,
  7669906661438108012LL,
  -3107701591998473783LL,
  5131557539528336346LL,
  -5528225071433045800LL,
  -7471159540005007421LL,
  -5364966475566429131LL,
  -4628013444495373636LL,
  -6036698122249638896LL,
  7028062189609305135LL,
  6280211929704790574LL,
  2066262232767611279LL,
  105214042201881750LL,
  -4081556552482423031LL,
  -1437079865854836520LL,
  -5684613154655114540LL,
  9087231322562546590LL,
  -3304500069183437575LL,
  -6652881708797021187LL,
  5893064900296320943LL,
  -6504581157767580266LL,
  -3889934763065932560LL,
  -8232673736216922134LL,
  3830029596686433866LL,
  -4886283714130391799LL,
  6217491026431780454LL,
  931510447224280733LL,
  8073623591748612815LL,
  -8815470321920749327LL,
  -3160602864672326233LL,
  3259283940229712883LL,
  923499501126173080LL,
  -540008316248060488LL,
  8938623110849850071LL,
  4042951091164485977LL,
  -6087874576110127909LL,
  -8975220378105340381LL,
  -7325591401942359994LL,
  2362355097372532748LL,
  -2168880146052031972LL,
  8992151036634442504LL,
  201615536367986563LL,
  4800543073265763705LL,
  6542024131170997916LL,
  6665720798215343872LL
}; // weak
__int64 CC_CAST_S_table1[128] =
{
  -1221698864094900076LL,
  4125090002506076030LL,
  -1241550409644781702LL,
  8285504360852003988LL,
  5628780684155285369LL,
  -2448772121311419442LL,
  6848857468140007663LL,
  -6784971961213854851LL,
  6478487461771423611LL,
  -1585309907138793324LL,
  -1183245649353375610LL,
  -2250728907705430861LL,
  4253978266246463873LL,
  -6492273681432579647LL,
  -1952328739299652133LL,
  -2197483912871936191LL,
  1701215492548857984LL,
  -119291905644509527LL,
  8640225491915017087LL,
  2664617482212512361LL,
  -5997668237333772197LL,
  -582546137528912603LL,
  960756985732804948LL,
  -4006300029912870623LL,
  -3539208356891472013LL,
  6658337998395440775LL,
  4176915477129290113LL,
  -1951370101712342908LL,
  -3009867329431444316LL,
  -7025330451447527883LL,
  -7384642574813629172LL,
  -5013364499849945223LL,
  2681798324591538324LL,
  -6773630306110967810LL,
  -8122931922502616214LL,
  5676569853991887399LL,
  4489500452270395241LL,
  3844214317701267592LL,
  2125773131804391564LL,
  -6519377469963920387LL,
  -8913043594803846349LL,
  -84653925351980904LL,
  2873749173669726945LL,
  -1975016737268645294LL,
  -1484375449053800995LL,
  -6794058263588238515LL,
  -2254782700603103575LL,
  -1886911073958739017LL,
  463858058530926724LL,
  -2676185645430693549LL,
  7518227305641172415LL,
  3273785132200913812LL,
  -6425562742072081957LL,
  -5802985934685192868LL,
  4142125104613335884LL,
  6849503361513958293LL,
  2370937827789467489LL,
  -2443093277451349128LL,
  -5087054572718087902LL,
  2501708341995743208LL,
  5941041993702780197LL,
  -4175530317948218084LL,
  1991996603217261570LL,
  2064924410714824555LL,
  -8614858993169584123LL,
  920645179526754251LL,
  -2080305587581681837LL,
  -6961675258284464075LL,
  7934548133093173033LL,
  2190585548828332678LL,
  3504412482589688262LL,
  6958922424329373776LL,
  -6834607874522905577LL,
  5965149278151400516LL,
  -6905400032246156378LL,
  2413933177639459234LL,
  -3801239977671943160LL,
  7251911042558318437LL,
  2713241185038902902LL,
  2361875113447969276LL,
  1694564279051662976LL,
  9081549211101493325LL,
  -8905239693909866005LL,
  -1270752907755030392LL,
  3449014987366291059LL,
  3528129694347440577LL,
  1549194268600101972LL,
  -3363381498135348788LL,
  -8128207767003464354LL,
  211738418171541882LL,
  -4448718536987010347LL,
  -4139290714571543632LL,
  -9186727338424261876LL,
  -3215656506324225080LL,
  -4701540994723996728LL,
  -4852358885792298520LL,
  2486601182268616698LL,
  -2821137339059441484LL,
  3467233942554031292LL,
  -5692573404629309063LL,
  50182163304921286LL,
  638429484134944319LL,
  -585019796352207270LL,
  -8134455696279619793LL,
  1632506150807549856LL,
  -6407655886541319241LL,
  1896357678041318974LL,
  5909679109854210236LL,
  3830290915242178475LL,
  674535190229406230LL,
  6839324939654460155LL,
  -7973535733038673163LL,
  2767366091587152251LL,
  -8337380161890983864LL,
  946847708930924756LL,
  785546397812817546LL,
  -2867259016365187060LL,
  -6031350234230962617LL,
  2526145214456627635LL,
  -1101778844840838009LL,
  -281397725023526221LL,
  5742240969907202938LL,
  -6681862598463478391LL,
  4493813670484345635LL,
  9110166853373105522LL,
  7812835984472530718LL,
  8340594425061221689LL,
  4982086134839866885LL
}; // weak
__int64 CC_CAST_S_table2[128] =
{
  2736602663443481152LL,
  -1724657621428847169LL,
  3936115611316289535LL,
  -5850516143732701628LL,
  -1226163363957835329LL,
  5899442484690229584LL,
  -1104220524389693434LL,
  -7894791635987559037LL,
  532688889745276319LL,
  4417794146667062484LL,
  -369714217698220000LL,
  -9047035460267334005LL,
  5231742592885961408LL,
  1323565420752377748LL,
  -9053611851887149842LL,
  1321703413873231341LL,
  -4980840453368020222LL,
  -8190055345603590425LL,
  -8313783965123872833LL,
  -3926376202037866776LL,
  6313978940180405687LL,
  -955537866949570767LL,
  2285451450475608001LL,
  4862732678948704961LL,
  -5342458504941650497LL,
  6161379047345340646LL,
  5332593386964109340LL,
  -3691984735077643736LL,
  7784265186390819347LL,
  1147816882339416380LL,
  -2900177980087080880LL,
  6556003024577265599LL,
  1608094230113669840LL,
  6395977339216379710LL,
  8249683865620721710LL,
  1116874574404760578LL,
  6713788073673948589LL,
  5826119394210191945LL,
  2210237795336189088LL,
  -9055933915898858423LL,
  -6452996918640542309LL,
  5041011244306809048LL,
  -1711716377911528445LL,
  -7902012212895451878LL,
  -6267901075895883455LL,
  177580707276066290LL,
  -7892129869939776140LL,
  177753056299375563LL,
  5641030795430695309LL,
  -2117391051724329505LL,
  3750625239066705283LL,
  -7803417329001580926LL,
  -7471560666437894997LL,
  -7864316233454211173LL,
  -8919473035316224480LL,
  2834685959422467222LL,
  -5395362653905322770LL,
  -1916645983928681245LL,
  4393297298468725086LL,
  -3630847698378894877LL,
  66032525272040974LL,
  -196092208706324351LL,
  -4780532946892427815LL,
  4866710379226312106LL,
  584301962956619920LL,
  -5100104383977193794LL,
  4450331555724803327LL,
  7221537745911067372LL,
  -2805980576001262597LL,
  6766095301498962305LL,
  8548643621945931882LL,
  -7899993726810586264LL,
  -5027795191969742854LL,
  -362659360599452917LL,
  1818536133312218659LL,
  -9053092345437588155LL,
  -3378189203425883232LL,
  1332682390676243700LL,
  -9046982394069742153LL,
  1332472902635799720LL,
  4351308810810008008LL,
  -1727743773229651291LL,
  -5405199702196409861LL,
  1044203286543362643LL,
  1190666287610077099LL,
  -432582466084206483LL,
  2551954043019447313LL,
  3188421839254637133LL,
  -509090828064379394LL,
  -7704151085768813862LL,
  -5064675960208865140LL,
  -8515344524777651524LL,
  -1440930152092494793LL,
  758657840288191059LL,
  8949891733063449664LL,
  5758790238980898550LL,
  -3260781207531540481LL,
  5815536273833526208LL,
  30989219954920657LL,
  -5051116025397559467LL,
  -7541497256595914730LL,
  -2445578576873882265LL,
  -6067387515584087245LL,
  8863267343498265949LL,
  6269140228594401281LL,
  -8556274559744243464LL,
  -8835214881760958977LL,
  -1254503831486867157LL,
  6847914734979788766LL,
  -1888829689309798035LL,
  7057079880137042898LL,
  -3710339633558837903LL,
  -5627947223927927624LL,
  9156026737989795426LL,
  1148492937941376056LL,
  6941237012509650374LL,
  3164212071825391944LL,
  2369383943128749880LL,
  7513556850732096015LL,
  6921264608087063525LL,
  -8992365321789664432LL,
  3129313038069207328LL,
  -7192149647183634895LL,
  1946496815071028872LL,
  4702582219355123669LL,
  -8993056487321281519LL,
  -6830899597948271050LL,
  -1282057479213853924LL
}; // weak
__int64 CC_CAST_S_table3[128] =
{
  2285271003274871840LL,
  -3282100926889952273LL,
  7254608881300503515LL,
  -431766321226513341LL,
  -1869074781900866817LL,
  8785380618014766497LL,
  7248464100358200643LL,
  2602599960388312931LL,
  5738352066503671647LL,
  919616273124622400LL,
  -4565181830569718992LL,
  2599838215513637081LL,
  1137811362685980954LL,
  3249734152563429452LL,
  870319520324346975LL,
  366763136035522762LL,
  -1711826361186582272LL,
  9216378050581296376LL,
  495187471948639839LL,
  8256199989682674570LL,
  -760677320707735585LL,
  -6742128081267963132LL,
  6088563121454351360LL,
  7850854924340251808LL,
  -5863466859113108513LL,
  5782479617301646529LL,
  1276270086972711009LL,
  -572323584482996733LL,
  -1401332110792306642LL,
  -2211435340200731007LL,
  7575109066862842880LL,
  -17164866727068775LL,
  9168586547718854661LL,
  6727195497427658438LL,
  -6948368602845384726LL,
  4148174237141967407LL,
  1244053768446635406LL,
  -3449389067230936318LL,
  4590883310005158840LL,
  2888593956289418014LL,
  -8370069168161526210LL,
  -5402383877849750335LL,
  8721654998165686821LL,
  2887435336448392065LL,
  1546082769709215032LL,
  2385853061072886626LL,
  5690503724703481LL,
  653373802643622600LL,
  -2030746480962575150LL,
  3427700288019527381LL,
  66116551895148766LL,
  5599306558583718636LL,
  -2507544376178721281LL,
  456872541573066805LL,
  6253463112720042160LL,
  6779150100903330076LL,
  -4753619764989539467LL,
  -8556851469273115273LL,
  -3392271387677701843LL,
  -8255877513016763721LL,
  -4366620286388837393LL,
  778614785258825856LL,
  -1395597156944358262LL,
  2095970815343000114LL,
  -7244749922631577556LL,
  -7987503163759101961LL,
  -4104033501734735909LL,
  6261510336321841736LL,
  5927721168271985899LL,
  1006977438649906868LL,
  -1307727332178080285LL,
  3177558776121769476LL,
  -6112801188657987683LL,
  3317300784877751584LL,
  -1894150443378826668LL,
  3178551940224991990LL,
  7301138710542873871LL,
  2671495951142760952LL,
  295252720025501602LL,
  942702212886992550LL,
  6776354948934360097LL,
  1763450458430177021LL,
  3535731047390887302LL,
  -3729524924761807860LL,
  913882766872597430LL,
  7814461135409644001LL,
  2277400217272378017LL,
  -4476995741754339025LL,
  -3799734233622659477LL,
  88252041152630614LL,
  -5422842141786951734LL,
  1343098408087857158LL,
  6412420766465123634LL,
  2643049768516774144LL,
  -7561074532189401147LL,
  -9014544189981099115LL,
  27356493822587894LL,
  -5935425451522681567LL,
  9204116835875718165LL,
  3011198593464710107LL,
  -6399347698631007131LL,
  -8476688666114506776LL,
  928960511538506983LL,
  98825315581349254LL,
  -2404857370782494461LL,
  -2385967859774237090LL,
  -8397555760632931138LL,
  -5141487965292919215LL,
  -1935493992889272622LL,
  3416525536650151209LL,
  -832949659329201413LL,
  -2968830417678832805LL,
  2295293436634613260LL,
  -3890983191400971473LL,
  2984524939489527032LL,
  -7897415253671034219LL,
  -7218181846328709527LL,
  -2655593135954066129LL,
  -697172946592050706LL,
  222589550948311916LL,
  -5198181117751713474LL,
  -3686480778714863329LL,
  -7841947868590884487LL,
  5683492713611641677LL,
  4375620234055592204LL,
  -7479540554244677378LL,
  -3170618310103600976LL,
  787987901287089601LL
}; // weak
__int64 CC_CAST_S_table4[128] =
{
  3201624725151878148LL,
  -6470695123078453537LL,
  2149159259570077695LL,
  1671141672603196740LL,
  -1732458913632357894LL,
  8312160574951987968LL,
  4063233998413308632LL,
  7619806322887107642LL,
  -1096871915531606145LL,
  -3677820761885030378LL,
  1562934098836919872LL,
  5524473083589236939LL,
  293493370545244673LL,
  4742554162193503484LL,
  2634244022524228749LL,
  4731483318799441150LL,
  -5003668737990784508LL,
  -7663693741301828377LL,
  -7519436830869623593LL,
  5582772947631323562LL,
  7543388917311207267LL,
  1275809022662712918LL,
  -8716181956419954040LL,
  5632312805838319958LL,
  203154566108603693LL,
  -7820512358925824507LL,
  6306506166902273834LL,
  -3007664815404368382LL,
  8409728840428135040LL,
  -3160992659915302464LL,
  -3986268740689117976LL,
  4437621787855454153LL,
  -128119933751002880LL,
  442487365199396619LL,
  5744738758070063025LL,
  -5717135607859800827LL,
  -1901465984534470145LL,
  -1843588101688644313LL,
  1064267224382242847LL,
  3016855766282622396LL,
  1706733921835578266LL,
  -8504884721039732452LL,
  5928801941135157632LL,
  921061655154408649LL,
  -8670494805069664086LL,
  122679297910141990LL,
  7272800467885651017LL,
  128714400878801370LL,
  947713042654216064LL,
  334887621971078631LL,
  5833769099601590198LL,
  -8157647568406878493LL,
  -76135059569113637LL,
  -2961125735847658953LL,
  3486908574577950153LL,
  -1429538999085200930LL,
  -5235887457582160235LL,
  -3418978994969317435LL,
  -4487050331182443519LL,
  -5329957065043520241LL,
  -8658400792975912728LL,
  2466396783763943137LL,
  -7059245108085132339LL,
  606861215482461767LL,
  -3347465493096306656LL,
  -431567148851767433LL,
  -3768433922522817853LL,
  -5704074210619226035LL,
  -2787170665227913003LL,
  4694083303292525142LL,
  1105446592175541078LL,
  7026941150026130798LL,
  -7712612547365821281LL,
  -6954871320632391538LL,
  389692303131542144LL,
  4940534224104745943LL,
  -4543740003448721245LL,
  5325840769720702383LL,
  3221821313628649295LL,
  5301291192393172339LL,
  593323195334962690LL,
  4920345347068402842LL,
  -4537049998966240072LL,
  9045089475751403462LL,
  3348164145585283439LL,
  -6156222285891294239LL,
  9013422190923994903LL,
  -5816913891289100272LL,
  5031441459939924434LL,
  -2610305991560734189LL,
  1688085081004243278LL,
  7142046858355676138LL,
  1681676596011824002LL,
  8123933564414093975LL,
  7952744461141659506LL,
  5015744056711667833LL,
  -6214350652259114277LL,
  2471986856575971676LL,
  -688457916765956750LL,
  -8348195630059683853LL,
  4640880239127222063LL,
  6675386296361489294LL,
  8471871798765409902LL,
  6407409982887521047LL,
  4559927089511878533LL,
  8626081547813826171LL,
  -6128144178365660481LL,
  3101053804779843626LL,
  912013242358363639LL,
  7410083575114975882LL,
  -7708666054268221324LL,
  -711896751612962904LL,
  2338917987252626553LL,
  -8577177676634195355LL,
  1150380478642284582LL,
  6000672259895492464LL,
  -2200310082029655000LL,
  3992487053390262399LL,
  1721498941162655657LL,
  -744946477553969305LL,
  7565799708297078165LL,
  -1036101974189849556LL,
  -1244883428896119398LL,
  -4958240297239640296LL,
  -8622412499437552107LL,
  -2710465120222420407LL,
  -5669111337394438232LL,
  -1159140529518682616LL
}; // weak
__int64 CC_CAST_S_table5[128] =
{
  3219067551430119325LL,
  -2147232425487153049LL,
  101405434503170279LL,
  3626897077091261628LL,
  -2332333621251208864LL,
  7200559510575929245LL,
  -2423290397008157000LL,
  -8536096760541481166LL,
  -4604587858249823903LL,
  -1989924185974585638LL,
  5630593808716065433LL,
  5233538353685321676LL,
  -87455336820861415LL,
  -1540204243492530358LL,
  695885786700372781LL,
  2954554425752445258LL,
  -2190675338048677487LL,
  3348689367298411616LL,
  -3526340409060681422LL,
  -3411441039577577518LL,
  -7852687096157234818LL,
  8412394035295806426LL,
  5512199545878848828LL,
  7906648330153088064LL,
  -6936969660097554009LL,
  5649497809420537776LL,
  7259161864762661889LL,
  -5008607735335459990LL,
  -1798124398550486136LL,
  6650934348284599003LL,
  -5151744264566960756LL,
  1937368934581901414LL,
  -6155714653150466167LL,
  4267760703579143763LL,
  -7283946749012259869LL,
  -535702019740482706LL,
  -4377317815667757650LL,
  -6443340415092483063LL,
  2458318498694979276LL,
  7530452147912679471LL,
  6034968888787216751LL,
  2720146184605349461LL,
  -3521200557750318404LL,
  -6221279201573703864LL,
  -1769919157572347035LL,
  840261508715249016LL,
  -1684034095732741183LL,
  -4032834943791893491LL,
  -6840684569912098919LL,
  -2712813217317160587LL,
  -7698677570801264305LL,
  -5154311774334115859LL,
  2460520760415110509LL,
  3297011779640805435LL,
  6338076252127268266LL,
  -3273713683709949250LL,
  2662405560452209557LL,
  -3671137794944574055LL,
  -6278991014993713153LL,
  -856208192449429242LL,
  -1807481503243426938LL,
  1548894065405440148LL,
  7576589372136060079LL,
  -930553481651431352LL,
  -6374878016394039523LL,
  8373960082549241462LL,
  5005304090726830179LL,
  6425221943460025351LL,
  5547148893058708771LL,
  7788181340882783936LL,
  -4247459194749086040LL,
  658022730957260715LL,
  6909294203258269883LL,
  8450463505966911283LL,
  5240454409351450765LL,
  -4032534091759578047LL,
  -4352109518773164154LL,
  7640663880582103045LL,
  -8574727426813998938LL,
  -6221169014146566526LL,
  -3144819353511106659LL,
  -3769262918003852032LL,
  2274232138013286922LL,
  -3730024942357262584LL,
  440376116053946726LL,
  6945363815333203848LL,
  -5275876239665557894LL,
  4593787576464354555LL,
  7266612206862533202LL,
  2190531013652279523LL,
  5850212911746189552LL,
  -9065875103256188745LL,
  8390662330908134718LL,
  518829665054828161LL,
  891363362812932057LL,
  -751300068253292436LL,
  -7674429512047054602LL,
  -4438603716689657923LL,
  -5659919995419448168LL,
  -7695357798038588374LL,
  3263938986109939218LL,
  -6560622145647019349LL,
  2701891761379643683LL,
  3896740387507187529LL,
  4041120851372896074LL,
  -7171326261682683837LL,
  -2719637640160719017LL,
  6121694285560206138LL,
  6428181201604793605LL,
  9194034506518828488LL,
  -5262774268208691275LL,
  5981583108527856698LL,
  6063689294671609450LL,
  5771316035687053290LL,
  4905383676819430792LL,
  -5123511837890664248LL,
  6984335808205066890LL,
  -1518670630541239402LL,
  143473009558247699LL,
  -7298374613836329478LL,
  5358689456214556575LL,
  -6853959294438269643LL,
  -773231638880760591LL,
  -4703007340978686684LL,
  5316832875643364047LL,
  8153947806989939505LL,
  -6459536563209950971LL,
  -2993258574314170435LL
}; // weak
__int64 CC_CAST_S_table6[128] =
{
  3687310544909385753LL,
  -3474994871395303425LL,
  -6081045791973212305LL,
  2317341838691797153LL,
  5576627413093694695LL,
  5832596383302599424LL,
  -5606164203049923823LL,
  -5582484561776437774LL,
  -3656498582030140170LL,
  -2139506975123169012LL,
  -151741908850785407LL,
  2201131308656734424LL,
  4088779459447574529LL,
  -7879157226227831267LL,
  -4976849274570053954LL,
  -5266173505710932273LL,
  8459376262679413248LL,
  -9058748030368080006LL,
  2887757917999836518LL,
  3849765082378067880LL,
  -5498075123817150018LL,
  846735132995005071LL,
  -1507861991028689622LL,
  -3220789417337132783LL,
  6864443624021244671LL,
  5296704318038532151LL,
  -8317231831728363826LL,
  -3802607600485772096LL,
  5126708497420658729LL,
  8851296334144786987LL,
  4915662530872654586LL,
  4815148543681993058LL,
  -2187130465161964920LL,
  8285506322543451751LL,
  -479572587839407625LL,
  -4700641633830838440LL,
  513721677158201904LL,
  -3936654601574250455LL,
  4088220531450365201LL,
  -4978632309870233068LL,
  5084371065162873926LL,
  -1070704167709228193LL,
  -5767404729971918177LL,
  1649322850670890605LL,
  43716322782366347LL,
  -7293175557483229971LL,
  -5582934744218880102LL,
  -4718780065428783959LL,
  6192940239423014121LL,
  -5502850784533596613LL,
  7061084826155006853LL,
  4324383802264937011LL,
  8767036695941811316LL,
  5904439023419842989LL,
  1596144123322626906LL,
  2105485481806932777LL,
  -4957804066667251302LL,
  7960014112233544059LL,
  -3147399554909500580LL,
  6971641179604598923LL,
  -6685781711755244168LL,
  -3988198041605584771LL,
  -1453281973393060814LL,
  8753303357156465965LL,
  -8397577816276577528LL,
  -2268749506681008732LL,
  6513075490212509807LL,
  6762718013693949856LL,
  -740346186169401424LL,
  628663892052722304LL,
  8778432767975871152LL,
  -7294086016085102195LL,
  4863976534103424352LL,
  4431521984535922856LL,
  5686784599014583279LL,
  3947921183647584037LL,
  -4440255894224679359LL,
  -4979932394484553265LL,
  -7877165851133112858LL,
  -7014350882430528026LL,
  -6622754972693233283LL,
  1576497337736238787LL,
  3143318842528348295LL,
  7638519906460940930LL,
  1533628132358101916LL,
  4613192541944604480LL,
  -2076793182522009054LL,
  3770599334539118009LL,
  5290884518060304750LL,
  -2314026643714258554LL,
  1864066584661869265LL,
  1562027292941506074LL,
  8166365657720581148LL,
  -7788085801890862294LL,
  6401007799938887920LL,
  5967082918368154875LL,
  3369824475347680870LL,
  132408887326888899LL,
  -6780426461047111416LL,
  7583730295052099494LL,
  5488496915187254936LL,
  6717456911021995606LL,
  4409618173611090767LL,
  5415177237751521684LL,
  4366660396692472367LL,
  4610273959316788484LL,
  2983234481356778766LL,
  -4170819521526903185LL,
  -636140354416563404LL,
  5652779936742913216LL,
  -5394587769696509123LL,
  6966707330084939903LL,
  3511896476606543979LL,
  -209972695224237955LL,
  6385492465388048795LL,
  2069639630625044710LL,
  -1551672639548989015LL,
  6240397880517415837LL,
  5057009637691122605LL,
  -900305321988130690LL,
  4657338787336705524LL,
  -5268339845354713291LL,
  4413791660421042364LL,
  5285187797676768686LL,
  -8885088110467139918LL,
  -8679040276768108925LL,
  -7719139073995933241LL,
  -7688899503697334453LL
}; // weak
__int64 CC_CAST_S_table7[128] =
{
  -4909486557174026227LL,
  3847341361936980669LL,
  -1819152865302021961LL,
  372928733488879104LL,
  -1903563517136875600LL,
  -7816329803133904630LL,
  8566832826912248604LL,
  4025617719313166138LL,
  724168237580476081LL,
  -8890095490757464023LL,
  -5417487511412524529LL,
  332965096379546096LL,
  5191860268360376669LL,
  7394327033280192090LL,
  4580044168783259697LL,
  8466823432311347483LL,
  6664709499687223889LL,
  -6178901972490490525LL,
  209366576212857866LL,
  8804962380180561326LL,
  -168265561234940436LL,
  8213236222180410344LL,
  -3561051169078734814LL,
  -7612252826629609043LL,
  7420691281857466453LL,
  2115363202865302350LL,
  -4427385087813885841LL,
  7240232376153436782LL,
  6956888879316491588LL,
  4169312867018870275LL,
  -5880663232937787229LL,
  8075108117165889074LL,
  2997390763439116361LL,
  -7246867319210838998LL,
  473305982793866579LL,
  4284312127873449479LL,
  -3294654928753000527LL,
  -6563115356758595635LL,
  1852239728715354850LL,
  3856594454954604056LL,
  2876452051600531175LL,
  3943734597722830459LL,
  -4501048000020977715LL,
  490077590684286843LL,
  3269106817291044360LL,
  7261720176941222234LL,
  4454075097086477868LL,
  8634837832723042100LL,
  2646096499503489551LL,
  5090516065604188062LL,
  4483194207562360142LL,
  -8819150169861403227LL,
  -4360672599479052408LL,
  465739569988876812LL,
  3568027912733104926LL,
  -522711735658504233LL,
  7923170891781897400LL,
  -213093064691568235LL,
  -2139089711271111860LL,
  -1002629099956792598LL,
  -6938705447073123473LL,
  3803030366844897657LL,
  -1550805519266905721LL,
  1456500429521252092LL,
  -4693051987957890809LL,
  -6896519278826508505LL,
  -7859701751640242023LL,
  1519545873827561837LL,
  -163712107412263201LL,
  2369741195939183477LL,
  -2217627837784341858LL,
  8180560389497831352LL,
  -6975402635143491781LL,
  -5757213286750378804LL,
  -8914281838006731379LL,
  4813180804482457564LL,
  6334413458888306186LL,
  2364193596661258998LL,
  8777850633121485864LL,
  3734554861678029819LL,
  4030718047844383571LL,
  -1909328438275700872LL,
  -4194905697832879884LL,
  4147792699866521250LL,
  4879532608962842516LL,
  2465179549305830684LL,
  4026098408493599394LL,
  -5984180253803380087LL,
  -1210209507811658057LL,
  -4519138041886449297LL,
  6791726942586569054LL,
  3376151216047924325LL,
  -1617178723482910492LL,
  -1165428465067234847LL,
  -2645769583716740261LL,
  6301099933405013426LL,
  6585442514727001788LL,
  5781548500039859791LL,
  8440821343576855170LL,
  -449943729395493227LL,
  -3348504841489276639LL,
  -5021217956933780915LL,
  -5975885358805255009LL,
  1873623265692047717LL,
  61481903080878226LL,
  -5971977600928785055LL,
  -6994719810355732560LL,
  -3537627491543030104LL,
  6153226307093608096LL,
  -4040978773971750391LL,
  -8551958494693620278LL,
  -4798274873085820373LL,
  -8699516251592792846LL,
  4240781275108385402LL,
  970275217021695567LL,
  3822856263407874390LL,
  4772020048186898959LL,
  1045971171970480903LL,
  -8979939632931586463LL,
  1245588534105574156LL,
  438205651587835301LL,
  898049516910224459LL,
  945854677417627010LL,
  -515087651160278242LL,
  -5012982748593155792LL,
  2226787082068971876LL,
  -1518161669519135360LL,
  -1545871985740701053LL
}; // weak
__int64 qword_579A0[2] = { 0LL, 0LL }; // weak
__int64 qword_579B0[2] = { 289644378169868803LL, 868365760874482187LL }; // weak
__int64 TD0[128] =
{
  9097664130796791632LL,
  4190421977067398339LL,
  2278053888909863883LL,
  5468218305355077803LL,
  -5947445354026698155LL,
  -791986860371183983LL,
  -4239351973597030404LL,
  -5376555169910274944LL,
  2718515458003262025LL,
  6773062963384946328LL,
  -9129658381212093182LL,
  7769828413313292195LL,
  1554476985220030439LL,
  -7686982817918649621LL,
  6373756566327165741LL,
  -8157769054382167767LL,
  -824588123483502230LL,
  2862444234745069163LL,
  -1114450590275907658LL,
  9065247648086273126LL,
  -1938182253538489832LL,
  7085146575652074336LL,
  -4941664281173919776LL,
  -502103439526748132LL,
  -8122808067193018280LL,
  5943617600605809799LL,
  8235679516139442979LL,
  7373988165576527703LL,
  3437867211709687815LL,
  -3227101100963955814LL,
  2575959696609937394LL,
  -1362758505550615878LL,
  -6378868855706543061LL,
  5650296392707994352LL,
  433962762857280717LL,
  -4276450923972894177LL,
  -6704921273665956963LL,
  -6559796919381860046LL,
  4638971129808284729LL,
  -4796878611440099578LL,
  -7575892138062148871LL,
  5613382097621091758LL,
  8197779500215602613LL,
  6940071210806334575LL,
  -2972963343815869660LL,
  7483186492093776844LL,
  543118649274679997LL,
  8775526502173858616LL,
  8953736505277024839LL,
  4169408201LL,
  3615244019547801219LL,
  7807678583837651116LL,
  1118362028735397883LL,
  3903839292701463838LL,
  7520068138634828132LL,
  2609323863357215953LL,
  -7829535375347783759LL,
  1989343771240601298LL,
  7051546998872261967LL,
  2022707865952144233LL,
  -4566602855316473334LL,
  1304661931384299587LL,
  -969461748689466101LL,
  1449782807285377209LL,
  -5803724520322688635LL,
  -6665502279738991173LL,
  6661656990781089439LL,
  6628030039556766661LL,
  -3808982209293768330LL,
  -5123705065467675544LL,
  4783813536573938890LL,
  -8879390785849056704LL,
  -3261945367502379907LL,
  -4095564893657681391LL,
  -2543917018778816693LL,
  8629428843738190572LL,
  -6237281416393845140LL,
  5181782943753062650LL,
  -6849905649493701436LL,
  2464472778724420824LL,
  -2794252939654771001LL,
  -7434304755963616514LL,
  -6522656844111576625LL,
  4588534310125407782LL,
  5798545105513782756LL,
  6088381134756891803LL,
  -8009448626570062910LL,
  -9024175760742942882LL,
  7624756332597182654LL,
  -3520386971835486807LL,
  1159815063627667771LL,
  -2649317814919470226LL,
  7951414056792258569LL,
  -8984792681266104575LL,
  -6124923535491764635LL,
  -1218812049027707128LL,
  5363565942748453849LL,
  3004038213957257684LL,
  3044190567318991535LL,
  3864764408389801008LL,
  -251415812914627529LL,
  3722181205518504112LL,
  4750412465404418122LL,
  1698409218363445262LL,
  4895325166718801549LL,
  -1975386027297452716LL,
  5506363146623890915LL,
  5072550164446322616LL,
  111522766965893636LL,
  -357120075418340237LL,
  -7864461071633408678LL,
  7914591545990338099LL,
  4008498864059801996LL,
  -1507794889621367666LL,
  -5232680059090163730LL,
  8811206071006586349LL,
  6193139154639052633LL,
  8342698128609431161LL,
  6916871911938444778LL,
  8666292955285122836LL,
  -5086600005147430015LL,
  -4421619631439727572LL,
  -4836262316001606798LL,
  -68234312260236917LL,
  580036219055374705LL,
  7230179079645095068LL,
  -3084202200565840799LL,
  -3406877175911261068LL
}; // weak
__int64 TD1[128] =
{
  6016318055608612007LL,
  -7621736131417466972LL,
  -1071965263202899093LL,
  -7832917521976198568LL,
  -671750557456649990LL,
  2735094876579875958LL,
  -2898863729543223849LL,
  -8091452701132049084LL,
  7432551386411086170LL,
  -2207328139199780338LL,
  1333431558400585589LL,
  -4148989562547517801LL,
  -7704090384959041697LL,
  -2696158255750615686LL,
  -3217694247904559485LL,
  4940107702795362409LL,
  8715747830997041801LL,
  -2510834370448107458LL,
  1725028936959582543LL,
  -5440966024565808980LL,
  -9014770280947261622LL,
  4999650342881153331LL,
  -8882387441219967881LL,
  -7712123905632206432LL,
  1841767727030618216LL,
  -5236987109321679252LL,
  -2129557201232366637LL,
  3055223591619731343LL,
  229601950976633640LL,
  -6497789327273704069LL,
  -5610429991319754617LL,
  6696032971692049258LL,
  -7879195208329801956LL,
  -6823399961993738254LL,
  -3096781376722707724LL,
  -8447443390654499742LL,
  -6871662532378743213LL,
  8477171888635087585LL,
  -6178832108181814292LL,
  5889984902775861663LL,
  4437977504841343370LL,
  5065958858218618373LL,
  392310549315146893LL,
  -44947943286634796LL,
  -7505602919935665925LL,
  8604084889174229059LL,
  -8644789726438430654LL,
  -2631851581738903205LL,
  -1622349132366513142LL,
  3388507166LL,
  5202268818672025734LL,
  5650991081003553136LL,
  6201321691704790783LL,
  2825495539565309653LL,
  2407275870542237657LL,
  4189533128074419028LL,
  1122337384764803687LL,
  -7053873340738113898LL,
  -6745868735493390139LL,
  1593168272828299083LL,
  -1891335741063916614LL,
  2094766563406914272LL,
  -5912509673783817971LL,
  -4029562052284664152LL,
  5525763942969241881LL,
  -170152307463382563LL,
  -4873894297445400282LL,
  3772885634746967611LL,
  -2536894629793807575LL,
  7185744930964696572LL,
  1171608279354651100LL,
  2343215483531269922LL,
  -517145134420899292LL,
  7910337042262718770LL,
  -874628307128312273LL,
  -3425055806929926574LL,
  -7374238973156412650LL,
  2470199540537463880LL,
  1918797597932846220LL,
  -1215352252238758612LL,
  -4478485880796395186LL,
  3934127422488496802LL,
  2928882230059529601LL,
  -6611374691454371954LL,
  959399292918577821LL,
  7085426955480883148LL,
  -1688611572207416045LL,
  -755826629478041097LL,
  8964925866234568064LL,
  -5490128658659027667LL,
  -6408595343595230055LL,
  8924792762770431075LL,
  -833630921238436232LL,
  -6304107530512721225LL,
  9127388959836902766LL,
  -1806200839239254833LL,
  -3581928272247986277LL,
  -3014684425353257207LL,
  3542713479375987890LL,
  -4596589284873034348LL,
  -6414107964257055044LL,
  1527749247731011792LL,
  -629962050550823784LL,
  3393341330238197072LL,
  5567557105926688214LL,
  -2313559363214857651LL,
  1967064273420997045LL,
  9171129092853017631LL,
  6701791879842586346LL,
  3385311920645638004LL,
  5950059754221168413LL,
  1400010660484026454LL,
  8806684671616931681LL,
  -8508685920179193836LL,
  3870669924671072551LL,
  4357874418960112869LL,
  4563820374648476383LL,
  -4651154949074840370LL,
  6584260090756593613LL,
  -8757173767683228305LL,
  4519758894243426291LL,
  6900257075100132404LL,
  917857074532457923LL,
  4755534667462491209LL,
  -2447692432637581311LL,
  -8042207645016738588LL,
  8130460461188238212LL,
  4814550687411629148LL
}; // weak
__int64 TD2[128] =
{
  7301318241342738932LL,
  6815699028014209559LL,
  5039844218983103403LL,
  257633043507555578LL,
  7923711320922595376LL,
  5487061111948806348LL,
  -3758318576214454299LL,
  -6660905890197002699LL,
  1974588443934187185LL,
  -4548250802919553558LL,
  -1147712789645442257LL,
  -448552556350436026LL,
  -7163795913844522097LL,
  6474651595780636525LL,
  2437389080777839806LL,
  -4015927972003755552LL,
  8753014768822547906LL,
  8204757772708583768LL,
  -5974041912440078623LL,
  4230144276616890656LL,
  3567665755905876959LL,
  9170844325693658961LL,
  -5871361943937175196LL,
  3140408655625387649LL,
  -208978379429875640LL,
  -524860141510093602LL,
  207854202802908019LL,
  -6112961026538872033LL,
  -4466673950314220821LL,
  623136358711068357LL,
  -6507099206070423512LL,
  -9053100470385901053LL,
  -5435097661677270321LL,
  -2116324132933012729LL,
  -4695840413812169254LL,
  -105055422399917772LL,
  6170110655461143598LL,
  -1480095522274278006LL,
  -1177057567579763837LL,
  1175929260532457073LL,
  449681416070446625LL,
  -4808069901838262978LL,
  6702888709415866708LL,
  1585091494135399430LL,
  -1614585829938423400LL,
  -7028034510645327552LL,
  -8392449619493277464LL,
  -1235259771352193255LL,
  1146584203224654204LL,
  516552836LL,
  -1348772925875746432LL,
  8236639904200400401LL,
  4059449182320852238LL,
  4118319954730565038LL,
  -6475780041188570609LL,
  3331014690163759963LL,
  -1797055722686444534LL,
  -7953889536825445138LL,
  2351549556271841472LL,
  1879720759373945463LL,
  3091088515808289427LL,
  1665507348132871202LL,
  -4058321010157744631LL,
  -6212693564045316682LL,
  525988173969709041LL,
  6988921966325132953LL,
  -739614593451559167LL,
  9093994682165576806LL,
  -4117192607443547325LL,
  -1052794594247788819LL,
  -8858507470364289231LL,
  1234132284412531607LL,
  4465550731750769994LL,
  -6814571681394086151LL,
  3527405591016971678LL,
  -2030991759740498554LL,
  -5072837206378533965LL,
  7215408620278256020LL,
  4547123728798230780LL,
  -8003140409623030147LL,
  -3332143413414361271LL,
  808001222194662602LL,
  -2438517252638660875LL,
  -4637511701637965129LL,
  -7922587690499560390LL,
  5071709033947949663LL,
  -5122685318415059315LL,
  -5767559969597673927LL,
  -7819258508787933347LL,
  1347648472100728789LL,
  9054223273687763116LL,
  -4937111518363850596LL,
  1798183481772723494LL,
  -7302442417385247590LL,
  -1837843583875029355LL,
  -1664380135320051268LL,
  3949175766467066599LL,
  8995423141775075999LL,
  2535854518255432100LL,
  7403976759784294053LL,
  -3844107593480440754LL,
  -2876335991152385904LL,
  -2668591769770594044LL,
  -707320703593054259LL,
  -5742859252451674547LL,
  351250630151818410LL,
  -8639227438978326831LL,
  5872489839252259116LL,
  3845231361577491806LL,
  4696967488010975879LL,
  -3291406907866434713LL,
  5121557968587057424LL,
  899092241879702231LL,
  4362276181266815480LL,
  -3948047870410699095LL,
  -5675526981174828772LL,
  8304450739512122578LL,
  4017056694637500436LL,
  -6171233320695081993LL,
  -2628281097207357635LL,
  -4305800333306508625LL,
  4638640147425671204LL,
  2669716361251526173LL,
  -7691586256038582212LL,
  -5485938448433858136LL,
  -4498985703377676108LL,
  -5300502348469011509LL,
  6287817520006318188LL
}; // weak
__int64 TD3[128] =
{
  4712264389848682577LL,
  2836869991538934554LL,
  -7114014918039581893LL,
  -2088663847637832788LL,
  8533748092648445216LL,
  165549023933403528LL,
  3083795614829902927LL,
  7107682645366308902LL,
  -5036318346696832546LL,
  -89824199072376763LL,
  5543951487349359299LL,
  -3172286298457660531LL,
  -7882261327972931837LL,
  5934014319359093695LL,
  8368201960737942996LL,
  -3906797296302872247LL,
  -8180374255510656395LL,
  -5084039343270630503LL,
  -8598189790240721218LL,
  -3586355703735097655LL,
  1887433642650245219LL,
  6016603915761705111LL,
  7759285148385271985LL,
  588727975644503294LL,
  5043215260910639216LL,
  8933091427436234644LL,
  5405131483995382699LL,
  6173074332345915363LL,
  -5349709903019112526LL,
  3965601800869616262LL,
  -4637104377234460112LL,
  1621960988841130498LL,
  8769795623829121930LL,
  7629838075231924467LL,
  414002441184660837LL,
  -6413536244031938607LL,
  -912646731050279628LL,
  -654299971287174651LL,
  6984988741142853899LL,
  7930928817896359518LL,
  -2520259226450069186LL,
  -1820221376820826403LL,
  -4297272485133568623LL,
  5770799285441687300LL,
  -4762108183022197735LL,
  -2765641835815383927LL,
  -8535579101207806544LL,
  -3967992908503893785LL,
  4760280052020168609LL,
  2216610296LL,
  3165265494853190409LL,
  6517357836814560286LL,
  -8847226845293184003LL,
  3258678921194184253LL,
  6676060228449559562LL,
  3904121854802317723LL,
  6334048525129068812LL,
  -7236829283403509068LL,
  -2584887645380980864LL,
  1304379353593506138LL,
  -6905454463344309534LL,
  1952060927220138812LL,
  -8374533727026607346LL,
  2209517083491350829LL,
  8432793145822512471LL,
  9178614919684013038LL,
  8283734192960217079LL,
  -324764555962235580LL,
  2577990602279450251LL,
  -1949667517756839754LL,
  7171155858147822295LL,
  -4174519625752362989LL,
  -4954530432966951547LL,
  2999799532452188590LL,
  -5606713407204013283LL,
  -4475504392902874099LL,
  8122692357072514091LL,
  -1629139474375443951LL,
  -1135159302339902296LL,
  3715732623988676694LL,
  4094266675238455175LL,
  -3167377883062403444LL,
  8853558610710613926LL,
  -5926837991026841894LL,
  8688021269383078956LL,
  9099068211672685418LL,
  -2830256658262539530LL,
  -4345985175722172882LL,
  -3417250900261945697LL,
  2671395234200267119LL,
  1764750319056141256LL,
  4304170052034457320LL,
  6420150022745622989LL,
  5736082358599614956LL,
  -7179078277896730LL,
  1578765586732615713LL,
  8013819408061028794LL,
  -5729469151671888662LL,
  4549534107795894065LL,
  -6744492055296986938LL,
  -9022215300900440204LL,
  -6352303963188973344LL,
  -1379518477213611279LL,
  -7929098314663850369LL,
  -1175354551279645322LL,
  -7636732894910786356LL,
  7676415581602833310LL,
  7300756410308999361LL,
  -8343660474165230435LL,
  810981065747624954LL,
  -2606930445545416013LL,
  -3006412867100068887LL,
  -6841959353627341670LL,
  1386134431895686745LL,
  7046222154931105486LL,
  5165976742739176929LL,
  -976367056366642788LL,
  -4091591235451586280LL,
  -168221568335484333LL,
  4961707265463620831LL,
  7549227842621637066LL,
  -6683236559582122952LL,
  -2151299244906810858LL,
  978761058506869544LL,
  915319278046114105LL,
  6251436519972969688LL,
  3654232211202531707LL,
  -5163584984818355128LL
}; // weak
__int64 Td4[128] =
{
  651061556772229714LL,
  -3038287261001422230LL,
  3906369333155082288LL,
  4051049680768181669LL,
  4629771063775969215LL,
  -7016996765209222237LL,
  -868082075977023103LL,
  -289360691958655017LL,
  -2025524841200976772LL,
  -9042521605989123783LL,
  3399988125208648603LL,
  -8680820738548039681LL,
  -8174439532218534860LL,
  4919131752972370755LL,
  -2387225704094448444LL,
  -3761688987074696727LL,
  8897841258426553428LL,
  3617008643554448532LL,
  -4412750543594281306LL,
  4412750542684758819LL,
  5497853138422394606LL,
  795741903543178645LL,
  -361700867289497022LL,
  5642533483340612547LL,
  3327647949911492616LL,
  7378697630477558177LL,
  -2748926570828126168LL,
  -5570193310923611100LL,
  6582955728719738486LL,
  5280832618678624930LL,
  -8391460049722184339LL,
  2676586397905834449LL,
  -506381212123499918LL,
  7234017286266746614LL,
  7523377975665264262LL,
  1591483804627277976LL,
  -6582955727456512812LL,
  -3689348816628327332LL,
  7306357456510999901LL,
  -7885078838744009034LL,
  8102099357797215340LL,
  5787213826911389768LL,
  -1302123110815891971LL,
  -2676586395564656199LL,
  1519143630829149790LL,
  6293595036626339398LL,
  -8246779703102822489LL,
  -8897841258662355555LL,
  -2821266741897686896LL,
  2880154539LL,
  -4846791580959601524LL,
  723401731766211539LL,
  -1953184666308053001LL,
  361700865588353112LL,
  -5497853135609612104LL,
  434041038089569605LL,
  3182967607637627088LL,
  -8102099359767847394LL,
  4557430891140008650LL,
  144680345895112463LL,
  -5787213826742959679LL,
  217020521647029693LL,
  1374463283620282625LL,
  7740398494196337290LL,
  -7957419013653775814LL,
  4702111233666519313LL,
  7451037801917665103LL,
  -1519143629835412260LL,
  -940422248427710569LL,
  -3544668469048913969LL,
  -5425512961845169936LL,
  8319119878315763430LL,
  -6004234345930910058LL,
  2459565877875733620LL,
  -5931894171745392665LL,
  -8825501087592794827LL,
  -434041037415849246LL,
  -1663823978256976073LL,
  8463800220555942940LL,
  7957419014091694047LL,
  -1012762422596384953LL,
  8174439529237322266LL,
  2965947086159027485LL,
  -8536140393882466875LL,
  -5208492445554217105LL,
  1012762421147886178LL,
  1736164150572919466LL,
  1953184669373480638LL,
  6221254866870533372LL,
  5425512962636791358LL,
  -3255307777915566394LL,
  2314885532317481337LL,
  -2604246223265555814LL,
  -72340173882343232LL,
  -3617008643335489416LL,
  -795741903812666790LL,
  -2459565879694778593LL,
  3689348816712542376LL,
  506381212039284872LL,
  3544668471592208327LL,
  1302123113763418545LL,
  6438275381359284240LL,
  -9187201951934765273LL,
  6872316421992148204LL,
  5859554000136855648LL,
  -6221254864782000257LL,
  -5353172792645183207LL,
  940422247922420298LL,
  -1880844496889107155LL,
  -6944656593078551942LL,
  -3906369334165662829LL,
  -1157442766807196516LL,
  -2242545359058329440LL,
  5570193308228729659LL,
  3038287261422497454LL,
  -5714873653045889547LL,
  -1446803457351038776LL,
  4340410372423662523LL,
  6004234346368828291LL,
  7016996766236645785LL,
  3110627431700436759LL,
  9114861775542813700LL,
  8608480568859605690LL,
  2748926570811283158LL,
  7595718150019211745LL,
  7161677109638992916LL,
  2387225704532366677LL,
  9042521602856324108LL
}; // weak
__int64 rcon[6] =
{
  144115188092633088LL,
  576460752370532352LL,
  2305843009482129408LL,
  -9223372035781033984LL,
  3891110078501093376LL,
  0LL
}; // weak
__int64 Tks0[128] =
{
  1011353932217188352LL,
  1304661930846329366LL,
  3903839292608492588LL,
  3044190567093644858LL,
  9097664131305597016LL,
  7085146574931259982LL,
  5072550162420489332LL,
  6518761901403293282LL,
  -1253727222249762640LL,
  -969461744688510298LL,
  -2972963340609592164LL,
  -3841584218448134518LL,
  -7002615956592084760LL,
  -9024175759156403458LL,
  -6416079365226066748LL,
  -4978839503689050414LL,
  -3084202198964651141LL,
  -3953970915403259539LL,
  -1362758501773897897LL,
  -1079570562787994303LL,
  -6522656844036189405LL,
  -5086600005471319755LL,
  -7106985856571283697LL,
  -9129658382083031783LL,
  3864764406058150859LL,
  3004038210687431133LL,
  974557542997778407LL,
  1266717634307376625LL,
  5037855180438307731LL,
  6482954265193159045LL,
  9065247646654347199LL,
  7051546998588386729LL,
  -6665502280831832586LL,
  -4652927687291799584LL,
  -7252109515819951654LL,
  -8698334586381253684LL,
  -3227101099513215570LL,
  -3520386970470834248LL,
  -1507794887287099006LL,
  -648194402844922988LL,
  4895325166040956230LL,
  6916871911904683856LL,
  8920369041059776874LL,
  7483186491566363516LL,
  3722181202785785118LL,
  3437867209273954056LL,
  829766486016581938LL,
  1698409216648739620LL,
  8666292953524590221LL,
  7230179077794024603LL,
  4638971130930651809LL,
  6661656988020897975LL,
  580036218189102805LL,
  1449782806956188867LL,
  3470172987383188217LL,
  3187033289469956335LL,
  -7434304755794639299LL,
  -8879390783848899541LL,
  -6849905649393257967LL,
  -4836262313370900473LL,
  -1685362832577302939LL,
  -824588121256004493LL,
  -3406877173390157239LL,
  -3699059117493370785LL,
  5757245177100819191LL,
  6050531030878568673LL,
  8629428845554495195LL,
  7769828412651926733LL,
  4588534307288822447LL,
  2575959696568920249LL,
  543118646466086531LL,
  1989343768566996117LL,
  -5803724519088846265LL,
  -5519410542756884399LL,
  -7543191410886295957LL,
  -8411834089978846083LL,
  -2360608494472371681LL,
  -4382155257515968503LL,
  -1794302676543973837LL,
  -357120075510952923LL,
  -7719613381592260212LL,
  -8589360021898953830LL,
  -5982371660811774560LL,
  -5699231945718673482LL,
  -1975386026794874412LL,
  -539272202603916350LL,
  -2543917015740236296LL,
  -4566602855650613266LL,
  8380842903043813692LL,
  7520068140182907690LL,
  5506363145073165584LL,
  5798545106356248326LL,
  290152799387244900LL,
  1735238775901897586LL,
  4333272370693008712LL,
  2319629051850520414LL,
  -2120146443479369983LL,
  -107628869925163753LL,
  -2686522355274054867LL,
  -4132734145796466373LL,
  -7864461072012222631LL,
  -8157769053461494449LL,
  -6124923536592671883LL,
  -5265274862617431709LL,
  145076960304457649LL,
  2166636780048645543LL,
  4190421977505092509LL,
  2753182064428468619LL,
  8235679515347652585LL,
  7951414054966269439LL,
  5363565940760263621LL,
  6232186767059198419LL,
  4154572242025516154LL,
  2718515455000254060LL,
  111522764586039382LL,
  2134195272917918272LL,
  5323370385572979746LL,
  6193139153551195700LL,
  8197779499921526798LL,
  7914591543755754008LL,
  -2794252938314130230LL,
  -4239351971529373988LL,
  -2225651305746667290LL,
  -211950674860576016LL,
  -6237281414317230958LL,
  -5376555167406903676LL,
  -7974593228964003650LL,
  -8266753337453471064LL
}; // weak
__int64 Tks1[128] =
{
  796584135530381312LL,
  2094766562649838106LL,
  2825495539799827508LL,
  3542713477412173358LL,
  6016318055744817256LL,
  4999650342576347762LL,
  9171129091704253532LL,
  7591511287542349382LL,
  -4904813766725562160LL,
  -5912509671167720758LL,
  -7505602917909875484LL,
  -9094263036981238018LL,
  -2044966601558533960LL,
  -755826626787489118LL,
  -3483827324025504628LL,
  -2757637165369987434LL,
  8130460461630700475LL,
  7406486987135723937LL,
  6696032970045857679LL,
  5400102480182709653LL,
  2928882227174142931LL,
  4519758892967092681LL,
  332526306871957479LL,
  1333431558783059453LL,
  -4596589282281673877LL,
  -3014684425215035023LL,
  -1437345015440109729LL,
  -427397448619549371LL,
  -7474310658229804285LL,
  -8189241680376368871LL,
  -5440966023078230217LL,
  -6745868733984549587LL,
  -170152306477795731LL,
  -1459327379721591689LL,
  -3342977878389861799LL,
  -4069203135518130109LL,
  -6497789326932492795LL,
  -5490128658402038753LL,
  -8508685920371402191LL,
  -6920061037211744213LL,
  5567557104732858045LL,
  6584260088172770471LL,
  7024431813396644489LL,
  8604084887829991571LL,
  1527749245270863573LL,
  229601950983896271LL,
  4110524135380454113LL,
  3393341330600597755LL,
  -8757173765660062250LL,
  -8042207645040746548LL,
  -6178832106432599582LL,
  -4873894297053529096LL,
  -2447692430963243586LL,
  -4029562052118177884LL,
  -995250953181585014LL,
  -2005163284090440816LL,
  3934127419251154182LL,
  2343215483186761500LL,
  1918797594930116914LL,
  917857072747572008LL,
  7991809489494250862LL,
  8715747831156737908LL,
  4814550686700056922LL,
  6110446043730715456LL,
  -265741137262547750LL,
  -1561632182146040128LL,
  -3425055807208044306LL,
  -4148989561845458188LL,
  -6611374689166973774LL,
  -5610429992276216152LL,
  -8644789727422480250LL,
  -7053873341771967844LL,
  5525763941933874186LL,
  6535680859867803152LL,
  6960121030414784574LL,
  8541994963716885028LL,
  1503988348049503330LL,
  199054988256552568LL,
  4100273865247756374LL,
  3385311919336653388LL,
  -8677066024239311007LL,
  -7959887531606620805LL,
  -6076206538670541995LL,
  -4778063831408647857LL,
  -2313559361065410807LL,
  -3893216610206970605LL,
  -874628304213984451LL,
  -1891335737240016601LL,
  3995905339759937457LL,
  2407275869575206315LL,
  1967064269874946949LL,
  959399289197326751LL,
  7999526347398057945LL,
  8725747154940206531LL,
  4844785645823077357LL,
  6133956544358660599LL,
  784351712221371063LL,
  2089249801742878893LL,
  2799751939646340739LL,
  3514678615286000793LL,
  5950059754202911455LL,
  4940107703436492997LL,
  9127388960101037803LL,
  7545479893966447857LL,
  -5043464875929761177LL,
  -6044374474347767683LL,
  -7621736132053291437LL,
  -9212617419231052727LL,
  -2129557197913570801LL,
  -833630917118373867LL,
  -3581928272591296965LL,
  -2857959282042178527LL,
  8158736924257263884LL,
  7432551386986558230LL,
  6701791881209021752LL,
  5412656252944881442LL,
  2975155771006249316LL,
  4563820374023469950LL,
  392310546512203088LL,
  1400010660022312778LL,
  -4478485880740826660LL,
  -2898863730072017978LL,
  -1305730643213216280LL,
  -289058308659935246LL,
  -7374238972510766668LL,
  -8091452701055161426LL,
  -5363412713456312960LL,
  -6661590656629911654LL
}; // weak
__int64 Tks2[128] =
{
  939860379054374912LL,
  1665507344808025106LL,
  4118319952030349348LL,
  2535854516033168438LL,
  7301318240286306376LL,
  9170844326749432922LL,
  5872489840271181932LL,
  5433973893727671422LL,
  -2469117626283925360LL,
  -4058321006976631678LL,
  -1614585830316386124LL,
  -900215593258793818LL,
  -5340250109543673640LL,
  -5767559971013227318LL,
  -9092865549078845188LL,
  -7216531424053906194LL,
  -5300502347544536261LL,
  -6023906309576997079LL,
  -9053100469771574497LL,
  -7472895629681623283LL,
  -2438517251274200205LL,
  -4305800334016137375LL,
  -1583968137998527657LL,
  -1147712787362246843LL,
  7403976759106550699LL,
  8995423143520446393LL,
  5975165676399559567LL,
  5258534843434737565LL,
  1051667384375659491LL,
  1481220249566402545LL,
  4230144274659767239LL,
  2351549553727598549LL,
  6988921964449934710LL,
  8857383569037767012LL,
  6111836985970300242LL,
  5676654604063639872LL,
  623136356732689726LL,
  1347648471866851628LL,
  4362276181752603930LL,
  2783214679136467208LL,
  -5742859252875768346LL,
  -6171233321342709260LL,
  -8907702473848097342LL,
  -7028034509308401200LL,
  -2876335991078109778LL,
  -4466673947512397380LL,
  -1425095146039278198LL,
  -707320700722734696LL,
  -2628281095471401395LL,
  -4498985703780423073LL,
  -1177057567740703127LL,
  -739614589926813061LL,
  -5485938445256423931LL,
  -6212693564111775209LL,
  -8650798983536886239LL,
  -7069476885013519821LL,
  808001219564771037LL,
  1234132284310522575LL,
  4547123727276551929LL,
  2669716358644085483LL,
  7164919815757751957LL,
  8753014768470850183LL,
  6287817519969984177LL,
  5572303670560670371LL,
  -2883341812784217620LL,
  -3608979827826008578LL,
  -2030991757124339256LL,
  -448552554787412518LL,
  -4637511701025050204LL,
  -6507099205520511562LL,
  -8392449618356270720LL,
  -7953889536728820334LL,
  525988173714006396LL,
  2115200230240664942LL,
  3702125268893931864LL,
  2987728523298178378LL,
  8004267892476465460LL,
  8431516061035777318LL,
  6573257752153680144LL,
  4696967487334774018LL,
  7820382960792410839LL,
  8543760379926972101LL,
  6389390687533572851LL,
  4809194488917835489LL,
  351250629019363999LL,
  2218577537607595661LL,
  3527405591263236795LL,
  3091088513356975785LL,
  -4884307389530211769LL,
  -6475780041964100011LL,
  -8639227439797484957LL,
  -7922587690480542095LL,
  -3139285987790419441LL,
  -3568794752256960995LL,
  -2286918065066593749LL,
  -408384796526498247LL,
  -5040967708829422438LL,
  -6909367895384919928LL,
  -8203629189494090562LL,
  -7768490942671370068LL,
  -3291406904611726126LL,
  -4015927970457747264LL,
  -1837843581776845578LL,
  -258755845500454684LL,
  7690461527336356874LL,
  8118897288713539608LL,
  6815699026652771374LL,
  4935987201907042364LL,
  207854199527537730LL,
  1798183480127873104LL,
  3949175764855112806LL,
  3231427828076730484LL,
  103927660332857249LL,
  1974588442795584435LL,
  3845231358596484997LL,
  3407850108052575127LL,
  7577669076129040361LL,
  8304450737882291195LL,
  6702888708381507533LL,
  5121557968383927263LL,
  -3332143411031779535LL,
  -3758318576501732573LL,
  -1878597955260846315LL,
  -1129134236306681LL,
  -5072837203725211783LL,
  -6660905888418317461LL,
  -8235516551453827235LL,
  -7520011618396634289LL
}; // weak
__int64 Tks3[128] =
{
  652189675796037632LL,
  1952060926938715676LL,
  3258678918868380728LL,
  4549534105941719588LL,
  4712264386958874736LL,
  6016603915916824172LL,
  7300756411386721352LL,
  8596080151153238612LL,
  -7359519582747316000LL,
  -8374533724984137988LL,
  -4762108182553912104LL,
  -5768123990346396988LL,
  -3335509677617649520LL,
  -4345985173295006068LL,
  -756094946312928088LL,
  -1757571832668040524LL,
  3654232208790223835LL,
  2354369719380831687LL,
  1621960989313292259LL,
  331096971787190783LL,
  8853558608534905771LL,
  7549227841310242231LL,
  6839284057946656659LL,
  5543951487727376783LL,
  -6744492055198971077LL,
  -5729469151228863193LL,
  -8767685981552777469LL,
  -7761679004213055201LL,
  -1581158993231637685LL,
  -570674735820995241LL,
  -3586355700940947597LL,
  -2584887645038597777LL,
  9178614919089485485LL,
  7888876904549703857LL,
  6581115420526893717LL,
  5282378796664867977LL,
  3965601798596964061LL,
  2671395231364004033LL,
  1386134428173042405LL,
  82929526495745273LL,
  -1175354548714387891LL,
  -168221568388345775LL,
  -3781790602911716747LL,
  -2765641833394242455LL,
  -6352303962685366723LL,
  -5349709903796697055LL,
  -8940708438499839483LL,
  -7929098315541830631LL,
  4961707265738837366LL,
  6251436518545333098LL,
  6984988740706017614LL,
  8283734195020805970LL,
  915319275323839750LL,
  2209517080823513882LL,
  2920569171908163902LL,
  4223782904038223650LL,
  -3167377882753909354LL,
  -4174519624813237366LL,
  -1135159302396178002LL,
  -2151299241460889678LL,
  -7177771435879930394LL,
  -8180374256501885958LL,
  -5163584983660869154LL,
  -6175186276166115390LL,
  -1884760498824874175LL,
  -590558329638428323LL,
  -4475504391785358471LL,
  -3172286295968527003LL,
  -5926837991343214799LL,
  -4637104374849947347LL,
  -8535579102948195575LL,
  -7236829284946635499LL,
  8432793143858579361LL,
  7430194686923395517LL,
  5851126544021220249LL,
  4839529615202745733LL,
  4426779907617595345LL,
  3419642529245039053LL,
  1827116638891553769LL,
  810981063513613813LL,
  -2419409826466394982LL,
  -3713620826105603450LL,
  -407387006971830110LL,
  -1710596341055375682LL,
  -7636732895099759382LL,
  -8926475342045789450LL,
  -5606713406716512046LL,
  -6905454462984786226LL,
  5673470603138514042LL,
  6676060229620935270LL,
  7676415579754139714LL,
  8688021270305900126LL,
  492140322526684170LL,
  1499268870446477846LL,
  2513082517786806322LL,
  3529226854898032174LL,
  -7801911436091391508LL,
  -9106246567002826768LL,
  -5220192197134832172LL,
  -6515529131040883768LL,
  -2606930443737540196LL,
  -3906797296833704064LL,
  -7179076642310748LL,
  -1298047457855183944LL,
  246213647572381964LL,
  1256693541296252688LL,
  2836869991919492404LL,
  3838333684135070504LL,
  5405131483160504700LL,
  6420150023443840864LL,
  8013819405890471236LL,
  9019822019543421784LL,
  -6340662308031923657LL,
  -5036318346667725781LL,
  -8343660473522563569LL,
  -7048332301349797869LL,
  -2276242739234069945LL,
  -976367055685143461LL,
  -4297272483107565953LL,
  -3006412863627978653LL,
  4094266674845108951LL,
  3083795611574000843LL,
  2082331953719731951LL,
  1080859499770867955LL,
  8122692356109800103LL,
  7107682646279226555LL,
  6092725506845752991LL,
  5086714131459516547LL
}; // weak
__int64 Te4_3[128] =
{
  8935141662364008448LL,
  8863084068661624832LL,
  7710162566118375424LL,
  -4251398046375477248LL,
  72057594843234304LL,
  3098476545358954496LL,
  -2954361351293632512LL,
  8502796099344400384LL,
  -9079256845389922304LL,
  9007199258113212416LL,
  6413125873569890304LL,
  -1152921503415664640LL,
  -3170534134766370816LL,
  -5836665114354253824LL,
  -6629298648872124416LL,
  -4611686016514785280LL,
  -216172779043553280LL,
  2738188575907512320LL,
  4539628425295429632LL,
  -3746994885828280320LL,
  -6557241056579026944LL,
  -1080863906726936576LL,
  -2882303759621292032LL,
  1513209475618570240LL,
  -4107282860094783488LL,
  -4395513235726401536LL,
  -7638104967617708032LL,
  -7349874591784763392LL,
  1297036692800143360LL,
  -2161727818990354432LL,
  2810246171421835264LL,
  8430738505423912960LL,
  -9007199254589997056LL,
  1873497445724323840LL,
  7926335344625057792LL,
  -6917529026131132416LL,
  4251398049613479936LL,
  -5548434737330126848LL,
  -2089670226412044288LL,
  -8935141659914534912LL,
  -3386706918390104064LL,
  -1369094286720630784LL,
  -288230375614840832LL,
  6557241060421009408LL,
  -3819052482231795712LL,
  4107282863349563392LL,
  5476377148124037120LL,
  -3530822106382073856LL,
  -1224979095155113984LL,
  -360287967337512960LL,
  5548434742044524544LL,
  -8863084065809498112LL,
  -504403157107867648LL,
  9151314442850402304LL,
  4323455643617853440LL,
  -6341068272670081024LL,
  -6701356244168343552LL,
  -8142508125212114944LL,
  -7133701807305392128LL,
  -792633533477683200LL,
  -5332261955652550656LL,
  2377900606909054976LL,
  -72057593769492480LL,
  -3314649321667821568LL,
  864691131894464512LL,
  -1441151880439791616LL,
  -7566047372388597760LL,
  1657324664013193216LL,
  -6413125866087251968LL,
  4395513238427533312LL,
  6701356247205019648LL,
  8286623314781143040LL,
  -9151314441206235136LL,
  -2594073384040005632LL,
  3026418950163398656LL,
  -8646911282135433216LL,
  -1297036691508297728LL,
  1441151883845566464LL,
  6773413843289767936LL,
  -2666130979218784256LL,
  3602879705654493184LL,
  720575941352357888LL,
  432345565452304384LL,
  6629298652093349888LL,
  -3242591728451977216LL,
  7061644218602618880LL,
  -7710162559625592832LL,
  8718968882414485504LL,
  -4035225262248427520LL,
  7854277751056891904LL,
  -3098476541265313792LL,
  -6269010679991107584LL,
  6196953089073741824LL,
  -1585267064740773888LL,
  8791026474321707008LL,
  576460755222659072LL,
  8646911287671914496LL,
  3314649326365442048LL,
  -6485183462943752192LL,
  -4179340451179921408LL,
  -2522015787435163648LL,
  2233785417121923072LL,
  -4827858799282880512LL,
  -8502796094143463424LL,
  4467570832230580224LL,
  7349874594905325568LL,
  216172783321743360LL,
  1008806320658186240LL,
  3819052485637570560LL,
  -5116089175233265664LL,
  -4539628422141313024LL,
  -7061644215230398464LL,
  -576460748528549888LL,
  1224979101194911744LL,
  -2810246165717581824LL,
  -7782220153713852416LL,
  2161727823738306560LL,
  -1657324660607418368LL,
  6124895496679981056LL,
  -2377900602580533248LL,
  -6845471431254343680LL,
  936748724791541760LL,
  -1873497441781678080LL,
  7493989781051801600LL,
  -7421932184816058368LL,
  1080863911323893760LL,
  6052837902138736640LL,
  1585267071971753984LL
}; // weak
__int64 Te4_2[128] =
{
  34902897118609408LL,
  34621422143209472LL,
  30117822523899904LL,
  55450570419273728LL,
  281474979856384LL,
  12103424005308416LL,
  60517120009437184LL,
  33214047263064064LL,
  36591746985623552LL,
  35184372102004736LL,
  25051272943632384LL,
  67553994415210496LL,
  59672695073996800LL,
  49258120934981632LL,
  46161896190771200LL,
  54043195535917056LL,
  71213169119789056LL,
  10696049124638720LL,
  17732923536310272LL,
  57420895265161216LL,
  46443371160666112LL,
  67835469402275840LL,
  60798594976907264LL,
  5910974514135040LL,
  56013520365682688LL,
  54887620460871680LL,
  42221246508171264LL,
  43347146413768704LL,
  5066549581250560LL,
  63613344744996864LL,
  10977524107116544LL,
  32932572286812160LL,
  36873221949685760LL,
  7318349397360640LL,
  30962247439941632LL,
  45035996279603200LL,
  16607023631302656LL,
  50384020845232128LL,
  63894819716005888LL,
  37154696928886784LL,
  58828270137966592LL,
  66709569480425472LL,
  70931694133182464LL,
  25614222892269568LL,
  57139420279209984LL,
  16044073684959232LL,
  21392098234859520LL,
  58265320184872960LL,
  67272519447478272LL,
  70650219165515776LL,
  21673573211111424LL,
  37436171905859584LL,
  70087269205475328LL,
  35747322042384384LL,
  16888498607882240LL,
  47287796097810432LL,
  45880421209145344LL,
  40250921673818112LL,
  44191571353141248LL,
  68961369297780736LL,
  51228445773660160LL,
  9288674245738496LL,
  71776119062265856LL,
  59109745125163008LL,
  3377699733962752LL,
  66428094504960000LL,
  42502721489534976LL,
  6473924468801536LL,
  47006321123524608LL,
  17169973587607552LL,
  26177172840644608LL,
  32369622323363840LL,
  36310272001966080LL,
  61924494881521664LL,
  11821949024075776LL,
  38280596842086400LL,
  66991044461723648LL,
  5629499546271744LL,
  26458647825350656LL,
  61643019900354560LL,
  14073748850212864LL,
  2814749770907648LL,
  1688849865048064LL,
  25895697859739648LL,
  59391220098662400LL,
  27584547728916480LL,
  41939771539390464LL,
  34058472196931584LL,
  56294995357270016LL,
  30680772465065984LL,
  59954170048610304LL,
  47569271069212672LL,
  24206848004194304LL,
  65865144566284288LL,
  34339947165319168LL,
  2251799825088512LL,
  33776997217468416LL,
  12947848931115008LL,
  46724846135803904LL,
  55732045400506368LL,
  62205969868259328LL,
  8725724285632512LL,
  53198770603229184LL,
  38843546795180032LL,
  17451448563400704LL,
  28710447636348928LL,
  844424934850560LL,
  3940649690071040LL,
  14918173772021760LL,
  52072870697172992LL,
  54324670513938432LL,
  44473046322184192LL,
  69805794238988288LL,
  4785074614042624LL,
  61080069953093632LL,
  41658296562483200LL,
  8444249311477760LL,
  65583669582430208LL,
  23925373033906176LL,
  62768919809097728LL,
  45317471259590656LL,
  3659174706216960LL,
  64739244655968256LL,
  29273397582233600LL,
  43065671440990208LL,
  4222124653608960LL,
  23643898055229440LL,
  6192449499889664LL
}; // weak
__int64 Te4_1[128] =
{
  136339441869568LL,
  135239930246912LL,
  117647744233984LL,
  216603790700288LL,
  1099511640064LL,
  47279000020736LL,
  236395000036864LL,
  129742372121344LL,
  142936511662592LL,
  137438953523456LL,
  97856534936064LL,
  263882790684416LL,
  233096465132800LL,
  192414534902272LL,
  180319906995200LL,
  211106232562176LL,
  278176441874176LL,
  41781441893120LL,
  69269232563712LL,
  224300372129536LL,
  181419418596352LL,
  264982302352640LL,
  237494511628544LL,
  23089744195840LL,
  218802813928448LL,
  214404767425280LL,
  164926744172544LL,
  169324790678784LL,
  19791209301760LL,
  248489627910144LL,
  42880953543424LL,
  128642860495360LL,
  144036023240960LL,
  28587302333440LL,
  120946279062272LL,
  175921860467200LL,
  64871186059776LL,
  196812581426688LL,
  249589139515648LL,
  145135534878464LL,
  229797930226432LL,
  260584255782912LL,
  277076930207744LL,
  100055558172928LL,
  223200860465664LL,
  62672162831872LL,
  83562883729920LL,
  227598906972160LL,
  262783279091712LL,
  275977418615296LL,
  84662395355904LL,
  146235046507264LL,
  273778395333888LL,
  139637976728064LL,
  65970697687040LL,
  184717953507072LL,
  179220395348224LL,
  157230162788352LL,
  172623325598208LL,
  269380348819456LL,
  200111116303360LL,
  36283883772416LL,
  280375465086976LL,
  230897441895168LL,
  13194139585792LL,
  259484744160000LL,
  166026255818496LL,
  25288767456256LL,
  183618441888768LL,
  67070209326592LL,
  102254581408768LL,
  126443837200640LL,
  141837000007680LL,
  241892558130944LL,
  46179488375296LL,
  149533581414400LL,
  261683767428608LL,
  21990232602624LL,
  103354093067776LL,
  240793046485760LL,
  54975581446144LL,
  10995116292608LL,
  6597069785344LL,
  101155069764608LL,
  231996953510400LL,
  107752139566080LL,
  163827232575744LL,
  133040907019264LL,
  219902325614336LL,
  119846767441664LL,
  234195976752384LL,
  185817465114112LL,
  94558000016384LL,
  257285720962048LL,
  134140418614528LL,
  8796093066752LL,
  131941395380736LL,
  50577534887168LL,
  182518930217984LL,
  217703302345728LL,
  242992069797888LL,
  34084860490752LL,
  207807697668864LL,
  151732604668672LL,
  68169720950784LL,
  112150186079488LL,
  3298534901760LL,
  15393162851840LL,
  58274116296960LL,
  203409651160832LL,
  212205744195072LL,
  173722837196032LL,
  272678883746048LL,
  18691697711104LL,
  238594023254272LL,
  162727720947200LL,
  32985348872960LL,
  256186209306368LL,
  93458488413696LL,
  245191093004288LL,
  177021372107776LL,
  14293651196160LL,
  252887674437376LL,
  114349209305600LL,
  168225279066368LL,
  16492674428160LL,
  92358976778240LL,
  24189255858944LL
}; // weak
__int64 Te4_0[128] =
{
  532575944803LL,
  528280977527LL,
  459561500914LL,
  846108557423LL,
  4294967344LL,
  184683593831LL,
  923417968894LL,
  506806141099LL,
  558345748682LL,
  536870912201LL,
  382252089594LL,
  1030792151111LL,
  910533066925LL,
  751619276962LL,
  704374636700LL,
  824633720946LL,
  1086626726071LL,
  163208757395LL,
  270582939702LL,
  876173328631LL,
  708669603892LL,
  1035087118565LL,
  927712936049LL,
  90194313265LL,
  854698491908LL,
  837518622755LL,
  644245094424LL,
  661424963589LL,
  77309411335LL,
  970662609024LL,
  167503724779LL,
  502511173810LL,
  562640715785LL,
  111669149740LL,
  472446402587LL,
  687194767450LL,
  253403070546LL,
  768799146198LL,
  974957576233LL,
  566935683119LL,
  897648164947LL,
  1017907249152LL,
  1082331758624LL,
  390842024113LL,
  871878361194LL,
  244813136062LL,
  326417514570LL,
  889058230360LL,
  1026497183952LL,
  1078036791466LL,
  330712481859LL,
  571230650419LL,
  1069446856773LL,
  545460846594LL,
  257698037840LL,
  721554505887LL,
  700079669329LL,
  614180323392LL,
  674309865618LL,
  1052266987576LL,
  781684048060LL,
  141733920986LL,
  1095216660496LL,
  901943132403LL,
  51539607757LL,
  1013612281875LL,
  648540061791LL,
  98784247876LL,
  717259538628LL,
  261993005182LL,
  399431958628LL,
  493921239065LL,
  554050781280LL,
  944892805199LL,
  180388626466LL,
  584115552400LL,
  1022202216518LL,
  85899346104LL,
  403726926046LL,
  940597837835LL,
  214748365024LL,
  42949673018LL,
  25769803849LL,
  395136991268LL,
  906238099650LL,
  420906795180LL,
  639950127249LL,
  519691043044LL,
  858993459431LL,
  468151435319LL,
  914828034189LL,
  725849473102LL,
  369367187564LL,
  1005022347508LL,
  523986010213LL,
  34359738542LL,
  515396075706LL,
  197568495653LL,
  712964571164LL,
  850403524788LL,
  949187772648LL,
  133143986292LL,
  811748819019LL,
  592705486987LL,
  266287972464LL,
  438086664373LL,
  12884901960LL,
  60129542390LL,
  227633266785LL,
  794568949847LL,
  828928688262LL,
  678604832797LL,
  1065151889633LL,
  73014444184LL,
  932007903337LL,
  635655159950LL,
  128849019035LL,
  1000727380103LL,
  365072220366LL,
  957777707048LL,
  691489734796LL,
  55834574985LL,
  987842478271LL,
  446676598850LL,
  657129996353LL,
  64424509485LL,
  360777253040LL,
  94489280699LL
}; // weak
__int64 TE0[128] =
{
  -541420945485372507LL,
  -685818668722456679LL,
  -2996182663297830387LL,
  -7942725399498231887LL,
  144397777062867024LL,
  6209104330594150313LL,
  -5343565613052789223LL,
  -1407807428406563866LL,
  2270520775382059589LL,
  -397023171052975808LL,
  -5595342192152741355LL,
  -292470043356739639LL,
  -5488528505909170708LL,
  5021425530703094525LL,
  6027123582689909951LL,
  -7223562301395012970LL,
  -2162293947392084030LL,
  5487115433738998702LL,
  9097059322532607578LL,
  -8949553715690866942LL,
  5883290957368276060LL,
  -436302668694690508LL,
  -6063858906022448749LL,
  3032353286078411091LL,
  -7653929886557142004LL,
  -7078599379625237659LL,
  4005554537633421352LL,
  3430224168543913231LL,
  2599159958319073033LL,
  -2314038506432724837LL,
  5631513194429934374LL,
  -1552205168656600371LL,
  2126688163979659547LL,
  3754342025716837492LL,
  -2562989426236253395LL,
  6602454056648202990LL,
  8519468324756804342LL,
  9057780976001144417LL,
  -2457871114808252037LL,
  1406394769301516145LL,
  -5057030659163073547LL,
  -4472658081674690560LL,
  -2018461321818791840LL,
  -5306546660418932280LL,
  -8229260389017556290LL,
  8230672790022176473LL,
  -7472513205809820962LL,
  -8804590826792920856LL,
  -4183862561985212309LL,
  -1298167994321818907LL,
  -7328115430596066363LL,
  1262562142963577685LL,
  -1586963544236800561LL,
  -108227673938853370LL,
  8663866048599052528LL,
  5451793042474246074LL,
  6747416980771394035LL,
  400696716879216832LL,
  2422265606441177773LL,
  -1011633141863401400LL,
  8626283077993020639LL,
  4765126592770529909LL,
  -1873498432290680784LL,
  -4624402497502186738LL,
  1732773238847425868LL,
  -4328825454563486923LL,
  3861721930727907297LL,
  3321148786075780300LL,
  6172086474637624407LL,
  8808263825594678914LL,
  -5017751159740013396LL,
  -1841000738036573909LL,
  1837892616006426784LL,
  -6639189309975998511LL,
  6064706570005127782LL,
  831064228471279787LL,
  -4040029953490270518LL,
  2887955510789191891LL,
  -4873353419196277127LL,
  -5918895985025938659LL,
  7219888501265850427LL,
  1443977756440934990LL,
  866386620789115355LL,
  -5162148937340803988LL,
  -4768235108375018915LL,
  -4295762628318548753LL,
  3572926411961373096LL,
  -974614202886593481LL,
  -8374223313418328270LL,
  -2707387166384244903LL,
  -5632361133457437300LL,
  5307960417992986322LL,
  -6028535415816033100LL,
  -3464699514580569081LL,
  -830216427596847697LL,
  1155182204795399913LL,
  -1119011980016174379LL,
  6642297568823158127LL,
  6315919082392394788LL,
  -7510097275457850169LL,
  -6783021935683377117LL,
  4476330785123890332LL,
  7042994147389295581LL,
  1119859761417522054LL,
  8952661566213353616LL,
  -3718171560109558332LL,
  433193311588075736LL,
  2021568757368288769LL,
  7653081827185353123LL,
  7618324550603200505LL,
  -7367394931305445743LL,
  2854893732180598055LL,
  -1443130916428717768LL,
  2454762183366056115LL,
  -6207691532763829829LL,
  3716759037594734217LL,
  4331933023719889846LL,
  -3897327678503024750LL,
  -6172933158257897911LL,
  -6494226421221152648LL,
  6458621427209505935LL,
  1877171012278782336LL,
  -2889369011076743206LL,
  -3429375974540426554LL,
  2997596010370384323LL,
  2165966512990268791LL,
  -6317330933910294325LL,
  3176751027361397718LL
}; // weak
__int64 TE1[128] =
{
  -8865198991679790237LL,
  -8217244699297155209LL,
  -4767505047984540942LL,
  6093869223692365679LL,
  216736837222281264LL,
  9031453569270572903LL,
  7112828505354141438LL,
  -7283316244322866261LL,
  -7124832589452424502LL,
  -8648462157504460343LL,
  -1463008689786979590LL,
  863548670535944007LL,
  7472550217699667373LL,
  -1565652125036338526LL,
  -624974895785993060LL,
  6601081612239401586LL,
  2081223769152927671LL,
  7659539013824254867LL,
  4719279000863389238LL,
  5729648327540471799LL,
  -841709525703052236LL,
  646814039779960293LL,
  8334993960084861297LL,
  4551473554748748081LL,
  5950882140354249732LL,
  6817820654893933347LL,
  -6829824738220566504LL,
  -5390920242471238395LL,
  3901263045880317703LL,
  4458531619012837504LL,
  7588045471274101739LL,
  -6923592328309394766LL,
  -7053336842274207479LL,
  3329314725457046572LL,
  -5558446420598056165LL,
  -334497134248961446LL,
  5581713916899381842LL,
  -3567497746787608874LL,
  4530027366370650409LL,
  -7560553645052383441LL,
  7546293350335664979LL,
  3225120410337017856LL,
  2297958395765858336LL,
  -1317765391195721295LL,
  5083943630311549546LL,
  5436470620812983998LL,
  -3127666048173323702LL,
  5369926622578235480LL,
  3082133331588665552LL,
  1652253697617406634LL,
  -2910929212719873213LL,
  -7777288273782230221LL,
  1218780025504548165LL,
  -9079679615193841150LL,
  4933759624217120848LL,
  -2068374161215610977LL,
  -117758090679266991LL,
  -8501230874019217344LL,
  -4890454419805597038LL,
  356336279370086456LL,
  -4505932005612733252LL,
  7152315585809865434LL,
  1938236686142017552LL,
  7908271270322369523LL,
  1447920527162330573LL,
  3441855039489315603LL,
  -6758328987216355489LL,
  4120256100073751620LL,
  -984696606339906364LL,
  5150496458251468414LL,
  -1748982848367860636LL,
  -7645296378931701479LL,
  -7486810511779012512LL,
  9197437704988413775LL,
  9102947108415349282LL,
  -9003952909427437424LL,
  3010637585288283718LL,
  4334736721023056056LL,
  -2108706765655253282LL,
  8551733001578744587LL,
  6225155776266887392LL,
  2167368360054241850LL,
  723960266623174985LL,
  -1965719685323807708LL,
  7979767019532698306LL,
  -6429906190917718868LL,
  -6615341907625537135LL,
  -8362487996538952476LL,
  4867204584475846631LL,
  -5198722503426558153LL,
  7255815587824242061LL,
  -2285108789065789874LL,
  -383836937832076180LL,
  2724654590459114740LL,
  -8145751158652508827LL,
  1733894690904190638LL,
  -8579224829768516934LL,
  8240512195151668517LL,
  -1056192355092653028LL,
  5879386395607545012LL,
  8980703073811556584LL,
  2395386272444609652LL,
  -2566561689460389045LL,
  -8858709614455321717LL,
  4790772542054887536LL,
  -6139419600444344907LL,
  361980135103023176LL,
  1304933445355960054LL,
  6875366285631447393LL,
  -3429005432856881321LL,
  6384346983881475718LL,
  -5104937250501026531LL,
  1435514651237016033LL,
  3684526212860582040LL,
  8118259332494289257LL,
  -6398607279888298354LL,
  2466879813146483611LL,
  2362676668920072071LL,
  -24113023202898226LL,
  8837715994673686568LL,
  -551231764114994036LL,
  1664657363900664201LL,
  3591593107032227775LL,
  -5129485176358550974LL,
  -5752898163299958463LL,
  1233439907047419181LL,
  -240849856896454480LL,
  4191749640517696443LL
}; // weak
__int64 TE2[128] =
{
  8972569570760312419LL,
  8903043048529325687LL,
  7763597091767451634LL,
  -4227593873025540497LL,
  72904223096856624LL,
  3133755660142497383LL,
  -2926576868551694338LL,
  8546403237323230635LL,
  -9035030630835580982LL,
  9045473794751236553LL,
  6479468588420165626LL,
  -1149548270447915449LL,
  -3141344489035251283LL,
  -5770723351865761886LL,
  -6559682365772323940LL,
  -4585900544688200590LL,
  -208043002939935305LL,
  2768108649542729107LL,
  4558063107835587638LL,
  -3724613450471377417LL,
  -6488471393144117196LL,
  -1078337293427355163LL,
  -2849745193088589199LL,
  1530988668839486001LL,
  -4084037230072690684LL,
  -4368881127064779229LL,
  -7592726378980036584LL,
  -7298875282527679995LL,
  1312276001271123463LL,
  -2144311683645236352LL,
  2839886973389688299LL,
  8475750814966448050LL,
  -8962693759790018039LL,
  1886502580934432812LL,
  7976680255487030811LL,
  -6846778064208219046LL,
  4273201618611512402LL,
  -5490312684368709674LL,
  -2071974810820586967LL,
  -8892617480767185361LL,
  -3357229213496793517LL,
  -1356496164093952000LL,
  -279253979859238880LL,
  6624151131872852401LL,
  -3799193330831666070LL,
  4128519075755812798LL,
  5536217295386219594LL,
  -3509845833985511336LL,
  -1212939515838350384LL,
  -353833855542276182LL,
  5609121517553419843LL,
  -8821406506322401741LL,
  -499642301810767291LL,
  9187904536260772866LL,
  4342728141143318608LL,
  -6277090268017973857LL,
  -6629758645406817711LL,
  -8103658466674966464LL,
  -7080747553478852718LL,
  -791241598996811720LL,
  -5277805661565000772LL,
  2405839339112476634LL,
  -64486360992440304LL,
  -3283757640541340173LL,
  870347071252365773LL,
  -1427707138151209453LL,
  -7520389503490343329LL,
  1673419414018230340LL,
  -6344914745969634364LL,
  4415632364194167934LL,
  6766581875431229540LL,
  8328816467909489177LL,
  -9108502202894204832LL,
  -2558145896695882161LL,
  3061977336601003042LL,
  -8610025381431657584LL,
  -1285276392433742778LL,
  1458084448826518456LL,
  6837234299947296734LL,
  -2632725775855577589LL,
  3627196719566674912LL,
  729042223841047610LL,
  435173535143137865LL,
  6693677652066977828LL,
  -3211420767071920190LL,
  7108584989938107308LL,
  -7663946147414132335LL,
  8758360505685890020LL,
  -4016212747695761945LL,
  7906027833487093303LL,
  -3070133515198922355LL,
  -6205879294995817394LL,
  6267511318630422636LL,
  -1574623886672661516LL,
  8831264725849328229LL,
  583233781224785838LL,
  8685456282705293242LL,
  3346838826356001317LL,
  -6417251623077922788LL,
  -4156374101393968204LL,
  -2486934919760851992LL,
  2243142394228566132LL,
  -4765826840015301045LL,
  -8465342836852257397LL,
  4486284786432794736LL,
  7397950079417741749LL,
  217586768174157896LL,
  1013903716694226934LL,
  3845909384301625953LL,
  -5057426136771219881LL,
  -4514689569973528698LL,
  -7009527784978040291LL,
  -570853273582904863LL,
  1239371781428685720LL,
  -2778534218840681879LL,
  -7735157121846016114LL,
  2171364072505093531LL,
  -1648095458778540665LL,
  6196858897794959310LL,
  -2343378278217658328LL,
  -6775567088996121716LL,
  943251292401174921LL,
  -1859467785066093121LL,
  7546010321874748482LL,
  -7372346851795434943LL,
  1085682035709139501LL,
  6123954675013876656LL,
  1601641093635927483LL
}; // weak
__int64 TE3[128] =
{
  8970190760169416134LL,
  8897861578506344942LL,
  7740489115462929919LL,
  -4195854494209166882LL,
  72342376947929184LL,
  3110717777684965838LL,
  -2893735701755586073LL,
  8536180485010482765LL,
  -9042492341934733937LL,
  9042533138026807433LL,
  6438436296250234351LL,
  -1085354336335705714LL,
  -3110747421195506623LL,
  -5787149411345564321LL,
  -6582864816826958045LL,
  -4557542099700771100LL,
  -144927857230495115LL,
  2749001499682254397LL,
  4557433358253447788LL,
  -3689486566503808267LL,
  -6510529056688808856LL,
  -1013018571784768303LL,
  -2821377986367745054LL,
  1519189899753902946LL,
  -4051178539825492984LL,
  -4340521586862627514LL,
  -7595706463514253264LL,
  -7306328231902900470LL,
  1302162770628118798LL,
  -2098046446258316517LL,
  2821339479257327309LL,
  8463846905081941375LL,
  -8970152185249064174LL,
  1880866595994891352LL,
  7957494250027560246LL,
  -6872216661590872396LL,
  4268090242446325412LL,
  -5497823680218635849LL,
  -2025706287787312302LL,
  -8897820855125446306LL,
  -3327763504473836122LL,
  -1302335509799895040LL,
  -217263621663596480LL,
  6583116649253161081LL,
  -3761835490548203820LL,
  4123409890032933223LL,
  5498002995659071124LL,
  -3472474851106035536LL,
  -1157659550110684229LL,
  -289612541008878257LL,
  5570345371680621958LL,
  -8825485093168261786LL,
  -434297294988914806LL,
  9187204692814464516LL,
  4340419424412561568LL,
  -6293530565310170587LL,
  -6655196147573722206LL,
  -8102105449458777984LL,
  -7089303386032263873LL,
  -723666731046254480LL,
  -5280820793033564317LL,
  2387298416273814959LL,
  -251903857381344LL,
  -3255418909436473603LL,
  868090924940217473LL,
  -1374671271369689818LL,
  -7523366302382038594LL,
  1663861458213325960LL,
  -6365853098077562989LL,
  4412761801326756604LL,
  6727788206095641800LL,
  8319157752179534642LL,
  -9114836936062623552LL,
  -2532008551102033506LL,
  3038379798429853252LL,
  -8608486602025424069LL,
  -1229999712371291508LL,
  1446847525888250731LL,
  6800121788196878759LL,
  -2604357473917002474LL,
  3617048442001439707LL,
  723423762371464820LL,
  434045461973293970LL,
  6655445828878232648LL,
  -3183078750325416545LL,
  7089412127075397443LL,
  -7668042257725806535LL,
  8753181226109777875LL,
  -3978856002611629355LL,
  7885160670452734318LL,
  -3038411659844678655LL,
  -6221194804784606564LL,
  6221435553719301336LL,
  -1519360349186029581LL,
  8825523600110235594LL,
  578739012046350663LL,
  8680838849262966127LL,
  3327722914759143242LL,
  -6438193260843490248LL,
  -4123518696797452429LL,
  -2459672786635316277LL,
  2242547691579284712LL,
  -4774417719509656170LL,
  -8463806247877245427LL,
  4485095381137395936LL,
  7378772835691775089LL,
  217022731584592016LL,
  1012766881816642039LL,
  3834075568280806338LL,
  -5063787155145885266LL,
  -4485206337474096873LL,
  -7016967591753341126LL,
  -506633054251304743LL,
  1229820396930904875LL,
  -2749042224612656174LL,
  -7740378019666163449LL,
  2170209714143213101LL,
  -1591704943364435435LL,
  6149101975323429255LL,
  -2314996833683212208LL,
  -6799880898858938621LL,
  940433299934380041LL,
  -1808703397637924251LL,
  7523466382126794372LL,
  -7378672823234870398LL,
  1085104856524224346LL,
  6076759598678723451LL,
  1591523482124670573LL
}; // weak
__int64 Te4_3_0[128] =
{
  8935141662364008448LL,
  8863084068661624832LL,
  7710162566118375424LL,
  -4251398046375477248LL,
  72057594843234304LL,
  3098476545358954496LL,
  -2954361351293632512LL,
  8502796099344400384LL,
  -9079256845389922304LL,
  9007199258113212416LL,
  6413125873569890304LL,
  -1152921503415664640LL,
  -3170534134766370816LL,
  -5836665114354253824LL,
  -6629298648872124416LL,
  -4611686016514785280LL,
  -216172779043553280LL,
  2738188575907512320LL,
  4539628425295429632LL,
  -3746994885828280320LL,
  -6557241056579026944LL,
  -1080863906726936576LL,
  -2882303759621292032LL,
  1513209475618570240LL,
  -4107282860094783488LL,
  -4395513235726401536LL,
  -7638104967617708032LL,
  -7349874591784763392LL,
  1297036692800143360LL,
  -2161727818990354432LL,
  2810246171421835264LL,
  8430738505423912960LL,
  -9007199254589997056LL,
  1873497445724323840LL,
  7926335344625057792LL,
  -6917529026131132416LL,
  4251398049613479936LL,
  -5548434737330126848LL,
  -2089670226412044288LL,
  -8935141659914534912LL,
  -3386706918390104064LL,
  -1369094286720630784LL,
  -288230375614840832LL,
  6557241060421009408LL,
  -3819052482231795712LL,
  4107282863349563392LL,
  5476377148124037120LL,
  -3530822106382073856LL,
  -1224979095155113984LL,
  -360287967337512960LL,
  5548434742044524544LL,
  -8863084065809498112LL,
  -504403157107867648LL,
  9151314442850402304LL,
  4323455643617853440LL,
  -6341068272670081024LL,
  -6701356244168343552LL,
  -8142508125212114944LL,
  -7133701807305392128LL,
  -792633533477683200LL,
  -5332261955652550656LL,
  2377900606909054976LL,
  -72057593769492480LL,
  -3314649321667821568LL,
  864691131894464512LL,
  -1441151880439791616LL,
  -7566047372388597760LL,
  1657324664013193216LL,
  -6413125866087251968LL,
  4395513238427533312LL,
  6701356247205019648LL,
  8286623314781143040LL,
  -9151314441206235136LL,
  -2594073384040005632LL,
  3026418950163398656LL,
  -8646911282135433216LL,
  -1297036691508297728LL,
  1441151883845566464LL,
  6773413843289767936LL,
  -2666130979218784256LL,
  3602879705654493184LL,
  720575941352357888LL,
  432345565452304384LL,
  6629298652093349888LL,
  -3242591728451977216LL,
  7061644218602618880LL,
  -7710162559625592832LL,
  8718968882414485504LL,
  -4035225262248427520LL,
  7854277751056891904LL,
  -3098476541265313792LL,
  -6269010679991107584LL,
  6196953089073741824LL,
  -1585267064740773888LL,
  8791026474321707008LL,
  576460755222659072LL,
  8646911287671914496LL,
  3314649326365442048LL,
  -6485183462943752192LL,
  -4179340451179921408LL,
  -2522015787435163648LL,
  2233785417121923072LL,
  -4827858799282880512LL,
  -8502796094143463424LL,
  4467570832230580224LL,
  7349874594905325568LL,
  216172783321743360LL,
  1008806320658186240LL,
  3819052485637570560LL,
  -5116089175233265664LL,
  -4539628422141313024LL,
  -7061644215230398464LL,
  -576460748528549888LL,
  1224979101194911744LL,
  -2810246165717581824LL,
  -7782220153713852416LL,
  2161727823738306560LL,
  -1657324660607418368LL,
  6124895496679981056LL,
  -2377900602580533248LL,
  -6845471431254343680LL,
  936748724791541760LL,
  -1873497441781678080LL,
  7493989781051801600LL,
  -7421932184816058368LL,
  1080863911323893760LL,
  6052837902138736640LL,
  1585267071971753984LL
}; // weak
__int64 Te4_2_0[128] =
{
  34902897118609408LL,
  34621422143209472LL,
  30117822523899904LL,
  55450570419273728LL,
  281474979856384LL,
  12103424005308416LL,
  60517120009437184LL,
  33214047263064064LL,
  36591746985623552LL,
  35184372102004736LL,
  25051272943632384LL,
  67553994415210496LL,
  59672695073996800LL,
  49258120934981632LL,
  46161896190771200LL,
  54043195535917056LL,
  71213169119789056LL,
  10696049124638720LL,
  17732923536310272LL,
  57420895265161216LL,
  46443371160666112LL,
  67835469402275840LL,
  60798594976907264LL,
  5910974514135040LL,
  56013520365682688LL,
  54887620460871680LL,
  42221246508171264LL,
  43347146413768704LL,
  5066549581250560LL,
  63613344744996864LL,
  10977524107116544LL,
  32932572286812160LL,
  36873221949685760LL,
  7318349397360640LL,
  30962247439941632LL,
  45035996279603200LL,
  16607023631302656LL,
  50384020845232128LL,
  63894819716005888LL,
  37154696928886784LL,
  58828270137966592LL,
  66709569480425472LL,
  70931694133182464LL,
  25614222892269568LL,
  57139420279209984LL,
  16044073684959232LL,
  21392098234859520LL,
  58265320184872960LL,
  67272519447478272LL,
  70650219165515776LL,
  21673573211111424LL,
  37436171905859584LL,
  70087269205475328LL,
  35747322042384384LL,
  16888498607882240LL,
  47287796097810432LL,
  45880421209145344LL,
  40250921673818112LL,
  44191571353141248LL,
  68961369297780736LL,
  51228445773660160LL,
  9288674245738496LL,
  71776119062265856LL,
  59109745125163008LL,
  3377699733962752LL,
  66428094504960000LL,
  42502721489534976LL,
  6473924468801536LL,
  47006321123524608LL,
  17169973587607552LL,
  26177172840644608LL,
  32369622323363840LL,
  36310272001966080LL,
  61924494881521664LL,
  11821949024075776LL,
  38280596842086400LL,
  66991044461723648LL,
  5629499546271744LL,
  26458647825350656LL,
  61643019900354560LL,
  14073748850212864LL,
  2814749770907648LL,
  1688849865048064LL,
  25895697859739648LL,
  59391220098662400LL,
  27584547728916480LL,
  41939771539390464LL,
  34058472196931584LL,
  56294995357270016LL,
  30680772465065984LL,
  59954170048610304LL,
  47569271069212672LL,
  24206848004194304LL,
  65865144566284288LL,
  34339947165319168LL,
  2251799825088512LL,
  33776997217468416LL,
  12947848931115008LL,
  46724846135803904LL,
  55732045400506368LL,
  62205969868259328LL,
  8725724285632512LL,
  53198770603229184LL,
  38843546795180032LL,
  17451448563400704LL,
  28710447636348928LL,
  844424934850560LL,
  3940649690071040LL,
  14918173772021760LL,
  52072870697172992LL,
  54324670513938432LL,
  44473046322184192LL,
  69805794238988288LL,
  4785074614042624LL,
  61080069953093632LL,
  41658296562483200LL,
  8444249311477760LL,
  65583669582430208LL,
  23925373033906176LL,
  62768919809097728LL,
  45317471259590656LL,
  3659174706216960LL,
  64739244655968256LL,
  29273397582233600LL,
  43065671440990208LL,
  4222124653608960LL,
  23643898055229440LL,
  6192449499889664LL
}; // weak
__int64 Te4_1_0[128] =
{
  136339441869568LL,
  135239930246912LL,
  117647744233984LL,
  216603790700288LL,
  1099511640064LL,
  47279000020736LL,
  236395000036864LL,
  129742372121344LL,
  142936511662592LL,
  137438953523456LL,
  97856534936064LL,
  263882790684416LL,
  233096465132800LL,
  192414534902272LL,
  180319906995200LL,
  211106232562176LL,
  278176441874176LL,
  41781441893120LL,
  69269232563712LL,
  224300372129536LL,
  181419418596352LL,
  264982302352640LL,
  237494511628544LL,
  23089744195840LL,
  218802813928448LL,
  214404767425280LL,
  164926744172544LL,
  169324790678784LL,
  19791209301760LL,
  248489627910144LL,
  42880953543424LL,
  128642860495360LL,
  144036023240960LL,
  28587302333440LL,
  120946279062272LL,
  175921860467200LL,
  64871186059776LL,
  196812581426688LL,
  249589139515648LL,
  145135534878464LL,
  229797930226432LL,
  260584255782912LL,
  277076930207744LL,
  100055558172928LL,
  223200860465664LL,
  62672162831872LL,
  83562883729920LL,
  227598906972160LL,
  262783279091712LL,
  275977418615296LL,
  84662395355904LL,
  146235046507264LL,
  273778395333888LL,
  139637976728064LL,
  65970697687040LL,
  184717953507072LL,
  179220395348224LL,
  157230162788352LL,
  172623325598208LL,
  269380348819456LL,
  200111116303360LL,
  36283883772416LL,
  280375465086976LL,
  230897441895168LL,
  13194139585792LL,
  259484744160000LL,
  166026255818496LL,
  25288767456256LL,
  183618441888768LL,
  67070209326592LL,
  102254581408768LL,
  126443837200640LL,
  141837000007680LL,
  241892558130944LL,
  46179488375296LL,
  149533581414400LL,
  261683767428608LL,
  21990232602624LL,
  103354093067776LL,
  240793046485760LL,
  54975581446144LL,
  10995116292608LL,
  6597069785344LL,
  101155069764608LL,
  231996953510400LL,
  107752139566080LL,
  163827232575744LL,
  133040907019264LL,
  219902325614336LL,
  119846767441664LL,
  234195976752384LL,
  185817465114112LL,
  94558000016384LL,
  257285720962048LL,
  134140418614528LL,
  8796093066752LL,
  131941395380736LL,
  50577534887168LL,
  182518930217984LL,
  217703302345728LL,
  242992069797888LL,
  34084860490752LL,
  207807697668864LL,
  151732604668672LL,
  68169720950784LL,
  112150186079488LL,
  3298534901760LL,
  15393162851840LL,
  58274116296960LL,
  203409651160832LL,
  212205744195072LL,
  173722837196032LL,
  272678883746048LL,
  18691697711104LL,
  238594023254272LL,
  162727720947200LL,
  32985348872960LL,
  256186209306368LL,
  93458488413696LL,
  245191093004288LL,
  177021372107776LL,
  14293651196160LL,
  252887674437376LL,
  114349209305600LL,
  168225279066368LL,
  16492674428160LL,
  92358976778240LL,
  24189255858944LL
}; // weak
__int64 Te4_0_0[128] =
{
  532575944803LL,
  528280977527LL,
  459561500914LL,
  846108557423LL,
  4294967344LL,
  184683593831LL,
  923417968894LL,
  506806141099LL,
  558345748682LL,
  536870912201LL,
  382252089594LL,
  1030792151111LL,
  910533066925LL,
  751619276962LL,
  704374636700LL,
  824633720946LL,
  1086626726071LL,
  163208757395LL,
  270582939702LL,
  876173328631LL,
  708669603892LL,
  1035087118565LL,
  927712936049LL,
  90194313265LL,
  854698491908LL,
  837518622755LL,
  644245094424LL,
  661424963589LL,
  77309411335LL,
  970662609024LL,
  167503724779LL,
  502511173810LL,
  562640715785LL,
  111669149740LL,
  472446402587LL,
  687194767450LL,
  253403070546LL,
  768799146198LL,
  974957576233LL,
  566935683119LL,
  897648164947LL,
  1017907249152LL,
  1082331758624LL,
  390842024113LL,
  871878361194LL,
  244813136062LL,
  326417514570LL,
  889058230360LL,
  1026497183952LL,
  1078036791466LL,
  330712481859LL,
  571230650419LL,
  1069446856773LL,
  545460846594LL,
  257698037840LL,
  721554505887LL,
  700079669329LL,
  614180323392LL,
  674309865618LL,
  1052266987576LL,
  781684048060LL,
  141733920986LL,
  1095216660496LL,
  901943132403LL,
  51539607757LL,
  1013612281875LL,
  648540061791LL,
  98784247876LL,
  717259538628LL,
  261993005182LL,
  399431958628LL,
  493921239065LL,
  554050781280LL,
  944892805199LL,
  180388626466LL,
  584115552400LL,
  1022202216518LL,
  85899346104LL,
  403726926046LL,
  940597837835LL,
  214748365024LL,
  42949673018LL,
  25769803849LL,
  395136991268LL,
  906238099650LL,
  420906795180LL,
  639950127249LL,
  519691043044LL,
  858993459431LL,
  468151435319LL,
  914828034189LL,
  725849473102LL,
  369367187564LL,
  1005022347508LL,
  523986010213LL,
  34359738542LL,
  515396075706LL,
  197568495653LL,
  712964571164LL,
  850403524788LL,
  949187772648LL,
  133143986292LL,
  811748819019LL,
  592705486987LL,
  266287972464LL,
  438086664373LL,
  12884901960LL,
  60129542390LL,
  227633266785LL,
  794568949847LL,
  828928688262LL,
  678604832797LL,
  1065151889633LL,
  73014444184LL,
  932007903337LL,
  635655159950LL,
  128849019035LL,
  1000727380103LL,
  365072220366LL,
  957777707048LL,
  691489734796LL,
  55834574985LL,
  987842478271LL,
  446676598850LL,
  657129996353LL,
  64424509485LL,
  360777253040LL,
  94489280699LL
}; // weak
__int64 rcon_0[6] =
{
  144115188092633088LL,
  576460752370532352LL,
  2305843009482129408LL,
  -9223372035781033984LL,
  3891110078501093376LL,
  0LL
}; // weak
__int64 Tks0_0[128] =
{
  1011353932217188352LL,
  1304661930846329366LL,
  3903839292608492588LL,
  3044190567093644858LL,
  9097664131305597016LL,
  7085146574931259982LL,
  5072550162420489332LL,
  6518761901403293282LL,
  -1253727222249762640LL,
  -969461744688510298LL,
  -2972963340609592164LL,
  -3841584218448134518LL,
  -7002615956592084760LL,
  -9024175759156403458LL,
  -6416079365226066748LL,
  -4978839503689050414LL,
  -3084202198964651141LL,
  -3953970915403259539LL,
  -1362758501773897897LL,
  -1079570562787994303LL,
  -6522656844036189405LL,
  -5086600005471319755LL,
  -7106985856571283697LL,
  -9129658382083031783LL,
  3864764406058150859LL,
  3004038210687431133LL,
  974557542997778407LL,
  1266717634307376625LL,
  5037855180438307731LL,
  6482954265193159045LL,
  9065247646654347199LL,
  7051546998588386729LL,
  -6665502280831832586LL,
  -4652927687291799584LL,
  -7252109515819951654LL,
  -8698334586381253684LL,
  -3227101099513215570LL,
  -3520386970470834248LL,
  -1507794887287099006LL,
  -648194402844922988LL,
  4895325166040956230LL,
  6916871911904683856LL,
  8920369041059776874LL,
  7483186491566363516LL,
  3722181202785785118LL,
  3437867209273954056LL,
  829766486016581938LL,
  1698409216648739620LL,
  8666292953524590221LL,
  7230179077794024603LL,
  4638971130930651809LL,
  6661656988020897975LL,
  580036218189102805LL,
  1449782806956188867LL,
  3470172987383188217LL,
  3187033289469956335LL,
  -7434304755794639299LL,
  -8879390783848899541LL,
  -6849905649393257967LL,
  -4836262313370900473LL,
  -1685362832577302939LL,
  -824588121256004493LL,
  -3406877173390157239LL,
  -3699059117493370785LL,
  5757245177100819191LL,
  6050531030878568673LL,
  8629428845554495195LL,
  7769828412651926733LL,
  4588534307288822447LL,
  2575959696568920249LL,
  543118646466086531LL,
  1989343768566996117LL,
  -5803724519088846265LL,
  -5519410542756884399LL,
  -7543191410886295957LL,
  -8411834089978846083LL,
  -2360608494472371681LL,
  -4382155257515968503LL,
  -1794302676543973837LL,
  -357120075510952923LL,
  -7719613381592260212LL,
  -8589360021898953830LL,
  -5982371660811774560LL,
  -5699231945718673482LL,
  -1975386026794874412LL,
  -539272202603916350LL,
  -2543917015740236296LL,
  -4566602855650613266LL,
  8380842903043813692LL,
  7520068140182907690LL,
  5506363145073165584LL,
  5798545106356248326LL,
  290152799387244900LL,
  1735238775901897586LL,
  4333272370693008712LL,
  2319629051850520414LL,
  -2120146443479369983LL,
  -107628869925163753LL,
  -2686522355274054867LL,
  -4132734145796466373LL,
  -7864461072012222631LL,
  -8157769053461494449LL,
  -6124923536592671883LL,
  -5265274862617431709LL,
  145076960304457649LL,
  2166636780048645543LL,
  4190421977505092509LL,
  2753182064428468619LL,
  8235679515347652585LL,
  7951414054966269439LL,
  5363565940760263621LL,
  6232186767059198419LL,
  4154572242025516154LL,
  2718515455000254060LL,
  111522764586039382LL,
  2134195272917918272LL,
  5323370385572979746LL,
  6193139153551195700LL,
  8197779499921526798LL,
  7914591543755754008LL,
  -2794252938314130230LL,
  -4239351971529373988LL,
  -2225651305746667290LL,
  -211950674860576016LL,
  -6237281414317230958LL,
  -5376555167406903676LL,
  -7974593228964003650LL,
  -8266753337453471064LL
}; // weak
__int64 Tks1_0[128] =
{
  796584135530381312LL,
  2094766562649838106LL,
  2825495539799827508LL,
  3542713477412173358LL,
  6016318055744817256LL,
  4999650342576347762LL,
  9171129091704253532LL,
  7591511287542349382LL,
  -4904813766725562160LL,
  -5912509671167720758LL,
  -7505602917909875484LL,
  -9094263036981238018LL,
  -2044966601558533960LL,
  -755826626787489118LL,
  -3483827324025504628LL,
  -2757637165369987434LL,
  8130460461630700475LL,
  7406486987135723937LL,
  6696032970045857679LL,
  5400102480182709653LL,
  2928882227174142931LL,
  4519758892967092681LL,
  332526306871957479LL,
  1333431558783059453LL,
  -4596589282281673877LL,
  -3014684425215035023LL,
  -1437345015440109729LL,
  -427397448619549371LL,
  -7474310658229804285LL,
  -8189241680376368871LL,
  -5440966023078230217LL,
  -6745868733984549587LL,
  -170152306477795731LL,
  -1459327379721591689LL,
  -3342977878389861799LL,
  -4069203135518130109LL,
  -6497789326932492795LL,
  -5490128658402038753LL,
  -8508685920371402191LL,
  -6920061037211744213LL,
  5567557104732858045LL,
  6584260088172770471LL,
  7024431813396644489LL,
  8604084887829991571LL,
  1527749245270863573LL,
  229601950983896271LL,
  4110524135380454113LL,
  3393341330600597755LL,
  -8757173765660062250LL,
  -8042207645040746548LL,
  -6178832106432599582LL,
  -4873894297053529096LL,
  -2447692430963243586LL,
  -4029562052118177884LL,
  -995250953181585014LL,
  -2005163284090440816LL,
  3934127419251154182LL,
  2343215483186761500LL,
  1918797594930116914LL,
  917857072747572008LL,
  7991809489494250862LL,
  8715747831156737908LL,
  4814550686700056922LL,
  6110446043730715456LL,
  -265741137262547750LL,
  -1561632182146040128LL,
  -3425055807208044306LL,
  -4148989561845458188LL,
  -6611374689166973774LL,
  -5610429992276216152LL,
  -8644789727422480250LL,
  -7053873341771967844LL,
  5525763941933874186LL,
  6535680859867803152LL,
  6960121030414784574LL,
  8541994963716885028LL,
  1503988348049503330LL,
  199054988256552568LL,
  4100273865247756374LL,
  3385311919336653388LL,
  -8677066024239311007LL,
  -7959887531606620805LL,
  -6076206538670541995LL,
  -4778063831408647857LL,
  -2313559361065410807LL,
  -3893216610206970605LL,
  -874628304213984451LL,
  -1891335737240016601LL,
  3995905339759937457LL,
  2407275869575206315LL,
  1967064269874946949LL,
  959399289197326751LL,
  7999526347398057945LL,
  8725747154940206531LL,
  4844785645823077357LL,
  6133956544358660599LL,
  784351712221371063LL,
  2089249801742878893LL,
  2799751939646340739LL,
  3514678615286000793LL,
  5950059754202911455LL,
  4940107703436492997LL,
  9127388960101037803LL,
  7545479893966447857LL,
  -5043464875929761177LL,
  -6044374474347767683LL,
  -7621736132053291437LL,
  -9212617419231052727LL,
  -2129557197913570801LL,
  -833630917118373867LL,
  -3581928272591296965LL,
  -2857959282042178527LL,
  8158736924257263884LL,
  7432551386986558230LL,
  6701791881209021752LL,
  5412656252944881442LL,
  2975155771006249316LL,
  4563820374023469950LL,
  392310546512203088LL,
  1400010660022312778LL,
  -4478485880740826660LL,
  -2898863730072017978LL,
  -1305730643213216280LL,
  -289058308659935246LL,
  -7374238972510766668LL,
  -8091452701055161426LL,
  -5363412713456312960LL,
  -6661590656629911654LL
}; // weak
__int64 Tks2_0[128] =
{
  939860379054374912LL,
  1665507344808025106LL,
  4118319952030349348LL,
  2535854516033168438LL,
  7301318240286306376LL,
  9170844326749432922LL,
  5872489840271181932LL,
  5433973893727671422LL,
  -2469117626283925360LL,
  -4058321006976631678LL,
  -1614585830316386124LL,
  -900215593258793818LL,
  -5340250109543673640LL,
  -5767559971013227318LL,
  -9092865549078845188LL,
  -7216531424053906194LL,
  -5300502347544536261LL,
  -6023906309576997079LL,
  -9053100469771574497LL,
  -7472895629681623283LL,
  -2438517251274200205LL,
  -4305800334016137375LL,
  -1583968137998527657LL,
  -1147712787362246843LL,
  7403976759106550699LL,
  8995423143520446393LL,
  5975165676399559567LL,
  5258534843434737565LL,
  1051667384375659491LL,
  1481220249566402545LL,
  4230144274659767239LL,
  2351549553727598549LL,
  6988921964449934710LL,
  8857383569037767012LL,
  6111836985970300242LL,
  5676654604063639872LL,
  623136356732689726LL,
  1347648471866851628LL,
  4362276181752603930LL,
  2783214679136467208LL,
  -5742859252875768346LL,
  -6171233321342709260LL,
  -8907702473848097342LL,
  -7028034509308401200LL,
  -2876335991078109778LL,
  -4466673947512397380LL,
  -1425095146039278198LL,
  -707320700722734696LL,
  -2628281095471401395LL,
  -4498985703780423073LL,
  -1177057567740703127LL,
  -739614589926813061LL,
  -5485938445256423931LL,
  -6212693564111775209LL,
  -8650798983536886239LL,
  -7069476885013519821LL,
  808001219564771037LL,
  1234132284310522575LL,
  4547123727276551929LL,
  2669716358644085483LL,
  7164919815757751957LL,
  8753014768470850183LL,
  6287817519969984177LL,
  5572303670560670371LL,
  -2883341812784217620LL,
  -3608979827826008578LL,
  -2030991757124339256LL,
  -448552554787412518LL,
  -4637511701025050204LL,
  -6507099205520511562LL,
  -8392449618356270720LL,
  -7953889536728820334LL,
  525988173714006396LL,
  2115200230240664942LL,
  3702125268893931864LL,
  2987728523298178378LL,
  8004267892476465460LL,
  8431516061035777318LL,
  6573257752153680144LL,
  4696967487334774018LL,
  7820382960792410839LL,
  8543760379926972101LL,
  6389390687533572851LL,
  4809194488917835489LL,
  351250629019363999LL,
  2218577537607595661LL,
  3527405591263236795LL,
  3091088513356975785LL,
  -4884307389530211769LL,
  -6475780041964100011LL,
  -8639227439797484957LL,
  -7922587690480542095LL,
  -3139285987790419441LL,
  -3568794752256960995LL,
  -2286918065066593749LL,
  -408384796526498247LL,
  -5040967708829422438LL,
  -6909367895384919928LL,
  -8203629189494090562LL,
  -7768490942671370068LL,
  -3291406904611726126LL,
  -4015927970457747264LL,
  -1837843581776845578LL,
  -258755845500454684LL,
  7690461527336356874LL,
  8118897288713539608LL,
  6815699026652771374LL,
  4935987201907042364LL,
  207854199527537730LL,
  1798183480127873104LL,
  3949175764855112806LL,
  3231427828076730484LL,
  103927660332857249LL,
  1974588442795584435LL,
  3845231358596484997LL,
  3407850108052575127LL,
  7577669076129040361LL,
  8304450737882291195LL,
  6702888708381507533LL,
  5121557968383927263LL,
  -3332143411031779535LL,
  -3758318576501732573LL,
  -1878597955260846315LL,
  -1129134236306681LL,
  -5072837203725211783LL,
  -6660905888418317461LL,
  -8235516551453827235LL,
  -7520011618396634289LL
}; // weak
__int64 Tks3_0[128] =
{
  652189675796037632LL,
  1952060926938715676LL,
  3258678918868380728LL,
  4549534105941719588LL,
  4712264386958874736LL,
  6016603915916824172LL,
  7300756411386721352LL,
  8596080151153238612LL,
  -7359519582747316000LL,
  -8374533724984137988LL,
  -4762108182553912104LL,
  -5768123990346396988LL,
  -3335509677617649520LL,
  -4345985173295006068LL,
  -756094946312928088LL,
  -1757571832668040524LL,
  3654232208790223835LL,
  2354369719380831687LL,
  1621960989313292259LL,
  331096971787190783LL,
  8853558608534905771LL,
  7549227841310242231LL,
  6839284057946656659LL,
  5543951487727376783LL,
  -6744492055198971077LL,
  -5729469151228863193LL,
  -8767685981552777469LL,
  -7761679004213055201LL,
  -1581158993231637685LL,
  -570674735820995241LL,
  -3586355700940947597LL,
  -2584887645038597777LL,
  9178614919089485485LL,
  7888876904549703857LL,
  6581115420526893717LL,
  5282378796664867977LL,
  3965601798596964061LL,
  2671395231364004033LL,
  1386134428173042405LL,
  82929526495745273LL,
  -1175354548714387891LL,
  -168221568388345775LL,
  -3781790602911716747LL,
  -2765641833394242455LL,
  -6352303962685366723LL,
  -5349709903796697055LL,
  -8940708438499839483LL,
  -7929098315541830631LL,
  4961707265738837366LL,
  6251436518545333098LL,
  6984988740706017614LL,
  8283734195020805970LL,
  915319275323839750LL,
  2209517080823513882LL,
  2920569171908163902LL,
  4223782904038223650LL,
  -3167377882753909354LL,
  -4174519624813237366LL,
  -1135159302396178002LL,
  -2151299241460889678LL,
  -7177771435879930394LL,
  -8180374256501885958LL,
  -5163584983660869154LL,
  -6175186276166115390LL,
  -1884760498824874175LL,
  -590558329638428323LL,
  -4475504391785358471LL,
  -3172286295968527003LL,
  -5926837991343214799LL,
  -4637104374849947347LL,
  -8535579102948195575LL,
  -7236829284946635499LL,
  8432793143858579361LL,
  7430194686923395517LL,
  5851126544021220249LL,
  4839529615202745733LL,
  4426779907617595345LL,
  3419642529245039053LL,
  1827116638891553769LL,
  810981063513613813LL,
  -2419409826466394982LL,
  -3713620826105603450LL,
  -407387006971830110LL,
  -1710596341055375682LL,
  -7636732895099759382LL,
  -8926475342045789450LL,
  -5606713406716512046LL,
  -6905454462984786226LL,
  5673470603138514042LL,
  6676060229620935270LL,
  7676415579754139714LL,
  8688021270305900126LL,
  492140322526684170LL,
  1499268870446477846LL,
  2513082517786806322LL,
  3529226854898032174LL,
  -7801911436091391508LL,
  -9106246567002826768LL,
  -5220192197134832172LL,
  -6515529131040883768LL,
  -2606930443737540196LL,
  -3906797296833704064LL,
  -7179076642310748LL,
  -1298047457855183944LL,
  246213647572381964LL,
  1256693541296252688LL,
  2836869991919492404LL,
  3838333684135070504LL,
  5405131483160504700LL,
  6420150023443840864LL,
  8013819405890471236LL,
  9019822019543421784LL,
  -6340662308031923657LL,
  -5036318346667725781LL,
  -8343660473522563569LL,
  -7048332301349797869LL,
  -2276242739234069945LL,
  -976367055685143461LL,
  -4297272483107565953LL,
  -3006412863627978653LL,
  4094266674845108951LL,
  3083795611574000843LL,
  2082331953719731951LL,
  1080859499770867955LL,
  8122692356109800103LL,
  7107682646279226555LL,
  6092725506845752991LL,
  5086714131459516547LL
}; // weak
__int64 ORIG_P[10] =
{
  -8817193942522041720LL,
  247824715720788526LL,
  2999170649027065890LL,
  -1419077496771511656LL,
  4093793464262074854LL,
  3812592220735039183LL,
  -3928175861623412297LL,
  -5384324834043570763LL,
  -8540519122497776167LL,
  0LL
}; // weak
__int64 ORIG_S[512] =
{
  -7431021106289374298LL,
  -3451200186548915493LL,
  7648940202947358701LL,
  -1068338713358200763LL,
  -5507501060570113721LL,
  -8822837448418397470LL,
  8167082663806050520LL,
  -823246692746199389LL,
  8254735656317973647LL,
  -9073263486300336808LL,
  -4442139446454475747LL,
  3094641531298043193LL,
  2909472628438839331LL,
  -5126441148289550056LL,
  6933881027089259696LL,
  -5756011273018012021LL,
  -4813983892198033471LL,
  6152018660399460314LL,
  -6172839058233547277LL,
  7199026270926575714LL,
  3074569545919904106LL,
  1243530946625166388LL,
  8967486628324214447LL,
  7165152423763121169LL,
  8365491241204958557LL,
  -7239656115454329322LL,
  7792581252251826739LL,
  2924391380756353921LL,
  7731477347581249688LL,
  7361170509481895963LL,
  -350812854081287732LL,
  6767925293483863136LL,
  -1619759083881734819LL,
  -1484750227525721342LL,
  -3200180522495426943LL,
  -8938446547584192525LL,
  -6592108752239115134LL,
  -7052747661634899894LL,
  -654872861558282174LL,
  -6065353707035517855LL,
  -2858607737966583598LL,
  -6102039242420017368LL,
  1403500086179072876LL,
  9149953903381180496LL,
  4156542590120977693LL,
  -9060382046336427714LL,
  5003393308839282201LL,
  4290627242243171779LL,
  -8808723714101250600LL,
  6251395020039537823LL,
  3908973870556883554LL,
  4799432289379344242LL,
  -3455929662670055644LL,
  5328251609783724755LL,
  -9180276150200208695LL,
  -655028592709436968LL,
  -5298119666512801766LL,
  342280970154336445LL,
  4656546937903337398LL,
  1831316207683018434LL,
  4498062166451777455LL,
  4274738958136095467LL,
  -7264142180892258017LL,
  -5809998619477981884LL,
  -2435520519787524092LL,
  1814470933760387079LL,
  5028396594967062615LL,
  -5056421035066564807LL,
  1900574062715453629LL,
  4624196785020141766LL,
  -351382147604470274LL,
  -2652018779627329032LL,
  -188751970420058401LL,
  -5979282035077800248LL,
  -206172310699395590LL,
  4467816581186026312LL,
  -3859711734153717829LL,
  -2371310289023445458LL,
  2918050825144281334LL,
  -8336350429937454394LL,
  -4915018425537189968LL,
  -5120573393636187299LL,
  -206739281300275816LL,
  3301652390892844396LL,
  -5262379861899221895LL,
  5475136420262595004LL,
  -6572020469919255846LL,
  -3538484705811033279LL,
  3924689171467258586LL,
  3166346874470702846LL,
  -5868030229928486323LL,
  7751774270743481969LL,
  -5780609949204819504LL,
  -8181469631962391761LL,
  -1003692056038153477LL,
  -8066527352210278382LL,
  7534455227977391776LL,
  -5500933992539491951LL,
  -4751834756169129448LL,
  -8430140772200403458LL,
  -5372947286795957233LL,
  -3564068482915175466LL,
  -210577677941714976LL,
  -3265768502719136895LL,
  -9181301401590979994LL,
  2385241456459412244LL,
  8626076118600720485LL,
  -316037232327376139LL,
  8880686839465094924LL,
  -5900139606216467501LL,
  2337846897892724269LL,
  6321049119815631035LL,
  -1150184690705746277LL,
  6476078039689302301LL,
  -2784821612623346807LL,
  208777214047378338LL,
  7103812314203554678LL,
  5652943600611760488LL,
  8868899704499809738LL,
  -7326466998172319662LL,
  -4856069087042775233LL,
  -9086447654096821130LL,
  6276866825170612149LL,
  3030316812577729643LL,
  -1749092417117395679LL,
  -4213866884556913362LL,
  -6224098236946174627LL,
  7963779668068943769LL,
  -5353890158712557335LL,
  -4316376832410318546LL,
  5307456416753034928LL,
  -8075602403016154952LL,
  7609420908350573681LL,
  -4417575068819500436LL,
  8433355516628304549LL,
  -2010793192968350912LL,
  6575991464094111898LL,
  -7352337676498901802LL,
  -1159623072507847673LL,
  -1142403845663672090LL,
  -8903357913580625786LL,
  152784143145691590LL,
  4520188822745017151LL,
  7740122747398199316LL,
  5954007774290457988LL,
  -6174427153239878907LL,
  9213993598847517724LL,
  6275470005265843436LL,
  -1130390253539239369LL,
  144313099902263044LL,
  4374530923833390362LL,
  -2591503014313100712LL,
  8982763668891571193LL,
  2518997638690522995LL,
  4018014456983709057LL,
  -7281232459831347660LL,
  1139414067228991814LL,
  -6596332810719934658LL,
  4317278791609666969LL,
  1747030729348397985LL,
  5723434146782808888LL,
  -717755970360308477LL,
  2636712966731862672LL,
  -4850506887491637134LL,
  -2768860477355559137LL,
  -2535738595372978670LL,
  3344891549112758815LL,
  -6952205943704986138LL,
  8361172502585034520LL,
  -1636075996205573444LL,
  -2628661843954308038LL,
  -4295092836232510618LL,
  3609029308801882183LL,
  2648884138211490615LL,
  3055063774724705603LL,
  1385671474560630786LL,
  1166799907355359390LL,
  6850284663893424086LL,
  -2908261671905175823LL,
  6423440427618211899LL,
  -7497934849365252371LL,
  2167705241455869740LL,
  -1519560590997895824LL,
  6502681862506176010LL,
  5637669981347899164LL,
  -7356879515330159431LL,
  5937653222125504982LL,
  -7201058535540733576LL,
  -7718349006919364934LL,
  2164593099684985939LL,
  3899320397442010791LL,
  1856179450077062671LL,
  -643149747512432888LL,
  -1530344921474662802LL,
  -6450341572491065963LL,
  111744641019099089LL,
  -4725302241027498513LL,
  7542289137019658629LL,
  -2652737570855803633LL,
  6588255549592141229LL,
  2956438814410782248LL,
  7034364305413588853LL,
  -1485697949780629456LL,
  -6195998921568485858LL,
  5508080713015188624LL,
  -3770957556904714741LL,
  6945162675136267964LL,
  -5551948305007944533LL,
  1854903373607411375LL,
  7303355700693133753LL,
  4335476273270250034LL,
  -4602193972367005227LL,
  -8692052174205351143LL,
  7078952352320297342LL,
  -7502102560980367206LL,
  1614560811973186399LL,
  -4042508611296589783LL,
  8671886251325841313LL,
  1955250959235253413LL,
  1928181203136136191LL,
  5993781294863157995LL,
  7907249096037583076LL,
  3803008317340250863LL,
  -1262067326687580831LL,
  -1701032572088609575LL,
  2323316311671725332LL,
  -6653316476802506058LL,
  -375118385502204139LL,
  -1194936361706916798LL,
  4741482289749244701LL,
  -8753504780004984418LL,
  4431931607876323178LL,
  6596971114736558578LL,
  -4483414088591309920LL,
  7623314966800835779LL,
  6238244719361821195LL,
  -5973663591426179328LL,
  2558622931804664180LL,
  888886888327449898LL,
  2539872277272327230LL,
  -8245107674284101486LL,
  7791856120684866801LL,
  -3751933481792424103LL,
  -3569328730557517145LL,
  1871334287493005444LL,
  7050832949422315605LL,
  -4248012915381524566LL,
  -9220052764359087108LL,
  -4376033718724953554LL,
  1017425156216547077LL,
  1176997295358729171LL,
  -2060604360607409543LL,
  8160022078523978853LL,
  -1049902624177325922LL,
  -8092901141991513625LL,
  -2629066477212033237LL,
  -7745838450586330520LL,
  -7752619976094374372LL,
  8503593204842832119LL,
  -3124934741594969298LL,
  3684213234850079857LL,
  5764714928417526967LL,
  -7555838107363707346LL,
  -4644468781905522828LL,
  -7586997815128753123LL,
  7394962563436699091LL,
  269538153415313900LL,
  3588107782499102160LL,
  6196171616199321523LL,
  -6068025887727204378LL,
  5981952333679982629LL,
  -1605975292136093990LL,
  -2933979706751708062LL,
  2855719693817135268LL,
  -1691192316917711198LL,
  8859942447162908678LL,
  -3227005137905115524LL,
  4641850307355648921LL,
  -2741700717969498571LL,
  4256550960722728875LL,
  5435027083452087031LL,
  -1521205457712880079LL,
  -2496327685439030668LL,
  -3857296816684800009LL,
  -2810611652099377842LL,
  -5023601388433680724LL,
  2342871843554409018LL,
  -3416379113690388041LL,
  -6839390754907330628LL,
  -7358359300623226525LL,
  4553462452413155926LL,
  -8058855647964004836LL,
  299260062587217922LL,
  371596324887008604LL,
  -1961760592364497592LL,
  -4102912454839692481LL,
  4684903661790939630LL,
  6739757888501676452LL,
  -3054635917514944021LL,
  4688587394754855311LL,
  6929522913422571572LL,
  2261770304847669411LL,
  207502712258081986LL,
  -3832139877778567215LL,
  3692550241353176544LL,
  -1243352890435433630LL,
  -1821128146293710322LL,
  3288462432836652172LL,
  -7658488823481731003LL,
  -1743748542872352670LL,
  -8683704266507312273LL,
  -919142110666556121LL,
  -7408703463124147391LL,
  -794606920585482581LL,
  -6779080754034713504LL,
  -2635051025817673396LL,
  7883318290800137488LL,
  -2535271485671091141LL,
  -3746713810381500985LL,
  7570530133173931922LL,
  -3541098571309015045LL,
  -2804231136101675253LL,
  5862469713221857160LL,
  8519639426980411839LL,
  -3742111438215762253LL,
  -851689208033123689LL,
  -4149456572516225625LL,
  8660124236351098060LL,
  -5219018963810106825LL,
  5475078967358766054LL,
  1282098702381158424LL,
  -2098180682296214056LL,
  725663957351798361LL,
  -3050086852862350226LL,
  -2700285798897457330LL,
  7270151197372180872LL,
  -1083185500954787753LL,
  6918479338018470904LL,
  -704778497385725114LL,
  -2938883751708086780LL,
  -1144288751283705037LL,
  4323559407351775623LL,
  -4762365134295246914LL,
  -4658922818937404775LL,
  -946321473136954225LL,
  -8680198161757311176LL,
  -3984684615026148925LL,
  5115202668405846613LL,
  -8422329968022051231LL,
  -7971488015305798023LL,
  2356604644751202702LL,
  -3962360301770943087LL,
  -4935375847628755743LL,
  8463576096102179400LL,
  -2258031803858216522LL,
  -4309304706868639327LL,
  716281853154828034LL,
  2120911820462596133LL,
  839258183707475229LL,
  2911842595078861327LL,
  6285062446666734211LL,
  5750392292102033051LL,
  -6411291908406944255LL,
  1001716835743348164LL,
  8592734230377172007LL,
  7036881795503115270LL,
  -4542576348474410456LL,
  3520826868563466410LL,
  2538036101246262999LL,
  -4412940270870274668LL,
  -8017332168876626346LL,
  -3577796032366608991LL,
  5439224135446553609LL,
  8976373502327261757LL,
  8236412713688068703LL,
  -3197915292466324556LL,
  647574647223899512LL,
  5597147250007309523LL,
  -5664992902920474786LL,
  7805040780355376345LL,
  6260371562618537959LL,
  -2466063606481240114LL,
  -7898326304039488972LL,
  4638708579993320078LL,
  -3171952717145190857LL,
  6540683630694528823LL,
  5738491108362577822LL,
  -7368843646390360256LL,
  -4679394950208450915LL,
  -4107066824008590210LL,
  2423376703608593259LL,
  7653426339129504190LL,
  -4858136630591444177LL,
  7298578838712448722LL,
  5083964184216991982LL,
  5535009458418289181LL,
  -6216579025528570917LL,
  -4729092949477611800LL,
  7650861066556724720LL,
  -7311895113516479262LL,
  4838043537029841592LL,
  -7137413053865393238LL,
  -7212116447101689414LL,
  -5015783107888605872LL,
  -6396735582010694919LL,
  -1200944971488062074LL,
  -625164879234338861LL,
  8645233813116710761LL,
  -8669281941672646379LL,
  4269101918917879469LL,
  -7046770469469356710LL,
  156371792836999129LL,
  107425024458468410LL,
  8970375686133858006LL,
  -5912460384394074673LL,
  6523733768536503410LL,
  -2298623723774038927LL,
  -1327441920314528515LL,
  2899007322734183565LL,
  8724367656655087145LL,
  -1336055794237505135LL,
  -2030175047471722940LL,
  -8578110744249930284LL,
  388700162461098277LL,
  4364084395505327637LL,
  -6252677253144569108LL,
  2189631180440038811LL,
  2800380361138005755LL,
  -5668345325300754136LL,
  -8450374976894782334LL,
  -4464516420732881135LL,
  -3698138779399466649LL,
  4048978568756598609LL,
  -7844996148780771230LL,
  -342700393006198590LL,
  8597299336992378468LL,
  -4383905388580904066LL,
  7211360867003028329LL,
  -2491139148448135152LL,
  650525394919042557LL,
  7225393235619366410LL,
  2026840483450117839LL,
  1970480144531257309LL,
  7761078453600911743LL,
  4204672402663303806LL,
  -4849024583015790012LL,
  -404052125934760280LL,
  -4666732760757038418LL,
  6066170159810995299LL,
  -698511556576381157LL,
  -1775804361737368179LL,
  -5813164851451521423LL,
  5671407197645490952LL,
  78161550905591402LL,
  -7667314465583397848LL,
  -3571741223247372364LL,
  3828248259965434754LL,
  2842378260948065611LL,
  -1759992813617585999LL,
  3766458160185506091LL,
  5894782327997348321LL,
  -6908597930120001097LL,
  -4843672015961536386LL,
  -6779981463858114109LL,
  -3148089144940394679LL,
  -3095421856306766133LL,
  -4138466766513126614LL,
  -2255974317651578500LL,
  4897026031772129719LL,
  2871402777816668671LL,
  1578225979973116460LL,
  -7236135452358345615LL,
  -3551478718794460319LL,
  1349576657218331737LL,
  -2088146249378101410LL,
  -3818027455423033243LL,
  1628292315517578766LL,
  3636913316092747966LL,
  -2246290299982736078LL,
  -8542780803931687893LL,
  5450257750402167873LL,
  -4362243282449829600LL,
  -7234699448034156659LL,
  1145023840998092615LL,
  2223327487189637716LL,
  -3657346828709234225LL,
  -203755975455755930LL,
  -649887678475545915LL,
  -6470979817769274537LL,
  6254599393461679409LL,
  6518375201211711842LL,
  -8587674217584642409LL,
  -9099160410688953710LL,
  8198334816854249499LL,
  3637241686109112253LL,
  -4327260389422018554LL,
  7108948507862127613LL,
  3872483211752095714LL,
  -5619364088102360827LL,
  -3641551537118130308LL,
  1603531964115140288LL,
  2686306950004325728LL,
  -628588088645902180LL,
  2338881548929966533LL,
  -8437337940781433266LL,
  5546651876284103088LL,
  214917741554483804LL,
  -2960023870473278748LL,
  -6459042984350123927LL,
  -4465065458906094291LL,
  -3569135296122560206LL,
  4234354408295489507LL
}; // weak
__int64 ecies_iv_data_0[2] = { 0LL, 0LL }; // weak
__int64 odd_parity[32] =
{
  506377898362536193LL,
  1012761315875948552LL,
  1591482698580561936LL,
  2242542046476376345LL,
  2748925463989788704LL,
  3399984811885603113LL,
  3978706194590216497LL,
  4485089612103628856LL,
  5063810994808242240LL,
  5714870342704056649LL,
  6293591725408670033LL,
  6799975142922082392LL,
  7451034490817896801LL,
  7957417908331309160LL,
  8536139291035922544LL,
  9187198638931736953LL,
  -8753162017264402304LL,
  -8102102669368587895LL,
  -7523381286663974511LL,
  -7016997869150562152LL,
  -6365938521254747743LL,
  -5859555103741335384LL,
  -5280833721036722000LL,
  -4629774373140907591LL,
  -4051052990436294207LL,
  -3544669572922881848LL,
  -2965948190218268464LL,
  -2314888842322454055LL,
  -1808505424809041696LL,
  -1157446076913227287LL,
  -578724694208613903LL,
  -72341276695201544LL
}; // weak
__int64 df_bcc_final_endmark[2] = { 128LL, 0LL }; // weak
__int64 f_2_2_11_t[4] =
{
  1012762419699387404LL,
  723403936061328910LL,
  795739693571967498LL,
  868082074056920076LL
}; // weak
__int64 qword_5EFC0[2] = { 289644378169868803LL, 868365760874482187LL }; // weak
__int64 ccn_trailing_zeros_nibble2zeros[2] = { 281483566710788LL, 281483566710787LL }; // weak
__int64 Bi[120] =
{
  -61658700799263867LL,
  15723428623623672LL,
  17243383147678903LL,
  -50484209505776590LL,
  8794400799014220LL,
  4012629018317137LL,
  13099611595371426LL,
  40402701518169724LL,
  21661169353890356LL,
  -66440472579960959LL,
  19282560609331835LL,
  -63501924685326983LL,
  -53100312757835871LL,
  50959353204631023LL,
  -19063405641338682LL,
  -41612035444795581LL,
  -33983490618731035LL,
  -71662654614574575LL,
  -63446068617543344LL,
  -6657302359070704LL,
  20259777363628645LL,
  -4734432656250164LL,
  -50710863531782832LL,
  -50577371652584175LL,
  48101804066821566LL,
  -25667261401343844LL,
  -67305848391843178LL,
  68358063851196557LL,
  43024087570883270LL,
  -42607604602426469LL,
  49276821222832947LL,
  8509081859937346LL,
  54021505913536803LL,
  62341909895808557LL,
  46468887542525472LL,
  27216112542799546LL,
  38941648154456050LL,
  28324007930757203LL,
  53617035962781276LL,
  23971531021267381LL,
  69520333537444504LL,
  17598714365319063LL,
  -18675011714009056LL,
  59486340755195278LL,
  -67928949374143557LL,
  42560055006897106LL,
  -11930877980684957LL,
  23694789141709013LL,
  22463254503178516LL,
  -65179414390733879LL,
  -14875653619703375LL,
  43309561162299214LL,
  7085502065967237LL,
  70953396621486025LL,
  33138683491312945LL,
  61768364254081973LL,
  15808730949180976LL,
  33686789986979197LL,
  5887150295932204LL,
  -6052897380458247LL,
  -28742698556431569LL,
  -37561645062900890LL,
  37987813183537786LL,
  -5847383206664989LL,
  59364513966239192LL,
  -33671985261346697LL,
  -20361847241474450LL,
  -45405131560025593LL,
  -30976708561402764LL,
  -61078519444165174LL,
  -55679389085094098LL,
  -52573354669514537LL,
  -12157196996706800LL,
  -44801387988642614LL,
  -13785917305016311LL,
  -17977960032097551LL,
  33796242983413240LL,
  -40475187688806405LL,
  -2847559034718068LL,
  -69288280760957223LL,
  -11610215698513080LL,
  -50744368579992020LL,
  39946996915329742LL,
  67541298491075253LL,
  27775201316085084LL,
  57193841825974761LL,
  -69361492732634363LL,
  -64591764055622957LL,
  -60507748365185829LL,
  69552176374698976LL,
  13661062574407533LL,
  60145381980993518LL,
  -69882721400384818LL,
  20312854570142078LL,
  39757657542505583LL,
  -17927751864434797LL,
  21633694467713550LL,
  48793482751112111LL,
  41926173686045013LL,
  2707143636645378LL,
  11336368595575691LL,
  -31810589402715540LL,
  -24799265711661157LL,
  -68551942953239479LL,
  -61380661824806760LL,
  -21672722806347085LL,
  29489868233876634LL,
  -3706651297635574LL,
  64569614891154785LL,
  -33908234200870178LL,
  68505703304449822LL,
  -62675693278150527LL,
  -21865352098084241LL,
  -11278098786417660LL,
  -67880321728541434LL,
  44345399889147177LL,
  32011181528828851LL,
  -11748891633589571LL,
  -69734983112245558LL,
  -52788077267997106LL
}; // weak
__int64 d[6] =
{
  59517139926218934LL,
  29847411351711421LL,
  -37743382323150807LL,
  -13948842602906436LL,
  -51776324693215488LL,
  0LL
}; // weak
__int64 sqrtm1[6] =
{
  -34118034821046096LL,
  15034167956805790LL,
  -1170262611653536LL,
  -8614214067270497LL,
  48990467152739358LL,
  0LL
}; // weak
__int64 base[512] =
{
  -61658700799263867LL,
  15723428623623672LL,
  17243383147678903LL,
  -50484209505776590LL,
  8794400799014220LL,
  4012629018317137LL,
  13099611595371426LL,
  40402701518169724LL,
  21661169353890356LL,
  -66440472579960959LL,
  19282560609331835LL,
  -63501924685326983LL,
  -53100312757835871LL,
  50959353204631023LL,
  -19063405641338682LL,
  -55732981704396310LL,
  50613273278984773LL,
  -11814235285939594LL,
  -16383453410020623LL,
  -43857208139937837LL,
  29651706865571259LL,
  27711013056860499LL,
  -41034134755430908LL,
  48057544405530073LL,
  -22845274671637358LL,
  47900125306780274LL,
  66125548449892479LL,
  -54770861758282321LL,
  -40978785485438458LL,
  -17895823067846063LL,
  -41612035444795581LL,
  -33983490618731035LL,
  -71662654614574575LL,
  -63446068617543344LL,
  -6657302359070704LL,
  20259777363628645LL,
  -4734432656250164LL,
  -50710863531782832LL,
  -50577371652584175LL,
  48101804066821566LL,
  -25667261401343844LL,
  -67305848391843178LL,
  68358063851196557LL,
  43024087570883270LL,
  -42607604602426469LL,
  59794075116374450LL,
  -25913388816726770LL,
  -69081881754364132LL,
  64450382294064894LL,
  -26957534028751272LL,
  -35674200200439617LL,
  -33017196026973504LL,
  45124451138402854LL,
  41474115478087299LL,
  44557695843779674LL,
  71770007330553628LL,
  12344105378925378LL,
  12178826473404645LL,
  -16488572690465656LL,
  -435183266941612LL,
  49276821222832947LL,
  8509081859937346LL,
  54021505913536803LL,
  62341909895808557LL,
  46468887542525472LL,
  27216112542799546LL,
  38941648154456050LL,
  28324007930757203LL,
  53617035962781276LL,
  23971531021267381LL,
  69520333537444504LL,
  17598714365319063LL,
  -18675011714009056LL,
  59486340755195278LL,
  -67928949374143557LL,
  -55245103486897852LL,
  20273091778250562LL,
  25235131860819030LL,
  -65390095416740755LL,
  -51908012676600636LL,
  472377674534052LL,
  9357466737781043LL,
  19513827449991050LL,
  -67387551539891312LL,
  63852149214995480LL,
  -68510835832894607LL,
  7332707459674294LL,
  16819530000614272LL,
  17005587972826500LL,
  18827642004959446LL,
  42560055006897106LL,
  -11930877980684957LL,
  23694789141709013LL,
  22463254503178516LL,
  -65179414390733879LL,
  -14875653619703375LL,
  43309561162299214LL,
  7085502065967237LL,
  70953396621486025LL,
  33138683491312945LL,
  61768364254081973LL,
  15808730949180976LL,
  33686789986979197LL,
  5887150295932204LL,
  -6052897380458247LL,
  -11723538421694833LL,
  -18273586934084809LL,
  61294625065978894LL,
  46714014256739119LL,
  10229439539846094LL,
  -11404692956236583LL,
  1985357207780437LL,
  16928068108613082LL,
  15874224889583159LL,
  65936148917330144LL,
  35314604114612506LL,
  -16784710701482760LL,
  51274848569298374LL,
  -12758260515271545LL,
  43419649785916613LL,
  -56502141628474320LL,
  50582255015470554LL,
  28000541091513050LL,
  -6990295499819564LL,
  -20461958627458833LL,
  61032868283568478LL,
  69909371660230059LL,
  -53249597264259310LL,
  51563397343343790LL,
  54990661001166129LL,
  -67266162865019705LL,
  1289095777955209LL,
  -50314579801190566LL,
  -38818747614539211LL,
  18887805896943717LL,
  -33029183280461819LL,
  -64227963141824107LL,
  -67010990257408724LL,
  62468263479202590LL,
  4220406646263936LL,
  67846322798878686LL,
  -42558328427754350LL,
  13587283669775679LL,
  -17695815008000989LL,
  42670345445889648LL,
  69887377133823511LL,
  -43449620062287902LL,
  5181951086256592LL,
  19584604178413286LL,
  -46806334578599826LL,
  -49362080635408432LL,
  -47401376481250271LL,
  -64477182890504123LL,
  19506440107532346LL,
  23624763951053115LL,
  -6430051360765900LL,
  48770891253424839LL,
  -36078425383749285LL,
  52674956369463232LL,
  58086576536022211LL,
  -13082427408386092LL,
  10782085900593051LL,
  -44733501703652998LL,
  21820873438223166LL,
  23092445695197125LL,
  -17227332893676360LL,
  -15193992287460291LL,
  8250194082225015LL,
  -40576454443178137LL,
  6738833756730840LL,
  62735002447484354LL,
  -34530751102517272LL,
  37691855629046073LL,
  13075130254012574LL,
  53875090540328792LL,
  12643928430862045LL,
  29108311888094359LL,
  -18434871499008626LL,
  -8859357917855275LL,
  62168161263666770LL,
  10540764574803363LL,
  200162633231464LL,
  -67346865322178479LL,
  -31081239506450875LL,
  -22563898428028664LL,
  46864827721821657LL,
  -36952764776651273LL,
  -4437972547265446LL,
  -57657925871064821LL,
  35741373491818896LL,
  -9503663093359734LL,
  -18623403381505547LL,
  45768614616826496LL,
  -51002040831452083LL,
  2612942128784950LL,
  -64800684164292892LL,
  -32457316974864861LL,
  48556958917321299LL,
  -29122356461885824LL,
  102593911028951LL,
  -1263596581596524LL,
  63190015568300584LL,
  -8717134372904035LL,
  -27660912223370144LL,
  71635566254822275LL,
  20489841535736628LL,
  60135293089154957LL,
  41080524675447327LL,
  -20594557164389168LL,
  21066617001215119LL,
  62779549874593445LL,
  -64595912975811611LL,
  -64022625035379891LL,
  69543595065925691LL,
  -18583606228125332LL,
  17716598345098080LL,
  -45757705406796226LL,
  -60581574536665590LL,
  3934336042217728LL,
  -12827052980372718LL,
  53340800849194501LL,
  -32097879748041749LL,
  71894213465509192LL,
  11870555167021815LL,
  17797811914415618LL,
  50849599583440472LL,
  -25721116030842946LL,
  3414314328000570LL,
  -62683600280928447LL,
  -70687722785276244LL,
  -47805038965416361LL,
  -67117020160734070LL,
  35976922412872379LL,
  17557319467281732LL,
  -61917163382358959LL,
  30915118763800835LL,
  -8687146894982890LL,
  1927697273470442LL,
  18716900546083983LL,
  -70106910091172143LL,
  59415817370374574LL,
  -6662838624383036LL,
  -50952575690595723LL,
  -16963969744599525LL,
  11853250750891690LL,
  8814909278340817LL,
  45226937658157085LL,
  -55344751000083605LL,
  -18313985341977519LL,
  -55489452749685540LL,
  30793283420326281LL,
  -12702752319994272LL,
  -11929443433075015LL,
  39549072487936269LL,
  70822567617914973LL,
  44563073131771179LL,
  57171177274796159LL,
  -70602433355113495LL,
  1710986108014322LL,
  -3062019654878327LL,
  42072988505200311LL,
  63904002350620346LL,
  63481179969443909LL,
  51870092413873865LL,
  21960953732626706LL,
  50671152262076128LL,
  59825127778912251LL,
  52778039871076818LL,
  -31509533676206061LL,
  23137942320139423LL,
  -14672536014654099LL,
  -10054372393223093LL,
  -44850672707105513LL,
  15538272562689717LL,
  44543071447476970LL,
  -61716008553615321LL,
  -2335225236084523LL,
  -44727772222167533LL,
  -37564179093252795LL,
  72000316386301404LL,
  43355817962060153LL,
  63333278535587381LL,
  -27799691249809838LL,
  67385915171489937LL,
  -70910305190811194LL,
  23505050324789944LL,
  20379594081611812LL,
  44520278084461275LL,
  40581488095302944LL,
  1573229311881162LL,
  49495082879173162LL,
  -27930777927373226LL,
  23281505858480177LL,
  34536317386785021LL,
  50129947746564853LL,
  11572531689712265LL,
  10812034700025394LL,
  47898639226093542LL,
  -65569182664326356LL,
  -43789566672633875LL,
  30641640999269744LL,
  -12089804648623305LL,
  44761045345081240LL,
  22918362103847777LL,
  -62367331767614719LL,
  -17091770802988038LL,
  69066505824847398LL,
  67585364875142469LL,
  -37877840567738289LL,
  69665859902941432LL,
  37167135132979970LL,
  -26167599334409658LL,
  -68774289116657329LL,
  -11285726661053555LL,
  -60842644160080180LL,
  -47108537060665554LL,
  -43298484418764116LL,
  -47459401536555493LL,
  12228991690308393LL,
  7466349675185781LL,
  -61216254796771353LL,
  -34371674107451885LL,
  34917319602930550LL,
  -13912842159103848LL,
  -47267304817508441LL,
  14765474794923190LL,
  1972575423573585LL,
  48685073532801093LL,
  25002757002702860LL,
  36700023108395636LL,
  -69274863219839935LL,
  16842701391258319LL,
  -14649506358851728LL,
  52473646994033457LL,
  -14585687444590503LL,
  -9570905152959361LL,
  -43581892161550029LL,
  2220601184490993LL,
  -16549509703388567LL,
  4520865426360936LL,
  57224507857749349LL,
  35115253212756265LL,
  54102362982685475LL,
  -19809931034919669LL,
  -40711526844169468LL,
  -61704296166098627LL,
  -65884454727896903LL,
  -25940348352515545LL,
  -70487882290244521LL,
  -40849691631221120LL,
  50776864321588418LL,
  -59925934921207821LL,
  -17888822222279342LL,
  -22398327457106136LL,
  21649246512429067LL,
  35212903536288465LL,
  49830743116726572LL,
  -12024877635358585LL,
  7392033881734094LL,
  -13450141078548722LL,
  45904339884589884LL,
  33236080441664611LL,
  -10969136039537508LL,
  34686795854443890LL,
  49557007670624944LL,
  16749779728977486LL,
  -29212512103236631LL,
  -48314989071564820LL,
  31983341561812317LL,
  24897998423679978LL,
  -16293250506447728LL,
  -34207576282419422LL,
  -36319098140759035LL,
  -38871919338013805LL,
  -25242983089841812LL,
  -49687359978399416LL,
  4923647401237276LL,
  71470656711115155LL,
  -15952363234643174LL,
  46435060410664099LL,
  -14687564072288542LL,
  46482953556916132LL,
  20182081421425668LL,
  67758937364950309LL,
  -51430309253138171LL,
  -34671011856051468LL,
  -22829108392884498LL,
  7935248461520355LL,
  -70801908829988957LL,
  -41234499240464333LL,
  35942743069421644LL,
  68590945564379005LL,
  8415782226209118LL,
  -19515609813906673LL,
  53000845608666540LL,
  -45011497767371639LL,
  28307871783032621LL,
  63876647700381760LL,
  35395478330978945LL,
  41268755919702763LL,
  -51601485149976749LL,
  -1681922069569207LL,
  58536246720545210LL,
  68144273233158332LL,
  -37856262681949629LL,
  -67335015476310252LL,
  1537800163824646LL,
  4264855272886453LL,
  3688079803236375LL,
  36166012654707799LL,
  56123312629028907LL,
  35495619763391254LL,
  64771602941165213LL,
  53757554466990003LL,
  -53587224642007843LL,
  44995048055741777LL,
  -63037338082549846LL,
  10595615606228262LL,
  31167460992833559LL,
  64724856511800416LL,
  -40252081309517318LL,
  -70105922220970237LL,
  -24490698114234799LL,
  20934654162926659LL,
  -10013243834884580LL,
  21432762986224050LL,
  31328715508991512LL,
  8266300208507526LL,
  -33983400450045767LL,
  39660818899887847LL,
  -38736258482425945LL,
  -31579459976881392LL,
  45743759636738670LL,
  -49862796453507697LL,
  57535665384036722LL,
  21337839932633775LL,
  18929441314161866LL,
  -35756375809249398LL,
  58389148362399881LL,
  30159079997400218LL,
  -28198624973324090LL,
  68414074475118243LL,
  18534596332280607LL,
  -16821995354976824LL,
  55267995625733088LL,
  58002081604898939LL,
  -9160138725196219LL,
  27673367662403793LL,
  9640058369393189LL,
  -34743631147306869LL,
  59242545467434552LL,
  1370682972090732LL,
  -26903237081202980LL,
  38474046487714898LL,
  23796360777534864LL,
  -49408878583051872LL,
  -32826744289092812LL,
  4393356409436824LL,
  -6738730696690712LL,
  -64264762442521104LL,
  3330257534929227LL,
  60158039279404423LL,
  19969506267625730LL,
  3831905398671579LL,
  -65309045081222589LL,
  41174726195090577LL,
  18571954010475614LL,
  -15247236978458593LL,
  20457474719148411LL,
  25199152927037346LL,
  8278940272298911LL,
  -20415246541513911LL,
  -68803228574309566LL,
  25474580636821407LL,
  -69185283148870087LL,
  -12788419715200771LL,
  7799033566691594LL,
  -13022070281566105LL,
  -65060478125137118LL,
  33219982877133780LL,
  -41192241069963800LL,
  26611784858620141LL,
  -57685211801992802LL,
  -13643504774038001LL,
  65721885934684921LL,
  -26505192404046753LL,
  -47466604173530073LL,
  58000874753982654LL,
  -27567080075192666LL,
  36184012833201870LL,
  36993738756108901LL,
  -13887965760844070LL,
  -7862126612025132LL,
  30644235193880666LL,
  -3560175712350210LL,
  -39591103012473613LL,
  54578378526323917LL,
  -44994382318050124LL,
  -63073574711439718LL,
  70861969626936014LL,
  29580118381296310LL,
  -29886912190889604LL,
  -40989793472343639LL,
  -60663140273616523LL,
  45285731448634817LL,
  66485080100851760LL,
  2443308092979757LL,
  48204372171629997LL,
  -71406292314929798LL,
  38125445406812803LL,
  -53974265624055347LL,
  -3636145098247342LL
}; // weak
__int64 d2[6] =
{
  -25080912518385319LL,
  59694818408455547LL,
  68628423429554258LL,
  -27897689500780169LL,
  40562525871631872LL,
  0LL
}; // weak
__int64 zero[4] = { 0LL, 0LL, 0LL, 0LL }; // weak
__int64 gcm_shift_table[64] =
{
  5044739684282990592LL,
  5622053662039083015LL,
  6200493574062018574LL,
  6776681634730612745LL,
  7358499298014470172LL,
  7933561441596614683LL,
  8507497685271654418LL,
  9085937580114196501LL,
  448886872743690296LL,
  1026200850499782719LL,
  1600137094174822454LL,
  2176325154843416625LL,
  2749135481431482404LL,
  3324197625013626915LL,
  3902637537036562474LL,
  4481077431879104557LL,
  -4147247413772320656LL,
  -3569933436016228233LL,
  -2991493523993292674LL,
  -2415305463324698503LL,
  -1842495136736632724LL,
  -1267432993154488213LL,
  -693496749479448478LL,
  -115056854636906395LL,
  -8770122235398995896LL,
  -8192808257642903473LL,
  -7618872013967863738LL,
  -7042683953299269567LL,
  -6460866290015412140LL,
  -5885804146433267629LL,
  -5307364234410332070LL,
  -4728924339567789987LL,
  5108072511835209953LL,
  5684823548227946726LL,
  6263826401614237935LL,
  6839451520919476456LL,
  7421832125566689533LL,
  7996331327785478394LL,
  8570830512823873779LL,
  9148707466303060212LL,
  494205026904326361LL,
  1070956063297063134LL,
  1645455248335458519LL,
  2221080367640697040LL,
  2794453635592118469LL,
  3368952837810907330LL,
  3947955691197198539LL,
  4525832644676384972LL,
  -4137958606394851183LL,
  -3561207570002114410LL,
  -2982204716615823201LL,
  -2406579597310584680LL,
  -1833206329359163251LL,
  -1258707127140374390LL,
  -684207942101979005LL,
  -106330988622792572LL,
  -8742818754629943127LL,
  -8166067718237206354LL,
  -7591568533198810969LL,
  -7015943413893572448LL,
  -6433562809246359371LL,
  -5859063607027570510LL,
  -5280060753641279301LL,
  -4702183800162092868LL
}; // weak
__int64 K_XMM_AR[10] =
{
  6521908909941356953LL,
  6521908909941356953LL,
  7987674492700322721LL,
  7987674492700322721LL,
  -8134700646434882340LL,
  -8134700646434882340LL,
  -3863312402067832362LL,
  -3863312402067832362LL,
  289644378169868803LL,
  868365760874482187LL
}; // weak
__int64 ccsha256_K[32] =
{
  8158064640682241944LL,
  -1606136187322303537LL,
  6480981066509632091LL,
  -6116909922501295452LL,
  1334009978109274776LL,
  6128411470023722430LL,
  -9160688885620122252LL,
  -4495734319865919833LL,
  -1171420208383170111LL,
  2597628982895680966LL,
  5365058922554666095LL,
  8573033837115779548LL,
  -6327057827470880430LL,
  -4658551843909851192LL,
  -3051310485054944269LL,
  1452737877275992913LL,
  3322285675184065157LL,
  5996557280394112508LL,
  8532644243977171796LL,
  -7894198244907759314LL,
  -6333637450904115039LL,
  -4076793798895891600LL,
  -2983346522951587815LL,
  1182934259129529733LL,
  2177327726902690070LL,
  3796741972107491148LL,
  5681478165690322099LL,
  7507060719877933647LL,
  8693463986056692462LL,
  -8302665152423495660LL,
  -6606660894350966790LL,
  -4147400797850065929LL
}; // weak
__int64 qword_67020[2] = { 289644378169868803LL, 868365760874482187LL }; // weak
__int64 K[80] =
{
  4794697086780616226LL,
  8158064640168781261LL,
  -5349999486874862801LL,
  -1606136188198331460LL,
  4131703408338449720LL,
  6480981068601479193LL,
  -7908458776815382629LL,
  -6116909921290321640LL,
  -2880145864133508542LL,
  1334009975649890238LL,
  2608012711638119052LL,
  6128411473006802146LL,
  8268148722764581231LL,
  -9160688886553864527LL,
  -7215885187991268811LL,
  -4495734319001033068LL,
  -1973867731355612462LL,
  -1171420211273849373LL,
  1135362057144423861LL,
  2597628984639134821LL,
  3308224258029322869LL,
  5365058923640841347LL,
  6679025012923562964LL,
  8573033837759648693LL,
  -7476448914759557205LL,
  -6327057829258317296LL,
  -5763719355590565569LL,
  -4658551843659510044LL,
  -4116276920077217854LL,
  -3051310485924567259LL,
  489312712824947311LL,
  1452737877330783856LL,
  2861767655752347644LL,
  3322285676063803686LL,
  5560940570517711597LL,
  5996557281743188959LL,
  7280758554555802590LL,
  8532644243296465576LL,
  -9096487096722542874LL,
  -7894198246740708037LL,
  -6719396339535248540LL,
  -6333637450476146687LL,
  -4446306890439682159LL,
  -4076793802049405392LL,
  -3345356375505022440LL,
  -2983346525034927856LL,
  -860691631967231958LL,
  1182934255886127544LL,
  1847814050463011016LL,
  2177327727835720531LL,
  2830643537854262169LL,
  3796741975233480872LL,
  4115178125766777443LL,
  5681478168544905931LL,
  6601373596472566643LL,
  7507060721942968483LL,
  8399075790359081724LL,
  8693463985226723168LL,
  -8878714635349349518LL,
  -8302665154208450068LL,
  -8016688836872298968LL,
  -6606660893046293015LL,
  -4685533653050689259LL,
  -4147400797238176981LL,
  -3880063495543823972LL,
  -3348786107499101689LL,
  -1523767162380948706LL,
  -757361751448694408LL,
  500013540394364858LL,
  748580250866718886LL,
  1242879168328830382LL,
  1977374033974150939LL,
  2944078676154940804LL,
  3659926193048069267LL,
  4368137639120453308LL,
  4836135668995329356LL,
  5532061633213252278LL,
  6448918945643986474LL,
  6902733635092675308LL,
  7801388544844847127LL
}; // weak
__int64 ccn_prime_table[64] =
{
  1970346312007682LL,
  5348097572798475LL,
  10414707284181015LL,
  14918375631945769LL,
  19985011113263163LL,
  25051629414711369LL,
  30118264896290913LL,
  36873767417348205LL,
  42503361442545801LL,
  48695888241164445LL,
  54325490855772339LL,
  62769826057617605LL,
  67273520176234723LL,
  74029022697947377LL,
  79095658179395853LL,
  87540036331176219LL,
  94858488806441273LL,
  101051032785453403LL,
  107806543897231727LL,
  115124987782562181LL,
  121880516074209699LL,
  128635992826118583LL,
  134828519624475085LL,
  141584056506319335LL,
  152280208702505469LL,
  160161679851455011LL,
  166917182373036603LL,
  172546767808168535LL,
  180428170236461673LL,
  185494814308369027LL,
  192250316830343829LL,
  202383553432715955LL,
  209139081724887767LL,
  216457525610218223LL,
  227716679299433221LL,
  232783340551602987LL,
  241227718703121213LL,
  247983221224964955LL,
  256427599376876403LL,
  264871977529967511LL,
  273316355681878963LL,
  280634808157537233LL,
  287390327859643377LL,
  295271713108198407LL,
  300901315723461659LL,
  308782743922345023LL,
  316101196397216847LL,
  327360350087218281LL,
  335804745419392147LL,
  344249123571827889LL,
  351567558866830541LL,
  361137888466633963LL,
  366204515357754633LL,
  371834109382296855LL,
  386471014333220143LL,
  400544943559869797LL,
  405048646268421523LL,
  410678231703422375LL,
  418559659902305727LL,
  425315128063624657LL,
  436011366159681011LL,
  442203918728365585LL,
  450648296880277035LL,
  455714915182249543LL
}; // weak
__int64 composite[16] =
{
  -5856188520313147057LL,
  -4512863755770294506LL,
  8963167833447751306LL,
  7957236958701324786LL,
  6982761966735007071LL,
  -2241963150096629415LL,
  -5282854472637583494LL,
  3901287057352828522LL,
  5234753011849385775LL,
  -5947871411424837014LL,
  3684403948054528991LL,
  -1810299181938966696LL,
  -8975618647474116433LL,
  -2974478168731928995LL,
  184921LL,
  0LL
}; // weak
__int64 qword_675B0[8] =
{
  -3004866341678491662LL,
  4689420875046029175LL,
  376750151072312945LL,
  2083934370768397824LL,
  -9031803101629256515LL,
  -5131918880170696144LL,
  4895533504762465624LL,
  3071808977602779175LL
}; // weak
char byte_675F1[7] = { '', '', '\x1F', '', '', '\x0E', '_' }; // weak
__int16 word_67672[3] = { 46798, 38564, 51324 }; // weak
char byte_676F3[5] = { '\x01', '', '\x0E', '', '' }; // weak
int dword_67724 = 7587908; // weak
char byte_6772F = '9'; // weak
__int64 qword_67740[2] = { -1250056671813402394LL, 5885737857504951040LL }; // weak
char byte_67763[5] = { '\x11', '\x11', '', '\x06', '' }; // weak
__int64 byteMap[2] = { 3978425819141910832LL, 7378413942531504440LL }; // weak
__int64 pc1[8] =
{
  2269495618449464LL,
  74609668456526137LL,
  146949841294602810LL,
  2751196062875857723LL,
  2678855889566176798LL,
  2606515716728100125LL,
  219290013610546204LL,
  0LL
}; // weak
__int64 bytebit[4] = { 274877907072LL, 68719476768LL, 17179869192LL, 4294967298LL }; // weak
__int64 totrot[2] = { 1012195045828461057LL, 2025240044361683215LL }; // weak
__int64 pc2[6] =
{
  1946122387410522125LL,
  219288892708488462LL,
  75456296250181401LL,
  2818468513606153000LL,
  3973916683233406002LL,
  2241705408574862369LL
}; // weak
__int64 bigbyte[12] =
{
  18014398517870592LL,
  4503599629467648LL,
  1125899907366912LL,
  281474976841728LL,
  70368744210432LL,
  17592186052608LL,
  4398046513152LL,
  1099511628288LL,
  274877907072LL,
  68719476768LL,
  17179869192LL,
  4294967298LL
}; // weak
__int64 SP7[32] =
{
  297237583998484480LL,
  67110914LL,
  288239180834670592LL,
  297246371501574146LL,
  9007199323949058LL,
  288230384741646336LL,
  288230376151711746LL,
  8804752162818LL,
  9016004004808704LL,
  288239172246831106LL,
  297237575473561602LL,
  9007207913883648LL,
  8796162228224LL,
  297246380089411586LL,
  8592033792LL,
  9015995414872064LL,
  9015995414872064LL,
  288239180836765696LL,
  297237584063498242LL,
  8659140610LL,
  288230376153808898LL,
  9007199321851904LL,
  8804752164864LL,
  297246371501574146LL,
  288230384741648386LL,
  297237575475660802LL,
  2099200LL,
  297246380089409538LL,
  9016003937697792LL,
  8796162228224LL,
  288239172311842818LL,
  9007207844677632LL
}; // weak
__int64 SP5[32] =
{
  146368087401169152LL,
  4755802306048950272LL,
  1099512152064LL,
  146366988963282944LL,
  2251800887951616LL,
  4613938917786255616LL,
  4758053007424225536LL,
  4611686018427912448LL,
  4613937818274627584LL,
  1074266112LL,
  4758054106902298880LL,
  144116288695304448LL,
  4611687119046836224LL,
  4755801206503243776LL,
  144115188109934848LL,
  2252900432609280LL,
  4755802306015395840LL,
  144115188075856128LL,
  146366988963282944LL,
  4613938918859997440LL,
  4611686018460942592LL,
  146368088508989440LL,
  1100585894144LL,
  4758053006350483456LL,
  2252900433133824LL,
  4758054106935853056LL,
  34078720LL,
  4755801207577509888LL,
  144116287588008192LL,
  2251800887427328LL,
  4613937818241073152LL,
  4611687117973094656LL
}; // weak
__int64 SP3[32] =
{
  577025901280100872LL,
  577023736616583168LL,
  134218240LL,
  576462951326810632LL,
  576460786663292936LL,
  562950087639048LL,
  562984447509000LL,
  2233517342720LL,
  34493956096LL,
  2199157604864LL,
  577023702256976384LL,
  565183470764040LL,
  565149110895112LL,
  576462985686548480LL,
  577025935639838728LL,
  576460752303424000LL,
  576460752437772800LL,
  2233383125000LL,
  577025901280231424LL,
  134218240LL,
  562984313160192LL,
  576462951461028360LL,
  2199157473288LL,
  577023736616583168LL,
  562950087639560LL,
  577025935774056448LL,
  565183336415240LL,
  576460786663293440LL,
  576462985820766208LL,
  577023702256845320LL,
  34359869960LL,
  565149111025672LL
}; // weak
__int64 SP1[32] =
{
  16843776LL,
  72343484241084416LL,
  285890219933700LL,
  281474976710660LL,
  72343467061150720LL,
  4398063354884LL,
  72339086211286020LL,
  17196646400LL,
  72061992084440068LL,
  285873040000000LL,
  72339069014705152LL,
  72062009281150976LL,
  72057611217862660LL,
  281492173357060LL,
  4415226380288LL,
  72057594037994500LL,
  72343484241084416LL,
  72339069014638596LL,
  72057594054771712LL,
  4398063288320LL,
  281474993553412LL,
  72057611217863680LL,
  17179870208LL,
  285890219869188LL,
  281492173423620LL,
  72062009281150976LL,
  4415243157508LL,
  72343467061216260LL,
  72061992084440068LL,
  16778240LL,
  285873023287300LL,
  72339086194507776LL
}; // weak
__int64 SP8[32] =
{
  17592454484032LL,
  1154065271577903104LL,
  1152939371939233792LL,
  1152921504606847040LL,
  1154047404513951808LL,
  1143492361588800LL,
  1143767239495680LL,
  274877911040LL,
  1152921779753451520LL,
  17867332390912LL,
  1126174785015808LL,
  1154064996968431680LL,
  4160LL,
  1154047679391596544LL,
  1152939097061326912LL,
  1125899907108928LL,
  1125899907108928LL,
  17592454746112LL,
  1154047679391596608LL,
  1143766970798080LL,
  275146346496LL,
  1154047404782125120LL,
  1152921504875544640LL,
  1152939371671060480LL,
  1154065271577640960LL,
  1152921779485016128LL,
  1152939097061588992LL,
  268439616LL,
  1143492361588800LL,
  17867064217600LL,
  1126174784753728LL,
  1154064996968169472LL
}; // weak
__int64 SP6[32] =
{
  2323857408260046864LL,
  2323927845186846720LL,
  69260541952LL,
  18014399050563600LL,
  18084836510023680LL,
  2305843077937364992LL,
  2305913377962065936LL,
  70438000525312LL,
  18014467228958720LL,
  70369281064976LL,
  2305913446681559040LL,
  2323857476442652688LL,
  541065232LL,
  2323927776471564304LL,
  18084767253676048LL,
  2305843009754775552LL,
  69256364032LL,
  18084767794724880LL,
  18014399050563600LL,
  2305843077933187088LL,
  2305913377962065920LL,
  70438000525312LL,
  2323927845723701264LL,
  2323857407727386624LL,
  2323927776471564304LL,
  2323857476442652672LL,
  70368744177680LL,
  18084836514201600LL,
  18014467228975104LL,
  536887312LL,
  2305843009754775552LL,
  2305913446681542672LL
}; // weak
__int64 SP4[32] =
{
  35738431266817LL,
  549755822209LL,
  36029351078142080LL,
  35188675444737LL,
  36063981391052800LL,
  36064535450230784LL,
  129LL,
  36028801322320000LL,
  35184372088833LL,
  36063985694408704LL,
  36028797018964096LL,
  35734127910913LL,
  4303356033LL,
  36029346774786176LL,
  36064531146874880LL,
  554059178113LL,
  36028801322320000LL,
  36064535450230784LL,
  129LL,
  36063981391052800LL,
  36029346774786176LL,
  4303356033LL,
  35738431266817LL,
  549755822209LL,
  554059178113LL,
  35184372088833LL,
  35188675444737LL,
  36029351078142080LL,
  35734127910913LL,
  36063985694408704LL,
  36028797018964096LL,
  36064531146874880LL
}; // weak
__int64 SP2[32] =
{
  -9223231297217855456LL,
  4644474554712064LL,
  137440002048LL,
  -9223231159778934752LL,
  -9218727560152612832LL,
  -9223372034706210816LL,
  4503601774886912LL,
  -9218868299788451808LL,
  4503737067405312LL,
  2147516448LL,
  140739635838976LL,
  -9218868437226323936LL,
  -9223371899414773728LL,
  4644337115725824LL,
  -9218727699739017184LL,
  140877075841024LL,
  4644474554679296LL,
  4503601775902752LL,
  -9218868435079888864LL,
  140739636920320LL,
  -9223231297217888256LL,
  -9218727562300096480LL,
  137440034848LL,
  -9223372036854743040LL,
  -9218727699739017184LL,
  -9223371899414773760LL,
  -9223231161926418400LL,
  4503739213807648LL,
  1081344LL,
  140877074825216LL,
  -9218868297640968192LL,
  4644339264290848LL
}; // weak
__int64 off_69010[2] = { 624016LL, 624024LL }; // weak
__int64 (*off_69020[4])[2] =
{
  &ccaes_intel_cbc_decrypt_aesni_mode,
  &ccaes_intel_cbc_encrypt_aesni_mode,
  &ccaes_intel_ecb_decrypt_aesni_mode,
  &ccaes_intel_ecb_encrypt_aesni_mode
}; // weak
__int64 (*off_69028[3])[2] =
{
  &ccaes_intel_cbc_encrypt_aesni_mode,
  &ccaes_intel_ecb_decrypt_aesni_mode,
  &ccaes_intel_ecb_encrypt_aesni_mode
}; // weak
__int64 (*off_69030[2])[2] = { &ccaes_intel_ecb_decrypt_aesni_mode, &ccaes_intel_ecb_encrypt_aesni_mode }; // weak
__int64 (*off_69038)[2] = &ccaes_intel_ecb_encrypt_aesni_mode; // weak
__int64 (*off_69040[2])[3] = { &ccaes_intel_xts_decrypt_aesni_mode, &ccaes_intel_xts_encrypt_aesni_mode }; // weak
__int64 (*off_69048)[3] = &ccaes_intel_xts_encrypt_aesni_mode; // weak
__int64 (*off_69050[3])[4] =
{
  &ccsha1_vng_intel_SupplementalSSE3_di,
  &ccsha224_vng_intel_SupplementalSSE3_di,
  &ccsha256_vng_intel_SupplementalSSE3_di
}; // weak
__int64 (*off_69058[2])[4] =
{
  &ccsha224_vng_intel_SupplementalSSE3_di,
  &ccsha256_vng_intel_SupplementalSSE3_di
}; // weak
__int64 (*off_69060)[4] = &ccsha256_vng_intel_SupplementalSSE3_di; // weak
__int64 ccaes_intel_cbc_decrypt_opt_mode[2] = { 244LL, 16LL }; // weak
__int64 ccaes_intel_cbc_encrypt_opt_mode[2] = { 244LL, 16LL }; // weak
__int64 ccaes_intel_ecb_decrypt_opt_mode[2] = { 244LL, 16LL }; // weak
__int64 ccaes_intel_ecb_encrypt_opt_mode[2] = { 244LL, 16LL }; // weak
__int64 ccaes_intel_xts_decrypt_opt_mode[3] = { 488LL, 16LL, 16LL }; // weak
__int64 ccaes_intel_xts_encrypt_opt_mode[3] = { 488LL, 16LL, 16LL }; // weak
__int64 ccaes_ltc_ecb_encrypt_mode[2] = { 484LL, 16LL }; // weak
__int64 ccblowfish_ltc_ecb_decrypt_mode[2] = { 4168LL, 8LL }; // weak
__int64 ccblowfish_ltc_ecb_encrypt_mode[2] = { 4168LL, 8LL }; // weak
__int64 cccast_eay_ecb_decrypt_mode[2] = { 132LL, 8LL }; // weak
__int64 cccast_eay_ecb_encrypt_mode[2] = { 132LL, 8LL }; // weak
__int64 ccdes3_ltc_ecb_decrypt_mode[2] = { 768LL, 8LL }; // weak
__int64 ccdes3_ltc_ecb_encrypt_mode[2] = { 768LL, 8LL }; // weak
__int64 ccdes_ltc_ecb_decrypt_mode[2] = { 256LL, 8LL }; // weak
__int64 ccdes_ltc_ecb_encrypt_mode[2] = { 256LL, 8LL }; // weak
__int64 ccec_cp192 = 3LL; // weak
__int64 ccec_cp224 = 4LL; // weak
__int64 ccec_cp256 = 4LL; // weak
__int64 ccec_cp384 = 6LL; // weak
__int64 ccec_cp521 = 9LL; // weak
__int64 ccmd5_ltc_di[4] = { 16LL, 16LL, 64LL, 10LL }; // weak
_UNKNOWN ccrc4_eay; // weak
__int64 ccsha1_vng_intel_NOSupplementalSSE3_di[4] = { 20LL, 20LL, 64LL, 7LL }; // weak
__int64 ccsha224_vng_intel_NOSupplementalSSE3_di[4] = { 28LL, 32LL, 64LL, 11LL }; // weak
__int64 ccsha256_vng_intel_AVX2_di[4] = { 32LL, 32LL, 64LL, 11LL }; // weak
__int64 ccsha256_vng_intel_AVX1_di[4] = { 32LL, 32LL, 64LL, 11LL }; // weak
__int64 ccsha256_vng_intel_NOSupplementalSSE3_di[4] = { 32LL, 32LL, 64LL, 11LL }; // weak
__int64 ccsha384_ltc_di[4] = { 48LL, 64LL, 128LL, 11LL }; // weak
__int64 ccsha512_ltc_di[4] = { 64LL, 64LL, 128LL, 11LL }; // weak
__int64 rfc5054_8192 = 128LL; // weak
__int64 rfc5054_1024 = 16LL; // weak
__int64 rfc5054_2048 = 32LL; // weak
__int64 rfc5054_4096 = 64LL; // weak
__int64 weak_keys[16] =
{
  72340172838076673LL,
  -72340172838076674LL,
  1012762420019404575LL,
  -1012762420019404576LL,
  -143554428589179391LL,
  143554428589179390LL,
  -1076658214702948321LL,
  1076658214702948320LL,
  -1080317445236727807LL,
  139895198055399904LL,
  -139895198055399905LL,
  1080317445236727806LL,
  1009103189485625089LL,
  75999403371856159LL,
  -75999403371856160LL,
  -1009103189485625090LL
}; // weak
__int64 sl_test_xor_RB = 0LL; // weak
__int64 kmod_info = 0LL; // weak
int (*_realmain)(void) = &corecrypto_kext_start; // weak
int (*_antimain)(void) = &corecrypto_kext_stop; // weak
__int64 cbc_blowfish_encrypt; // weak
__int64 qword_6B698; // weak
__int64 qword_6B6A0; // weak
__int64 qword_6B6A8; // weak
__int64 qword_6B6B0; // weak
__int64 cbc_blowfish_decrypt; // weak
__int64 qword_6B6C0; // weak
__int64 qword_6B6C8; // weak
__int64 qword_6B6D0; // weak
__int64 qword_6B6D8; // weak
__int64 cfb_blowfish_encrypt; // weak
__int64 qword_6B6E8; // weak
__int64 qword_6B6F0; // weak
__int64 qword_6B6F8; // weak
__int64 qword_6B700; // weak
__int64 cfb_blowfish_decrypt; // weak
__int64 qword_6B710; // weak
__int64 qword_6B718; // weak
__int64 qword_6B720; // weak
__int64 qword_6B728; // weak
__int64 cfb8_blowfish_encrypt; // weak
__int64 qword_6B738; // weak
__int64 qword_6B740; // weak
__int64 qword_6B748; // weak
__int64 qword_6B750; // weak
__int64 cfb8_blowfish_decrypt; // weak
__int64 qword_6B760; // weak
__int64 qword_6B768; // weak
__int64 qword_6B770; // weak
__int64 qword_6B778; // weak
__int64 ctr_blowfish; // weak
__int64 qword_6B788; // weak
__int64 qword_6B790; // weak
__int64 qword_6B798; // weak
__int64 qword_6B7A0; // weak
__int64 ofb_blowfish; // weak
__int64 qword_6B7B0; // weak
__int64 qword_6B7B8; // weak
__int64 qword_6B7C0; // weak
__int64 qword_6B7C8; // weak
__int64 cbc_cast_encrypt; // weak
__int64 qword_6B7D8; // weak
__int64 qword_6B7E0; // weak
__int64 qword_6B7E8; // weak
__int64 qword_6B7F0; // weak
__int64 cbc_cast_decrypt; // weak
__int64 qword_6B800; // weak
__int64 qword_6B808; // weak
__int64 qword_6B810; // weak
__int64 qword_6B818; // weak
__int64 cfb_cast_encrypt; // weak
__int64 qword_6B828; // weak
__int64 qword_6B830; // weak
__int64 qword_6B838; // weak
__int64 qword_6B840; // weak
__int64 cfb_cast_decrypt; // weak
__int64 qword_6B850; // weak
__int64 qword_6B858; // weak
__int64 qword_6B860; // weak
__int64 qword_6B868; // weak
__int64 cfb8_cast_encrypt; // weak
__int64 qword_6B878; // weak
__int64 qword_6B880; // weak
__int64 qword_6B888; // weak
__int64 qword_6B890; // weak
__int64 cfb8_cast_decrypt; // weak
__int64 qword_6B8A0; // weak
__int64 qword_6B8A8; // weak
__int64 qword_6B8B0; // weak
__int64 qword_6B8B8; // weak
__int64 ctr_cast; // weak
__int64 qword_6B8C8; // weak
__int64 qword_6B8D0; // weak
__int64 qword_6B8D8; // weak
__int64 qword_6B8E0; // weak
__int64 ofb_cast; // weak
__int64 qword_6B8F0; // weak
__int64 qword_6B8F8; // weak
__int64 qword_6B900; // weak
__int64 qword_6B908; // weak
__int64 gcm_aes_decrypt; // weak
__int64 qword_6B918; // weak
__int64 qword_6B920; // weak
__int64 qword_6B928; // weak
__int64 qword_6B930; // weak
__int64 qword_6B938; // weak
__int64 qword_6B940; // weak
__int64 qword_6B948; // weak
__int64 qword_6B950; // weak
__int64 cbc_des_encrypt; // weak
__int64 qword_6B960; // weak
__int64 qword_6B968; // weak
__int64 qword_6B970; // weak
__int64 qword_6B978; // weak
__int64 cbc_des_decrypt; // weak
__int64 qword_6B988; // weak
__int64 qword_6B990; // weak
__int64 qword_6B998; // weak
__int64 qword_6B9A0; // weak
__int64 cfb_des_encrypt; // weak
__int64 qword_6B9B0; // weak
__int64 qword_6B9B8; // weak
__int64 qword_6B9C0; // weak
__int64 qword_6B9C8; // weak
__int64 cfb_des_decrypt; // weak
__int64 qword_6B9D8; // weak
__int64 qword_6B9E0; // weak
__int64 qword_6B9E8; // weak
__int64 qword_6B9F0; // weak
__int64 cfb8_des_encrypt; // weak
__int64 qword_6BA00; // weak
__int64 qword_6BA08; // weak
__int64 qword_6BA10; // weak
__int64 qword_6BA18; // weak
__int64 cfb8_des_decrypt; // weak
__int64 qword_6BA28; // weak
__int64 qword_6BA30; // weak
__int64 qword_6BA38; // weak
__int64 qword_6BA40; // weak
__int64 ctr_des; // weak
__int64 qword_6BA50; // weak
__int64 qword_6BA58; // weak
__int64 qword_6BA60; // weak
__int64 qword_6BA68; // weak
__int64 ofb_des; // weak
__int64 qword_6BA78; // weak
__int64 qword_6BA80; // weak
__int64 qword_6BA88; // weak
__int64 qword_6BA90; // weak
__int64 cbc_des3_encrypt; // weak
__int64 qword_6BAA0; // weak
__int64 qword_6BAA8; // weak
__int64 qword_6BAB0; // weak
__int64 qword_6BAB8; // weak
__int64 cbc_des3_decrypt; // weak
__int64 qword_6BAC8; // weak
__int64 qword_6BAD0; // weak
__int64 qword_6BAD8; // weak
__int64 qword_6BAE0; // weak
__int64 cfb_des3_encrypt; // weak
__int64 qword_6BAF0; // weak
__int64 qword_6BAF8; // weak
__int64 qword_6BB00; // weak
__int64 qword_6BB08; // weak
__int64 cfb_des3_decrypt; // weak
__int64 qword_6BB18; // weak
__int64 qword_6BB20; // weak
__int64 qword_6BB28; // weak
__int64 qword_6BB30; // weak
__int64 cfb8_des3_encrypt; // weak
__int64 qword_6BB40; // weak
__int64 qword_6BB48; // weak
__int64 qword_6BB50; // weak
__int64 qword_6BB58; // weak
__int64 cfb8_des3_decrypt; // weak
__int64 qword_6BB68; // weak
__int64 qword_6BB70; // weak
__int64 qword_6BB78; // weak
__int64 qword_6BB80; // weak
__int64 ctr_des3; // weak
__int64 qword_6BB90; // weak
__int64 qword_6BB98; // weak
__int64 qword_6BBA0; // weak
__int64 qword_6BBA8; // weak
__int64 ofb_des3; // weak
__int64 qword_6BBB8; // weak
__int64 qword_6BBC0; // weak
__int64 qword_6BBC8; // weak
__int64 qword_6BBD0; // weak
__int64 ccm_aes_encrypt; // weak
__int64 qword_6BBE0; // weak
__int64 qword_6BBE8; // weak
__int64 qword_6BBF0; // weak
__int64 qword_6BBF8; // weak
__int64 qword_6BC00; // weak
__int64 qword_6BC08; // weak
__int64 qword_6BC10; // weak
__int64 qword_6BC18; // weak
__int64 qword_6BC20; // weak
__int64 gcm_aes_encrypt; // weak
__int64 qword_6BC30; // weak
__int64 qword_6BC38; // weak
__int64 qword_6BC40; // weak
__int64 qword_6BC48; // weak
__int64 qword_6BC50; // weak
__int64 qword_6BC58; // weak
__int64 qword_6BC60; // weak
__int64 qword_6BC68; // weak
__int64 kpis; // weak
__int64 qword_6BC78; // weak
__int64 qword_6BC88; // weak
__int64 qword_6BC90; // weak
__int64 qword_6BC98; // weak
__int64 qword_6BCA0; // weak
__int64 qword_6BCA8; // weak
__int64 qword_6BCB0; // weak
__int64 qword_6BCB8; // weak
__int64 qword_6BCC0; // weak
__int64 qword_6BCC8; // weak
__int64 qword_6BCD0; // weak
__int64 qword_6BCD8; // weak
__int64 qword_6BCE0; // weak
__int64 qword_6BCE8; // weak
__int64 qword_6BCF0; // weak
__int64 qword_6BCF8; // weak
__int64 qword_6BD00; // weak
__int64 qword_6BD08; // weak
__int64 qword_6BD10; // weak
__int64 qword_6BD18; // weak
__int64 qword_6BD20; // weak
__int64 qword_6BD28; // weak
__int64 qword_6BD30; // weak
__int64 qword_6BD38; // weak
__int64 qword_6BD40; // weak
__int64 qword_6BD48; // weak
__int64 qword_6BD50; // weak
__int64 qword_6BD58; // weak
__int64 qword_6BD60; // weak
__int64 qword_6BD68; // weak
__int64 qword_6BD70; // weak
__int64 qword_6BD78; // weak
__int64 qword_6BD80; // weak
__int64 qword_6BD88; // weak
_UNKNOWN DRBG_POST_info; // weak
int (__fastcall *qword_6BD98)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD); // weak
int (__fastcall *qword_6BDA0)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD); // weak
int (__fastcall *qword_6BDA8)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD); // weak
__int64 ccm_aes_decrypt; // weak
__int64 qword_6BDC8; // weak
__int64 qword_6BDD0; // weak
__int64 qword_6BDD8; // weak
__int64 qword_6BDE0; // weak
__int64 qword_6BDE8; // weak
__int64 qword_6BDF0; // weak
__int64 qword_6BDF8; // weak
__int64 qword_6BE00; // weak
__int64 qword_6BE08; // weak


//----- (0000000000000DC0) ----------------------------------------------------
__int64 __fastcall ccansikdf_x963(__int64 a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, void *a7)
{
  __int64 v7; // r15@1
  __int64 v8; // r12@1
  unsigned __int64 v9; // rdi@1
  __int64 v10; // r8@1
  __int64 v11; // r11@1
  char *v12; // rbx@1
  signed int v13; // er14@1
  unsigned int i; // er14@4
  char *v15; // rsi@5
  char *v16; // r14@5
  void *v17; // rbx@5
  unsigned int v18; // er13@6
  __int64 v19; // rax@6
  signed __int64 v20; // rax@6
  const void *v21; // r13@11
  __int64 result; // rax@12
  void *v23; // [sp+0h] [bp-80h]@1
  unsigned __int64 v24; // [sp+8h] [bp-78h]@4
  unsigned __int64 v25; // [sp+10h] [bp-70h]@4
  const void *v26; // [sp+18h] [bp-68h]@1
  bool v27; // [sp+27h] [bp-59h]@4
  unsigned __int64 v28; // [sp+28h] [bp-58h]@4
  __int64 v29; // [sp+30h] [bp-50h]@1
  const void *v30; // [sp+38h] [bp-48h]@1
  void *v31; // [sp+40h] [bp-40h]@4
  unsigned int v32; // [sp+4Ch] [bp-34h]@6
  __int64 v33; // [sp+50h] [bp-30h]@1

  v26 = a5;
  v30 = a3;
  v7 = a1;
  v8 = off_69010[0];
  v33 = *(_QWORD *)off_69010[0];
  v9 = *(_QWORD *)a1;
  v10 = *(_QWORD *)(v7 + 8);
  v29 = a6 / v9;
  v11 = *(_QWORD *)(v7 + 16);
  v12 = (char *)&v23 - ((((v10 + v11 + 19) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v13 = -1;
  if ( a6 / v9 <= 0xFFFFFFFE && a2 <= 0xFFFFFFFB && 4294967291u - a2 >= a4 )
  {
    v23 = (char *)&v23 - ((v9 + 15) & 0xFFFFFFFFFFFFFFF0LL);
    v28 = a2;
    v24 = a6;
    v31 = a7;
    v25 = a4;
    v27 = v26 != 0LL && a4 != 0;
    for ( i = 0; ; i = v18 )
    {
      v32 = _byteswap_ulong(i + 1);
      v18 = _byteswap_ulong(v32);
      v19 = ccdigest_init(v7, (__int64)v12);
      LODWORD(v20) = ccdigest_update(v19, v28, v30, v7, (__int64)v12);
      ccdigest_update(v20, 4uLL, &v32, v7, (__int64)v12);
      if ( v18 > (unsigned int)v29 )
        break;
      if ( v27 )
        ccdigest_update(v29, v25, v26, v7, (__int64)v12);
      v15 = v12;
      v16 = v12;
      v17 = v31;
      (*(void (__fastcall **)(__int64, char *, void *))(v7 + 56))(v7, v15, v31);
      v31 = (char *)v17 + *(_QWORD *)v7;
      v12 = v16;
    }
    if ( v27 )
      ccdigest_update(v29, v25, v26, v7, (__int64)v12);
    v21 = v23;
    (*(void (__fastcall **)(__int64, char *, void *))(v7 + 56))(v7, v12, v23);
    memcpy(v31, v21, v24 - *(_QWORD *)v7 * i);
    v10 = *(_QWORD *)(v7 + 8);
    v11 = *(_QWORD *)(v7 + 16);
    v13 = 0;
    v8 = off_69010[0];
  }
  cc_clear(v10 + v11 + 12, v12);
  result = *(_QWORD *)v8;
  if ( *(_QWORD *)v8 == v33 )
    result = (unsigned int)v13;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000000F9A) ----------------------------------------------------
__int64 __fastcall ccecies_encrypt_gcm_composite(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, unsigned __int64 a8, const void *a9, __int64 a10, __int64 a11)
{
  __int64 v11; // rbx@1
  __int64 v12; // r12@1
  __int64 *v13; // rsi@1
  __int64 v14; // rdi@1
  __int64 v15; // r15@1
  char *v16; // r14@1
  signed int v17; // er13@1
  __int64 v18; // r14@3
  __int64 v19; // r13@4
  __int64 v20; // r12@4
  __int64 v21; // r15@4
  __int64 **v22; // rbx@4
  unsigned __int64 v23; // rax@4
  __int64 v24; // rdi@4
  unsigned __int64 v25; // rsi@4
  unsigned __int64 v26; // r9@4
  __int64 v27; // rbx@4
  const void *v28; // rdx@5
  unsigned __int64 v29; // rcx@5
  const void *v30; // r8@5
  void *v31; // r13@8
  __int64 v32; // rsi@8
  __int64 result; // rax@14
  __int64 v34; // [sp+10h] [bp-90h]@1
  __int64 v35; // [sp+18h] [bp-88h]@1
  __int64 v36; // [sp+20h] [bp-80h]@1
  __int64 v37; // [sp+28h] [bp-78h]@1
  __int64 v38; // [sp+30h] [bp-70h]@1
  __int64 v39; // [sp+38h] [bp-68h]@2
  size_t v40; // [sp+40h] [bp-60h]@1
  __int64 v41; // [sp+48h] [bp-58h]@1
  void *v42; // [sp+50h] [bp-50h]@1
  void *v43; // [sp+58h] [bp-48h]@2
  void *v44; // [sp+60h] [bp-40h]@2
  size_t v45; // [sp+68h] [bp-38h]@1
  __int64 v46; // [sp+70h] [bp-30h]@1

  v35 = a6;
  v37 = a5;
  v36 = a4;
  v38 = a3;
  v11 = a2;
  v12 = (__int64)a1;
  v46 = *(_QWORD *)off_69010[0];
  v13 = *a1;
  v14 = **a1;
  v15 = (__int64)((char *)&v34 - (32 * v14 | 0x10));
  v40 = (unsigned __int64)(ccn_bitlen(v14, (__int64)(v13 + 2)) + 7) >> 3;
  v45 = v40;
  v16 = (char *)&v34 - ((v40 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v41 = *(_QWORD *)(v11 + 16);
  v42 = (char *)&v34 - ((*(_QWORD *)v41 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  v17 = -1;
  if ( ccec_generate_key_internal_fips(*(__int64 **)v12, *(_QWORD *)(v11 + 8), v15) )
  {
    v44 = v16;
    v43 = (char *)&v34 - (32 * v14 | 0x10);
    v39 = v12;
    goto LABEL_13;
  }
  v43 = (char *)&v34 - (32 * v14 | 0x10);
  v39 = v12;
  v44 = v16;
  if ( (unsigned int)ccec_compute_key((__int64 **)v15, v12, (__int64)&v45, v16) )
  {
LABEL_13:
    v18 = v41;
    goto LABEL_14;
  }
  v17 = -2;
  v18 = v41;
  if ( *(_BYTE *)(v11 + 32) & 2 )
  {
    v19 = v11;
    v20 = v37;
    v21 = v38;
    v22 = (__int64 **)v43;
    ccec_x963_export(0, v38, (__int64 **)v43);
    v23 = ccecies_pub_key_size(v22, v19);
    v24 = *(_QWORD *)v19;
    v25 = v45;
    v26 = *(_DWORD *)(v19 + 24);
    v27 = v19;
    if ( *(_BYTE *)(v19 + 32) & 1 )
    {
      v28 = v44;
      v45 = (size_t)v44;
      v29 = v23;
      v30 = (const void *)v21;
    }
    else
    {
      v30 = a9;
      v29 = a8;
      v28 = v44;
      v45 = (size_t)v44;
    }
    v17 = -1;
    if ( !(unsigned int)ccansikdf_x963(v24, v25, v28, v29, v30, v26, (void *)v45) )
    {
      v31 = v42;
      (*(void (__fastcall **)(__int64, void *, _QWORD, void *))(v18 + 16))(v18, v42, *(_DWORD *)(v27 + 24), v44);
      (*(void (__fastcall **)(void *, signed __int64, _QWORD))(v18 + 24))(v31, 16LL, ecies_iv_data);
      v32 = a10;
      if ( !a11 || !a10 )
        v32 = 0LL;
      (*(void (__fastcall **)(void *, __int64))(v18 + 32))(v31, v32);
      (*(void (__fastcall **)(void *, __int64, __int64, __int64))(v18 + 40))(v31, v35, a7, v36);
      (*(void (__fastcall **)(void *, _QWORD, __int64))(v18 + 48))(v31, *(_DWORD *)(v27 + 28), v20);
      v17 = 0;
    }
  }
LABEL_14:
  cc_clear(*(_QWORD *)v18, v42);
  cc_clear(v40, v44);
  cc_clear(32LL * **(_QWORD **)v39 | 0x10, v43);
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v46 )
    result = (unsigned int)v17;
  return result;
}
// 4B920: using guessed type __int64 ecies_iv_data[2];
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000011E4) ----------------------------------------------------
bool __fastcall ccrsa_pairwise_consistency_check(__int64 *a1)
{
  __int64 v1; // r14@1
  signed __int64 v2; // rax@1
  signed __int64 v3; // r15@1
  unsigned __int64 v4; // rax@1
  unsigned __int64 v5; // rbx@1
  int v6; // ecx@2
  bool result; // al@2
  unsigned __int64 v8; // rax@4
  __int64 v9; // rbx@4
  char *v10; // r13@4
  __int64 v11; // r15@5
  __int64 v12; // rcx@7
  __int64 v13; // [sp+10h] [bp-70h]@1
  __int64 v14; // [sp+18h] [bp-68h]@4
  char v15; // [sp+27h] [bp-59h]@2
  unsigned __int64 v16; // [sp+28h] [bp-58h]@1
  char v17; // [sp+30h] [bp-50h]@1
  __int64 v18; // [sp+50h] [bp-30h]@1
  __int64 v19; // [sp+58h] [bp-28h]@4

  v18 = *(_QWORD *)off_69010[0];
  memcpy(&v17, "ABCEDFGHIJKLMNOPRSTU", 0x14uLL);
  v1 = *a1;
  v2 = ccn_bitlen(*a1, (__int64)(a1 + 2));
  v3 = v2;
  v4 = (unsigned __int64)(v2 + 7) >> 3;
  v5 = (unsigned __int64)((char *)&v13 - ((v4 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v16 = v4;
  if ( (unsigned int)ccrsa_sign_pkcs1v15(
                       a1,
                       (__int64)"\x06\x05+\x0E\x03\x02\x1A",
                       0x14uLL,
                       &v17,
                       (__int64)&v16,
                       (char *)&v13 - ((v4 + 15) & 0xFFFFFFFFFFFFFFF0LL)) )
    goto LABEL_6;
  v6 = ccrsa_verify_pkcs1v15(a1, (__int64)"\x06\x05+\x0E\x03\x02\x1A", 0x14uLL, &v17, v16, v5, (__int64)&v15);
  result = 0;
  if ( !v6 && v15 )
  {
    v8 = (8 * v1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
    v9 = (__int64)((char *)&v19 - v8);
    v10 = (char *)&v19 - v8;
    v14 = (__int64)((char *)&v19 - v8);
    *(__int64 *)((char *)&v19 - v8) = 42LL;
    bzero((char *)&v19 + -v8 + 8, 8 * v1 - 8);
    *(_QWORD *)&v10[8 * ((unsigned __int64)(v3 - 9) >> 6)] |= 1LL << ((unsigned __int8)v3 - 9);
    ccrsa_priv_crypt(&a1[4 * *a1 + 3], v9, v10);
    if ( (unsigned int)ccn_cmp(v1, (__int64)v10, v9) )
    {
      v11 = v14;
      ccrsa_pub_crypt(a1, v14, v9);
      result = (unsigned int)ccn_cmp(v1, v11, (__int64)v10) == 0;
      goto LABEL_7;
    }
LABEL_6:
    result = 0;
  }
LABEL_7:
  v12 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000000138E) ----------------------------------------------------
int __fastcall ccrsa_dump_key(__int64 *a1)
{
  __int64 *v1; // rbx@1
  signed __int64 v2; // rax@1
  __int64 v3; // rax@1
  __int64 v4; // rax@1
  __int64 v5; // rax@1
  signed __int64 v6; // r12@1
  signed __int64 v7; // r15@1
  __int64 v8; // r14@1
  __int64 v9; // rax@1

  v1 = a1;
  v2 = ccn_bitlen(*a1, (__int64)(a1 + 2));
  LODWORD(v3) = printf("%lu bit rsa key\n", v2);
  LODWORD(v4) = ccn_lprint(v3, (__int64)(v1 + 2), *v1);
  ccn_lprint(v4, (__int64)&v1[*v1 + 2], *v1 + 1);
  ccn_lprint(16 * *v1, (__int64)&v1[2 * *v1 + 3], *v1);
  LODWORD(v5) = ccn_lprint(3 * *v1, (__int64)&v1[3 * *v1 + 3], *v1);
  v6 = 4 * *a1;
  v7 = (signed __int64)&a1[v6 + 3];
  v8 = *(_QWORD *)v7;
  ccn_lprint(v5, (__int64)&v1[v6 + 5], v1[v6 + 3]);
  ccn_lprint(v1[v6 + 3], v7 + 8 * v1[v6 + 3] + 16, v8 + 1);
  ccn_lprint(16 * v1[v6 + 3], 16 * v1[v6 + 3] + v7 + 40, *(_QWORD *)(16 * v1[v6 + 3] + v7 + 24));
  v9 = *(_QWORD *)v7;
  ccn_lprint(
    *(_QWORD *)(v7 + 16 * v9 + 24) + 2 * v9 + 3,
    v7 + 8 * (*(_QWORD *)(v7 + 16 * v9 + 24) + 2 * v9 + 3) + 16,
    *(_QWORD *)(v7 + 16 * v9 + 24) + 1LL);
  ccn_lprint(32 * v1[v6 + 3], 32 * v1[v6 + 3] + v7 + 48, v8);
  ccn_lprint(5 * v1[v6 + 3], v7 + 40 * v1[v6 + 3] + 48, *(_QWORD *)(16 * v1[v6 + 3] + v7 + 24));
  ccn_lprint(6 * v1[v6 + 3], v7 + 48 * v1[v6 + 3] + 48, v8);
  return printf("\n", "qinv=0x");
}

//----- (0000000000001519) ----------------------------------------------------
signed __int64 __fastcall ccrsa_crt_makekey(__int64 a1, const void *a2, __int64 a3, __int64 a4, void *a5, void *a6, unsigned __int64 a7, void *a8)
{
  __int64 v8; // r14@1
  __int64 v9; // rbx@1
  signed __int64 v10; // r15@1
  __int64 v11; // r13@1
  __int64 v12; // rbx@2
  __int64 v13; // r12@2
  __int64 v14; // rdx@6
  unsigned __int64 v15; // r14@6
  __int64 v16; // r12@6
  __int64 v17; // rax@6
  __int64 v18; // rbx@6
  __int64 v19; // rax@6
  __int64 v20; // rbx@6
  __int64 v21; // rax@6
  const void *v22; // rbx@6
  unsigned __int64 v23; // r13@6
  char *v24; // r14@6
  unsigned __int64 v25; // r12@6
  __int64 v26; // rbx@6
  unsigned __int64 v27; // rdx@6
  __int64 v28; // r12@6
  int v29; // ecx@8
  signed __int64 result; // rax@8
  __int64 v31; // rcx@10
  __int64 v32; // [sp+0h] [bp-90h]@6
  unsigned __int64 v33; // [sp+8h] [bp-88h]@2
  __int64 v34; // [sp+10h] [bp-80h]@1
  __int64 v35; // [sp+18h] [bp-78h]@1
  const void *v36; // [sp+20h] [bp-70h]@1
  __int64 v37; // [sp+28h] [bp-68h]@1
  void *v38; // [sp+30h] [bp-60h]@1
  void *v39; // [sp+38h] [bp-58h]@1
  __int64 v40; // [sp+40h] [bp-50h]@2
  __int64 v41; // [sp+48h] [bp-48h]@1
  __int64 v42; // [sp+50h] [bp-40h]@2
  unsigned __int64 v43; // [sp+58h] [bp-38h]@1
  __int64 v44; // [sp+60h] [bp-30h]@1

  v39 = a6;
  v38 = a5;
  v8 = a4;
  v37 = a3;
  v36 = a2;
  v9 = a1;
  v44 = *(_QWORD *)off_69010[0];
  v43 = *(_QWORD *)a1;
  v10 = *(_QWORD *)a7;
  v35 = a1 + 16;
  v34 = a4 + 16;
  v41 = a7 + 16;
  ccn_mul(v10, a1 + 16, a4 + 16, (unsigned __int64 *)(a7 + 16));
  v11 = *(_QWORD *)v8;
  if ( *(_QWORD *)v8 > (unsigned __int64)v10 )
  {
    v33 = a1;
    v12 = *(_QWORD *)(v8 + 8 * v11 + 8);
    v42 = v8;
    v40 = v11;
    v13 = 0LL;
    if ( v12 )
    {
      v13 = 0LL;
      do
      {
        v13 += ccn_add(v10, v33 + 8 * v10 + 16, v33 + 8 * v10 + 16, v41);
        --v12;
      }
      while ( v12 );
    }
    v9 = v33;
    *(_QWORD *)(16 * v10 + v33 + 16) = v13;
    v11 = v40;
    v8 = v42;
  }
  v42 = v8;
  v40 = v11;
  cczp_init(v9);
  v14 = v43;
  v15 = (unsigned __int64)((char *)&v32 - ((16 * v43 + 39) & 0xFFFFFFFFFFFFFFF0LL));
  *(_QWORD *)v15 = v43;
  v16 = v15 + 16;
  v17 = v9;
  v18 = v14 - v11;
  v33 = v15 + 8 * v11 + 16;
  v32 = v17 + 8 * v11 + 16;
  v19 = ccn_sub(v11, v15 + 16, v35, v34);
  ccn_sub1(v18, v33, v32, v19);
  v20 = v43 - v10;
  v21 = ccn_sub(v10, v16, v16, v41);
  ccn_sub1(v20, v15 + 8 * v10 + 16, v15 + 8 * v10 + 16, v21);
  ccn_add1(v43, v16, v16, 1LL);
  cczp_init(v15);
  v22 = (const void *)v37;
  cczp_mod_inv_slow(v15, v37, v36);
  *(_BYTE *)(v42 + 16) &= 0xFEu;
  v23 = a7;
  *(_BYTE *)(v23 + 16) &= 0xFEu;
  v24 = (char *)&v32 - ((8 * v43 + 31) & 0xFFFFFFFFFFFFFFF0LL);
  v25 = v43;
  ccn_set(v43, v24, v22);
  bzero(&v24[8 * v25], 0x10uLL);
  v26 = v42;
  cczp_init(v42);
  cczp_modn(v26, v38, v25, (__int64)v24);
  cczp_init(a7);
  v27 = v25;
  v28 = v40;
  cczp_modn(a7, a8, v27, (__int64)v24);
  *(_BYTE *)(v26 + 16) |= 1u;
  *(_BYTE *)(v23 + 16) |= 1u;
  if ( v28 > (unsigned __int64)v10 )
    *(_QWORD *)(a7 + 8 * v28 + 8) = 0LL;
  v29 = cczp_mod_inv(v26, v39, (const void *)v41);
  result = 0xFFFFFFFFLL;
  if ( !v29 )
  {
    cczp_init(v26);
    cczp_init(a7);
    result = 0LL;
  }
  v31 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000017BC) ----------------------------------------------------
signed __int64 __fastcall ccrsa_generate_key_raw(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  unsigned __int64 v6; // rax@1
  __int64 v7; // r15@1
  __int64 v8; // r12@1
  unsigned __int64 v9; // rdi@1
  __int64 v10; // rbx@1
  unsigned __int64 v11; // rax@1
  int v12; // ecx@4
  signed __int64 result; // rax@4

  v5 = a5;
  v6 = a1 >> 1;
  v7 = (a1 >> 1) + 1;
  v8 = a1 - v7;
  v9 = (a1 + 63) >> 6;
  *(_QWORD *)a2 = v9;
  v10 = a2 + 32 * v9 + 24;
  v11 = (v6 + 64) >> 6;
  *(_QWORD *)v10 = v11;
  *(_QWORD *)(16 * v11 + v10 + 24) = (unsigned __int64)(v8 + 63) >> 6;
  if ( !(unsigned int)ccn_read_uint(v9, 16LL * *(_QWORD *)a2 + a2 + 24, a3, a4) )
  {
    while ( 1 )
    {
      *(_QWORD *)(v10 + 24LL * *(_QWORD *)v10 + 32) = 0LL;
      if ( (unsigned int)cczp_random_prime(v7, v10, 16LL * *(_QWORD *)a2 + a2 + 24, v5)
        || (unsigned int)cczp_random_prime(v8, 16LL * *(_QWORD *)v10 + v10 + 24, 16LL * *(_QWORD *)a2 + a2 + 24, v5) )
        break;
      v12 = ccrsa_crt_makekey(
              a2,
              (const void *)(16LL * *(_QWORD *)a2 + a2 + 24),
              a2 + 24LL * *(_QWORD *)a2 + 24,
              v10,
              (void *)(32LL * *(_QWORD *)v10 + v10 + 48),
              (void *)(v10 + 48LL * *(_QWORD *)v10 + 48),
              16LL * *(_QWORD *)v10 + v10 + 24,
              (void *)(v10 + 40LL * *(_QWORD *)v10 + 48));
      result = 0LL;
      if ( !v12 )
        return result;
    }
  }
  return 0xFFFFFFFFLL;
}

//----- (000000000000190C) ----------------------------------------------------
signed __int64 __fastcall ccrsa_generate_key(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4, __int64 a5)
{
  signed __int64 result; // rax@1

  result = ccrsa_generate_key_raw(a1, a2, a3, a4, a5);
  if ( !(_DWORD)result )
    result = (unsigned int)((unsigned __int8)~ccrsa_pairwise_consistency_check((__int64 *)a2) << 31 >> 31);
  return result;
}

//----- (0000000000001938) ----------------------------------------------------
__int64 __fastcall cccmac_final(__int64 a1, signed __int64 a2, unsigned __int64 a3, signed __int64 a4, __int64 a5)
{
  signed __int64 v5; // r14@1
  unsigned __int64 v6; // r12@1
  signed __int64 v7; // r15@1
  unsigned __int64 v8; // r13@1
  unsigned __int64 v9; // rbx@1
  size_t v10; // rdx@1
  signed __int64 v11; // rbx@3
  signed __int64 v12; // r12@5
  signed __int64 v13; // rax@5
  signed __int64 v14; // rdi@7
  __int64 v15; // r8@7
  unsigned __int64 v16; // ST08_8@8
  signed __int64 v17; // rsi@8
  signed __int64 v18; // rcx@8
  signed __int64 v19; // r14@8
  signed __int64 v20; // r12@8
  __int64 v21; // r15@8
  __int64 v23; // [sp+0h] [bp-50h]@1
  __int64 v24; // [sp+8h] [bp-48h]@1
  char v25[16]; // [sp+10h] [bp-40h]@1
  __int64 v26; // [sp+20h] [bp-30h]@1

  v24 = a5;
  v5 = a4;
  v6 = a3;
  v7 = a2;
  v23 = a1;
  v26 = *(_QWORD *)off_69010[0];
  v8 = a3 >> 4;
  v9 = a3 & 0xF;
  cc_clear(0x10uLL, v25);
  v10 = v9;
  if ( !v6 || v9 )
  {
    v11 = a2 + 16;
    v25[v10] = -128;
  }
  else
  {
    --v8;
    v10 = 16LL;
    v11 = a2;
  }
  v12 = v5;
  memcpy(v25, (const void *)(v5 + 16 * v8), v10);
  v13 = 15LL;
  do
  {
    v25[v13] ^= *(_BYTE *)(v11 + v13);
    --v13;
  }
  while ( v13 != -1 );
  v14 = a2 + 32;
  v15 = v24;
  if ( v8 )
  {
    do
    {
      v16 = v8;
      v17 = v7 + *(_QWORD *)v23 + 32;
      v18 = v12;
      v19 = v12;
      v20 = v7;
      v21 = v15;
      (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(v23 + 24))(
        v14,
        v17,
        1LL,
        v18);
      v15 = v21;
      v7 = v20;
      v12 = v19 + 16;
      --v8;
    }
    while ( v16 != 1 );
  }
  (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, _QWORD, __int64))(v23 + 24))(
    v14,
    v7 + *(_QWORD *)v23 + 32,
    1LL,
    v25,
    v15);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];
// 1938: using guessed type char var_40[16];

//----- (0000000000001A57) ----------------------------------------------------
signed __int64 __fastcall ccec_check_pub(__int64 **a1)
{
  signed __int64 *v1; // r15@1
  __int64 v2; // r14@1
  __int64 v3; // rbx@1
  __int64 v4; // r13@1
  __int64 v5; // r12@3
  signed __int64 result; // rax@5

  v1 = *a1;
  v2 = (__int64)(a1 + 2);
  v3 = **a1;
  v4 = (__int64)(*a1 + 2);
  if ( (signed int)ccn_cmp(**a1, v4, (__int64)(a1 + 2)) > 0
    && (signed int)ccn_cmp(v3, v4, v2 + 8 * v3) > 0
    && (v5 = (__int64)&a1[2 * v3 + 2], (signed int)ccn_cmp(v3, v4, v5) > 0)
    && ccn_n(v3, v5) )
    result = (unsigned int)((unsigned __int8)~ccec_is_point(v1, v2) << 31 >> 31);
  else
    result = 0xFFFFFFFFLL;
  return result;
}

//----- (0000000000001AF7) ----------------------------------------------------
void __fastcall ccmode_ccm_reset(__int64 a1, __int64 a2)
{
  bzero((void *)(a2 + 32), 0x10uLL);
  bzero((void *)(a2 + 48), 0x10uLL);
  *(_DWORD *)(a2 + 68) = 0;
  *(_QWORD *)(a2 + 88) = 0LL;
  *(_QWORD *)(a2 + 80) = 0LL;
  *(_DWORD *)(a2 + 72) = 0;
  *(_DWORD *)(a2 + 64) = 0;
}

//----- (0000000000001B80) ----------------------------------------------------
void __usercall ONE(__int64 a1@<rax>)
{
  *(_DWORD *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  TWO(a1);
}

//----- (0000000000001B90) ----------------------------------------------------
void __usercall TWO(__int64 a1@<rax>)
{
  LOBYTE(a1) = *(_BYTE *)a1 + a1;
  *(_BYTE *)a1 = a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  JUMPOUT(Lbswap_mask);
}

//----- (0000000000001BB0) ----------------------------------------------------
int __usercall ctr_crypt@<eax>(unsigned __int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __m128i a6@<xmm4>, __m128i a7@<xmm5>, __m128i a8@<xmm6>, __m128i a9@<xmm7>, __m128i a10@<xmm8>, __m128i a11@<xmm9>, __m128i a12@<xmm10>, __m128i a13@<xmm11>, __m128i a14@<xmm12>, __m128i a15@<xmm13>, __m128i a16@<xmm14>, __m128i a17@<xmm15>)
{
  __int128 v18; // [sp+0h] [bp-D0h]@1
  __int128 v19; // [sp+10h] [bp-C0h]@1
  __int128 v20; // [sp+20h] [bp-B0h]@1
  __int128 v21; // [sp+30h] [bp-A0h]@1
  __int128 v22; // [sp+40h] [bp-90h]@1
  __int128 v23; // [sp+50h] [bp-80h]@1
  __int128 v24; // [sp+60h] [bp-70h]@1
  __int128 v25; // [sp+70h] [bp-60h]@1
  __int128 v26; // [sp+80h] [bp-50h]@1
  __int128 v27; // [sp+90h] [bp-40h]@1
  __int128 v28; // [sp+A0h] [bp-30h]@1
  __int128 v29; // [sp+B0h] [bp-20h]@1

  _mm_store_si128((__m128i *)&v18, a6);
  _mm_store_si128((__m128i *)&v19, a7);
  _mm_store_si128((__m128i *)&v20, a8);
  _mm_store_si128((__m128i *)&v21, a9);
  _mm_store_si128((__m128i *)&v22, a10);
  _mm_store_si128((__m128i *)&v23, a11);
  _mm_store_si128((__m128i *)&v24, a12);
  _mm_store_si128((__m128i *)&v25, a13);
  _mm_store_si128((__m128i *)&v26, a14);
  _mm_store_si128((__m128i *)&v27, a15);
  _mm_store_si128((__m128i *)&v28, a16);
  _mm_store_si128((__m128i *)&v29, a17);
  JUMPOUT(a1 < 0x80, &loc_20F7);
  return Main_Decrypt_Loop(a3, a4, a1 - 128, a2, a5, *(_DWORD *)(a5 + 240));
}
// 1C80: using guessed type int __fastcall Main_Decrypt_Loop(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD);

//----- (0000000000001C80) ----------------------------------------------------
#error "2279: positive sp value has been found (funcsize=0)"

//----- (000000000000227A) ----------------------------------------------------
unsigned __int64 __fastcall ccecies_decrypt_gcm_plaintext_size(__int64 **a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rbx@1
  unsigned __int64 v4; // rax@1
  unsigned __int64 v5; // rcx@1

  v3 = a3;
  v4 = ccecies_pub_key_size(a1, a2);
  v5 = 0LL;
  if ( v4 )
    v5 = v3 - v4 - *(_DWORD *)(a2 + 28);
  return v5;
}

//----- (00000000000022A8) ----------------------------------------------------
__int64 __usercall ccsha256_vng_intel_avx1_compress@<rax>(__int64 _RDX@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM4@<xmm4>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>)
{
  signed __int64 v27; // rbx@1
  int v28; // er9@2
  int v29; // er10@2
  int v30; // er12@2
  int v31; // er13@2
  int v32; // er14@2
  int v35; // eax@2
  unsigned __int64 v36; // rt0@2
  int v37; // ecx@2
  int v40; // eax@2
  int v42; // ecx@2
  int v44; // eax@2
  int v46; // er15@2
  int v49; // er11@2
  int v52; // er15@2
  int v55; // eax@2
  int v58; // ecx@2
  int v60; // eax@2
  int v62; // ecx@2
  int v63; // eax@2
  int v65; // er14@2
  int v67; // er10@2
  int v69; // er14@2
  int v70; // eax@2
  int v73; // ecx@2
  int v74; // eax@2
  int v76; // ecx@2
  int v78; // eax@2
  int v80; // er13@2
  int v82; // er9@2
  int v84; // er13@2
  int v85; // eax@2
  int v87; // ecx@2
  int v89; // eax@2
  int v90; // ecx@2
  int v91; // eax@2
  int v93; // er12@2
  int v95; // er8@2
  int v97; // er12@2
  __int128 v98; // ST00_16@2
  int v99; // eax@2
  int v101; // ecx@2
  int v103; // eax@2
  int v105; // ecx@2
  int v107; // eax@2
  int v109; // er11@2
  int v111; // er15@2
  int v114; // er11@2
  int v116; // eax@2
  int v118; // ecx@2
  int v120; // eax@2
  int v122; // ecx@2
  int v124; // eax@2
  int v126; // er10@2
  int v128; // er14@2
  int v131; // er10@2
  int v133; // eax@2
  int v135; // ecx@2
  int v137; // eax@2
  int v139; // ecx@2
  int v141; // eax@2
  int v143; // er9@2
  int v145; // er13@2
  int v148; // er9@2
  int v150; // eax@2
  int v152; // ecx@2
  int v153; // eax@2
  int v155; // ecx@2
  int v157; // eax@2
  int v159; // er8@2
  int v160; // er12@2
  int v162; // er8@2
  __int128 v163; // ST10_16@2
  int v166; // eax@2
  int v167; // ecx@2
  int v170; // eax@2
  int v172; // ecx@2
  int v174; // eax@2
  int v176; // er15@2
  int v179; // er11@2
  int v182; // er15@2
  int v185; // eax@2
  int v188; // ecx@2
  int v190; // eax@2
  int v192; // ecx@2
  int v193; // eax@2
  int v195; // er14@2
  int v197; // er10@2
  int v199; // er14@2
  int v200; // eax@2
  int v203; // ecx@2
  int v204; // eax@2
  int v206; // ecx@2
  int v208; // eax@2
  int v210; // er13@2
  int v212; // er9@2
  int v214; // er13@2
  int v215; // eax@2
  int v217; // ecx@2
  int v219; // eax@2
  int v220; // ecx@2
  int v221; // eax@2
  int v223; // er12@2
  int v225; // er8@2
  int v227; // er12@2
  __int128 v228; // ST20_16@2
  int v229; // eax@2
  int v231; // ecx@2
  int v233; // eax@2
  int v235; // ecx@2
  int v237; // eax@2
  int v239; // er11@2
  int v241; // er15@2
  int v244; // er11@2
  int v246; // eax@2
  int v248; // ecx@2
  int v250; // eax@2
  int v252; // ecx@2
  int v254; // eax@2
  int v256; // er10@2
  int v258; // er14@2
  int v261; // er10@2
  int v263; // eax@2
  int v265; // ecx@2
  int v267; // eax@2
  int v269; // ecx@2
  int v271; // eax@2
  int v273; // er9@2
  int v275; // er13@2
  int v278; // er9@2
  int v280; // eax@2
  int v282; // ecx@2
  int v283; // eax@2
  int v285; // ecx@2
  int v287; // eax@2
  int v289; // er8@2
  int v290; // er12@2
  int v292; // er8@2
  __int128 v293; // ST30_16@2
  int v296; // eax@2
  int v297; // ecx@2
  int v300; // eax@2
  int v302; // ecx@2
  int v304; // eax@2
  int v306; // er15@2
  int v309; // er11@2
  int v312; // er15@2
  int v315; // eax@2
  int v318; // ecx@2
  int v320; // eax@2
  int v322; // ecx@2
  int v323; // eax@2
  int v325; // er14@2
  int v327; // er10@2
  int v329; // er14@2
  int v330; // eax@2
  int v333; // ecx@2
  int v334; // eax@2
  int v336; // ecx@2
  int v338; // eax@2
  int v340; // er13@2
  int v342; // er9@2
  int v344; // er13@2
  int v345; // eax@2
  int v347; // ecx@2
  int v349; // eax@2
  int v350; // ecx@2
  int v351; // eax@2
  int v353; // er12@2
  int v355; // er8@2
  int v357; // er12@2
  __int128 v358; // ST00_16@2
  int v359; // eax@2
  int v361; // ecx@2
  int v363; // eax@2
  int v365; // ecx@2
  int v367; // eax@2
  int v369; // er11@2
  int v371; // er15@2
  int v374; // er11@2
  int v376; // eax@2
  int v378; // ecx@2
  int v380; // eax@2
  int v382; // ecx@2
  int v384; // eax@2
  int v386; // er10@2
  int v388; // er14@2
  int v391; // er10@2
  int v393; // eax@2
  int v395; // ecx@2
  int v397; // eax@2
  int v399; // ecx@2
  int v401; // eax@2
  int v403; // er9@2
  int v405; // er13@2
  int v408; // er9@2
  int v410; // eax@2
  int v412; // ecx@2
  int v413; // eax@2
  int v415; // ecx@2
  int v417; // eax@2
  int v419; // er8@2
  int v420; // er12@2
  int v422; // er8@2
  __int128 v423; // ST10_16@2
  int v426; // eax@2
  int v427; // ecx@2
  int v430; // eax@2
  int v432; // ecx@2
  int v434; // eax@2
  int v436; // er15@2
  int v439; // er11@2
  int v442; // er15@2
  int v445; // eax@2
  int v448; // ecx@2
  int v450; // eax@2
  int v452; // ecx@2
  int v453; // eax@2
  int v455; // er14@2
  int v457; // er10@2
  int v459; // er14@2
  int v460; // eax@2
  int v463; // ecx@2
  int v464; // eax@2
  int v466; // ecx@2
  int v468; // eax@2
  int v470; // er13@2
  int v472; // er9@2
  int v474; // er13@2
  int v475; // eax@2
  int v477; // ecx@2
  int v479; // eax@2
  int v480; // ecx@2
  int v481; // eax@2
  int v483; // er12@2
  int v485; // er8@2
  int v487; // er12@2
  __int128 v488; // ST20_16@2
  int v489; // eax@2
  int v491; // ecx@2
  int v493; // eax@2
  int v495; // ecx@2
  int v497; // eax@2
  int v499; // er11@2
  int v501; // er15@2
  int v504; // er11@2
  int v506; // eax@2
  int v508; // ecx@2
  int v510; // eax@2
  int v512; // ecx@2
  int v514; // eax@2
  int v516; // er10@2
  int v518; // er14@2
  int v521; // er10@2
  int v523; // eax@2
  int v525; // ecx@2
  int v527; // eax@2
  int v529; // ecx@2
  int v531; // eax@2
  int v533; // er9@2
  int v535; // er13@2
  int v538; // er9@2
  int v540; // eax@2
  int v542; // ecx@2
  int v543; // eax@2
  int v545; // ecx@2
  int v547; // eax@2
  int v549; // er8@2
  int v550; // er12@2
  int v552; // er8@2
  __int128 v553; // ST30_16@2
  int v556; // eax@2
  int v557; // ecx@2
  int v560; // eax@2
  int v562; // ecx@2
  int v564; // eax@2
  int v566; // er15@2
  int v569; // er11@2
  int v572; // er15@2
  int v575; // eax@2
  int v578; // ecx@2
  int v580; // eax@2
  int v582; // ecx@2
  int v583; // eax@2
  int v585; // er14@2
  int v587; // er10@2
  int v589; // er14@2
  int v590; // eax@2
  int v593; // ecx@2
  int v594; // eax@2
  int v596; // ecx@2
  int v598; // eax@2
  int v600; // er13@2
  int v602; // er9@2
  int v604; // er13@2
  int v605; // eax@2
  int v607; // ecx@2
  int v609; // eax@2
  int v610; // ecx@2
  int v611; // eax@2
  int v613; // er12@2
  int v615; // er8@2
  int v617; // er12@2
  int v618; // eax@2
  int v620; // ecx@2
  int v622; // eax@2
  int v624; // ecx@2
  int v626; // eax@2
  int v628; // er11@2
  int v630; // er15@2
  int v633; // er11@2
  int v635; // eax@2
  int v637; // ecx@2
  int v639; // eax@2
  int v641; // ecx@2
  int v643; // eax@2
  int v645; // er10@2
  int v647; // er14@2
  int v650; // er10@2
  int v652; // eax@2
  int v654; // ecx@2
  int v656; // eax@2
  int v658; // ecx@2
  int v660; // eax@2
  int v662; // er9@2
  int v664; // er13@2
  int v667; // er9@2
  int v669; // eax@2
  int v671; // ecx@2
  int v672; // eax@2
  int v674; // ecx@2
  int v676; // eax@2
  int v678; // er8@2
  int v679; // er12@2
  int v681; // er8@2
  int v684; // eax@2
  int v685; // ecx@2
  int v688; // eax@2
  int v690; // ecx@2
  int v692; // eax@2
  int v694; // er15@2
  int v697; // er11@2
  int v700; // er15@2
  int v703; // eax@2
  int v706; // ecx@2
  int v708; // eax@2
  int v710; // ecx@2
  int v711; // eax@2
  int v713; // er14@2
  int v715; // er10@2
  int v717; // er14@2
  int v718; // eax@2
  int v721; // ecx@2
  int v722; // eax@2
  int v724; // ecx@2
  int v726; // eax@2
  int v728; // er13@2
  int v730; // er9@2
  int v732; // er13@2
  int v733; // eax@2
  int v735; // ecx@2
  int v737; // eax@2
  int v738; // ecx@2
  int v739; // eax@2
  int v741; // er12@2
  int v743; // er8@2
  int v745; // er12@2
  int v746; // eax@2
  int v748; // ecx@2
  int v750; // eax@2
  int v752; // ecx@2
  int v754; // eax@2
  int v756; // er11@2
  int v758; // er15@2
  int v761; // er11@2
  int v763; // eax@2
  int v765; // ecx@2
  int v767; // eax@2
  int v769; // ecx@2
  int v771; // eax@2
  int v773; // er10@2
  int v775; // er14@2
  int v778; // er10@2
  int v780; // eax@2
  int v782; // ecx@2
  int v784; // eax@2
  int v786; // ecx@2
  int v788; // eax@2
  int v790; // er9@2
  int v792; // er13@2
  int v794; // er9@2
  int v796; // eax@2
  int v797; // ecx@2
  int v798; // eax@2
  int v800; // ecx@2
  int v802; // eax@2
  int v804; // er8@2
  int v805; // er12@2
  int v807; // er8@2
  signed __int64 v808; // rbx@2
  int v810; // eax@3
  unsigned __int64 v811; // rt0@3
  int v812; // ecx@3
  int v813; // eax@3
  int v814; // ecx@3
  int v815; // eax@3
  int v816; // er15@3
  int v817; // er11@3
  int v818; // er15@3
  int v819; // eax@3
  int v820; // ecx@3
  int v821; // eax@3
  int v822; // ecx@3
  int v823; // eax@3
  int v824; // er14@3
  int v825; // er10@3
  int v826; // er14@3
  int v827; // eax@3
  int v828; // ecx@3
  int v829; // eax@3
  int v830; // ecx@3
  int v831; // eax@3
  int v832; // er13@3
  int v833; // er9@3
  int v834; // er13@3
  int v836; // eax@3
  int v837; // ecx@3
  int v838; // eax@3
  int v839; // ecx@3
  int v840; // eax@3
  int v841; // er12@3
  int v842; // er8@3
  int v843; // er12@3
  int v845; // eax@3
  int v846; // ecx@3
  int v847; // eax@3
  int v848; // ecx@3
  int v849; // eax@3
  int v850; // er11@3
  int v851; // er15@3
  int v852; // er11@3
  int v853; // eax@3
  int v854; // ecx@3
  int v855; // eax@3
  int v856; // ecx@3
  int v857; // eax@3
  int v858; // er10@3
  int v859; // er14@3
  int v860; // er10@3
  int v861; // eax@3
  int v862; // ecx@3
  int v863; // eax@3
  int v864; // ecx@3
  int v865; // eax@3
  int v866; // er9@3
  int v867; // er13@3
  int v868; // er9@3
  int v870; // eax@3
  int v871; // ecx@3
  int v872; // eax@3
  int v873; // ecx@3
  int v874; // eax@3
  int v875; // er8@3
  int v876; // er12@3
  int v877; // er8@3
  int v879; // eax@3
  int v880; // ecx@3
  int v881; // eax@3
  int v882; // ecx@3
  int v883; // eax@3
  int v884; // er15@3
  int v885; // er11@3
  int v886; // er15@3
  int v887; // eax@3
  int v888; // ecx@3
  int v889; // eax@3
  int v890; // ecx@3
  int v891; // eax@3
  int v892; // er14@3
  int v893; // er10@3
  int v894; // er14@3
  int v895; // eax@3
  int v896; // ecx@3
  int v897; // eax@3
  int v898; // ecx@3
  int v899; // eax@3
  int v900; // er13@3
  int v901; // er9@3
  int v902; // er13@3
  int v904; // eax@3
  int v905; // ecx@3
  int v906; // eax@3
  int v907; // ecx@3
  int v908; // eax@3
  int v909; // er12@3
  int v910; // er8@3
  int v911; // er12@3
  int v913; // eax@3
  int v914; // ecx@3
  int v915; // eax@3
  int v916; // ecx@3
  int v917; // eax@3
  int v918; // er11@3
  int v919; // er15@3
  int v920; // er11@3
  int v921; // eax@3
  int v922; // ecx@3
  int v923; // eax@3
  int v924; // ecx@3
  int v925; // eax@3
  int v926; // er10@3
  int v927; // er14@3
  int v928; // er10@3
  int v929; // eax@3
  int v930; // ecx@3
  int v931; // eax@3
  int v932; // ecx@3
  int v933; // eax@3
  int v934; // er9@3
  int v935; // er13@3
  int v936; // er9@3
  int v937; // eax@3
  int v938; // ecx@3
  int v939; // eax@3
  int v940; // ecx@3
  int v941; // eax@3
  int v942; // er8@3
  int v943; // eax@4
  unsigned __int64 v944; // rt0@4
  int v945; // eax@4
  int v946; // ecx@4
  int v947; // eax@4
  int v948; // ecx@4
  int v949; // eax@4
  int v950; // er15@4
  int v951; // er11@4
  int v952; // er15@4
  int v953; // eax@4
  int v954; // eax@4
  int v955; // ecx@4
  int v956; // eax@4
  int v957; // ecx@4
  int v958; // eax@4
  int v959; // er14@4
  int v960; // er10@4
  int v961; // er14@4
  int v962; // eax@4
  int v963; // eax@4
  int v964; // ecx@4
  int v965; // eax@4
  int v966; // ecx@4
  int v967; // eax@4
  int v968; // er13@4
  int v969; // er9@4
  int v970; // er13@4
  int v971; // eax@4
  int v972; // eax@4
  int v973; // ecx@4
  int v974; // eax@4
  int v975; // ecx@4
  int v976; // eax@4
  int v977; // er12@4
  int v978; // er8@4
  int v979; // er12@4
  int v980; // eax@4
  int v981; // eax@4
  int v982; // ecx@4
  int v983; // eax@4
  int v984; // ecx@4
  int v985; // eax@4
  int v986; // er11@4
  int v987; // er15@4
  int v988; // er11@4
  int v989; // eax@4
  int v990; // eax@4
  int v991; // ecx@4
  int v992; // eax@4
  int v993; // ecx@4
  int v994; // eax@4
  int v995; // er10@4
  int v996; // er14@4
  int v997; // er10@4
  int v998; // eax@4
  int v999; // eax@4
  int v1000; // ecx@4
  int v1001; // eax@4
  int v1002; // ecx@4
  int v1003; // eax@4
  int v1004; // er9@4
  int v1005; // er13@4
  int v1006; // er9@4
  int v1007; // eax@4
  int v1008; // eax@4
  int v1009; // ecx@4
  int v1010; // eax@4
  int v1011; // ecx@4
  int v1012; // eax@4
  int v1013; // er8@4
  int v1014; // er12@4
  int v1015; // er8@4
  int v1016; // eax@4
  int v1017; // eax@4
  int v1018; // ecx@4
  int v1019; // eax@4
  int v1020; // ecx@4
  int v1021; // eax@4
  int v1022; // er15@4
  int v1023; // er11@4
  int v1024; // er15@4
  int v1025; // eax@4
  int v1026; // eax@4
  int v1027; // ecx@4
  int v1028; // eax@4
  int v1029; // ecx@4
  int v1030; // eax@4
  int v1031; // er14@4
  int v1032; // er10@4
  int v1033; // er14@4
  int v1034; // eax@4
  int v1035; // eax@4
  int v1036; // ecx@4
  int v1037; // eax@4
  int v1038; // ecx@4
  int v1039; // eax@4
  int v1040; // er13@4
  int v1041; // er9@4
  int v1042; // er13@4
  int v1043; // eax@4
  int v1044; // eax@4
  int v1045; // ecx@4
  int v1046; // eax@4
  int v1047; // ecx@4
  int v1048; // eax@4
  int v1049; // er12@4
  int v1050; // er8@4
  int v1051; // er12@4
  int v1052; // eax@4
  int v1053; // eax@4
  int v1054; // ecx@4
  int v1055; // eax@4
  int v1056; // ecx@4
  int v1057; // eax@4
  int v1058; // er11@4
  int v1059; // er15@4
  int v1060; // er11@4
  int v1061; // eax@4
  int v1062; // eax@4
  int v1063; // ecx@4
  int v1064; // eax@4
  int v1065; // ecx@4
  int v1066; // eax@4
  int v1067; // er10@4
  int v1068; // er14@4
  int v1069; // er10@4
  int v1070; // eax@4
  int v1071; // eax@4
  int v1072; // ecx@4
  int v1073; // eax@4
  int v1074; // ecx@4
  int v1075; // eax@4
  int v1076; // er9@4
  int v1077; // er13@4
  unsigned int v1078; // er9@4
  int v1079; // eax@4
  int v1080; // eax@4
  int v1081; // ecx@4
  int v1082; // eax@4
  int v1083; // ecx@4
  int v1084; // eax@4
  int v1085; // er8@4
  __int64 result; // rax@4
  __int128 v1096; // [sp+0h] [bp-108h]@1
  __int128 v1097; // [sp+0h] [bp-108h]@2
  __int128 v1099; // [sp+10h] [bp-F8h]@1
  __int128 v1100; // [sp+10h] [bp-F8h]@2
  __int128 v1102; // [sp+20h] [bp-E8h]@1
  __int128 v1103; // [sp+20h] [bp-E8h]@2
  __int128 v1105; // [sp+30h] [bp-D8h]@1
  __int128 v1106; // [sp+30h] [bp-D8h]@2

  __asm
  {
    vmovdqa [rsp+108h+var_B8], xmm0
    vmovdqa [rsp+108h+var_A8], xmm1
    vmovdqa [rsp+108h+var_98], xmm2
    vmovdqa [rsp+108h+var_88], xmm3
    vmovdqa [rsp+108h+var_78], xmm4
    vmovdqa [rsp+108h+var_68], xmm5
    vmovdqa [rsp+108h+var_58], xmm6
    vmovdqa [rsp+108h+var_48], xmm7
  }
  _RAX = (__int64)qword_4B930;
  __asm
  {
    vmovdqa xmm0, xmmword ptr [rax]
    vmovdqa [rsp+108h+var_C8], xmm0
    vmovdqu xmm0, xmmword ptr [rdx]
    vmovdqu xmm1, xmmword ptr [rdx+10h]
    vmovdqu xmm2, xmmword ptr [rdx+20h]
    vmovdqu xmm3, xmmword ptr [rdx+30h]
  }
  _RDX = _RDX + 64;
  __asm
  {
    vpshufb xmm0, xmm0, [rsp+108h+var_C8]
    vpshufb xmm1, xmm1, [rsp+108h+var_C8]
    vpshufb xmm2, xmm2, [rsp+108h+var_C8]
    vpshufb xmm3, xmm3, [rsp+108h+var_C8]
    vpaddd  xmm4, xmm0, xmmword ptr [rbx]
    vpaddd  xmm5, xmm1, xmmword ptr [rbx+10h]
    vpaddd  xmm6, xmm2, xmmword ptr [rbx+20h]
    vpaddd  xmm7, xmm3, xmmword ptr [rbx+30h]
  }
  v27 = (signed __int64)&ccsha256_K[8];
  __asm
  {
    vmovdqa [rsp+108h+var_108], xmm4
    vmovdqa [rsp+108h+var_F8], xmm5
    vmovdqa [rsp+108h+var_E8], xmm6
    vmovdqa [rsp+108h+var_D8], xmm7
  }
  while ( 1 )
  {
    v28 = *(_DWORD *)(a2 + 4);
    v29 = *(_DWORD *)(a2 + 8);
    v30 = *(_DWORD *)(a2 + 16);
    v31 = *(_DWORD *)(a2 + 20);
    v32 = *(_DWORD *)(a2 + 24);
    __asm
    {
      vpalignr xmm6, xmm3, xmm2, 4
      vpalignr xmm4, xmm1, xmm0, 4
    }
    LODWORD(v36) = *(_DWORD *)(a2 + 16);
    HIDWORD(v36) = *(_DWORD *)(a2 + 16);
    v35 = v36 >> 14;
    LODWORD(v36) = *(_DWORD *)a2;
    HIDWORD(v36) = *(_DWORD *)a2;
    v37 = *(_DWORD *)a2 ^ (v36 >> 9);
    __asm { vpaddd  xmm0, xmm0, xmm6 }
    LODWORD(v36) = v30 ^ v35;
    HIDWORD(v36) = v30 ^ v35;
    __asm { vpslld  xmm7, xmm4, 0Eh }
    v40 = v30 ^ (v36 >> 5);
    __asm { vpsrld  xmm6, xmm4, 7 }
    LODWORD(v36) = v37;
    HIDWORD(v36) = v37;
    v42 = *(_DWORD *)a2 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm4, 3 }
    LODWORD(v36) = v40;
    HIDWORD(v36) = v40;
    v44 = v36 >> 6;
    LODWORD(v36) = v42;
    HIDWORD(v36) = v42;
    __asm { vpxor   xmm4, xmm4, xmm7 }
    v46 = v1096 + v44 + (v32 ^ v30 & (v32 ^ *(_DWORD *)(a2 + 20))) + *(_DWORD *)(a2 + 28);
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      vpslld  xmm7, xmm7, 0Bh
    }
    v49 = v46 + *(_DWORD *)(a2 + 12);
    __asm
    {
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm6
    }
    v52 = (v29 & *(_DWORD *)a2 | v28 & (v29 | *(_DWORD *)a2)) + (v36 >> 2) + v46;
    __asm
    {
      vunpckhps xmm6, xmm3, xmm3
      vpxor   xmm4, xmm4, xmm7
    }
    LODWORD(v36) = v49;
    HIDWORD(v36) = v49;
    v55 = v36 >> 14;
    LODWORD(v36) = v52;
    HIDWORD(v36) = v52;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v58 = v52 ^ (v36 >> 9);
    LODWORD(v36) = v49 ^ v55;
    HIDWORD(v36) = v49 ^ v55;
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    v60 = v49 ^ (v36 >> 5);
    LODWORD(v36) = v58;
    HIDWORD(v36) = v58;
    __asm { vpsrld  xmm4, xmm3, 0Ah }
    v62 = v52 ^ (v36 >> 11);
    LODWORD(v36) = v60;
    HIDWORD(v36) = v60;
    v63 = v36 >> 6;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v62;
    HIDWORD(v36) = v62;
    v65 = DWORD1(v1096) + v63 + (v31 ^ v49 & (v31 ^ *(_DWORD *)(a2 + 16))) + v32;
    __asm { vpshufd xmm6, xmm6, 80h }
    v67 = v65 + v29;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v69 = (v28 & v52 | *(_DWORD *)a2 & (v28 | v52)) + (v36 >> 2) + v65;
    LODWORD(v36) = v67;
    HIDWORD(v36) = v67;
    v70 = v36 >> 14;
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v69;
    HIDWORD(v36) = v69;
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    v73 = v69 ^ (v36 >> 9);
    LODWORD(v36) = v67 ^ v70;
    HIDWORD(v36) = v67 ^ v70;
    v74 = v67 ^ (v36 >> 5);
    __asm { vunpcklps xmm6, xmm0, xmm0 }
    LODWORD(v36) = v73;
    HIDWORD(v36) = v73;
    v76 = v69 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm0, 0Ah }
    LODWORD(v36) = v74;
    HIDWORD(v36) = v74;
    v78 = v36 >> 6;
    LODWORD(v36) = v76;
    HIDWORD(v36) = v76;
    __asm { vpsrlq  xmm7, xmm6, 11h }
    v80 = DWORD2(v1096) + v78 + (v30 ^ v67 & (v30 ^ v49)) + v31;
    __asm { vpsrlq  xmm6, xmm6, 13h }
    v82 = v80 + v28;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    v84 = (*(_DWORD *)a2 & v69 | v52 & (*(_DWORD *)a2 | v69)) + (v36 >> 2) + v80;
    LODWORD(v36) = v82;
    HIDWORD(v36) = v82;
    v85 = v36 >> 14;
    __asm { vpshufd xmm6, xmm6, 8 }
    LODWORD(v36) = v84;
    HIDWORD(v36) = v84;
    v87 = v84 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v82 ^ v85;
    HIDWORD(v36) = v82 ^ v85;
    v89 = v82 ^ (v36 >> 5);
    LODWORD(v36) = v87;
    HIDWORD(v36) = v87;
    v90 = v84 ^ (v36 >> 11);
    LODWORD(v36) = v89;
    HIDWORD(v36) = v89;
    v91 = v36 >> 6;
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v90;
    HIDWORD(v36) = v90;
    v93 = DWORD3(v1096) + v91 + (v49 ^ v82 & (v49 ^ v67)) + v30;
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    v95 = v93 + *(_DWORD *)a2;
    __asm { vpaddd  xmm4, xmm0, xmmword ptr [rbx] }
    v97 = (v52 & v84 | v69 & (v52 | v84)) + (v36 >> 2) + v93;
    __asm { vmovdqa [rsp+108h+var_108], xmm4 }
    LODWORD(v36) = v95;
    HIDWORD(v36) = v95;
    v99 = v36 >> 14;
    LODWORD(v36) = v97;
    HIDWORD(v36) = v97;
    __asm { vpalignr xmm6, xmm0, xmm3, 4 }
    v101 = v97 ^ (v36 >> 9);
    __asm { vpalignr xmm4, xmm2, xmm1, 4 }
    LODWORD(v36) = v95 ^ v99;
    HIDWORD(v36) = v95 ^ v99;
    v103 = v95 ^ (v36 >> 5);
    __asm { vpaddd  xmm1, xmm1, xmm6 }
    LODWORD(v36) = v101;
    HIDWORD(v36) = v101;
    v105 = v97 ^ (v36 >> 11);
    __asm { vpslld  xmm7, xmm4, 0Eh }
    LODWORD(v36) = v103;
    HIDWORD(v36) = v103;
    v107 = v36 >> 6;
    LODWORD(v36) = v105;
    HIDWORD(v36) = v105;
    __asm { vpsrld  xmm6, xmm4, 7 }
    v109 = v1099 + v107 + (v67 ^ v95 & (v67 ^ v82)) + v49;
    __asm { vpsrld  xmm4, xmm4, 3 }
    v111 = v109 + v52;
    __asm
    {
      vpxor   xmm4, xmm4, xmm7
      vpxor   xmm4, xmm4, xmm6
    }
    v114 = (v69 & v97 | v84 & (v69 | v97)) + (v36 >> 2) + v109;
    __asm { vpsrld  xmm6, xmm6, 0Bh }
    LODWORD(v36) = v111;
    HIDWORD(v36) = v111;
    v116 = v36 >> 14;
    LODWORD(v36) = v114;
    HIDWORD(v36) = v114;
    __asm { vpslld  xmm7, xmm7, 0Bh }
    v118 = v114 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v111 ^ v116;
    HIDWORD(v36) = v111 ^ v116;
    v120 = v111 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm7 }
    LODWORD(v36) = v118;
    HIDWORD(v36) = v118;
    v122 = v114 ^ (v36 >> 11);
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    LODWORD(v36) = v120;
    HIDWORD(v36) = v120;
    v124 = v36 >> 6;
    LODWORD(v36) = v122;
    HIDWORD(v36) = v122;
    __asm { vunpckhps xmm6, xmm0, xmm0 }
    v126 = DWORD1(v1099) + v124 + (v82 ^ v111 & (v82 ^ v95)) + v67;
    __asm { vpsrld  xmm4, xmm0, 0Ah }
    v128 = v126 + v69;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v131 = (v84 & v114 | v97 & (v84 | v114)) + (v36 >> 2) + v126;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v128;
    HIDWORD(v36) = v128;
    v133 = v36 >> 14;
    LODWORD(v36) = v131;
    HIDWORD(v36) = v131;
    __asm { vpshufd xmm6, xmm6, 80h }
    v135 = v131 ^ (v36 >> 9);
    LODWORD(v36) = v128 ^ v133;
    HIDWORD(v36) = v128 ^ v133;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v137 = v128 ^ (v36 >> 5);
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v135;
    HIDWORD(v36) = v135;
    v139 = v131 ^ (v36 >> 11);
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    LODWORD(v36) = v137;
    HIDWORD(v36) = v137;
    v141 = v36 >> 6;
    LODWORD(v36) = v139;
    HIDWORD(v36) = v139;
    __asm { vunpcklps xmm6, xmm1, xmm1 }
    v143 = DWORD2(v1099) + v141 + (v95 ^ v128 & (v95 ^ v111)) + v82;
    __asm { vpsrld  xmm4, xmm1, 0Ah }
    v145 = v143 + v84;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v148 = (v97 & v131 | v114 & (v97 | v131)) + (v36 >> 2) + v143;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v145;
    HIDWORD(v36) = v145;
    v150 = v36 >> 14;
    LODWORD(v36) = v148;
    HIDWORD(v36) = v148;
    __asm { vpshufd xmm6, xmm6, 8 }
    v152 = v148 ^ (v36 >> 9);
    LODWORD(v36) = v145 ^ v150;
    HIDWORD(v36) = v145 ^ v150;
    v153 = v145 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v152;
    HIDWORD(v36) = v152;
    v155 = v148 ^ (v36 >> 11);
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v153;
    HIDWORD(v36) = v153;
    v157 = v36 >> 6;
    LODWORD(v36) = v155;
    HIDWORD(v36) = v155;
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    v159 = DWORD3(v1099) + v157 + (v111 ^ v145 & (v111 ^ v128)) + v95;
    v160 = v159 + v97;
    __asm { vpaddd  xmm4, xmm1, xmmword ptr [rbx] }
    v162 = (v114 & v148 | v131 & (v114 | v148)) + (v36 >> 2) + v159;
    __asm
    {
      vmovdqa [rsp+108h+var_F8], xmm4
      vpalignr xmm6, xmm1, xmm0, 4
      vpalignr xmm4, xmm3, xmm2, 4
    }
    LODWORD(v36) = v160;
    HIDWORD(v36) = v160;
    v166 = v36 >> 14;
    LODWORD(v36) = v162;
    HIDWORD(v36) = v162;
    v167 = v162 ^ (v36 >> 9);
    __asm { vpaddd  xmm2, xmm2, xmm6 }
    LODWORD(v36) = v160 ^ v166;
    HIDWORD(v36) = v160 ^ v166;
    __asm { vpslld  xmm7, xmm4, 0Eh }
    v170 = v160 ^ (v36 >> 5);
    __asm { vpsrld  xmm6, xmm4, 7 }
    LODWORD(v36) = v167;
    HIDWORD(v36) = v167;
    v172 = v162 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm4, 3 }
    LODWORD(v36) = v170;
    HIDWORD(v36) = v170;
    v174 = v36 >> 6;
    LODWORD(v36) = v172;
    HIDWORD(v36) = v172;
    __asm { vpxor   xmm4, xmm4, xmm7 }
    v176 = v1102 + v174 + (v128 ^ v160 & (v128 ^ v145)) + v111;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      vpslld  xmm7, xmm7, 0Bh
    }
    v179 = v176 + v114;
    __asm
    {
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm6
    }
    v182 = (v131 & v162 | v148 & (v131 | v162)) + (v36 >> 2) + v176;
    __asm
    {
      vunpckhps xmm6, xmm1, xmm1
      vpxor   xmm4, xmm4, xmm7
    }
    LODWORD(v36) = v179;
    HIDWORD(v36) = v179;
    v185 = v36 >> 14;
    LODWORD(v36) = v182;
    HIDWORD(v36) = v182;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v188 = v182 ^ (v36 >> 9);
    LODWORD(v36) = v179 ^ v185;
    HIDWORD(v36) = v179 ^ v185;
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    v190 = v179 ^ (v36 >> 5);
    LODWORD(v36) = v188;
    HIDWORD(v36) = v188;
    __asm { vpsrld  xmm4, xmm1, 0Ah }
    v192 = v182 ^ (v36 >> 11);
    LODWORD(v36) = v190;
    HIDWORD(v36) = v190;
    v193 = v36 >> 6;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v192;
    HIDWORD(v36) = v192;
    v195 = DWORD1(v1102) + v193 + (v145 ^ v179 & (v145 ^ v160)) + v128;
    __asm { vpshufd xmm6, xmm6, 80h }
    v197 = v195 + v131;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v199 = (v148 & v182 | v162 & (v148 | v182)) + (v36 >> 2) + v195;
    LODWORD(v36) = v197;
    HIDWORD(v36) = v197;
    v200 = v36 >> 14;
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v199;
    HIDWORD(v36) = v199;
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    v203 = v199 ^ (v36 >> 9);
    LODWORD(v36) = v197 ^ v200;
    HIDWORD(v36) = v197 ^ v200;
    v204 = v197 ^ (v36 >> 5);
    __asm { vunpcklps xmm6, xmm2, xmm2 }
    LODWORD(v36) = v203;
    HIDWORD(v36) = v203;
    v206 = v199 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm2, 0Ah }
    LODWORD(v36) = v204;
    HIDWORD(v36) = v204;
    v208 = v36 >> 6;
    LODWORD(v36) = v206;
    HIDWORD(v36) = v206;
    __asm { vpsrlq  xmm7, xmm6, 11h }
    v210 = DWORD2(v1102) + v208 + (v160 ^ v197 & (v160 ^ v179)) + v145;
    __asm { vpsrlq  xmm6, xmm6, 13h }
    v212 = v210 + v148;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    v214 = (v162 & v199 | v182 & (v162 | v199)) + (v36 >> 2) + v210;
    LODWORD(v36) = v212;
    HIDWORD(v36) = v212;
    v215 = v36 >> 14;
    __asm { vpshufd xmm6, xmm6, 8 }
    LODWORD(v36) = v214;
    HIDWORD(v36) = v214;
    v217 = v214 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v212 ^ v215;
    HIDWORD(v36) = v212 ^ v215;
    v219 = v212 ^ (v36 >> 5);
    LODWORD(v36) = v217;
    HIDWORD(v36) = v217;
    v220 = v214 ^ (v36 >> 11);
    LODWORD(v36) = v219;
    HIDWORD(v36) = v219;
    v221 = v36 >> 6;
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v220;
    HIDWORD(v36) = v220;
    v223 = DWORD3(v1102) + v221 + (v179 ^ v212 & (v179 ^ v197)) + v160;
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    v225 = v223 + v162;
    __asm { vpaddd  xmm4, xmm2, xmmword ptr [rbx] }
    v227 = (v182 & v214 | v199 & (v182 | v214)) + (v36 >> 2) + v223;
    __asm { vmovdqa [rsp+108h+var_E8], xmm4 }
    LODWORD(v36) = v225;
    HIDWORD(v36) = v225;
    v229 = v36 >> 14;
    LODWORD(v36) = v227;
    HIDWORD(v36) = v227;
    __asm { vpalignr xmm6, xmm2, xmm1, 4 }
    v231 = v227 ^ (v36 >> 9);
    __asm { vpalignr xmm4, xmm0, xmm3, 4 }
    LODWORD(v36) = v225 ^ v229;
    HIDWORD(v36) = v225 ^ v229;
    v233 = v225 ^ (v36 >> 5);
    __asm { vpaddd  xmm3, xmm3, xmm6 }
    LODWORD(v36) = v231;
    HIDWORD(v36) = v231;
    v235 = v227 ^ (v36 >> 11);
    __asm { vpslld  xmm7, xmm4, 0Eh }
    LODWORD(v36) = v233;
    HIDWORD(v36) = v233;
    v237 = v36 >> 6;
    LODWORD(v36) = v235;
    HIDWORD(v36) = v235;
    __asm { vpsrld  xmm6, xmm4, 7 }
    v239 = v1105 + v237 + (v197 ^ v225 & (v197 ^ v212)) + v179;
    __asm { vpsrld  xmm4, xmm4, 3 }
    v241 = v239 + v182;
    __asm
    {
      vpxor   xmm4, xmm4, xmm7
      vpxor   xmm4, xmm4, xmm6
    }
    v244 = (v199 & v227 | v214 & (v199 | v227)) + (v36 >> 2) + v239;
    __asm { vpsrld  xmm6, xmm6, 0Bh }
    LODWORD(v36) = v241;
    HIDWORD(v36) = v241;
    v246 = v36 >> 14;
    LODWORD(v36) = v244;
    HIDWORD(v36) = v244;
    __asm { vpslld  xmm7, xmm7, 0Bh }
    v248 = v244 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v241 ^ v246;
    HIDWORD(v36) = v241 ^ v246;
    v250 = v241 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm7 }
    LODWORD(v36) = v248;
    HIDWORD(v36) = v248;
    v252 = v244 ^ (v36 >> 11);
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    LODWORD(v36) = v250;
    HIDWORD(v36) = v250;
    v254 = v36 >> 6;
    LODWORD(v36) = v252;
    HIDWORD(v36) = v252;
    __asm { vunpckhps xmm6, xmm2, xmm2 }
    v256 = DWORD1(v1105) + v254 + (v212 ^ v241 & (v212 ^ v225)) + v197;
    __asm { vpsrld  xmm4, xmm2, 0Ah }
    v258 = v256 + v199;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v261 = (v214 & v244 | v227 & (v214 | v244)) + (v36 >> 2) + v256;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v258;
    HIDWORD(v36) = v258;
    v263 = v36 >> 14;
    LODWORD(v36) = v261;
    HIDWORD(v36) = v261;
    __asm { vpshufd xmm6, xmm6, 80h }
    v265 = v261 ^ (v36 >> 9);
    LODWORD(v36) = v258 ^ v263;
    HIDWORD(v36) = v258 ^ v263;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v267 = v258 ^ (v36 >> 5);
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v265;
    HIDWORD(v36) = v265;
    v269 = v261 ^ (v36 >> 11);
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    LODWORD(v36) = v267;
    HIDWORD(v36) = v267;
    v271 = v36 >> 6;
    LODWORD(v36) = v269;
    HIDWORD(v36) = v269;
    __asm { vunpcklps xmm6, xmm3, xmm3 }
    v273 = DWORD2(v1105) + v271 + (v225 ^ v258 & (v225 ^ v241)) + v212;
    __asm { vpsrld  xmm4, xmm3, 0Ah }
    v275 = v273 + v214;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v278 = (v227 & v261 | v244 & (v227 | v261)) + (v36 >> 2) + v273;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v275;
    HIDWORD(v36) = v275;
    v280 = v36 >> 14;
    LODWORD(v36) = v278;
    HIDWORD(v36) = v278;
    __asm { vpshufd xmm6, xmm6, 8 }
    v282 = v278 ^ (v36 >> 9);
    LODWORD(v36) = v275 ^ v280;
    HIDWORD(v36) = v275 ^ v280;
    v283 = v275 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v282;
    HIDWORD(v36) = v282;
    v285 = v278 ^ (v36 >> 11);
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v283;
    HIDWORD(v36) = v283;
    v287 = v36 >> 6;
    LODWORD(v36) = v285;
    HIDWORD(v36) = v285;
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    v289 = DWORD3(v1105) + v287 + (v241 ^ v275 & (v241 ^ v258)) + v225;
    v290 = v289 + v227;
    __asm { vpaddd  xmm4, xmm3, xmmword ptr [rbx] }
    v292 = (v244 & v278 | v261 & (v244 | v278)) + (v36 >> 2) + v289;
    __asm
    {
      vmovdqa [rsp+108h+var_D8], xmm4
      vpalignr xmm6, xmm3, xmm2, 4
      vpalignr xmm4, xmm1, xmm0, 4
    }
    LODWORD(v36) = v290;
    HIDWORD(v36) = v290;
    v296 = v36 >> 14;
    LODWORD(v36) = v292;
    HIDWORD(v36) = v292;
    v297 = v292 ^ (v36 >> 9);
    __asm { vpaddd  xmm0, xmm0, xmm6 }
    LODWORD(v36) = v290 ^ v296;
    HIDWORD(v36) = v290 ^ v296;
    __asm { vpslld  xmm7, xmm4, 0Eh }
    v300 = v290 ^ (v36 >> 5);
    __asm { vpsrld  xmm6, xmm4, 7 }
    LODWORD(v36) = v297;
    HIDWORD(v36) = v297;
    v302 = v292 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm4, 3 }
    LODWORD(v36) = v300;
    HIDWORD(v36) = v300;
    v304 = v36 >> 6;
    LODWORD(v36) = v302;
    HIDWORD(v36) = v302;
    __asm { vpxor   xmm4, xmm4, xmm7 }
    v306 = v98 + v304 + (v258 ^ v290 & (v258 ^ v275)) + v241;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      vpslld  xmm7, xmm7, 0Bh
    }
    v309 = v306 + v244;
    __asm
    {
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm6
    }
    v312 = (v261 & v292 | v278 & (v261 | v292)) + (v36 >> 2) + v306;
    __asm
    {
      vunpckhps xmm6, xmm3, xmm3
      vpxor   xmm4, xmm4, xmm7
    }
    LODWORD(v36) = v309;
    HIDWORD(v36) = v309;
    v315 = v36 >> 14;
    LODWORD(v36) = v312;
    HIDWORD(v36) = v312;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v318 = v312 ^ (v36 >> 9);
    LODWORD(v36) = v309 ^ v315;
    HIDWORD(v36) = v309 ^ v315;
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    v320 = v309 ^ (v36 >> 5);
    LODWORD(v36) = v318;
    HIDWORD(v36) = v318;
    __asm { vpsrld  xmm4, xmm3, 0Ah }
    v322 = v312 ^ (v36 >> 11);
    LODWORD(v36) = v320;
    HIDWORD(v36) = v320;
    v323 = v36 >> 6;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v322;
    HIDWORD(v36) = v322;
    v325 = DWORD1(v98) + v323 + (v275 ^ v309 & (v275 ^ v290)) + v258;
    __asm { vpshufd xmm6, xmm6, 80h }
    v327 = v325 + v261;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v329 = (v278 & v312 | v292 & (v278 | v312)) + (v36 >> 2) + v325;
    LODWORD(v36) = v327;
    HIDWORD(v36) = v327;
    v330 = v36 >> 14;
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v329;
    HIDWORD(v36) = v329;
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    v333 = v329 ^ (v36 >> 9);
    LODWORD(v36) = v327 ^ v330;
    HIDWORD(v36) = v327 ^ v330;
    v334 = v327 ^ (v36 >> 5);
    __asm { vunpcklps xmm6, xmm0, xmm0 }
    LODWORD(v36) = v333;
    HIDWORD(v36) = v333;
    v336 = v329 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm0, 0Ah }
    LODWORD(v36) = v334;
    HIDWORD(v36) = v334;
    v338 = v36 >> 6;
    LODWORD(v36) = v336;
    HIDWORD(v36) = v336;
    __asm { vpsrlq  xmm7, xmm6, 11h }
    v340 = DWORD2(v98) + v338 + (v290 ^ v327 & (v290 ^ v309)) + v275;
    __asm { vpsrlq  xmm6, xmm6, 13h }
    v342 = v340 + v278;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    v344 = (v292 & v329 | v312 & (v292 | v329)) + (v36 >> 2) + v340;
    LODWORD(v36) = v342;
    HIDWORD(v36) = v342;
    v345 = v36 >> 14;
    __asm { vpshufd xmm6, xmm6, 8 }
    LODWORD(v36) = v344;
    HIDWORD(v36) = v344;
    v347 = v344 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v342 ^ v345;
    HIDWORD(v36) = v342 ^ v345;
    v349 = v342 ^ (v36 >> 5);
    LODWORD(v36) = v347;
    HIDWORD(v36) = v347;
    v350 = v344 ^ (v36 >> 11);
    LODWORD(v36) = v349;
    HIDWORD(v36) = v349;
    v351 = v36 >> 6;
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v350;
    HIDWORD(v36) = v350;
    v353 = DWORD3(v98) + v351 + (v309 ^ v342 & (v309 ^ v327)) + v290;
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    v355 = v353 + v292;
    __asm { vpaddd  xmm4, xmm0, xmmword ptr [rbx] }
    v357 = (v312 & v344 | v329 & (v312 | v344)) + (v36 >> 2) + v353;
    __asm { vmovdqa [rsp+108h+var_108], xmm4 }
    LODWORD(v36) = v355;
    HIDWORD(v36) = v355;
    v359 = v36 >> 14;
    LODWORD(v36) = v357;
    HIDWORD(v36) = v357;
    __asm { vpalignr xmm6, xmm0, xmm3, 4 }
    v361 = v357 ^ (v36 >> 9);
    __asm { vpalignr xmm4, xmm2, xmm1, 4 }
    LODWORD(v36) = v355 ^ v359;
    HIDWORD(v36) = v355 ^ v359;
    v363 = v355 ^ (v36 >> 5);
    __asm { vpaddd  xmm1, xmm1, xmm6 }
    LODWORD(v36) = v361;
    HIDWORD(v36) = v361;
    v365 = v357 ^ (v36 >> 11);
    __asm { vpslld  xmm7, xmm4, 0Eh }
    LODWORD(v36) = v363;
    HIDWORD(v36) = v363;
    v367 = v36 >> 6;
    LODWORD(v36) = v365;
    HIDWORD(v36) = v365;
    __asm { vpsrld  xmm6, xmm4, 7 }
    v369 = v163 + v367 + (v327 ^ v355 & (v327 ^ v342)) + v309;
    __asm { vpsrld  xmm4, xmm4, 3 }
    v371 = v369 + v312;
    __asm
    {
      vpxor   xmm4, xmm4, xmm7
      vpxor   xmm4, xmm4, xmm6
    }
    v374 = (v329 & v357 | v344 & (v329 | v357)) + (v36 >> 2) + v369;
    __asm { vpsrld  xmm6, xmm6, 0Bh }
    LODWORD(v36) = v371;
    HIDWORD(v36) = v371;
    v376 = v36 >> 14;
    LODWORD(v36) = v374;
    HIDWORD(v36) = v374;
    __asm { vpslld  xmm7, xmm7, 0Bh }
    v378 = v374 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v371 ^ v376;
    HIDWORD(v36) = v371 ^ v376;
    v380 = v371 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm7 }
    LODWORD(v36) = v378;
    HIDWORD(v36) = v378;
    v382 = v374 ^ (v36 >> 11);
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    LODWORD(v36) = v380;
    HIDWORD(v36) = v380;
    v384 = v36 >> 6;
    LODWORD(v36) = v382;
    HIDWORD(v36) = v382;
    __asm { vunpckhps xmm6, xmm0, xmm0 }
    v386 = DWORD1(v163) + v384 + (v342 ^ v371 & (v342 ^ v355)) + v327;
    __asm { vpsrld  xmm4, xmm0, 0Ah }
    v388 = v386 + v329;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v391 = (v344 & v374 | v357 & (v344 | v374)) + (v36 >> 2) + v386;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v388;
    HIDWORD(v36) = v388;
    v393 = v36 >> 14;
    LODWORD(v36) = v391;
    HIDWORD(v36) = v391;
    __asm { vpshufd xmm6, xmm6, 80h }
    v395 = v391 ^ (v36 >> 9);
    LODWORD(v36) = v388 ^ v393;
    HIDWORD(v36) = v388 ^ v393;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v397 = v388 ^ (v36 >> 5);
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v395;
    HIDWORD(v36) = v395;
    v399 = v391 ^ (v36 >> 11);
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    LODWORD(v36) = v397;
    HIDWORD(v36) = v397;
    v401 = v36 >> 6;
    LODWORD(v36) = v399;
    HIDWORD(v36) = v399;
    __asm { vunpcklps xmm6, xmm1, xmm1 }
    v403 = DWORD2(v163) + v401 + (v355 ^ v388 & (v355 ^ v371)) + v342;
    __asm { vpsrld  xmm4, xmm1, 0Ah }
    v405 = v403 + v344;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v408 = (v357 & v391 | v374 & (v357 | v391)) + (v36 >> 2) + v403;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v405;
    HIDWORD(v36) = v405;
    v410 = v36 >> 14;
    LODWORD(v36) = v408;
    HIDWORD(v36) = v408;
    __asm { vpshufd xmm6, xmm6, 8 }
    v412 = v408 ^ (v36 >> 9);
    LODWORD(v36) = v405 ^ v410;
    HIDWORD(v36) = v405 ^ v410;
    v413 = v405 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v412;
    HIDWORD(v36) = v412;
    v415 = v408 ^ (v36 >> 11);
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v413;
    HIDWORD(v36) = v413;
    v417 = v36 >> 6;
    LODWORD(v36) = v415;
    HIDWORD(v36) = v415;
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    v419 = DWORD3(v163) + v417 + (v371 ^ v405 & (v371 ^ v388)) + v355;
    v420 = v419 + v357;
    __asm { vpaddd  xmm4, xmm1, xmmword ptr [rbx] }
    v422 = (v374 & v408 | v391 & (v374 | v408)) + (v36 >> 2) + v419;
    __asm
    {
      vmovdqa [rsp+108h+var_F8], xmm4
      vpalignr xmm6, xmm1, xmm0, 4
      vpalignr xmm4, xmm3, xmm2, 4
    }
    LODWORD(v36) = v420;
    HIDWORD(v36) = v420;
    v426 = v36 >> 14;
    LODWORD(v36) = v422;
    HIDWORD(v36) = v422;
    v427 = v422 ^ (v36 >> 9);
    __asm { vpaddd  xmm2, xmm2, xmm6 }
    LODWORD(v36) = v420 ^ v426;
    HIDWORD(v36) = v420 ^ v426;
    __asm { vpslld  xmm7, xmm4, 0Eh }
    v430 = v420 ^ (v36 >> 5);
    __asm { vpsrld  xmm6, xmm4, 7 }
    LODWORD(v36) = v427;
    HIDWORD(v36) = v427;
    v432 = v422 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm4, 3 }
    LODWORD(v36) = v430;
    HIDWORD(v36) = v430;
    v434 = v36 >> 6;
    LODWORD(v36) = v432;
    HIDWORD(v36) = v432;
    __asm { vpxor   xmm4, xmm4, xmm7 }
    v436 = v228 + v434 + (v388 ^ v420 & (v388 ^ v405)) + v371;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      vpslld  xmm7, xmm7, 0Bh
    }
    v439 = v436 + v374;
    __asm
    {
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm6
    }
    v442 = (v391 & v422 | v408 & (v391 | v422)) + (v36 >> 2) + v436;
    __asm
    {
      vunpckhps xmm6, xmm1, xmm1
      vpxor   xmm4, xmm4, xmm7
    }
    LODWORD(v36) = v439;
    HIDWORD(v36) = v439;
    v445 = v36 >> 14;
    LODWORD(v36) = v442;
    HIDWORD(v36) = v442;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v448 = v442 ^ (v36 >> 9);
    LODWORD(v36) = v439 ^ v445;
    HIDWORD(v36) = v439 ^ v445;
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    v450 = v439 ^ (v36 >> 5);
    LODWORD(v36) = v448;
    HIDWORD(v36) = v448;
    __asm { vpsrld  xmm4, xmm1, 0Ah }
    v452 = v442 ^ (v36 >> 11);
    LODWORD(v36) = v450;
    HIDWORD(v36) = v450;
    v453 = v36 >> 6;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v452;
    HIDWORD(v36) = v452;
    v455 = DWORD1(v228) + v453 + (v405 ^ v439 & (v405 ^ v420)) + v388;
    __asm { vpshufd xmm6, xmm6, 80h }
    v457 = v455 + v391;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v459 = (v408 & v442 | v422 & (v408 | v442)) + (v36 >> 2) + v455;
    LODWORD(v36) = v457;
    HIDWORD(v36) = v457;
    v460 = v36 >> 14;
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v459;
    HIDWORD(v36) = v459;
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    v463 = v459 ^ (v36 >> 9);
    LODWORD(v36) = v457 ^ v460;
    HIDWORD(v36) = v457 ^ v460;
    v464 = v457 ^ (v36 >> 5);
    __asm { vunpcklps xmm6, xmm2, xmm2 }
    LODWORD(v36) = v463;
    HIDWORD(v36) = v463;
    v466 = v459 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm2, 0Ah }
    LODWORD(v36) = v464;
    HIDWORD(v36) = v464;
    v468 = v36 >> 6;
    LODWORD(v36) = v466;
    HIDWORD(v36) = v466;
    __asm { vpsrlq  xmm7, xmm6, 11h }
    v470 = DWORD2(v228) + v468 + (v420 ^ v457 & (v420 ^ v439)) + v405;
    __asm { vpsrlq  xmm6, xmm6, 13h }
    v472 = v470 + v408;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    v474 = (v422 & v459 | v442 & (v422 | v459)) + (v36 >> 2) + v470;
    LODWORD(v36) = v472;
    HIDWORD(v36) = v472;
    v475 = v36 >> 14;
    __asm { vpshufd xmm6, xmm6, 8 }
    LODWORD(v36) = v474;
    HIDWORD(v36) = v474;
    v477 = v474 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v472 ^ v475;
    HIDWORD(v36) = v472 ^ v475;
    v479 = v472 ^ (v36 >> 5);
    LODWORD(v36) = v477;
    HIDWORD(v36) = v477;
    v480 = v474 ^ (v36 >> 11);
    LODWORD(v36) = v479;
    HIDWORD(v36) = v479;
    v481 = v36 >> 6;
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v480;
    HIDWORD(v36) = v480;
    v483 = DWORD3(v228) + v481 + (v439 ^ v472 & (v439 ^ v457)) + v420;
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    v485 = v483 + v422;
    __asm { vpaddd  xmm4, xmm2, xmmword ptr [rbx] }
    v487 = (v442 & v474 | v459 & (v442 | v474)) + (v36 >> 2) + v483;
    __asm { vmovdqa [rsp+108h+var_E8], xmm4 }
    LODWORD(v36) = v485;
    HIDWORD(v36) = v485;
    v489 = v36 >> 14;
    LODWORD(v36) = v487;
    HIDWORD(v36) = v487;
    __asm { vpalignr xmm6, xmm2, xmm1, 4 }
    v491 = v487 ^ (v36 >> 9);
    __asm { vpalignr xmm4, xmm0, xmm3, 4 }
    LODWORD(v36) = v485 ^ v489;
    HIDWORD(v36) = v485 ^ v489;
    v493 = v485 ^ (v36 >> 5);
    __asm { vpaddd  xmm3, xmm3, xmm6 }
    LODWORD(v36) = v491;
    HIDWORD(v36) = v491;
    v495 = v487 ^ (v36 >> 11);
    __asm { vpslld  xmm7, xmm4, 0Eh }
    LODWORD(v36) = v493;
    HIDWORD(v36) = v493;
    v497 = v36 >> 6;
    LODWORD(v36) = v495;
    HIDWORD(v36) = v495;
    __asm { vpsrld  xmm6, xmm4, 7 }
    v499 = v293 + v497 + (v457 ^ v485 & (v457 ^ v472)) + v439;
    __asm { vpsrld  xmm4, xmm4, 3 }
    v501 = v499 + v442;
    __asm
    {
      vpxor   xmm4, xmm4, xmm7
      vpxor   xmm4, xmm4, xmm6
    }
    v504 = (v459 & v487 | v474 & (v459 | v487)) + (v36 >> 2) + v499;
    __asm { vpsrld  xmm6, xmm6, 0Bh }
    LODWORD(v36) = v501;
    HIDWORD(v36) = v501;
    v506 = v36 >> 14;
    LODWORD(v36) = v504;
    HIDWORD(v36) = v504;
    __asm { vpslld  xmm7, xmm7, 0Bh }
    v508 = v504 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v501 ^ v506;
    HIDWORD(v36) = v501 ^ v506;
    v510 = v501 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm7 }
    LODWORD(v36) = v508;
    HIDWORD(v36) = v508;
    v512 = v504 ^ (v36 >> 11);
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    LODWORD(v36) = v510;
    HIDWORD(v36) = v510;
    v514 = v36 >> 6;
    LODWORD(v36) = v512;
    HIDWORD(v36) = v512;
    __asm { vunpckhps xmm6, xmm2, xmm2 }
    v516 = DWORD1(v293) + v514 + (v472 ^ v501 & (v472 ^ v485)) + v457;
    __asm { vpsrld  xmm4, xmm2, 0Ah }
    v518 = v516 + v459;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v521 = (v474 & v504 | v487 & (v474 | v504)) + (v36 >> 2) + v516;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v518;
    HIDWORD(v36) = v518;
    v523 = v36 >> 14;
    LODWORD(v36) = v521;
    HIDWORD(v36) = v521;
    __asm { vpshufd xmm6, xmm6, 80h }
    v525 = v521 ^ (v36 >> 9);
    LODWORD(v36) = v518 ^ v523;
    HIDWORD(v36) = v518 ^ v523;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v527 = v518 ^ (v36 >> 5);
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v525;
    HIDWORD(v36) = v525;
    v529 = v521 ^ (v36 >> 11);
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    LODWORD(v36) = v527;
    HIDWORD(v36) = v527;
    v531 = v36 >> 6;
    LODWORD(v36) = v529;
    HIDWORD(v36) = v529;
    __asm { vunpcklps xmm6, xmm3, xmm3 }
    v533 = DWORD2(v293) + v531 + (v485 ^ v518 & (v485 ^ v501)) + v472;
    __asm { vpsrld  xmm4, xmm3, 0Ah }
    v535 = v533 + v474;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v538 = (v487 & v521 | v504 & (v487 | v521)) + (v36 >> 2) + v533;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v535;
    HIDWORD(v36) = v535;
    v540 = v36 >> 14;
    LODWORD(v36) = v538;
    HIDWORD(v36) = v538;
    __asm { vpshufd xmm6, xmm6, 8 }
    v542 = v538 ^ (v36 >> 9);
    LODWORD(v36) = v535 ^ v540;
    HIDWORD(v36) = v535 ^ v540;
    v543 = v535 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v542;
    HIDWORD(v36) = v542;
    v545 = v538 ^ (v36 >> 11);
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v543;
    HIDWORD(v36) = v543;
    v547 = v36 >> 6;
    LODWORD(v36) = v545;
    HIDWORD(v36) = v545;
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    v549 = DWORD3(v293) + v547 + (v501 ^ v535 & (v501 ^ v518)) + v485;
    v550 = v549 + v487;
    __asm { vpaddd  xmm4, xmm3, xmmword ptr [rbx] }
    v552 = (v504 & v538 | v521 & (v504 | v538)) + (v36 >> 2) + v549;
    __asm
    {
      vmovdqa [rsp+108h+var_D8], xmm4
      vpalignr xmm6, xmm3, xmm2, 4
      vpalignr xmm4, xmm1, xmm0, 4
    }
    LODWORD(v36) = v550;
    HIDWORD(v36) = v550;
    v556 = v36 >> 14;
    LODWORD(v36) = v552;
    HIDWORD(v36) = v552;
    v557 = v552 ^ (v36 >> 9);
    __asm { vpaddd  xmm0, xmm0, xmm6 }
    LODWORD(v36) = v550 ^ v556;
    HIDWORD(v36) = v550 ^ v556;
    __asm { vpslld  xmm7, xmm4, 0Eh }
    v560 = v550 ^ (v36 >> 5);
    __asm { vpsrld  xmm6, xmm4, 7 }
    LODWORD(v36) = v557;
    HIDWORD(v36) = v557;
    v562 = v552 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm4, 3 }
    LODWORD(v36) = v560;
    HIDWORD(v36) = v560;
    v564 = v36 >> 6;
    LODWORD(v36) = v562;
    HIDWORD(v36) = v562;
    __asm { vpxor   xmm4, xmm4, xmm7 }
    v566 = v358 + v564 + (v518 ^ v550 & (v518 ^ v535)) + v501;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      vpslld  xmm7, xmm7, 0Bh
    }
    v569 = v566 + v504;
    __asm
    {
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm6
    }
    v572 = (v521 & v552 | v538 & (v521 | v552)) + (v36 >> 2) + v566;
    __asm
    {
      vunpckhps xmm6, xmm3, xmm3
      vpxor   xmm4, xmm4, xmm7
    }
    LODWORD(v36) = v569;
    HIDWORD(v36) = v569;
    v575 = v36 >> 14;
    LODWORD(v36) = v572;
    HIDWORD(v36) = v572;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v578 = v572 ^ (v36 >> 9);
    LODWORD(v36) = v569 ^ v575;
    HIDWORD(v36) = v569 ^ v575;
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    v580 = v569 ^ (v36 >> 5);
    LODWORD(v36) = v578;
    HIDWORD(v36) = v578;
    __asm { vpsrld  xmm4, xmm3, 0Ah }
    v582 = v572 ^ (v36 >> 11);
    LODWORD(v36) = v580;
    HIDWORD(v36) = v580;
    v583 = v36 >> 6;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v582;
    HIDWORD(v36) = v582;
    v585 = DWORD1(v358) + v583 + (v535 ^ v569 & (v535 ^ v550)) + v518;
    __asm { vpshufd xmm6, xmm6, 80h }
    v587 = v585 + v521;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v589 = (v538 & v572 | v552 & (v538 | v572)) + (v36 >> 2) + v585;
    LODWORD(v36) = v587;
    HIDWORD(v36) = v587;
    v590 = v36 >> 14;
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v589;
    HIDWORD(v36) = v589;
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    v593 = v589 ^ (v36 >> 9);
    LODWORD(v36) = v587 ^ v590;
    HIDWORD(v36) = v587 ^ v590;
    v594 = v587 ^ (v36 >> 5);
    __asm { vunpcklps xmm6, xmm0, xmm0 }
    LODWORD(v36) = v593;
    HIDWORD(v36) = v593;
    v596 = v589 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm0, 0Ah }
    LODWORD(v36) = v594;
    HIDWORD(v36) = v594;
    v598 = v36 >> 6;
    LODWORD(v36) = v596;
    HIDWORD(v36) = v596;
    __asm { vpsrlq  xmm7, xmm6, 11h }
    v600 = DWORD2(v358) + v598 + (v550 ^ v587 & (v550 ^ v569)) + v535;
    __asm { vpsrlq  xmm6, xmm6, 13h }
    v602 = v600 + v538;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    v604 = (v552 & v589 | v572 & (v552 | v589)) + (v36 >> 2) + v600;
    LODWORD(v36) = v602;
    HIDWORD(v36) = v602;
    v605 = v36 >> 14;
    __asm { vpshufd xmm6, xmm6, 8 }
    LODWORD(v36) = v604;
    HIDWORD(v36) = v604;
    v607 = v604 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v602 ^ v605;
    HIDWORD(v36) = v602 ^ v605;
    v609 = v602 ^ (v36 >> 5);
    LODWORD(v36) = v607;
    HIDWORD(v36) = v607;
    v610 = v604 ^ (v36 >> 11);
    LODWORD(v36) = v609;
    HIDWORD(v36) = v609;
    v611 = v36 >> 6;
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v610;
    HIDWORD(v36) = v610;
    v613 = DWORD3(v358) + v611 + (v569 ^ v602 & (v569 ^ v587)) + v550;
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    v615 = v613 + v552;
    __asm { vpaddd  xmm4, xmm0, xmmword ptr [rbx] }
    v617 = (v572 & v604 | v589 & (v572 | v604)) + (v36 >> 2) + v613;
    __asm { vmovdqa [rsp+108h+var_108], xmm4 }
    LODWORD(v36) = v615;
    HIDWORD(v36) = v615;
    v618 = v36 >> 14;
    LODWORD(v36) = v617;
    HIDWORD(v36) = v617;
    __asm { vpalignr xmm6, xmm0, xmm3, 4 }
    v620 = v617 ^ (v36 >> 9);
    __asm { vpalignr xmm4, xmm2, xmm1, 4 }
    LODWORD(v36) = v615 ^ v618;
    HIDWORD(v36) = v615 ^ v618;
    v622 = v615 ^ (v36 >> 5);
    __asm { vpaddd  xmm1, xmm1, xmm6 }
    LODWORD(v36) = v620;
    HIDWORD(v36) = v620;
    v624 = v617 ^ (v36 >> 11);
    __asm { vpslld  xmm7, xmm4, 0Eh }
    LODWORD(v36) = v622;
    HIDWORD(v36) = v622;
    v626 = v36 >> 6;
    LODWORD(v36) = v624;
    HIDWORD(v36) = v624;
    __asm { vpsrld  xmm6, xmm4, 7 }
    v628 = v423 + v626 + (v587 ^ v615 & (v587 ^ v602)) + v569;
    __asm { vpsrld  xmm4, xmm4, 3 }
    v630 = v628 + v572;
    __asm
    {
      vpxor   xmm4, xmm4, xmm7
      vpxor   xmm4, xmm4, xmm6
    }
    v633 = (v589 & v617 | v604 & (v589 | v617)) + (v36 >> 2) + v628;
    __asm { vpsrld  xmm6, xmm6, 0Bh }
    LODWORD(v36) = v630;
    HIDWORD(v36) = v630;
    v635 = v36 >> 14;
    LODWORD(v36) = v633;
    HIDWORD(v36) = v633;
    __asm { vpslld  xmm7, xmm7, 0Bh }
    v637 = v633 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v630 ^ v635;
    HIDWORD(v36) = v630 ^ v635;
    v639 = v630 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm7 }
    LODWORD(v36) = v637;
    HIDWORD(v36) = v637;
    v641 = v633 ^ (v36 >> 11);
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    LODWORD(v36) = v639;
    HIDWORD(v36) = v639;
    v643 = v36 >> 6;
    LODWORD(v36) = v641;
    HIDWORD(v36) = v641;
    __asm { vunpckhps xmm6, xmm0, xmm0 }
    v645 = DWORD1(v423) + v643 + (v602 ^ v630 & (v602 ^ v615)) + v587;
    __asm { vpsrld  xmm4, xmm0, 0Ah }
    v647 = v645 + v589;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v650 = (v604 & v633 | v617 & (v604 | v633)) + (v36 >> 2) + v645;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v647;
    HIDWORD(v36) = v647;
    v652 = v36 >> 14;
    LODWORD(v36) = v650;
    HIDWORD(v36) = v650;
    __asm { vpshufd xmm6, xmm6, 80h }
    v654 = v650 ^ (v36 >> 9);
    LODWORD(v36) = v647 ^ v652;
    HIDWORD(v36) = v647 ^ v652;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v656 = v647 ^ (v36 >> 5);
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v654;
    HIDWORD(v36) = v654;
    v658 = v650 ^ (v36 >> 11);
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    LODWORD(v36) = v656;
    HIDWORD(v36) = v656;
    v660 = v36 >> 6;
    LODWORD(v36) = v658;
    HIDWORD(v36) = v658;
    __asm { vunpcklps xmm6, xmm1, xmm1 }
    v662 = DWORD2(v423) + v660 + (v615 ^ v647 & (v615 ^ v630)) + v602;
    __asm { vpsrld  xmm4, xmm1, 0Ah }
    v664 = v662 + v604;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v667 = (v617 & v650 | v633 & (v617 | v650)) + (v36 >> 2) + v662;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v664;
    HIDWORD(v36) = v664;
    v669 = v36 >> 14;
    LODWORD(v36) = v667;
    HIDWORD(v36) = v667;
    __asm { vpshufd xmm6, xmm6, 8 }
    v671 = v667 ^ (v36 >> 9);
    LODWORD(v36) = v664 ^ v669;
    HIDWORD(v36) = v664 ^ v669;
    v672 = v664 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v671;
    HIDWORD(v36) = v671;
    v674 = v667 ^ (v36 >> 11);
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v672;
    HIDWORD(v36) = v672;
    v676 = v36 >> 6;
    LODWORD(v36) = v674;
    HIDWORD(v36) = v674;
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    v678 = DWORD3(v423) + v676 + (v630 ^ v664 & (v630 ^ v647)) + v615;
    v679 = v678 + v617;
    __asm { vpaddd  xmm4, xmm1, xmmword ptr [rbx] }
    v681 = (v633 & v667 | v650 & (v633 | v667)) + (v36 >> 2) + v678;
    __asm
    {
      vmovdqa [rsp+108h+var_F8], xmm4
      vpalignr xmm6, xmm1, xmm0, 4
      vpalignr xmm4, xmm3, xmm2, 4
    }
    LODWORD(v36) = v679;
    HIDWORD(v36) = v679;
    v684 = v36 >> 14;
    LODWORD(v36) = v681;
    HIDWORD(v36) = v681;
    v685 = v681 ^ (v36 >> 9);
    __asm { vpaddd  xmm2, xmm2, xmm6 }
    LODWORD(v36) = v679 ^ v684;
    HIDWORD(v36) = v679 ^ v684;
    __asm { vpslld  xmm7, xmm4, 0Eh }
    v688 = v679 ^ (v36 >> 5);
    __asm { vpsrld  xmm6, xmm4, 7 }
    LODWORD(v36) = v685;
    HIDWORD(v36) = v685;
    v690 = v681 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm4, 3 }
    LODWORD(v36) = v688;
    HIDWORD(v36) = v688;
    v692 = v36 >> 6;
    LODWORD(v36) = v690;
    HIDWORD(v36) = v690;
    __asm { vpxor   xmm4, xmm4, xmm7 }
    v694 = v488 + v692 + (v647 ^ v679 & (v647 ^ v664)) + v630;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      vpslld  xmm7, xmm7, 0Bh
    }
    v697 = v694 + v633;
    __asm
    {
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm6
    }
    v700 = (v650 & v681 | v667 & (v650 | v681)) + (v36 >> 2) + v694;
    __asm
    {
      vunpckhps xmm6, xmm1, xmm1
      vpxor   xmm4, xmm4, xmm7
    }
    LODWORD(v36) = v697;
    HIDWORD(v36) = v697;
    v703 = v36 >> 14;
    LODWORD(v36) = v700;
    HIDWORD(v36) = v700;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v706 = v700 ^ (v36 >> 9);
    LODWORD(v36) = v697 ^ v703;
    HIDWORD(v36) = v697 ^ v703;
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    v708 = v697 ^ (v36 >> 5);
    LODWORD(v36) = v706;
    HIDWORD(v36) = v706;
    __asm { vpsrld  xmm4, xmm1, 0Ah }
    v710 = v700 ^ (v36 >> 11);
    LODWORD(v36) = v708;
    HIDWORD(v36) = v708;
    v711 = v36 >> 6;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v710;
    HIDWORD(v36) = v710;
    v713 = DWORD1(v488) + v711 + (v664 ^ v697 & (v664 ^ v679)) + v647;
    __asm { vpshufd xmm6, xmm6, 80h }
    v715 = v713 + v650;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v717 = (v667 & v700 | v681 & (v667 | v700)) + (v36 >> 2) + v713;
    LODWORD(v36) = v715;
    HIDWORD(v36) = v715;
    v718 = v36 >> 14;
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v717;
    HIDWORD(v36) = v717;
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    v721 = v717 ^ (v36 >> 9);
    LODWORD(v36) = v715 ^ v718;
    HIDWORD(v36) = v715 ^ v718;
    v722 = v715 ^ (v36 >> 5);
    __asm { vunpcklps xmm6, xmm2, xmm2 }
    LODWORD(v36) = v721;
    HIDWORD(v36) = v721;
    v724 = v717 ^ (v36 >> 11);
    __asm { vpsrld  xmm4, xmm2, 0Ah }
    LODWORD(v36) = v722;
    HIDWORD(v36) = v722;
    v726 = v36 >> 6;
    LODWORD(v36) = v724;
    HIDWORD(v36) = v724;
    __asm { vpsrlq  xmm7, xmm6, 11h }
    v728 = DWORD2(v488) + v726 + (v679 ^ v715 & (v679 ^ v697)) + v664;
    __asm { vpsrlq  xmm6, xmm6, 13h }
    v730 = v728 + v667;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    v732 = (v681 & v717 | v700 & (v681 | v717)) + (v36 >> 2) + v728;
    LODWORD(v36) = v730;
    HIDWORD(v36) = v730;
    v733 = v36 >> 14;
    __asm { vpshufd xmm6, xmm6, 8 }
    LODWORD(v36) = v732;
    HIDWORD(v36) = v732;
    v735 = v732 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v730 ^ v733;
    HIDWORD(v36) = v730 ^ v733;
    v737 = v730 ^ (v36 >> 5);
    LODWORD(v36) = v735;
    HIDWORD(v36) = v735;
    v738 = v732 ^ (v36 >> 11);
    LODWORD(v36) = v737;
    HIDWORD(v36) = v737;
    v739 = v36 >> 6;
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v738;
    HIDWORD(v36) = v738;
    v741 = DWORD3(v488) + v739 + (v697 ^ v730 & (v697 ^ v715)) + v679;
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    v743 = v741 + v681;
    __asm { vpaddd  xmm4, xmm2, xmmword ptr [rbx] }
    v745 = (v700 & v732 | v717 & (v700 | v732)) + (v36 >> 2) + v741;
    __asm { vmovdqa [rsp+108h+var_E8], xmm4 }
    LODWORD(v36) = v743;
    HIDWORD(v36) = v743;
    v746 = v36 >> 14;
    LODWORD(v36) = v745;
    HIDWORD(v36) = v745;
    __asm { vpalignr xmm6, xmm2, xmm1, 4 }
    v748 = v745 ^ (v36 >> 9);
    __asm { vpalignr xmm4, xmm0, xmm3, 4 }
    LODWORD(v36) = v743 ^ v746;
    HIDWORD(v36) = v743 ^ v746;
    v750 = v743 ^ (v36 >> 5);
    __asm { vpaddd  xmm3, xmm3, xmm6 }
    LODWORD(v36) = v748;
    HIDWORD(v36) = v748;
    v752 = v745 ^ (v36 >> 11);
    __asm { vpslld  xmm7, xmm4, 0Eh }
    LODWORD(v36) = v750;
    HIDWORD(v36) = v750;
    v754 = v36 >> 6;
    LODWORD(v36) = v752;
    HIDWORD(v36) = v752;
    __asm { vpsrld  xmm6, xmm4, 7 }
    v756 = v553 + v754 + (v715 ^ v743 & (v715 ^ v730)) + v697;
    __asm { vpsrld  xmm4, xmm4, 3 }
    v758 = v756 + v700;
    __asm
    {
      vpxor   xmm4, xmm4, xmm7
      vpxor   xmm4, xmm4, xmm6
    }
    v761 = (v717 & v745 | v732 & (v717 | v745)) + (v36 >> 2) + v756;
    __asm { vpsrld  xmm6, xmm6, 0Bh }
    LODWORD(v36) = v758;
    HIDWORD(v36) = v758;
    v763 = v36 >> 14;
    LODWORD(v36) = v761;
    HIDWORD(v36) = v761;
    __asm { vpslld  xmm7, xmm7, 0Bh }
    v765 = v761 ^ (v36 >> 9);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v758 ^ v763;
    HIDWORD(v36) = v758 ^ v763;
    v767 = v758 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm7 }
    LODWORD(v36) = v765;
    HIDWORD(v36) = v765;
    v769 = v761 ^ (v36 >> 11);
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    LODWORD(v36) = v767;
    HIDWORD(v36) = v767;
    v771 = v36 >> 6;
    LODWORD(v36) = v769;
    HIDWORD(v36) = v769;
    __asm { vunpckhps xmm6, xmm2, xmm2 }
    v773 = DWORD1(v553) + v771 + (v730 ^ v758 & (v730 ^ v743)) + v715;
    __asm { vpsrld  xmm4, xmm2, 0Ah }
    v775 = v773 + v717;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v778 = (v732 & v761 | v745 & (v732 | v761)) + (v36 >> 2) + v773;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v775;
    HIDWORD(v36) = v775;
    v780 = v36 >> 14;
    LODWORD(v36) = v778;
    HIDWORD(v36) = v778;
    __asm { vpshufd xmm6, xmm6, 80h }
    v782 = v778 ^ (v36 >> 9);
    LODWORD(v36) = v775 ^ v780;
    HIDWORD(v36) = v775 ^ v780;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    v784 = v775 ^ (v36 >> 5);
    __asm { vpsrldq xmm4, xmm4, 8 }
    LODWORD(v36) = v782;
    HIDWORD(v36) = v782;
    v786 = v778 ^ (v36 >> 11);
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    LODWORD(v36) = v784;
    HIDWORD(v36) = v784;
    v788 = v36 >> 6;
    LODWORD(v36) = v786;
    HIDWORD(v36) = v786;
    __asm { vunpcklps xmm6, xmm3, xmm3 }
    v790 = DWORD2(v553) + v788 + (v743 ^ v775 & (v743 ^ v758)) + v730;
    __asm { vpsrld  xmm4, xmm3, 0Ah }
    v792 = v790 + v732;
    __asm
    {
      vpsrlq  xmm7, xmm6, 11h
      vpsrlq  xmm6, xmm6, 13h
    }
    v794 = (v745 & v778 | v761 & (v745 | v778)) + (v36 >> 2) + v790;
    __asm { vpxor   xmm6, xmm6, xmm7 }
    LODWORD(v36) = v792;
    HIDWORD(v36) = v792;
    v796 = v36 >> 14;
    LODWORD(v36) = v794;
    HIDWORD(v36) = v794;
    __asm { vpshufd xmm6, xmm6, 8 }
    v797 = v794 ^ (v36 >> 9);
    LODWORD(v36) = v792 ^ v796;
    HIDWORD(v36) = v792 ^ v796;
    v798 = v792 ^ (v36 >> 5);
    __asm { vpxor   xmm4, xmm4, xmm6 }
    LODWORD(v36) = v797;
    HIDWORD(v36) = v797;
    v800 = v794 ^ (v36 >> 11);
    __asm { vpslldq xmm4, xmm4, 8 }
    LODWORD(v36) = v798;
    HIDWORD(v36) = v798;
    v802 = v36 >> 6;
    LODWORD(v36) = v800;
    HIDWORD(v36) = v800;
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    v804 = DWORD3(v553) + v802 + (v758 ^ v792 & (v758 ^ v775)) + v743;
    v805 = v804 + v745;
    __asm { vpaddd  xmm4, xmm3, xmmword ptr [rbx] }
    v807 = (v761 & v794 | v778 & (v761 | v794)) + (v36 >> 2) + v804;
    __asm { vmovdqa [rsp+108h+var_D8], xmm4 }
    v808 = v27 - 64;
    --a3;
    if ( !a3 )
      break;
    __asm { vmovdqu xmm0, xmmword ptr [rdx] }
    LODWORD(v811) = v805;
    HIDWORD(v811) = v805;
    v810 = v811 >> 14;
    LODWORD(v811) = v807;
    HIDWORD(v811) = v807;
    v812 = v807 ^ (v811 >> 9);
    LODWORD(v811) = v805 ^ v810;
    HIDWORD(v811) = v805 ^ v810;
    v813 = v805 ^ (v811 >> 5);
    LODWORD(v811) = v812;
    HIDWORD(v811) = v812;
    v814 = v807 ^ (v811 >> 11);
    LODWORD(v811) = v813;
    HIDWORD(v811) = v813;
    v815 = v811 >> 6;
    LODWORD(v811) = v814;
    HIDWORD(v811) = v814;
    v816 = v1097 + v815 + (v775 ^ v805 & (v775 ^ v792)) + v758;
    v817 = v816 + v761;
    v818 = (v778 & v807 | v794 & (v778 | v807)) + (v811 >> 2) + v816;
    __asm { vpshufb xmm0, xmm0, [rsp+108h+var_C8] }
    LODWORD(v811) = v817;
    HIDWORD(v811) = v817;
    v819 = v811 >> 14;
    LODWORD(v811) = v818;
    HIDWORD(v811) = v818;
    v820 = v818 ^ (v811 >> 9);
    LODWORD(v811) = v817 ^ v819;
    HIDWORD(v811) = v817 ^ v819;
    v821 = v817 ^ (v811 >> 5);
    LODWORD(v811) = v820;
    HIDWORD(v811) = v820;
    v822 = v818 ^ (v811 >> 11);
    LODWORD(v811) = v821;
    HIDWORD(v811) = v821;
    v823 = v811 >> 6;
    LODWORD(v811) = v822;
    HIDWORD(v811) = v822;
    v824 = DWORD1(v1097) + v823 + (v792 ^ v817 & (v792 ^ v805)) + v775;
    v825 = v824 + v778;
    v826 = (v794 & v818 | v807 & (v794 | v818)) + (v811 >> 2) + v824;
    LODWORD(v811) = v825;
    HIDWORD(v811) = v825;
    v827 = v811 >> 14;
    LODWORD(v811) = v826;
    HIDWORD(v811) = v826;
    v828 = v826 ^ (v811 >> 9);
    LODWORD(v811) = v825 ^ v827;
    HIDWORD(v811) = v825 ^ v827;
    v829 = v825 ^ (v811 >> 5);
    LODWORD(v811) = v828;
    HIDWORD(v811) = v828;
    v830 = v826 ^ (v811 >> 11);
    LODWORD(v811) = v829;
    HIDWORD(v811) = v829;
    v831 = v811 >> 6;
    LODWORD(v811) = v830;
    HIDWORD(v811) = v830;
    v832 = DWORD2(v1097) + v831 + (v805 ^ v825 & (v805 ^ v817)) + v792;
    v833 = v832 + v794;
    v834 = (v807 & v826 | v818 & (v807 | v826)) + (v811 >> 2) + v832;
    __asm { vpaddd  xmm4, xmm0, xmmword ptr [rbx] }
    LODWORD(v811) = v833;
    HIDWORD(v811) = v833;
    v836 = v811 >> 14;
    LODWORD(v811) = v834;
    HIDWORD(v811) = v834;
    v837 = v834 ^ (v811 >> 9);
    LODWORD(v811) = v833 ^ v836;
    HIDWORD(v811) = v833 ^ v836;
    v838 = v833 ^ (v811 >> 5);
    LODWORD(v811) = v837;
    HIDWORD(v811) = v837;
    v839 = v834 ^ (v811 >> 11);
    LODWORD(v811) = v838;
    HIDWORD(v811) = v838;
    v840 = v811 >> 6;
    LODWORD(v811) = v839;
    HIDWORD(v811) = v839;
    v841 = DWORD3(v1097) + v840 + (v817 ^ v833 & (v817 ^ v825)) + v805;
    v842 = v841 + v807;
    v843 = (v818 & v834 | v826 & (v818 | v834)) + (v811 >> 2) + v841;
    __asm
    {
      vmovdqa [rsp+108h+var_108], xmm4
      vmovdqu xmm1, xmmword ptr [rdx+10h]
    }
    LODWORD(v811) = v842;
    HIDWORD(v811) = v842;
    v845 = v811 >> 14;
    LODWORD(v811) = v843;
    HIDWORD(v811) = v843;
    v846 = v843 ^ (v811 >> 9);
    LODWORD(v811) = v842 ^ v845;
    HIDWORD(v811) = v842 ^ v845;
    v847 = v842 ^ (v811 >> 5);
    LODWORD(v811) = v846;
    HIDWORD(v811) = v846;
    v848 = v843 ^ (v811 >> 11);
    LODWORD(v811) = v847;
    HIDWORD(v811) = v847;
    v849 = v811 >> 6;
    LODWORD(v811) = v848;
    HIDWORD(v811) = v848;
    v850 = v1100 + v849 + (v825 ^ v842 & (v825 ^ v833)) + v817;
    v851 = v850 + v818;
    v852 = (v826 & v843 | v834 & (v826 | v843)) + (v811 >> 2) + v850;
    __asm { vpshufb xmm1, xmm1, [rsp+108h+var_C8] }
    LODWORD(v811) = v851;
    HIDWORD(v811) = v851;
    v853 = v811 >> 14;
    LODWORD(v811) = v852;
    HIDWORD(v811) = v852;
    v854 = v852 ^ (v811 >> 9);
    LODWORD(v811) = v851 ^ v853;
    HIDWORD(v811) = v851 ^ v853;
    v855 = v851 ^ (v811 >> 5);
    LODWORD(v811) = v854;
    HIDWORD(v811) = v854;
    v856 = v852 ^ (v811 >> 11);
    LODWORD(v811) = v855;
    HIDWORD(v811) = v855;
    v857 = v811 >> 6;
    LODWORD(v811) = v856;
    HIDWORD(v811) = v856;
    v858 = DWORD1(v1100) + v857 + (v833 ^ v851 & (v833 ^ v842)) + v825;
    v859 = v858 + v826;
    v860 = (v834 & v852 | v843 & (v834 | v852)) + (v811 >> 2) + v858;
    LODWORD(v811) = v859;
    HIDWORD(v811) = v859;
    v861 = v811 >> 14;
    LODWORD(v811) = v860;
    HIDWORD(v811) = v860;
    v862 = v860 ^ (v811 >> 9);
    LODWORD(v811) = v859 ^ v861;
    HIDWORD(v811) = v859 ^ v861;
    v863 = v859 ^ (v811 >> 5);
    LODWORD(v811) = v862;
    HIDWORD(v811) = v862;
    v864 = v860 ^ (v811 >> 11);
    LODWORD(v811) = v863;
    HIDWORD(v811) = v863;
    v865 = v811 >> 6;
    LODWORD(v811) = v864;
    HIDWORD(v811) = v864;
    v866 = DWORD2(v1100) + v865 + (v842 ^ v859 & (v842 ^ v851)) + v833;
    v867 = v866 + v834;
    v868 = (v843 & v860 | v852 & (v843 | v860)) + (v811 >> 2) + v866;
    __asm { vpaddd  xmm4, xmm1, xmmword ptr [rbx+10h] }
    LODWORD(v811) = v867;
    HIDWORD(v811) = v867;
    v870 = v811 >> 14;
    LODWORD(v811) = v868;
    HIDWORD(v811) = v868;
    v871 = v868 ^ (v811 >> 9);
    LODWORD(v811) = v867 ^ v870;
    HIDWORD(v811) = v867 ^ v870;
    v872 = v867 ^ (v811 >> 5);
    LODWORD(v811) = v871;
    HIDWORD(v811) = v871;
    v873 = v868 ^ (v811 >> 11);
    LODWORD(v811) = v872;
    HIDWORD(v811) = v872;
    v874 = v811 >> 6;
    LODWORD(v811) = v873;
    HIDWORD(v811) = v873;
    v875 = DWORD3(v1100) + v874 + (v851 ^ v867 & (v851 ^ v859)) + v842;
    v876 = v875 + v843;
    v877 = (v852 & v868 | v860 & (v852 | v868)) + (v811 >> 2) + v875;
    __asm
    {
      vmovdqa [rsp+108h+var_F8], xmm4
      vmovdqu xmm2, xmmword ptr [rdx+20h]
    }
    LODWORD(v811) = v876;
    HIDWORD(v811) = v876;
    v879 = v811 >> 14;
    LODWORD(v811) = v877;
    HIDWORD(v811) = v877;
    v880 = v877 ^ (v811 >> 9);
    LODWORD(v811) = v876 ^ v879;
    HIDWORD(v811) = v876 ^ v879;
    v881 = v876 ^ (v811 >> 5);
    LODWORD(v811) = v880;
    HIDWORD(v811) = v880;
    v882 = v877 ^ (v811 >> 11);
    LODWORD(v811) = v881;
    HIDWORD(v811) = v881;
    v883 = v811 >> 6;
    LODWORD(v811) = v882;
    HIDWORD(v811) = v882;
    v884 = v1103 + v883 + (v859 ^ v876 & (v859 ^ v867)) + v851;
    v885 = v884 + v852;
    v886 = (v860 & v877 | v868 & (v860 | v877)) + (v811 >> 2) + v884;
    __asm { vpshufb xmm2, xmm2, [rsp+108h+var_C8] }
    LODWORD(v811) = v885;
    HIDWORD(v811) = v885;
    v887 = v811 >> 14;
    LODWORD(v811) = v886;
    HIDWORD(v811) = v886;
    v888 = v886 ^ (v811 >> 9);
    LODWORD(v811) = v885 ^ v887;
    HIDWORD(v811) = v885 ^ v887;
    v889 = v885 ^ (v811 >> 5);
    LODWORD(v811) = v888;
    HIDWORD(v811) = v888;
    v890 = v886 ^ (v811 >> 11);
    LODWORD(v811) = v889;
    HIDWORD(v811) = v889;
    v891 = v811 >> 6;
    LODWORD(v811) = v890;
    HIDWORD(v811) = v890;
    v892 = DWORD1(v1103) + v891 + (v867 ^ v885 & (v867 ^ v876)) + v859;
    v893 = v892 + v860;
    v894 = (v868 & v886 | v877 & (v868 | v886)) + (v811 >> 2) + v892;
    LODWORD(v811) = v893;
    HIDWORD(v811) = v893;
    v895 = v811 >> 14;
    LODWORD(v811) = v894;
    HIDWORD(v811) = v894;
    v896 = v894 ^ (v811 >> 9);
    LODWORD(v811) = v893 ^ v895;
    HIDWORD(v811) = v893 ^ v895;
    v897 = v893 ^ (v811 >> 5);
    LODWORD(v811) = v896;
    HIDWORD(v811) = v896;
    v898 = v894 ^ (v811 >> 11);
    LODWORD(v811) = v897;
    HIDWORD(v811) = v897;
    v899 = v811 >> 6;
    LODWORD(v811) = v898;
    HIDWORD(v811) = v898;
    v900 = DWORD2(v1103) + v899 + (v876 ^ v893 & (v876 ^ v885)) + v867;
    v901 = v900 + v868;
    v902 = (v877 & v894 | v886 & (v877 | v894)) + (v811 >> 2) + v900;
    __asm { vpaddd  xmm4, xmm2, xmmword ptr [rbx+20h] }
    LODWORD(v811) = v901;
    HIDWORD(v811) = v901;
    v904 = v811 >> 14;
    LODWORD(v811) = v902;
    HIDWORD(v811) = v902;
    v905 = v902 ^ (v811 >> 9);
    LODWORD(v811) = v901 ^ v904;
    HIDWORD(v811) = v901 ^ v904;
    v906 = v901 ^ (v811 >> 5);
    LODWORD(v811) = v905;
    HIDWORD(v811) = v905;
    v907 = v902 ^ (v811 >> 11);
    LODWORD(v811) = v906;
    HIDWORD(v811) = v906;
    v908 = v811 >> 6;
    LODWORD(v811) = v907;
    HIDWORD(v811) = v907;
    v909 = DWORD3(v1103) + v908 + (v885 ^ v901 & (v885 ^ v893)) + v876;
    v910 = v909 + v877;
    v911 = (v886 & v902 | v894 & (v886 | v902)) + (v811 >> 2) + v909;
    __asm
    {
      vmovdqa [rsp+108h+var_E8], xmm4
      vmovdqu xmm3, xmmword ptr [rdx+30h]
    }
    LODWORD(v811) = v910;
    HIDWORD(v811) = v910;
    v913 = v811 >> 14;
    LODWORD(v811) = v911;
    HIDWORD(v811) = v911;
    v914 = v911 ^ (v811 >> 9);
    LODWORD(v811) = v910 ^ v913;
    HIDWORD(v811) = v910 ^ v913;
    v915 = v910 ^ (v811 >> 5);
    LODWORD(v811) = v914;
    HIDWORD(v811) = v914;
    v916 = v911 ^ (v811 >> 11);
    LODWORD(v811) = v915;
    HIDWORD(v811) = v915;
    v917 = v811 >> 6;
    LODWORD(v811) = v916;
    HIDWORD(v811) = v916;
    v918 = v1106 + v917 + (v893 ^ v910 & (v893 ^ v901)) + v885;
    v919 = v918 + v886;
    v920 = (v894 & v911 | v902 & (v894 | v911)) + (v811 >> 2) + v918;
    __asm { vpshufb xmm3, xmm3, [rsp+108h+var_C8] }
    LODWORD(v811) = v919;
    HIDWORD(v811) = v919;
    v921 = v811 >> 14;
    LODWORD(v811) = v920;
    HIDWORD(v811) = v920;
    v922 = v920 ^ (v811 >> 9);
    LODWORD(v811) = v919 ^ v921;
    HIDWORD(v811) = v919 ^ v921;
    v923 = v919 ^ (v811 >> 5);
    LODWORD(v811) = v922;
    HIDWORD(v811) = v922;
    v924 = v920 ^ (v811 >> 11);
    LODWORD(v811) = v923;
    HIDWORD(v811) = v923;
    v925 = v811 >> 6;
    LODWORD(v811) = v924;
    HIDWORD(v811) = v924;
    v926 = DWORD1(v1106) + v925 + (v901 ^ v919 & (v901 ^ v910)) + v893;
    v927 = v926 + v894;
    v928 = (v902 & v920 | v911 & (v902 | v920)) + (v811 >> 2) + v926;
    LODWORD(v811) = v927;
    HIDWORD(v811) = v927;
    v929 = v811 >> 14;
    LODWORD(v811) = v928;
    HIDWORD(v811) = v928;
    v930 = v928 ^ (v811 >> 9);
    LODWORD(v811) = v927 ^ v929;
    HIDWORD(v811) = v927 ^ v929;
    v931 = v927 ^ (v811 >> 5);
    LODWORD(v811) = v930;
    HIDWORD(v811) = v930;
    v932 = v928 ^ (v811 >> 11);
    LODWORD(v811) = v931;
    HIDWORD(v811) = v931;
    v933 = v811 >> 6;
    LODWORD(v811) = v932;
    HIDWORD(v811) = v932;
    v934 = DWORD2(v1106) + v933 + (v910 ^ v927 & (v910 ^ v919)) + v901;
    v935 = v934 + v902;
    v936 = (v911 & v928 | v920 & (v911 | v928)) + (v811 >> 2) + v934;
    __asm { vpaddd  xmm4, xmm3, xmmword ptr [rbx+30h] }
    LODWORD(v811) = v935;
    HIDWORD(v811) = v935;
    v937 = v811 >> 14;
    LODWORD(v811) = v936;
    HIDWORD(v811) = v936;
    v938 = v936 ^ (v811 >> 9);
    LODWORD(v811) = v935 ^ v937;
    HIDWORD(v811) = v935 ^ v937;
    v939 = v935 ^ (v811 >> 5);
    LODWORD(v811) = v938;
    HIDWORD(v811) = v938;
    v940 = v936 ^ (v811 >> 11);
    LODWORD(v811) = v939;
    HIDWORD(v811) = v939;
    v941 = v811 >> 6;
    LODWORD(v811) = v940;
    HIDWORD(v811) = v940;
    v942 = DWORD3(v1106) + v941 + (v919 ^ v935 & (v919 ^ v927)) + v910;
    __asm { vmovdqa [rsp+108h+var_D8], xmm4 }
    v27 = v808 + 64;
    _RDX += 64LL;
    *(_DWORD *)a2 += (v920 & v936 | v928 & (v920 | v936)) + (v811 >> 2) + v942;
    *(_DWORD *)(a2 + 4) += v936;
    *(_DWORD *)(a2 + 8) += v928;
    *(_DWORD *)(a2 + 12) += v920;
    *(_DWORD *)(a2 + 16) += v942 + v911;
    *(_DWORD *)(a2 + 20) += v935;
    *(_DWORD *)(a2 + 24) += v927;
    *(_DWORD *)(a2 + 28) += v919;
  }
  LODWORD(v944) = v805;
  HIDWORD(v944) = v805;
  v943 = v944 >> 14;
  LODWORD(v944) = v807;
  HIDWORD(v944) = v807;
  v945 = v805 ^ v943;
  v946 = v807 ^ (v944 >> 9);
  LODWORD(v944) = v945;
  HIDWORD(v944) = v945;
  v947 = v805 ^ (v944 >> 5);
  LODWORD(v944) = v946;
  HIDWORD(v944) = v946;
  v948 = v807 ^ (v944 >> 11);
  LODWORD(v944) = v947;
  HIDWORD(v944) = v947;
  v949 = v944 >> 6;
  LODWORD(v944) = v948;
  HIDWORD(v944) = v948;
  v950 = v1097 + v949 + (v775 ^ v805 & (v775 ^ v792)) + v758;
  v951 = v950 + v761;
  v952 = (v778 & v807 | v794 & (v778 | v807)) + (v944 >> 2) + v950;
  LODWORD(v944) = v951;
  HIDWORD(v944) = v951;
  v953 = v944 >> 14;
  LODWORD(v944) = v952;
  HIDWORD(v944) = v952;
  v954 = v951 ^ v953;
  v955 = v952 ^ (v944 >> 9);
  LODWORD(v944) = v954;
  HIDWORD(v944) = v954;
  v956 = v951 ^ (v944 >> 5);
  LODWORD(v944) = v955;
  HIDWORD(v944) = v955;
  v957 = v952 ^ (v944 >> 11);
  LODWORD(v944) = v956;
  HIDWORD(v944) = v956;
  v958 = v944 >> 6;
  LODWORD(v944) = v957;
  HIDWORD(v944) = v957;
  v959 = DWORD1(v1097) + v958 + (v792 ^ v951 & (v792 ^ v805)) + v775;
  v960 = v959 + v778;
  v961 = (v794 & v952 | v807 & (v794 | v952)) + (v944 >> 2) + v959;
  LODWORD(v944) = v960;
  HIDWORD(v944) = v960;
  v962 = v944 >> 14;
  LODWORD(v944) = v961;
  HIDWORD(v944) = v961;
  v963 = v960 ^ v962;
  v964 = v961 ^ (v944 >> 9);
  LODWORD(v944) = v963;
  HIDWORD(v944) = v963;
  v965 = v960 ^ (v944 >> 5);
  LODWORD(v944) = v964;
  HIDWORD(v944) = v964;
  v966 = v961 ^ (v944 >> 11);
  LODWORD(v944) = v965;
  HIDWORD(v944) = v965;
  v967 = v944 >> 6;
  LODWORD(v944) = v966;
  HIDWORD(v944) = v966;
  v968 = DWORD2(v1097) + v967 + (v805 ^ v960 & (v805 ^ v951)) + v792;
  v969 = v968 + v794;
  v970 = (v807 & v961 | v952 & (v807 | v961)) + (v944 >> 2) + v968;
  LODWORD(v944) = v969;
  HIDWORD(v944) = v969;
  v971 = v944 >> 14;
  LODWORD(v944) = v970;
  HIDWORD(v944) = v970;
  v972 = v969 ^ v971;
  v973 = v970 ^ (v944 >> 9);
  LODWORD(v944) = v972;
  HIDWORD(v944) = v972;
  v974 = v969 ^ (v944 >> 5);
  LODWORD(v944) = v973;
  HIDWORD(v944) = v973;
  v975 = v970 ^ (v944 >> 11);
  LODWORD(v944) = v974;
  HIDWORD(v944) = v974;
  v976 = v944 >> 6;
  LODWORD(v944) = v975;
  HIDWORD(v944) = v975;
  v977 = DWORD3(v1097) + v976 + (v951 ^ v969 & (v951 ^ v960)) + v805;
  v978 = v977 + v807;
  v979 = (v952 & v970 | v961 & (v952 | v970)) + (v944 >> 2) + v977;
  LODWORD(v944) = v978;
  HIDWORD(v944) = v978;
  v980 = v944 >> 14;
  LODWORD(v944) = v979;
  HIDWORD(v944) = v979;
  v981 = v978 ^ v980;
  v982 = v979 ^ (v944 >> 9);
  LODWORD(v944) = v981;
  HIDWORD(v944) = v981;
  v983 = v978 ^ (v944 >> 5);
  LODWORD(v944) = v982;
  HIDWORD(v944) = v982;
  v984 = v979 ^ (v944 >> 11);
  LODWORD(v944) = v983;
  HIDWORD(v944) = v983;
  v985 = v944 >> 6;
  LODWORD(v944) = v984;
  HIDWORD(v944) = v984;
  v986 = v1100 + v985 + (v960 ^ v978 & (v960 ^ v969)) + v951;
  v987 = v986 + v952;
  v988 = (v961 & v979 | v970 & (v961 | v979)) + (v944 >> 2) + v986;
  LODWORD(v944) = v987;
  HIDWORD(v944) = v987;
  v989 = v944 >> 14;
  LODWORD(v944) = v988;
  HIDWORD(v944) = v988;
  v990 = v987 ^ v989;
  v991 = v988 ^ (v944 >> 9);
  LODWORD(v944) = v990;
  HIDWORD(v944) = v990;
  v992 = v987 ^ (v944 >> 5);
  LODWORD(v944) = v991;
  HIDWORD(v944) = v991;
  v993 = v988 ^ (v944 >> 11);
  LODWORD(v944) = v992;
  HIDWORD(v944) = v992;
  v994 = v944 >> 6;
  LODWORD(v944) = v993;
  HIDWORD(v944) = v993;
  v995 = DWORD1(v1100) + v994 + (v969 ^ v987 & (v969 ^ v978)) + v960;
  v996 = v995 + v961;
  v997 = (v970 & v988 | v979 & (v970 | v988)) + (v944 >> 2) + v995;
  LODWORD(v944) = v996;
  HIDWORD(v944) = v996;
  v998 = v944 >> 14;
  LODWORD(v944) = v997;
  HIDWORD(v944) = v997;
  v999 = v996 ^ v998;
  v1000 = v997 ^ (v944 >> 9);
  LODWORD(v944) = v999;
  HIDWORD(v944) = v999;
  v1001 = v996 ^ (v944 >> 5);
  LODWORD(v944) = v1000;
  HIDWORD(v944) = v1000;
  v1002 = v997 ^ (v944 >> 11);
  LODWORD(v944) = v1001;
  HIDWORD(v944) = v1001;
  v1003 = v944 >> 6;
  LODWORD(v944) = v1002;
  HIDWORD(v944) = v1002;
  v1004 = DWORD2(v1100) + v1003 + (v978 ^ v996 & (v978 ^ v987)) + v969;
  v1005 = v1004 + v970;
  v1006 = (v979 & v997 | v988 & (v979 | v997)) + (v944 >> 2) + v1004;
  LODWORD(v944) = v1005;
  HIDWORD(v944) = v1005;
  v1007 = v944 >> 14;
  LODWORD(v944) = v1006;
  HIDWORD(v944) = v1006;
  v1008 = v1005 ^ v1007;
  v1009 = v1006 ^ (v944 >> 9);
  LODWORD(v944) = v1008;
  HIDWORD(v944) = v1008;
  v1010 = v1005 ^ (v944 >> 5);
  LODWORD(v944) = v1009;
  HIDWORD(v944) = v1009;
  v1011 = v1006 ^ (v944 >> 11);
  LODWORD(v944) = v1010;
  HIDWORD(v944) = v1010;
  v1012 = v944 >> 6;
  LODWORD(v944) = v1011;
  HIDWORD(v944) = v1011;
  v1013 = DWORD3(v1100) + v1012 + (v987 ^ v1005 & (v987 ^ v996)) + v978;
  v1014 = v1013 + v979;
  v1015 = (v988 & v1006 | v997 & (v988 | v1006)) + (v944 >> 2) + v1013;
  LODWORD(v944) = v1014;
  HIDWORD(v944) = v1014;
  v1016 = v944 >> 14;
  LODWORD(v944) = v1015;
  HIDWORD(v944) = v1015;
  v1017 = v1014 ^ v1016;
  v1018 = v1015 ^ (v944 >> 9);
  LODWORD(v944) = v1017;
  HIDWORD(v944) = v1017;
  v1019 = v1014 ^ (v944 >> 5);
  LODWORD(v944) = v1018;
  HIDWORD(v944) = v1018;
  v1020 = v1015 ^ (v944 >> 11);
  LODWORD(v944) = v1019;
  HIDWORD(v944) = v1019;
  v1021 = v944 >> 6;
  LODWORD(v944) = v1020;
  HIDWORD(v944) = v1020;
  v1022 = v1103 + v1021 + (v996 ^ v1014 & (v996 ^ v1005)) + v987;
  v1023 = v1022 + v988;
  v1024 = (v997 & v1015 | v1006 & (v997 | v1015)) + (v944 >> 2) + v1022;
  LODWORD(v944) = v1023;
  HIDWORD(v944) = v1023;
  v1025 = v944 >> 14;
  LODWORD(v944) = v1024;
  HIDWORD(v944) = v1024;
  v1026 = v1023 ^ v1025;
  v1027 = v1024 ^ (v944 >> 9);
  LODWORD(v944) = v1026;
  HIDWORD(v944) = v1026;
  v1028 = v1023 ^ (v944 >> 5);
  LODWORD(v944) = v1027;
  HIDWORD(v944) = v1027;
  v1029 = v1024 ^ (v944 >> 11);
  LODWORD(v944) = v1028;
  HIDWORD(v944) = v1028;
  v1030 = v944 >> 6;
  LODWORD(v944) = v1029;
  HIDWORD(v944) = v1029;
  v1031 = DWORD1(v1103) + v1030 + (v1005 ^ v1023 & (v1005 ^ v1014)) + v996;
  v1032 = v1031 + v997;
  v1033 = (v1006 & v1024 | v1015 & (v1006 | v1024)) + (v944 >> 2) + v1031;
  LODWORD(v944) = v1032;
  HIDWORD(v944) = v1032;
  v1034 = v944 >> 14;
  LODWORD(v944) = v1033;
  HIDWORD(v944) = v1033;
  v1035 = v1032 ^ v1034;
  v1036 = v1033 ^ (v944 >> 9);
  LODWORD(v944) = v1035;
  HIDWORD(v944) = v1035;
  v1037 = v1032 ^ (v944 >> 5);
  LODWORD(v944) = v1036;
  HIDWORD(v944) = v1036;
  v1038 = v1033 ^ (v944 >> 11);
  LODWORD(v944) = v1037;
  HIDWORD(v944) = v1037;
  v1039 = v944 >> 6;
  LODWORD(v944) = v1038;
  HIDWORD(v944) = v1038;
  v1040 = DWORD2(v1103) + v1039 + (v1014 ^ v1032 & (v1014 ^ v1023)) + v1005;
  v1041 = v1040 + v1006;
  v1042 = (v1015 & v1033 | v1024 & (v1015 | v1033)) + (v944 >> 2) + v1040;
  LODWORD(v944) = v1041;
  HIDWORD(v944) = v1041;
  v1043 = v944 >> 14;
  LODWORD(v944) = v1042;
  HIDWORD(v944) = v1042;
  v1044 = v1041 ^ v1043;
  v1045 = v1042 ^ (v944 >> 9);
  LODWORD(v944) = v1044;
  HIDWORD(v944) = v1044;
  v1046 = v1041 ^ (v944 >> 5);
  LODWORD(v944) = v1045;
  HIDWORD(v944) = v1045;
  v1047 = v1042 ^ (v944 >> 11);
  LODWORD(v944) = v1046;
  HIDWORD(v944) = v1046;
  v1048 = v944 >> 6;
  LODWORD(v944) = v1047;
  HIDWORD(v944) = v1047;
  v1049 = DWORD3(v1103) + v1048 + (v1023 ^ v1041 & (v1023 ^ v1032)) + v1014;
  v1050 = v1049 + v1015;
  v1051 = (v1024 & v1042 | v1033 & (v1024 | v1042)) + (v944 >> 2) + v1049;
  LODWORD(v944) = v1050;
  HIDWORD(v944) = v1050;
  v1052 = v944 >> 14;
  LODWORD(v944) = v1051;
  HIDWORD(v944) = v1051;
  v1053 = v1050 ^ v1052;
  v1054 = v1051 ^ (v944 >> 9);
  LODWORD(v944) = v1053;
  HIDWORD(v944) = v1053;
  v1055 = v1050 ^ (v944 >> 5);
  LODWORD(v944) = v1054;
  HIDWORD(v944) = v1054;
  v1056 = v1051 ^ (v944 >> 11);
  LODWORD(v944) = v1055;
  HIDWORD(v944) = v1055;
  v1057 = v944 >> 6;
  LODWORD(v944) = v1056;
  HIDWORD(v944) = v1056;
  v1058 = v1106 + v1057 + (v1032 ^ v1050 & (v1032 ^ v1041)) + v1023;
  v1059 = v1058 + v1024;
  v1060 = (v1033 & v1051 | v1042 & (v1033 | v1051)) + (v944 >> 2) + v1058;
  LODWORD(v944) = v1059;
  HIDWORD(v944) = v1059;
  v1061 = v944 >> 14;
  LODWORD(v944) = v1060;
  HIDWORD(v944) = v1060;
  v1062 = v1059 ^ v1061;
  v1063 = v1060 ^ (v944 >> 9);
  LODWORD(v944) = v1062;
  HIDWORD(v944) = v1062;
  v1064 = v1059 ^ (v944 >> 5);
  LODWORD(v944) = v1063;
  HIDWORD(v944) = v1063;
  v1065 = v1060 ^ (v944 >> 11);
  LODWORD(v944) = v1064;
  HIDWORD(v944) = v1064;
  v1066 = v944 >> 6;
  LODWORD(v944) = v1065;
  HIDWORD(v944) = v1065;
  v1067 = DWORD1(v1106) + v1066 + (v1041 ^ v1059 & (v1041 ^ v1050)) + v1032;
  v1068 = v1067 + v1033;
  v1069 = (v1042 & v1060 | v1051 & (v1042 | v1060)) + (v944 >> 2) + v1067;
  LODWORD(v944) = v1068;
  HIDWORD(v944) = v1068;
  v1070 = v944 >> 14;
  LODWORD(v944) = v1069;
  HIDWORD(v944) = v1069;
  v1071 = v1068 ^ v1070;
  v1072 = v1069 ^ (v944 >> 9);
  LODWORD(v944) = v1071;
  HIDWORD(v944) = v1071;
  v1073 = v1068 ^ (v944 >> 5);
  LODWORD(v944) = v1072;
  HIDWORD(v944) = v1072;
  v1074 = v1069 ^ (v944 >> 11);
  LODWORD(v944) = v1073;
  HIDWORD(v944) = v1073;
  v1075 = v944 >> 6;
  LODWORD(v944) = v1074;
  HIDWORD(v944) = v1074;
  v1076 = DWORD2(v1106) + v1075 + (v1050 ^ v1068 & (v1050 ^ v1059)) + v1041;
  v1077 = v1076 + v1042;
  v1078 = (v1051 & v1069 | v1060 & (v1051 | v1069)) + (v944 >> 2) + v1076;
  LODWORD(v944) = v1077;
  HIDWORD(v944) = v1077;
  v1079 = v944 >> 14;
  LODWORD(v944) = v1078;
  HIDWORD(v944) = v1078;
  v1080 = v1077 ^ v1079;
  v1081 = v1078 ^ (v944 >> 9);
  LODWORD(v944) = v1080;
  HIDWORD(v944) = v1080;
  v1082 = v1077 ^ (v944 >> 5);
  LODWORD(v944) = v1081;
  HIDWORD(v944) = v1081;
  v1083 = v1078 ^ (v944 >> 11);
  LODWORD(v944) = v1082;
  HIDWORD(v944) = v1082;
  v1084 = v944 >> 6;
  LODWORD(v944) = v1083;
  HIDWORD(v944) = v1083;
  v1085 = DWORD3(v1106) + v1084 + (v1059 ^ v1077 & (v1059 ^ v1068)) + v1050;
  result = v1060 & v1078 | v1069 & (v1060 | v1078);
  *(_DWORD *)a2 += result + (v944 >> 2) + v1085;
  *(_DWORD *)(a2 + 4) += v1078;
  *(_DWORD *)(a2 + 8) += v1069;
  *(_DWORD *)(a2 + 12) += v1060;
  *(_DWORD *)(a2 + 16) += v1085 + v1051;
  *(_DWORD *)(a2 + 20) += v1077;
  *(_DWORD *)(a2 + 24) += v1068;
  *(_DWORD *)(a2 + 28) += v1059;
  __asm
  {
    vmovdqa xmm0, [rsp+108h+var_B8]
    vmovdqa xmm1, [rsp+108h+var_A8]
    vmovdqa xmm2, [rsp+108h+var_98]
    vmovdqa xmm3, [rsp+108h+var_88]
    vmovdqa xmm4, [rsp+108h+var_78]
    vmovdqa xmm5, [rsp+108h+var_68]
    vmovdqa xmm6, [rsp+108h+var_58]
    vmovdqa xmm7, [rsp+108h+var_48]
  }
  return result;
}
// 4B930: using guessed type __int64 qword_4B930[2];
// 66F00: using guessed type __int64 ccsha256_K[32];

//----- (00000000000048F0) ----------------------------------------------------
signed __int64 __fastcall vng_aes_encrypt_opt(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rax@1
  signed __int64 result; // rax@4
  __int64 v5; // r11@5
  int v6; // eax@5
  __m128i v7; // xmm0@5
  __m128i v8; // xmm1@5
  __m128i v9; // xmm2@5
  signed __int64 v10; // r10@5
  int v11; // er8@5
  int v12; // ebx@5
  int v13; // ecx@5
  int v14; // edx@5
  int v15; // er8@6
  int v16; // edx@6
  unsigned int v17; // eax@6
  int v18; // ecx@6
  int v19; // ebx@6
  unsigned int v20; // eax@6
  int v21; // ebx@6
  int v22; // er8@6
  int v23; // edx@6
  int v24; // ecx@6
  unsigned int v25; // eax@6
  int v26; // ecx@6
  int v27; // ebx@6
  int v28; // er8@6
  int v29; // edx@6
  unsigned int v30; // eax@6
  int v31; // edx@6
  int v32; // ecx@6
  int v33; // ebx@6
  int v34; // er8@7
  int v35; // edx@7
  unsigned int v36; // eax@7
  int v37; // ecx@7
  int v38; // ebx@7
  unsigned int v39; // eax@7
  int v40; // ebx@7
  int v41; // er8@7
  int v42; // edx@7
  int v43; // ecx@7
  unsigned int v44; // eax@7
  int v45; // ecx@7
  int v46; // ebx@7
  int v47; // er8@7
  int v48; // edx@7
  unsigned int v49; // eax@7
  int v50; // edx@7
  int v51; // ecx@7
  int v52; // ebx@7

  v3 = *(_BYTE *)(a3 + 240);
  if ( v3 == 160 || v3 == 192 || v3 == 224 )
  {
    v5 = a3 + v3;
    v6 = *(_DWORD *)a3 ^ *(_DWORD *)a1;
    v7 = _mm_cvtsi32_si128(*(_DWORD *)(a3 + 4) ^ *(_DWORD *)(a1 + 4));
    v8 = _mm_cvtsi32_si128(*(_DWORD *)(a3 + 8) ^ *(_DWORD *)(a1 + 8));
    v9 = _mm_cvtsi32_si128(*(_DWORD *)(a3 + 12) ^ *(_DWORD *)(a1 + 12));
    v10 = a3 + 16;
    v11 = *(_DWORD *)(a3 + 16);
    v12 = *(_DWORD *)(a3 + 20);
    v13 = *(_DWORD *)(a3 + 24);
    v14 = *(_DWORD *)(a3 + 28);
    do
    {
      v15 = AESEncryptTable[(unsigned __int64)(unsigned __int8)v6] ^ v11;
      v16 = AESEncryptTable[BYTE1(v6) + 256] ^ v14;
      v17 = (unsigned int)v6 >> 16;
      v18 = AESEncryptTable[(unsigned __int8)v17 + 512] ^ v13;
      v19 = AESEncryptTable[BYTE1(v17) + 768] ^ v12;
      v20 = _mm_cvtsi128_si32(v7);
      v21 = AESEncryptTable[(unsigned __int64)(unsigned __int8)v20] ^ v19;
      v22 = AESEncryptTable[BYTE1(v20) + 256] ^ v15;
      v20 >>= 16;
      v23 = AESEncryptTable[(unsigned __int8)v20 + 512] ^ v16;
      v24 = AESEncryptTable[BYTE1(v20) + 768] ^ v18;
      v25 = _mm_cvtsi128_si32(v8);
      v26 = AESEncryptTable[(unsigned __int64)(unsigned __int8)v25] ^ v24;
      v27 = AESEncryptTable[BYTE1(v25) + 256] ^ v21;
      v25 >>= 16;
      v28 = AESEncryptTable[(unsigned __int8)v25 + 512] ^ v22;
      v29 = AESEncryptTable[BYTE1(v25) + 768] ^ v23;
      v30 = _mm_cvtsi128_si32(v9);
      v31 = AESEncryptTable[(unsigned __int64)(unsigned __int8)v30] ^ v29;
      v32 = AESEncryptTable[BYTE1(v30) + 256] ^ v26;
      v30 >>= 16;
      v33 = AESEncryptTable[(unsigned __int8)v30 + 512] ^ v27;
      v10 += 16LL;
      v6 = AESEncryptTable[BYTE1(v30) + 768] ^ v28;
      v11 = *(_DWORD *)v10;
      v7 = _mm_cvtsi32_si128(v33);
      v12 = *(_DWORD *)(v10 + 4);
      v8 = _mm_cvtsi32_si128(v32);
      v13 = *(_DWORD *)(v10 + 8);
      v9 = _mm_cvtsi32_si128(v31);
      v14 = *(_DWORD *)(v10 + 12);
    }
    while ( v10 != v5 );
    v34 = AESSubBytesWordTable[(unsigned __int64)(unsigned __int8)v6] ^ v11;
    v35 = AESSubBytesWordTable[BYTE1(v6) + 256] ^ v14;
    v36 = (unsigned int)v6 >> 16;
    v37 = AESSubBytesWordTable[(unsigned __int8)v36 + 512] ^ v13;
    v38 = AESSubBytesWordTable[BYTE1(v36) + 768] ^ v12;
    v39 = _mm_cvtsi128_si32(v7);
    v40 = AESSubBytesWordTable[(unsigned __int64)(unsigned __int8)v39] ^ v38;
    v41 = AESSubBytesWordTable[BYTE1(v39) + 256] ^ v34;
    v39 >>= 16;
    v42 = AESSubBytesWordTable[(unsigned __int8)v39 + 512] ^ v35;
    v43 = AESSubBytesWordTable[BYTE1(v39) + 768] ^ v37;
    v44 = _mm_cvtsi128_si32(v8);
    v45 = AESSubBytesWordTable[(unsigned __int64)(unsigned __int8)v44] ^ v43;
    v46 = AESSubBytesWordTable[BYTE1(v44) + 256] ^ v40;
    v44 >>= 16;
    v47 = AESSubBytesWordTable[(unsigned __int8)v44 + 512] ^ v41;
    v48 = AESSubBytesWordTable[BYTE1(v44) + 768] ^ v42;
    v49 = _mm_cvtsi128_si32(v9);
    v50 = AESSubBytesWordTable[(unsigned __int64)(unsigned __int8)v49] ^ v48;
    v51 = AESSubBytesWordTable[BYTE1(v49) + 256] ^ v45;
    v49 >>= 16;
    v52 = AESSubBytesWordTable[(unsigned __int8)v49 + 512] ^ v46;
    *(_DWORD *)a2 = AESSubBytesWordTable[BYTE1(v49) + 768] ^ v47;
    *(_DWORD *)(a2 + 4) = v52;
    *(_DWORD *)(a2 + 8) = v51;
    *(_DWORD *)(a2 + 12) = v50;
    result = 0LL;
  }
  else
  {
    result = -1LL;
  }
  return result;
}
// 4C94C: using guessed type int AESEncryptTable[];
// 4E94C: using guessed type int AESSubBytesWordTable[];

//----- (0000000000004B61) ----------------------------------------------------
signed __int64 __fastcall vng_aes_decrypt_opt(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rax@1
  signed __int64 result; // rax@4
  __int64 v5; // r11@5
  __int64 v6; // r10@5
  int v7; // eax@5
  __m128i v8; // xmm0@5
  __m128i v9; // xmm1@5
  __m128i v10; // xmm2@5
  signed __int64 v11; // r10@5
  int v12; // er8@5
  int v13; // ebx@5
  int v14; // ecx@5
  int v15; // edx@5
  int v16; // er8@6
  int v17; // ebx@6
  unsigned int v18; // eax@6
  int v19; // ecx@6
  int v20; // edx@6
  unsigned int v21; // eax@6
  int v22; // ebx@6
  int v23; // ecx@6
  int v24; // edx@6
  int v25; // er8@6
  unsigned int v26; // eax@6
  int v27; // ecx@6
  int v28; // edx@6
  int v29; // er8@6
  int v30; // ebx@6
  unsigned int v31; // eax@6
  int v32; // edx@6
  int v33; // er8@6
  int v34; // ebx@6
  int v35; // ecx@6
  int v36; // er8@7
  int v37; // ebx@7
  unsigned int v38; // eax@7
  int v39; // ecx@7
  int v40; // edx@7
  unsigned int v41; // eax@7
  int v42; // ebx@7
  int v43; // ecx@7
  int v44; // edx@7
  int v45; // er8@7
  unsigned int v46; // eax@7
  int v47; // ecx@7
  int v48; // edx@7
  int v49; // er8@7
  int v50; // ebx@7
  unsigned int v51; // eax@7
  int v52; // edx@7
  int v53; // er8@7
  int v54; // ebx@7
  int v55; // ecx@7

  v3 = *(_BYTE *)(a3 + 240);
  if ( v3 == 160 || v3 == 192 || v3 == 224 )
  {
    v5 = a3;
    v6 = v3 + a3;
    v7 = *(_DWORD *)(v3 + a3) ^ *(_DWORD *)a1;
    v8 = _mm_cvtsi32_si128(*(_DWORD *)(v6 + 4) ^ *(_DWORD *)(a1 + 4));
    v9 = _mm_cvtsi32_si128(*(_DWORD *)(v6 + 8) ^ *(_DWORD *)(a1 + 8));
    v10 = _mm_cvtsi32_si128(*(_DWORD *)(v6 + 12) ^ *(_DWORD *)(a1 + 12));
    v11 = v6 - 16;
    v12 = *(_DWORD *)v11;
    v13 = *(_DWORD *)(v11 + 4);
    v14 = *(_DWORD *)(v11 + 8);
    v15 = *(_DWORD *)(v11 + 12);
    do
    {
      v16 = AESDecryptTable[(unsigned __int64)(unsigned __int8)v7] ^ v12;
      v17 = AESDecryptTable[BYTE1(v7) + 256] ^ v13;
      v18 = (unsigned int)v7 >> 16;
      v19 = AESDecryptTable[(unsigned __int8)v18 + 512] ^ v14;
      v20 = AESDecryptTable[BYTE1(v18) + 768] ^ v15;
      v21 = _mm_cvtsi128_si32(v8);
      v22 = AESDecryptTable[(unsigned __int64)(unsigned __int8)v21] ^ v17;
      v23 = AESDecryptTable[BYTE1(v21) + 256] ^ v19;
      v21 >>= 16;
      v24 = AESDecryptTable[(unsigned __int8)v21 + 512] ^ v20;
      v25 = AESDecryptTable[BYTE1(v21) + 768] ^ v16;
      v26 = _mm_cvtsi128_si32(v9);
      v27 = AESDecryptTable[(unsigned __int64)(unsigned __int8)v26] ^ v23;
      v28 = AESDecryptTable[BYTE1(v26) + 256] ^ v24;
      v26 >>= 16;
      v29 = AESDecryptTable[(unsigned __int8)v26 + 512] ^ v25;
      v30 = AESDecryptTable[BYTE1(v26) + 768] ^ v22;
      v31 = _mm_cvtsi128_si32(v10);
      v32 = AESDecryptTable[(unsigned __int64)(unsigned __int8)v31] ^ v28;
      v33 = AESDecryptTable[BYTE1(v31) + 256] ^ v29;
      v31 >>= 16;
      v34 = AESDecryptTable[(unsigned __int8)v31 + 512] ^ v30;
      v35 = AESDecryptTable[BYTE1(v31) + 768] ^ v27;
      v11 -= 16LL;
      v7 = v33;
      v12 = *(_DWORD *)v11;
      v8 = _mm_cvtsi32_si128(v34);
      v13 = *(_DWORD *)(v11 + 4);
      v9 = _mm_cvtsi32_si128(v35);
      v14 = *(_DWORD *)(v11 + 8);
      v10 = _mm_cvtsi32_si128(v32);
      v15 = *(_DWORD *)(v11 + 12);
    }
    while ( v11 != v5 );
    v36 = AESInvSubBytesWordTable[(unsigned __int64)(unsigned __int8)v7] ^ v12;
    v37 = AESInvSubBytesWordTable[BYTE1(v7) + 256] ^ v13;
    v38 = (unsigned int)v7 >> 16;
    v39 = AESInvSubBytesWordTable[(unsigned __int8)v38 + 512] ^ v14;
    v40 = AESInvSubBytesWordTable[BYTE1(v38) + 768] ^ v15;
    v41 = _mm_cvtsi128_si32(v8);
    v42 = AESInvSubBytesWordTable[(unsigned __int64)(unsigned __int8)v41] ^ v37;
    v43 = AESInvSubBytesWordTable[BYTE1(v41) + 256] ^ v39;
    v41 >>= 16;
    v44 = AESInvSubBytesWordTable[(unsigned __int8)v41 + 512] ^ v40;
    v45 = AESInvSubBytesWordTable[BYTE1(v41) + 768] ^ v36;
    v46 = _mm_cvtsi128_si32(v9);
    v47 = AESInvSubBytesWordTable[(unsigned __int64)(unsigned __int8)v46] ^ v43;
    v48 = AESInvSubBytesWordTable[BYTE1(v46) + 256] ^ v44;
    v46 >>= 16;
    v49 = AESInvSubBytesWordTable[(unsigned __int8)v46 + 512] ^ v45;
    v50 = AESInvSubBytesWordTable[BYTE1(v46) + 768] ^ v42;
    v51 = _mm_cvtsi128_si32(v10);
    v52 = AESInvSubBytesWordTable[(unsigned __int64)(unsigned __int8)v51] ^ v48;
    v53 = AESInvSubBytesWordTable[BYTE1(v51) + 256] ^ v49;
    v51 >>= 16;
    v54 = AESInvSubBytesWordTable[(unsigned __int8)v51 + 512] ^ v50;
    v55 = AESInvSubBytesWordTable[BYTE1(v51) + 768] ^ v47;
    *(_DWORD *)a2 = v53;
    *(_DWORD *)(a2 + 4) = v54;
    *(_DWORD *)(a2 + 8) = v55;
    *(_DWORD *)(a2 + 12) = v52;
    result = 0LL;
  }
  else
  {
    result = -1LL;
  }
  return result;
}
// 4D94C: using guessed type int AESDecryptTable[];
// 4F94C: using guessed type int AESInvSubBytesWordTable[];

//----- (0000000000004DD2) ----------------------------------------------------
signed __int64 __fastcall aes_encrypt_xmm_no_save(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rax@1
  signed __int64 result; // rax@4
  __int64 v5; // r11@5
  int v6; // eax@5
  __m128i v7; // xmm0@5
  __m128i v8; // xmm1@5
  __m128i v9; // xmm2@5
  signed __int64 v10; // r10@5
  int v11; // er8@5
  int v12; // ebx@5
  int v13; // ecx@5
  int v14; // edx@5
  int v15; // er8@6
  int v16; // edx@6
  unsigned int v17; // eax@6
  int v18; // ecx@6
  int v19; // ebx@6
  unsigned int v20; // eax@6
  int v21; // ebx@6
  int v22; // er8@6
  int v23; // edx@6
  int v24; // ecx@6
  unsigned int v25; // eax@6
  int v26; // ecx@6
  int v27; // ebx@6
  int v28; // er8@6
  int v29; // edx@6
  unsigned int v30; // eax@6
  int v31; // edx@6
  int v32; // ecx@6
  int v33; // ebx@6
  int v34; // er8@7
  int v35; // edx@7
  unsigned int v36; // eax@7
  int v37; // ecx@7
  int v38; // ebx@7
  unsigned int v39; // eax@7
  int v40; // ebx@7
  int v41; // er8@7
  int v42; // edx@7
  int v43; // ecx@7
  unsigned int v44; // eax@7
  int v45; // ecx@7
  int v46; // ebx@7
  int v47; // er8@7
  int v48; // edx@7
  unsigned int v49; // eax@7
  int v50; // edx@7
  int v51; // ecx@7
  int v52; // ebx@7

  v3 = *(_BYTE *)(a3 + 240);
  if ( v3 == 160 || v3 == 192 || v3 == 224 )
  {
    v5 = a3 + v3;
    v6 = *(_DWORD *)a3 ^ *(_DWORD *)a1;
    v7 = _mm_cvtsi32_si128(*(_DWORD *)(a3 + 4) ^ *(_DWORD *)(a1 + 4));
    v8 = _mm_cvtsi32_si128(*(_DWORD *)(a3 + 8) ^ *(_DWORD *)(a1 + 8));
    v9 = _mm_cvtsi32_si128(*(_DWORD *)(a3 + 12) ^ *(_DWORD *)(a1 + 12));
    v10 = a3 + 16;
    v11 = *(_DWORD *)(a3 + 16);
    v12 = *(_DWORD *)(a3 + 20);
    v13 = *(_DWORD *)(a3 + 24);
    v14 = *(_DWORD *)(a3 + 28);
    do
    {
      v15 = AESEncryptTable[(unsigned __int64)(unsigned __int8)v6] ^ v11;
      v16 = AESEncryptTable[BYTE1(v6) + 256] ^ v14;
      v17 = (unsigned int)v6 >> 16;
      v18 = AESEncryptTable[(unsigned __int8)v17 + 512] ^ v13;
      v19 = AESEncryptTable[BYTE1(v17) + 768] ^ v12;
      v20 = _mm_cvtsi128_si32(v7);
      v21 = AESEncryptTable[(unsigned __int64)(unsigned __int8)v20] ^ v19;
      v22 = AESEncryptTable[BYTE1(v20) + 256] ^ v15;
      v20 >>= 16;
      v23 = AESEncryptTable[(unsigned __int8)v20 + 512] ^ v16;
      v24 = AESEncryptTable[BYTE1(v20) + 768] ^ v18;
      v25 = _mm_cvtsi128_si32(v8);
      v26 = AESEncryptTable[(unsigned __int64)(unsigned __int8)v25] ^ v24;
      v27 = AESEncryptTable[BYTE1(v25) + 256] ^ v21;
      v25 >>= 16;
      v28 = AESEncryptTable[(unsigned __int8)v25 + 512] ^ v22;
      v29 = AESEncryptTable[BYTE1(v25) + 768] ^ v23;
      v30 = _mm_cvtsi128_si32(v9);
      v31 = AESEncryptTable[(unsigned __int64)(unsigned __int8)v30] ^ v29;
      v32 = AESEncryptTable[BYTE1(v30) + 256] ^ v26;
      v30 >>= 16;
      v33 = AESEncryptTable[(unsigned __int8)v30 + 512] ^ v27;
      v10 += 16LL;
      v6 = AESEncryptTable[BYTE1(v30) + 768] ^ v28;
      v11 = *(_DWORD *)v10;
      v7 = _mm_cvtsi32_si128(v33);
      v12 = *(_DWORD *)(v10 + 4);
      v8 = _mm_cvtsi32_si128(v32);
      v13 = *(_DWORD *)(v10 + 8);
      v9 = _mm_cvtsi32_si128(v31);
      v14 = *(_DWORD *)(v10 + 12);
    }
    while ( v10 != v5 );
    v34 = AESSubBytesWordTable[(unsigned __int64)(unsigned __int8)v6] ^ v11;
    v35 = AESSubBytesWordTable[BYTE1(v6) + 256] ^ v14;
    v36 = (unsigned int)v6 >> 16;
    v37 = AESSubBytesWordTable[(unsigned __int8)v36 + 512] ^ v13;
    v38 = AESSubBytesWordTable[BYTE1(v36) + 768] ^ v12;
    v39 = _mm_cvtsi128_si32(v7);
    v40 = AESSubBytesWordTable[(unsigned __int64)(unsigned __int8)v39] ^ v38;
    v41 = AESSubBytesWordTable[BYTE1(v39) + 256] ^ v34;
    v39 >>= 16;
    v42 = AESSubBytesWordTable[(unsigned __int8)v39 + 512] ^ v35;
    v43 = AESSubBytesWordTable[BYTE1(v39) + 768] ^ v37;
    v44 = _mm_cvtsi128_si32(v8);
    v45 = AESSubBytesWordTable[(unsigned __int64)(unsigned __int8)v44] ^ v43;
    v46 = AESSubBytesWordTable[BYTE1(v44) + 256] ^ v40;
    v44 >>= 16;
    v47 = AESSubBytesWordTable[(unsigned __int8)v44 + 512] ^ v41;
    v48 = AESSubBytesWordTable[BYTE1(v44) + 768] ^ v42;
    v49 = _mm_cvtsi128_si32(v9);
    v50 = AESSubBytesWordTable[(unsigned __int64)(unsigned __int8)v49] ^ v48;
    v51 = AESSubBytesWordTable[BYTE1(v49) + 256] ^ v45;
    v49 >>= 16;
    v52 = AESSubBytesWordTable[(unsigned __int8)v49 + 512] ^ v46;
    *(_DWORD *)a2 = AESSubBytesWordTable[BYTE1(v49) + 768] ^ v47;
    *(_DWORD *)(a2 + 4) = v52;
    *(_DWORD *)(a2 + 8) = v51;
    *(_DWORD *)(a2 + 12) = v50;
    result = 0LL;
  }
  else
  {
    result = -1LL;
  }
  return result;
}
// 4C94C: using guessed type int AESEncryptTable[];
// 4E94C: using guessed type int AESSubBytesWordTable[];

//----- (0000000000005027) ----------------------------------------------------
signed __int64 __fastcall aes_decrypt_xmm_no_save(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rax@1
  signed __int64 result; // rax@4
  __int64 v5; // r11@5
  __int64 v6; // r10@5
  int v7; // eax@5
  __m128i v8; // xmm0@5
  __m128i v9; // xmm1@5
  __m128i v10; // xmm2@5
  signed __int64 v11; // r10@5
  int v12; // er8@5
  int v13; // ebx@5
  int v14; // ecx@5
  int v15; // edx@5
  int v16; // er8@6
  int v17; // ebx@6
  unsigned int v18; // eax@6
  int v19; // ecx@6
  int v20; // edx@6
  unsigned int v21; // eax@6
  int v22; // ebx@6
  int v23; // ecx@6
  int v24; // edx@6
  int v25; // er8@6
  unsigned int v26; // eax@6
  int v27; // ecx@6
  int v28; // edx@6
  int v29; // er8@6
  int v30; // ebx@6
  unsigned int v31; // eax@6
  int v32; // edx@6
  int v33; // er8@6
  int v34; // ebx@6
  int v35; // ecx@6
  int v36; // er8@7
  int v37; // ebx@7
  unsigned int v38; // eax@7
  int v39; // ecx@7
  int v40; // edx@7
  unsigned int v41; // eax@7
  int v42; // ebx@7
  int v43; // ecx@7
  int v44; // edx@7
  int v45; // er8@7
  unsigned int v46; // eax@7
  int v47; // ecx@7
  int v48; // edx@7
  int v49; // er8@7
  int v50; // ebx@7
  unsigned int v51; // eax@7
  int v52; // edx@7
  int v53; // er8@7
  int v54; // ebx@7
  int v55; // ecx@7

  v3 = *(_BYTE *)(a3 + 240);
  if ( v3 == 160 || v3 == 192 || v3 == 224 )
  {
    v5 = a3;
    v6 = v3 + a3;
    v7 = *(_DWORD *)(v3 + a3) ^ *(_DWORD *)a1;
    v8 = _mm_cvtsi32_si128(*(_DWORD *)(v6 + 4) ^ *(_DWORD *)(a1 + 4));
    v9 = _mm_cvtsi32_si128(*(_DWORD *)(v6 + 8) ^ *(_DWORD *)(a1 + 8));
    v10 = _mm_cvtsi32_si128(*(_DWORD *)(v6 + 12) ^ *(_DWORD *)(a1 + 12));
    v11 = v6 - 16;
    v12 = *(_DWORD *)v11;
    v13 = *(_DWORD *)(v11 + 4);
    v14 = *(_DWORD *)(v11 + 8);
    v15 = *(_DWORD *)(v11 + 12);
    do
    {
      v16 = AESDecryptTable[(unsigned __int64)(unsigned __int8)v7] ^ v12;
      v17 = AESDecryptTable[BYTE1(v7) + 256] ^ v13;
      v18 = (unsigned int)v7 >> 16;
      v19 = AESDecryptTable[(unsigned __int8)v18 + 512] ^ v14;
      v20 = AESDecryptTable[BYTE1(v18) + 768] ^ v15;
      v21 = _mm_cvtsi128_si32(v8);
      v22 = AESDecryptTable[(unsigned __int64)(unsigned __int8)v21] ^ v17;
      v23 = AESDecryptTable[BYTE1(v21) + 256] ^ v19;
      v21 >>= 16;
      v24 = AESDecryptTable[(unsigned __int8)v21 + 512] ^ v20;
      v25 = AESDecryptTable[BYTE1(v21) + 768] ^ v16;
      v26 = _mm_cvtsi128_si32(v9);
      v27 = AESDecryptTable[(unsigned __int64)(unsigned __int8)v26] ^ v23;
      v28 = AESDecryptTable[BYTE1(v26) + 256] ^ v24;
      v26 >>= 16;
      v29 = AESDecryptTable[(unsigned __int8)v26 + 512] ^ v25;
      v30 = AESDecryptTable[BYTE1(v26) + 768] ^ v22;
      v31 = _mm_cvtsi128_si32(v10);
      v32 = AESDecryptTable[(unsigned __int64)(unsigned __int8)v31] ^ v28;
      v33 = AESDecryptTable[BYTE1(v31) + 256] ^ v29;
      v31 >>= 16;
      v34 = AESDecryptTable[(unsigned __int8)v31 + 512] ^ v30;
      v35 = AESDecryptTable[BYTE1(v31) + 768] ^ v27;
      v11 -= 16LL;
      v7 = v33;
      v12 = *(_DWORD *)v11;
      v8 = _mm_cvtsi32_si128(v34);
      v13 = *(_DWORD *)(v11 + 4);
      v9 = _mm_cvtsi32_si128(v35);
      v14 = *(_DWORD *)(v11 + 8);
      v10 = _mm_cvtsi32_si128(v32);
      v15 = *(_DWORD *)(v11 + 12);
    }
    while ( v11 != v5 );
    v36 = AESInvSubBytesWordTable[(unsigned __int64)(unsigned __int8)v7] ^ v12;
    v37 = AESInvSubBytesWordTable[BYTE1(v7) + 256] ^ v13;
    v38 = (unsigned int)v7 >> 16;
    v39 = AESInvSubBytesWordTable[(unsigned __int8)v38 + 512] ^ v14;
    v40 = AESInvSubBytesWordTable[BYTE1(v38) + 768] ^ v15;
    v41 = _mm_cvtsi128_si32(v8);
    v42 = AESInvSubBytesWordTable[(unsigned __int64)(unsigned __int8)v41] ^ v37;
    v43 = AESInvSubBytesWordTable[BYTE1(v41) + 256] ^ v39;
    v41 >>= 16;
    v44 = AESInvSubBytesWordTable[(unsigned __int8)v41 + 512] ^ v40;
    v45 = AESInvSubBytesWordTable[BYTE1(v41) + 768] ^ v36;
    v46 = _mm_cvtsi128_si32(v9);
    v47 = AESInvSubBytesWordTable[(unsigned __int64)(unsigned __int8)v46] ^ v43;
    v48 = AESInvSubBytesWordTable[BYTE1(v46) + 256] ^ v44;
    v46 >>= 16;
    v49 = AESInvSubBytesWordTable[(unsigned __int8)v46 + 512] ^ v45;
    v50 = AESInvSubBytesWordTable[BYTE1(v46) + 768] ^ v42;
    v51 = _mm_cvtsi128_si32(v10);
    v52 = AESInvSubBytesWordTable[(unsigned __int64)(unsigned __int8)v51] ^ v48;
    v53 = AESInvSubBytesWordTable[BYTE1(v51) + 256] ^ v49;
    v51 >>= 16;
    v54 = AESInvSubBytesWordTable[(unsigned __int8)v51 + 512] ^ v50;
    v55 = AESInvSubBytesWordTable[BYTE1(v51) + 768] ^ v47;
    *(_DWORD *)a2 = v53;
    *(_DWORD *)(a2 + 4) = v54;
    *(_DWORD *)(a2 + 8) = v55;
    *(_DWORD *)(a2 + 12) = v52;
    result = 0LL;
  }
  else
  {
    result = -1LL;
  }
  return result;
}
// 4D94C: using guessed type int AESDecryptTable[];
// 4F94C: using guessed type int AESInvSubBytesWordTable[];

//----- (000000000000527C) ----------------------------------------------------
void __fastcall vng_aes_encrypt_opt_key(__int64 a1, signed int a2)
{
  int v2; // er9@1
  bool v3; // zf@1

  v2 = a2;
  v3 = a2 == 128;
  if ( a2 < 128 )
  {
    v2 = 8 * a2;
    v3 = 8 * a2 == 128;
  }
  JUMPOUT(v3, EKeyHas4Words);
  JUMPOUT(v2, 192, EKeyHas6Words);
  JUMPOUT(v2, 256, EKeyHas8Words);
  JUMPOUT(&loc_53D1);
}
// 5320: using guessed type int EKeyHas4Words(void);
// 5430: using guessed type int EKeyHas6Words(void);
// 559E: using guessed type int EKeyHas8Words(void);

//----- (00000000000052F1) ----------------------------------------------------
int vng_aes_encrypt_key128()
{
  return EKeyHas4Words();
}
// 5320: using guessed type int EKeyHas4Words(void);

//----- (0000000000005320) ----------------------------------------------------
#error "53FF: positive sp value has been found (funcsize=0)"

//----- (0000000000005401) ----------------------------------------------------
int vng_aes_encrypt_key192()
{
  return EKeyHas6Words();
}
// 5430: using guessed type int EKeyHas6Words(void);

//----- (0000000000005430) ----------------------------------------------------
#error "556D: positive sp value has been found (funcsize=0)"

//----- (000000000000556F) ----------------------------------------------------
int vng_aes_encrypt_key256()
{
  return EKeyHas8Words();
}
// 559E: using guessed type int EKeyHas8Words(void);

//----- (000000000000559E) ----------------------------------------------------
#error "5783: positive sp value has been found (funcsize=0)"

//----- (0000000000005785) ----------------------------------------------------
__int64 __usercall InvMixColumn@<rax>(unsigned int a1@<eax>)
{
  return a1 >> 16;
}

//----- (00000000000057C6) ----------------------------------------------------
void __fastcall vng_aes_decrypt_opt_key(__int64 a1, signed int a2)
{
  int v2; // er9@1
  bool v3; // zf@1

  v2 = a2;
  v3 = a2 == 128;
  if ( a2 < 128 )
  {
    v2 = 8 * a2;
    v3 = 8 * a2 == 128;
  }
  JUMPOUT(v3, DKeyHas4Words);
  JUMPOUT(v2, 192, DKeyHas6Words);
  JUMPOUT(v2, 256, DKeyHas8Words);
  JUMPOUT(&loc_5AAF);
}
// 586A: using guessed type int DKeyHas4Words(void);
// 5B0E: using guessed type int DKeyHas6Words(void);
// 5E40: using guessed type int DKeyHas8Words(void);

//----- (000000000000583B) ----------------------------------------------------
int vng_aes_decrypt_key128()
{
  return DKeyHas4Words();
}
// 586A: using guessed type int DKeyHas4Words(void);

//----- (000000000000586A) ----------------------------------------------------
#error "5ADD: positive sp value has been found (funcsize=0)"

//----- (0000000000005ADF) ----------------------------------------------------
int vng_aes_decrypt_key192()
{
  return DKeyHas6Words();
}
// 5B0E: using guessed type int DKeyHas6Words(void);

//----- (0000000000005B0E) ----------------------------------------------------
#error "5E0F: positive sp value has been found (funcsize=0)"

//----- (0000000000005E11) ----------------------------------------------------
int vng_aes_decrypt_key256()
{
  return DKeyHas8Words();
}
// 5E40: using guessed type int DKeyHas8Words(void);

//----- (0000000000005E40) ----------------------------------------------------
#error "63D9: positive sp value has been found (funcsize=0)"

//----- (00000000000063DB) ----------------------------------------------------
__int64 __fastcall ccrsa_decrypt_oaep(__int64 *a1, __int64 a2, __int64 a3, void *a4, unsigned __int64 a5, unsigned __int64 a6, unsigned __int64 a7, const void *a8)
{
  __int64 v8; // r12@1
  unsigned __int64 v9; // r15@1
  __int64 v10; // r13@1
  __int64 result; // rax@1
  int v12; // ecx@2
  __int64 v13; // rcx@5
  __int64 v14; // [sp+10h] [bp-60h]@2
  __int64 v15; // [sp+18h] [bp-58h]@1
  __int64 v16; // [sp+20h] [bp-50h]@1
  void *v17; // [sp+28h] [bp-48h]@1
  unsigned __int64 v18; // [sp+30h] [bp-40h]@1
  unsigned __int64 v19; // [sp+38h] [bp-38h]@1
  __int64 v20; // [sp+40h] [bp-30h]@1

  v19 = a6;
  v18 = a5;
  v17 = a4;
  v8 = a3;
  v16 = a3;
  v15 = a2;
  v20 = *(_QWORD *)off_69010[0];
  v9 = *a1;
  v10 = ccn_write_uint_size(*a1, (__int64)(a1 + 2));
  result = 4294967294LL;
  if ( *(_QWORD *)v8 >= (unsigned __int64)v10 )
  {
    bzero((char *)&v14 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL), 8 * v9);
    v12 = ccn_read_uint(v9, (__int64)((char *)&v14 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL)), v18, v19);
    result = 0xFFFFFFFFLL;
    if ( !v12 )
    {
      result = ccrsa_priv_crypt(
                 &a1[4 * *a1 + 3],
                 (__int64)((char *)&v14 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL)),
                 (char *)&v14 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL));
      if ( !(_DWORD)result )
        result = ccrsa_oaep_decode_parameter(
                   v15,
                   v16,
                   v17,
                   v10,
                   (__int64)((char *)&v14 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL)),
                   a7,
                   a8);
    }
  }
  v13 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000064E0) ----------------------------------------------------
signed __int64 __fastcall vng_aes_encrypt_aesni(__m128i *a1, __int64 a2, __int64 _RDX)
{
  __m128i v3; // xmm0@1
  int v4; // eax@1
  signed __int64 result; // rax@4

  v3 = *(__m128i *)&a1->m128i_i64[0];
  v4 = *(_DWORD *)(_RDX + 240);
  switch ( v4 )
  {
    case 160:
      if ( _RDX & 0xF )
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)_RDX);
        _XMM1 = *(_OWORD *)(_RDX + 16);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 32);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 48);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 64);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 80);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 96);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 112);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 128);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 144);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 160);
        __asm { aesenclast xmm0, xmm1 }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      else
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)_RDX);
        __asm
        {
          aesenc  xmm0, xmmword ptr [rdx+10h]
          aesenc  xmm0, xmmword ptr [rdx+20h]
          aesenc  xmm0, xmmword ptr [rdx+30h]
          aesenc  xmm0, xmmword ptr [rdx+40h]
          aesenc  xmm0, xmmword ptr [rdx+50h]
          aesenc  xmm0, xmmword ptr [rdx+60h]
          aesenc  xmm0, xmmword ptr [rdx+70h]
          aesenc  xmm0, xmmword ptr [rdx+80h]
          aesenc  xmm0, xmmword ptr [rdx+90h]
          aesenclast xmm0, xmmword ptr [rdx+0A0h]
        }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      break;
    case 192:
      if ( _RDX & 0xF )
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)_RDX);
        _XMM1 = *(_OWORD *)(_RDX + 16);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 32);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 48);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 64);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 80);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 96);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 112);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 128);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 144);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 160);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 176);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 192);
        __asm { aesenclast xmm0, xmm1 }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      else
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)_RDX);
        __asm
        {
          aesenc  xmm0, xmmword ptr [rdx+10h]
          aesenc  xmm0, xmmword ptr [rdx+20h]
          aesenc  xmm0, xmmword ptr [rdx+30h]
          aesenc  xmm0, xmmword ptr [rdx+40h]
          aesenc  xmm0, xmmword ptr [rdx+50h]
          aesenc  xmm0, xmmword ptr [rdx+60h]
          aesenc  xmm0, xmmword ptr [rdx+70h]
          aesenc  xmm0, xmmword ptr [rdx+80h]
          aesenc  xmm0, xmmword ptr [rdx+90h]
          aesenc  xmm0, xmmword ptr [rdx+0A0h]
          aesenc  xmm0, xmmword ptr [rdx+0B0h]
          aesenclast xmm0, xmmword ptr [rdx+0C0h]
        }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      break;
    case 224:
      if ( _RDX & 0xF )
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)_RDX);
        _XMM1 = *(_OWORD *)(_RDX + 16);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 32);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 48);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 64);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 80);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 96);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 112);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 128);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 144);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 160);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 176);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 192);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 208);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 224);
        __asm { aesenclast xmm0, xmm1 }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      else
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)_RDX);
        __asm
        {
          aesenc  xmm0, xmmword ptr [rdx+10h]
          aesenc  xmm0, xmmword ptr [rdx+20h]
          aesenc  xmm0, xmmword ptr [rdx+30h]
          aesenc  xmm0, xmmword ptr [rdx+40h]
          aesenc  xmm0, xmmword ptr [rdx+50h]
          aesenc  xmm0, xmmword ptr [rdx+60h]
          aesenc  xmm0, xmmword ptr [rdx+70h]
          aesenc  xmm0, xmmword ptr [rdx+80h]
          aesenc  xmm0, xmmword ptr [rdx+90h]
          aesenc  xmm0, xmmword ptr [rdx+0A0h]
          aesenc  xmm0, xmmword ptr [rdx+0B0h]
          aesenc  xmm0, xmmword ptr [rdx+0C0h]
          aesenc  xmm0, xmmword ptr [rdx+0D0h]
          aesenclast xmm0, xmmword ptr [rdx+0E0h]
        }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      break;
    default:
      result = 0xFFFFFFFFLL;
      break;
  }
  return result;
}

//----- (0000000000006840) ----------------------------------------------------
signed __int64 __fastcall vng_aes_decrypt_aesni(__m128i *a1, __int64 a2, __int64 _RDX)
{
  __m128i v3; // xmm0@1
  int v4; // eax@1
  signed __int64 result; // rax@4

  v3 = *(__m128i *)&a1->m128i_i64[0];
  v4 = *(_DWORD *)(_RDX + 240);
  switch ( v4 )
  {
    case 160:
      if ( _RDX & 0xF )
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)(_RDX + 160));
        _XMM1 = *(_OWORD *)(_RDX + 144);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 128);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 112);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 96);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 80);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 64);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 48);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 32);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 16);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)_RDX;
        __asm { aesdeclast xmm0, xmm1 }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      else
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)(_RDX + 160));
        __asm
        {
          aesdec  xmm0, xmmword ptr [rdx+90h]
          aesdec  xmm0, xmmword ptr [rdx+80h]
          aesdec  xmm0, xmmword ptr [rdx+70h]
          aesdec  xmm0, xmmword ptr [rdx+60h]
          aesdec  xmm0, xmmword ptr [rdx+50h]
          aesdec  xmm0, xmmword ptr [rdx+40h]
          aesdec  xmm0, xmmword ptr [rdx+30h]
          aesdec  xmm0, xmmword ptr [rdx+20h]
          aesdec  xmm0, xmmword ptr [rdx+10h]
          aesdeclast xmm0, xmmword ptr [rdx]
        }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      break;
    case 192:
      if ( _RDX & 0xF )
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)(_RDX + 192));
        _XMM1 = *(_OWORD *)(_RDX + 176);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 160);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 144);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 128);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 112);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 96);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 80);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 64);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 48);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 32);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 16);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)_RDX;
        __asm { aesdeclast xmm0, xmm1 }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      else
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)(_RDX + 192));
        __asm
        {
          aesdec  xmm0, xmmword ptr [rdx+0B0h]
          aesdec  xmm0, xmmword ptr [rdx+0A0h]
          aesdec  xmm0, xmmword ptr [rdx+90h]
          aesdec  xmm0, xmmword ptr [rdx+80h]
          aesdec  xmm0, xmmword ptr [rdx+70h]
          aesdec  xmm0, xmmword ptr [rdx+60h]
          aesdec  xmm0, xmmword ptr [rdx+50h]
          aesdec  xmm0, xmmword ptr [rdx+40h]
          aesdec  xmm0, xmmword ptr [rdx+30h]
          aesdec  xmm0, xmmword ptr [rdx+20h]
          aesdec  xmm0, xmmword ptr [rdx+10h]
          aesdeclast xmm0, xmmword ptr [rdx]
        }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      break;
    case 224:
      if ( _RDX & 0xF )
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)(_RDX + 224));
        _XMM1 = *(_OWORD *)(_RDX + 208);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 192);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 176);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 160);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 144);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 128);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 112);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 96);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 80);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 64);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 48);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 32);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(_RDX + 16);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)_RDX;
        __asm { aesdeclast xmm0, xmm1 }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      else
      {
        _XMM0 = _mm_xor_si128(v3, *(__m128i *)(_RDX + 224));
        __asm
        {
          aesdec  xmm0, xmmword ptr [rdx+0D0h]
          aesdec  xmm0, xmmword ptr [rdx+0C0h]
          aesdec  xmm0, xmmword ptr [rdx+0B0h]
          aesdec  xmm0, xmmword ptr [rdx+0A0h]
          aesdec  xmm0, xmmword ptr [rdx+90h]
          aesdec  xmm0, xmmword ptr [rdx+80h]
          aesdec  xmm0, xmmword ptr [rdx+70h]
          aesdec  xmm0, xmmword ptr [rdx+60h]
          aesdec  xmm0, xmmword ptr [rdx+50h]
          aesdec  xmm0, xmmword ptr [rdx+40h]
          aesdec  xmm0, xmmword ptr [rdx+30h]
          aesdec  xmm0, xmmword ptr [rdx+20h]
          aesdec  xmm0, xmmword ptr [rdx+10h]
          aesdeclast xmm0, xmmword ptr [rdx]
        }
        result = 0LL;
        *(_OWORD *)a2 = _XMM0;
      }
      break;
    default:
      result = 0xFFFFFFFFLL;
      break;
  }
  return result;
}

//----- (0000000000006BA0) ----------------------------------------------------
signed __int64 __fastcall vng_aes_encrypt_aesni_key(__int64 a1, signed __int64 a2, __int64 a3)
{
  signed __int64 result; // rax@6
  __int64 v7; // rdx@8
  __int64 v8; // rcx@8
  __int64 v10; // rdx@8
  __int64 v11; // rcx@8
  __int64 v13; // rdx@8
  __int64 v14; // rcx@8
  __int64 v16; // rdx@8
  __int64 v17; // rcx@8
  __int64 v19; // rdx@8
  __int64 v20; // rcx@8
  __int64 v22; // rdx@8
  __int64 v23; // rcx@8
  __int64 v25; // rdx@8
  __int64 v26; // rcx@8
  __int64 v28; // rdx@8
  __int64 v29; // rcx@8
  __int64 v31; // rdx@8
  __int64 v32; // rcx@8
  __m128i v33; // xmm1@9
  __int64 v37; // rcx@9
  __int64 v39; // rcx@9
  __int64 v41; // rcx@9
  __int64 v43; // rcx@9
  __int64 v45; // rcx@9
  __int64 v47; // rcx@9
  __int64 v49; // rcx@9
  __m128i v50; // xmm1@10
  __int64 v54; // rcx@10
  __int64 v56; // rcx@10
  __int64 v58; // rcx@10
  __int64 v60; // rcx@10
  __int64 v62; // rcx@10
  __int64 v64; // rcx@10

  if ( a2 <= 32 )
    a2 *= 8LL;
  switch ( a2 )
  {
    case 128LL:
      *(_DWORD *)(a3 + 240) = 160;
      _XMM1 = *(__m128i *)a1;
      *(_OWORD *)a3 = *(_OWORD *)a1;
      __asm { aeskeygenassist xmm2, xmm1, 1 }
      sub_6C90(a3, 0LL, _XMM1, _XMM2);
      __asm { aeskeygenassist xmm2, xmm1, 2 }
      sub_6C90(v7, v8, _XMM1, _XMM2);
      __asm { aeskeygenassist xmm2, xmm1, 4 }
      sub_6C90(v10, v11, _XMM1, _XMM2);
      __asm { aeskeygenassist xmm2, xmm1, 8 }
      sub_6C90(v13, v14, _XMM1, _XMM2);
      __asm { aeskeygenassist xmm2, xmm1, 10h }
      sub_6C90(v16, v17, _XMM1, _XMM2);
      __asm { aeskeygenassist xmm2, xmm1, 20h }
      sub_6C90(v19, v20, _XMM1, _XMM2);
      __asm { aeskeygenassist xmm2, xmm1, 40h }
      sub_6C90(v22, v23, _XMM1, _XMM2);
      __asm { aeskeygenassist xmm2, xmm1, 80h }
      sub_6C90(v25, v26, _XMM1, _XMM2);
      __asm { aeskeygenassist xmm2, xmm1, 1Bh }
      sub_6C90(v28, v29, _XMM1, _XMM2);
      __asm { aeskeygenassist xmm2, xmm1, 36h }
      sub_6C90(v31, v32, _XMM1, _XMM2);
      result = 0LL;
      break;
    case 192LL:
      *(_DWORD *)(a3 + 240) = 192;
      v33 = *(__m128i *)a1;
      _XMM3 = _mm_loadl_epi64((const __m128i *)(a1 + 16));
      *(_OWORD *)a3 = *(_OWORD *)a1;
      _mm_storel_epi64((__m128i *)(a3 + 16), _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 1 }
      sub_6D70(a3 + 24, v33, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 2 }
      sub_6D70(v37, v33, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 4 }
      sub_6D70(v39, v33, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 8 }
      sub_6D70(v41, v33, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 10h }
      sub_6D70(v43, v33, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 20h }
      sub_6D70(v45, v33, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 40h }
      sub_6D70(v47, v33, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 80h }
      sub_6D70(v49, v33, _XMM2, _XMM3);
      result = 0LL;
      break;
    case 256LL:
      *(_DWORD *)(a3 + 240) = 224;
      v50 = *(__m128i *)a1;
      _XMM3 = *(__m128i *)(a1 + 16);
      *(_OWORD *)a3 = *(_OWORD *)a1;
      *(_OWORD *)(a3 + 16) = _XMM3;
      __asm { aeskeygenassist xmm2, xmm3, 1 }
      sub_6E50(a3 + 32, v50, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 2 }
      sub_6E50(v54, v50, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 4 }
      sub_6E50(v56, v50, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 8 }
      sub_6E50(v58, v50, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 10h }
      sub_6E50(v60, v50, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 20h }
      sub_6E50(v62, v50, _XMM2, _XMM3);
      __asm { aeskeygenassist xmm2, xmm3, 40h }
      sub_6EB0(v64, v50, _XMM2);
      result = 0LL;
      break;
    default:
      result = 1LL;
      break;
  }
  return result;
}

//----- (0000000000006C90) ----------------------------------------------------
void __usercall sub_6C90(__int64 a1@<rdx>, __int64 a2@<rcx>, __m128i a3@<xmm1>, __m128i a4@<xmm2>)
{
  __m128i v4; // xmm1@1
  __m128i v5; // xmm1@1

  v4 = _mm_xor_si128(a3, _mm_slli_si128(a3, 4));
  v5 = v4;
  *(_OWORD *)(a1 + a2 + 16) = _mm_xor_si128(_mm_xor_si128(v5, _mm_slli_si128(v5, 4)), _mm_shuffle_epi32(a4, -1));
}

//----- (0000000000006D70) ----------------------------------------------------
void __usercall sub_6D70(__int64 a1@<rcx>, __m128i a2@<xmm1>, __m128i a3@<xmm2>, __m128i a4@<xmm3>)
{
  __m128i v4; // xmm4@1
  __m128i v5; // xmm1@1
  __m128i v6; // xmm4@1
  __m128i v7; // xmm1@1

  v4 = _mm_slli_si128(a2, 4);
  v5 = _mm_xor_si128(a2, v4);
  v6 = _mm_slli_si128(v4, 4);
  v7 = _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v5, v6), v6), _mm_shuffle_epi32(a3, 85));
  *(_OWORD *)a1 = v7;
  _mm_storel_epi64(
    (__m128i *)(a1 + 16),
    _mm_xor_si128(_mm_xor_si128(a4, _mm_slli_si128(a4, 4)), _mm_shuffle_epi32(v7, -1)));
}

//----- (0000000000006E50) ----------------------------------------------------
void __usercall sub_6E50(__int64 a1@<rcx>, __m128i a2@<xmm1>, __m128i a3@<xmm2>, __m128i a4@<xmm3>)
{
  __m128i v4; // xmm4@1
  __m128i v5; // xmm1@1
  __m128i v6; // xmm4@1
  __m128i v9; // xmm2@1
  __m128i v10; // xmm4@1
  __m128i v11; // xmm3@1
  __m128i v12; // xmm4@1

  v4 = _mm_slli_si128(a2, 4);
  v5 = _mm_xor_si128(a2, v4);
  v6 = _mm_slli_si128(v4, 4);
  _XMM1 = _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v5, v6), v6), _mm_shuffle_epi32(a3, -1));
  *(_OWORD *)a1 = _XMM1;
  __asm { aeskeygenassist xmm4, xmm1, 0 }
  v9 = _mm_shuffle_epi32(_XMM4, -86);
  v10 = _mm_slli_si128(a4, 4);
  v11 = _mm_xor_si128(a4, v10);
  v12 = _mm_slli_si128(v10, 4);
  *(_OWORD *)(a1 + 16) = _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v11, v12), v12), v9);
}

//----- (0000000000006EB0) ----------------------------------------------------
void __usercall sub_6EB0(__int64 a1@<rcx>, __m128i a2@<xmm1>, __m128i a3@<xmm2>)
{
  __m128i v3; // xmm4@1
  __m128i v4; // xmm1@1
  __m128i v5; // xmm4@1

  v3 = _mm_slli_si128(a2, 4);
  v4 = _mm_xor_si128(a2, v3);
  v5 = _mm_slli_si128(v3, 4);
  *(_OWORD *)a1 = _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v4, v5), v5), _mm_shuffle_epi32(a3, -1));
}

//----- (0000000000006EE0) ----------------------------------------------------
signed __int64 __fastcall vng_aes_decrypt_aesni_key(__int64 a1, signed __int64 a2, __int64 a3)
{
  signed __int64 result; // rax@1
  __int64 v4; // rdx@1
  signed __int64 v5; // rcx@3
  unsigned __int8 v8; // of@6

  result = vng_aes_encrypt_aesni_key(a1, a2, a3);
  if ( !(_DWORD)result )
  {
    v5 = 9LL;
    if ( a2 != 128 )
    {
      v5 = 11LL;
      if ( a2 != 192 )
        v5 = 13LL;
    }
    do
    {
      v4 += 16LL;
      _XMM0 = *(_OWORD *)v4;
      __asm { aesimc  xmm0, xmm0 }
      *(_OWORD *)v4 = _XMM0;
      v8 = __OFSUB__(v5--, 1LL);
    }
    while ( !((unsigned __int8)((v5 < 0) ^ v8) | (v5 == 0)) );
  }
  return result;
}

//----- (0000000000006F40) ----------------------------------------------------
__int64 __fastcall vng_aes_encrypt_opt_cbc(__m128i *a1, __int64 a2, signed int a3, __int64 a4, __int64 a5)
{
  __m128i *v5; // rbx@1
  __m128i v6; // xmm7@1
  signed int v7; // er13@1
  __int64 v8; // r14@1
  __int64 v9; // r15@1
  unsigned __int8 v10; // of@2

  v5 = a1;
  v6 = *(__m128i *)a2;
  v7 = a3;
  v8 = a4;
  v9 = a5;
  if ( a3 >= 1 )
  {
    do
    {
      *(_OWORD *)a2 = _mm_xor_si128(v6, *v5);
      aes_encrypt_xmm_no_save(a2, a2, v9);
      v6 = *(__m128i *)a2;
      *(_OWORD *)v8 = *(_OWORD *)a2;
      ++v5;
      v8 += 16LL;
      v10 = __OFSUB__(v7--, 1);
    }
    while ( !((unsigned __int8)((v7 < 0) ^ v10) | (v7 == 0)) );
  }
  *(_OWORD *)a2 = v6;
  return 0LL;
}

//----- (0000000000006FF0) ----------------------------------------------------
void __fastcall vng_aes_decrypt_opt_cbc(__int64 a1, __m128i *a2, int a3, __int64 a4, __int64 a5)
{
  __int64 v5; // rbx@1
  __m128i v6; // xmm7@1
  int v7; // er13@1
  __int64 v8; // r14@1
  __int64 v9; // r15@1
  __m128i v10; // xmm6@2
  __m128i v11; // xmm0@2
  unsigned __int8 v12; // of@2

  v5 = a1;
  v6 = *(__m128i *)&a2->m128i_i64[0];
  v7 = a3;
  v8 = a4;
  v9 = a5;
  JUMPOUT(a3, 1, &loc_6FB3);
  do
  {
    v10 = *(__m128i *)v5;
    aes_decrypt_xmm_no_save(v5, v8, v9);
    v11 = _mm_xor_si128(*(__m128i *)v8, v6);
    v6 = v10;
    *(_OWORD *)v8 = v11;
    v5 += 16LL;
    v8 += 16LL;
    v12 = __OFSUB__(v7--, 1);
  }
  while ( !((unsigned __int8)((v7 < 0) ^ v12) | (v7 == 0)) );
  JUMPOUT(&loc_6FB3);
}

//----- (0000000000007070) ----------------------------------------------------
signed __int64 __fastcall vng_aes_encrypt_cbc_hw(__m128i *a1, __m128i *a2, signed int a3, __int64 a4, __int64 a5)
{
  __m128i *v5; // rbx@1
  signed int v7; // er13@1
  __int64 v8; // r14@1
  int v9; // eax@1
  __m128i v11; // xmm2@6
  unsigned __int8 v32; // of@7
  __m128i v33; // xmm2@11
  __m128i v58; // xmm2@15

  v5 = a1;
  _XMM0 = *(__m128i *)&a2->m128i_i64[0];
  v7 = a3;
  v8 = a4;
  v9 = *(_DWORD *)(a5 + 240);
  if ( v9 == 160 )
  {
    if ( a3 >= 1 )
    {
      v11 = *(__m128i *)a5;
      _XMM3 = *(_OWORD *)(a5 + 16);
      _XMM4 = *(_OWORD *)(a5 + 32);
      _XMM5 = *(_OWORD *)(a5 + 48);
      _XMM6 = *(_OWORD *)(a5 + 64);
      _XMM7 = *(_OWORD *)(a5 + 80);
      _XMM8 = *(_OWORD *)(a5 + 96);
      _XMM9 = *(_OWORD *)(a5 + 112);
      _XMM10 = *(_OWORD *)(a5 + 128);
      _XMM11 = *(_OWORD *)(a5 + 144);
      _XMM12 = *(_OWORD *)(a5 + 160);
      do
      {
        _XMM0 = _mm_xor_si128(_mm_xor_si128(_XMM0, v11), *v5);
        __asm
        {
          aesenc  xmm0, xmm3
          aesenc  xmm0, xmm4
          aesenc  xmm0, xmm5
          aesenc  xmm0, xmm6
          aesenc  xmm0, xmm7
          aesenc  xmm0, xmm8
          aesenc  xmm0, xmm9
          aesenc  xmm0, xmm10
          aesenc  xmm0, xmm11
          aesenclast xmm0, xmm12
        }
        *(_OWORD *)v8 = _XMM0;
        v8 += 16LL;
        ++v5;
        v32 = __OFSUB__(v7--, 1);
      }
      while ( !((unsigned __int8)((v7 < 0) ^ v32) | (v7 == 0)) );
    }
  }
  else if ( v9 == 192 )
  {
    if ( a3 >= 1 )
    {
      v33 = *(__m128i *)a5;
      _XMM3 = *(_OWORD *)(a5 + 16);
      _XMM4 = *(_OWORD *)(a5 + 32);
      _XMM5 = *(_OWORD *)(a5 + 48);
      _XMM6 = *(_OWORD *)(a5 + 64);
      _XMM7 = *(_OWORD *)(a5 + 80);
      _XMM8 = *(_OWORD *)(a5 + 96);
      _XMM9 = *(_OWORD *)(a5 + 112);
      _XMM10 = *(_OWORD *)(a5 + 128);
      _XMM11 = *(_OWORD *)(a5 + 144);
      _XMM12 = *(_OWORD *)(a5 + 160);
      _XMM13 = *(_OWORD *)(a5 + 176);
      _XMM14 = *(_OWORD *)(a5 + 192);
      do
      {
        _XMM0 = _mm_xor_si128(_mm_xor_si128(_XMM0, *v5), v33);
        __asm
        {
          aesenc  xmm0, xmm3
          aesenc  xmm0, xmm4
          aesenc  xmm0, xmm5
          aesenc  xmm0, xmm6
          aesenc  xmm0, xmm7
          aesenc  xmm0, xmm8
          aesenc  xmm0, xmm9
          aesenc  xmm0, xmm10
          aesenc  xmm0, xmm11
          aesenc  xmm0, xmm12
          aesenc  xmm0, xmm13
          aesenclast xmm0, xmm14
        }
        *(_OWORD *)v8 = _XMM0;
        ++v5;
        v8 += 16LL;
        v32 = __OFSUB__(v7--, 1);
      }
      while ( !((unsigned __int8)((v7 < 0) ^ v32) | (v7 == 0)) );
    }
  }
  else
  {
    if ( v9 != 224 )
      return 0xFFFFFFFFLL;
    if ( a3 >= 1 )
    {
      v58 = *(__m128i *)a5;
      _XMM3 = *(_OWORD *)(a5 + 16);
      _XMM4 = *(_OWORD *)(a5 + 32);
      _XMM5 = *(_OWORD *)(a5 + 48);
      _XMM6 = *(_OWORD *)(a5 + 64);
      _XMM7 = *(_OWORD *)(a5 + 80);
      _XMM8 = *(_OWORD *)(a5 + 96);
      _XMM9 = *(_OWORD *)(a5 + 112);
      _XMM10 = *(_OWORD *)(a5 + 128);
      _XMM11 = *(_OWORD *)(a5 + 144);
      _XMM12 = *(_OWORD *)(a5 + 160);
      _XMM13 = *(_OWORD *)(a5 + 176);
      _XMM14 = *(_OWORD *)(a5 + 192);
      _XMM15 = *(_OWORD *)(a5 + 208);
      do
      {
        _XMM0 = _mm_xor_si128(_mm_xor_si128(_XMM0, *v5), v58);
        __asm
        {
          aesenc  xmm0, xmm3
          aesenc  xmm0, xmm4
          aesenc  xmm0, xmm5
          aesenc  xmm0, xmm6
          aesenc  xmm0, xmm7
        }
        _XMM1 = *(_OWORD *)(a5 + 224);
        __asm
        {
          aesenc  xmm0, xmm8
          aesenc  xmm0, xmm9
          aesenc  xmm0, xmm10
          aesenc  xmm0, xmm11
          aesenc  xmm0, xmm12
          aesenc  xmm0, xmm13
          aesenc  xmm0, xmm14
          aesenc  xmm0, xmm15
          aesenclast xmm0, xmm1
        }
        *(_OWORD *)v8 = _XMM0;
        ++v5;
        v8 += 16LL;
        v32 = __OFSUB__(v7--, 1);
      }
      while ( !((unsigned __int8)((v7 < 0) ^ v32) | (v7 == 0)) );
    }
  }
  *(_OWORD *)&a2->m128i_i64[0] = _XMM0;
  return 0LL;
}

//----- (00000000000073F0) ----------------------------------------------------
void __fastcall vng_aes_decrypt_cbc_hw(__int64 a1, __m128i *a2, int a3, __int64 a4, __int64 a5)
{
  __int64 v5; // rbx@1
  __m128i v6; // xmm0@1
  __int64 v7; // r14@1
  int v8; // eax@1
  __m128i v9; // xmm3@5
  int v20; // er13@5
  __m128i v65; // xmm1@6
  __m128i v66; // xmm2@6
  __m128i v67; // xmm14@6
  __m128i v68; // xmm15@6
  unsigned __int8 v69; // of@6
  int v70; // er13@7
  __m128i v93; // xmm1@8
  __m128i v94; // xmm2@8
  __m128i v106; // xmm2@10
  __m128i v107; // xmm0@10
  __m128i v108; // xmm3@11
  __int128 v119; // xmm14@11
  __int128 v120; // xmm15@11
  int v121; // er13@11
  __m128i v176; // xmm1@12
  __m128i v177; // xmm2@12
  __m128i v178; // xmm14@12
  __m128i v179; // xmm15@12
  int v180; // er13@13
  __m128i v196; // xmm2@14
  __m128i v197; // xmm3@16
  __int128 v208; // xmm14@16
  __int128 v209; // xmm15@16
  int v210; // er13@16
  __m128i v275; // xmm1@17
  __m128i v276; // xmm2@17
  __m128i v277; // xmm14@17
  __m128i v278; // xmm15@17
  int v279; // er13@18
  __m128i v299; // xmm2@19

  v5 = a1;
  v6 = *(__m128i *)&a2->m128i_i64[0];
  v7 = a4;
  v8 = *(_DWORD *)(a5 + 240);
  if ( v8 != 160 )
  {
    if ( v8 != 192 )
    {
      if ( v8 != 224 )
        JUMPOUT(&loc_71D0);
      JUMPOUT(a3, 1, &loc_71CB);
      v197 = *(__m128i *)(a5 + 224);
      _XMM4 = *(_OWORD *)(a5 + 208);
      _XMM5 = *(_OWORD *)(a5 + 192);
      _XMM6 = *(_OWORD *)(a5 + 176);
      _XMM7 = *(_OWORD *)(a5 + 160);
      _XMM8 = *(_OWORD *)(a5 + 144);
      _XMM9 = *(_OWORD *)(a5 + 128);
      _XMM10 = *(_OWORD *)(a5 + 112);
      _XMM11 = *(_OWORD *)(a5 + 96);
      _XMM12 = *(_OWORD *)(a5 + 80);
      _XMM13 = *(_OWORD *)(a5 + 64);
      v208 = *(_OWORD *)(a5 + 48);
      v209 = *(_OWORD *)(a5 + 32);
      v210 = a3 - 4;
      if ( a3 >= 4 )
      {
        do
        {
          _XMM1 = _mm_xor_si128(*(__m128i *)v5, v197);
          _XMM2 = _mm_xor_si128(*(__m128i *)(v5 + 16), v197);
          _XMM14 = _mm_xor_si128(*(__m128i *)(v5 + 32), v197);
          _XMM15 = _mm_xor_si128(*(__m128i *)(v5 + 48), v197);
          __asm
          {
            aesdec  xmm1, xmm4
            aesdec  xmm2, xmm4
            aesdec  xmm14, xmm4
            aesdec  xmm15, xmm4
            aesdec  xmm1, xmm5
            aesdec  xmm2, xmm5
            aesdec  xmm14, xmm5
            aesdec  xmm15, xmm5
            aesdec  xmm1, xmm6
            aesdec  xmm2, xmm6
            aesdec  xmm14, xmm6
            aesdec  xmm15, xmm6
            aesdec  xmm1, xmm7
            aesdec  xmm2, xmm7
            aesdec  xmm14, xmm7
            aesdec  xmm15, xmm7
            aesdec  xmm1, xmm8
            aesdec  xmm2, xmm8
            aesdec  xmm14, xmm8
            aesdec  xmm15, xmm8
            aesdec  xmm1, xmm9
            aesdec  xmm2, xmm9
            aesdec  xmm14, xmm9
            aesdec  xmm15, xmm9
            aesdec  xmm1, xmm10
            aesdec  xmm2, xmm10
            aesdec  xmm14, xmm10
            aesdec  xmm15, xmm10
            aesdec  xmm1, xmm11
            aesdec  xmm2, xmm11
            aesdec  xmm14, xmm11
            aesdec  xmm15, xmm11
            aesdec  xmm1, xmm12
            aesdec  xmm2, xmm12
            aesdec  xmm14, xmm12
            aesdec  xmm15, xmm12
          }
          _XMM12 = *(_OWORD *)(a5 + 48);
          __asm
          {
            aesdec  xmm1, xmm13
            aesdec  xmm2, xmm13
            aesdec  xmm14, xmm13
            aesdec  xmm15, xmm13
          }
          _XMM13 = *(_OWORD *)(a5 + 32);
          __asm
          {
            aesdec  xmm1, xmm12
            aesdec  xmm2, xmm12
            aesdec  xmm14, xmm12
            aesdec  xmm15, xmm12
          }
          _XMM12 = *(_OWORD *)(a5 + 16);
          __asm
          {
            aesdec  xmm1, xmm13
            aesdec  xmm2, xmm13
            aesdec  xmm14, xmm13
            aesdec  xmm15, xmm13
          }
          _XMM13 = *(_OWORD *)a5;
          __asm
          {
            aesdec  xmm1, xmm12
            aesdec  xmm2, xmm12
            aesdec  xmm14, xmm12
            aesdec  xmm15, xmm12
          }
          _XMM12 = *(_OWORD *)(a5 + 80);
          __asm
          {
            aesdeclast xmm1, xmm13
            aesdeclast xmm2, xmm13
            aesdeclast xmm14, xmm13
            aesdeclast xmm15, xmm13
          }
          _XMM13 = *(_OWORD *)(a5 + 64);
          v275 = _mm_xor_si128(_XMM1, v6);
          v276 = _mm_xor_si128(_XMM2, *(__m128i *)v5);
          v277 = _mm_xor_si128(_XMM14, *(__m128i *)(v5 + 16));
          v278 = _mm_xor_si128(_XMM15, *(__m128i *)(v5 + 32));
          v6 = *(__m128i *)(v5 + 48);
          *(_OWORD *)v7 = v275;
          *(_OWORD *)(v7 + 16) = v276;
          *(_OWORD *)(v7 + 32) = v277;
          *(_OWORD *)(v7 + 48) = v278;
          v5 += 64LL;
          v7 += 64LL;
          v69 = __OFSUB__(v210, 4);
          v210 -= 4;
        }
        while ( !((v210 < 0) ^ v69) );
      }
      v279 = v210 + 4;
      JUMPOUT(v279, 0, &loc_71CB);
      _XMM14 = *(_OWORD *)(a5 + 48);
      _XMM15 = *(_OWORD *)(a5 + 32);
      do
      {
        _XMM2 = _mm_xor_si128(*(__m128i *)v5, v197);
        __asm
        {
          aesdec  xmm2, xmm4
          aesdec  xmm2, xmm5
          aesdec  xmm2, xmm6
          aesdec  xmm2, xmm7
          aesdec  xmm2, xmm8
          aesdec  xmm2, xmm9
          aesdec  xmm2, xmm10
          aesdec  xmm2, xmm11
          aesdec  xmm2, xmm12
          aesdec  xmm2, xmm13
          aesdec  xmm2, xmm14
          aesdec  xmm2, xmm15
        }
        _XMM1 = *(_OWORD *)(a5 + 16);
        __asm { aesdec  xmm2, xmm1 }
        _XMM1 = *(_OWORD *)a5;
        __asm { aesdeclast xmm2, xmm1 }
        v299 = _mm_xor_si128(_XMM2, v6);
        v6 = *(__m128i *)v5;
        *(_OWORD *)v7 = v299;
        v5 += 16LL;
        v7 += 16LL;
        v69 = __OFSUB__(v279--, 1);
      }
      while ( !((unsigned __int8)((v279 < 0) ^ v69) | (v279 == 0)) );
      JUMPOUT(&loc_71CB);
    }
    JUMPOUT(a3, 1, &loc_71CB);
    v108 = *(__m128i *)(a5 + 192);
    _XMM4 = *(_OWORD *)(a5 + 176);
    _XMM5 = *(_OWORD *)(a5 + 160);
    _XMM6 = *(_OWORD *)(a5 + 144);
    _XMM7 = *(_OWORD *)(a5 + 128);
    _XMM8 = *(_OWORD *)(a5 + 112);
    _XMM9 = *(_OWORD *)(a5 + 96);
    _XMM10 = *(_OWORD *)(a5 + 80);
    _XMM11 = *(_OWORD *)(a5 + 64);
    _XMM12 = *(_OWORD *)(a5 + 48);
    _XMM13 = *(_OWORD *)(a5 + 32);
    v119 = *(_OWORD *)(a5 + 16);
    v120 = *(_OWORD *)a5;
    v121 = a3 - 4;
    if ( a3 >= 4 )
    {
      do
      {
        _XMM1 = _mm_xor_si128(*(__m128i *)v5, v108);
        _XMM2 = _mm_xor_si128(*(__m128i *)(v5 + 16), v108);
        _XMM14 = _mm_xor_si128(*(__m128i *)(v5 + 32), v108);
        _XMM15 = _mm_xor_si128(*(__m128i *)(v5 + 48), v108);
        __asm
        {
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm14, xmm4
          aesdec  xmm15, xmm4
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm14, xmm5
          aesdec  xmm15, xmm5
          aesdec  xmm1, xmm6
          aesdec  xmm2, xmm6
          aesdec  xmm14, xmm6
          aesdec  xmm15, xmm6
          aesdec  xmm1, xmm7
          aesdec  xmm2, xmm7
          aesdec  xmm14, xmm7
          aesdec  xmm15, xmm7
          aesdec  xmm1, xmm8
          aesdec  xmm2, xmm8
          aesdec  xmm14, xmm8
          aesdec  xmm15, xmm8
          aesdec  xmm1, xmm9
          aesdec  xmm2, xmm9
          aesdec  xmm14, xmm9
          aesdec  xmm15, xmm9
          aesdec  xmm1, xmm10
          aesdec  xmm2, xmm10
          aesdec  xmm14, xmm10
          aesdec  xmm15, xmm10
          aesdec  xmm1, xmm11
          aesdec  xmm2, xmm11
          aesdec  xmm14, xmm11
          aesdec  xmm15, xmm11
          aesdec  xmm1, xmm12
          aesdec  xmm2, xmm12
          aesdec  xmm14, xmm12
          aesdec  xmm15, xmm12
        }
        _XMM12 = *(_OWORD *)(a5 + 16);
        __asm
        {
          aesdec  xmm1, xmm13
          aesdec  xmm2, xmm13
          aesdec  xmm14, xmm13
          aesdec  xmm15, xmm13
        }
        _XMM13 = *(_OWORD *)a5;
        __asm
        {
          aesdec  xmm1, xmm12
          aesdec  xmm2, xmm12
          aesdec  xmm14, xmm12
          aesdec  xmm15, xmm12
        }
        _XMM12 = *(_OWORD *)(a5 + 48);
        __asm
        {
          aesdeclast xmm1, xmm13
          aesdeclast xmm2, xmm13
          aesdeclast xmm14, xmm13
          aesdeclast xmm15, xmm13
        }
        _XMM13 = *(_OWORD *)(a5 + 32);
        v176 = _mm_xor_si128(_XMM1, v6);
        v177 = _mm_xor_si128(_XMM2, *(__m128i *)v5);
        v178 = _mm_xor_si128(_XMM14, *(__m128i *)(v5 + 16));
        v179 = _mm_xor_si128(_XMM15, *(__m128i *)(v5 + 32));
        v6 = *(__m128i *)(v5 + 48);
        *(_OWORD *)v7 = v176;
        *(_OWORD *)(v7 + 16) = v177;
        *(_OWORD *)(v7 + 32) = v178;
        *(_OWORD *)(v7 + 48) = v179;
        v5 += 64LL;
        v7 += 64LL;
        v69 = __OFSUB__(v121, 4);
        v121 -= 4;
      }
      while ( !((v121 < 0) ^ v69) );
    }
    v180 = v121 + 4;
    JUMPOUT(v180, 0, &loc_71CB);
    _XMM14 = *(_OWORD *)(a5 + 16);
    _XMM15 = *(_OWORD *)a5;
    do
    {
      _XMM2 = _mm_xor_si128(*(__m128i *)v5, v108);
      __asm
      {
        aesdec  xmm2, xmm4
        aesdec  xmm2, xmm5
        aesdec  xmm2, xmm6
        aesdec  xmm2, xmm7
        aesdec  xmm2, xmm8
        aesdec  xmm2, xmm9
        aesdec  xmm2, xmm10
        aesdec  xmm2, xmm11
        aesdec  xmm2, xmm12
        aesdec  xmm2, xmm13
        aesdec  xmm2, xmm14
        aesdeclast xmm2, xmm15
      }
      v196 = _mm_xor_si128(_XMM2, v6);
      v6 = *(__m128i *)v5;
      *(_OWORD *)v7 = v196;
      v5 += 16LL;
      v7 += 16LL;
      v69 = __OFSUB__(v180--, 1);
    }
    while ( !((unsigned __int8)((v180 < 0) ^ v69) | (v180 == 0)) );
    JUMPOUT(&loc_71CB);
  }
  JUMPOUT(a3, 1, &loc_71CB);
  v9 = *(__m128i *)(a5 + 160);
  _XMM4 = *(_OWORD *)(a5 + 144);
  _XMM5 = *(_OWORD *)(a5 + 128);
  _XMM6 = *(_OWORD *)(a5 + 112);
  _XMM7 = *(_OWORD *)(a5 + 96);
  _XMM8 = *(_OWORD *)(a5 + 80);
  _XMM9 = *(_OWORD *)(a5 + 64);
  _XMM10 = *(_OWORD *)(a5 + 48);
  _XMM11 = *(_OWORD *)(a5 + 32);
  _XMM12 = *(_OWORD *)(a5 + 16);
  _XMM13 = *(_OWORD *)a5;
  v20 = a3 - 4;
  if ( a3 >= 4 )
  {
    do
    {
      _XMM1 = _mm_xor_si128(*(__m128i *)v5, v9);
      _XMM2 = _mm_xor_si128(*(__m128i *)(v5 + 16), v9);
      _XMM14 = _mm_xor_si128(*(__m128i *)(v5 + 32), v9);
      _XMM15 = _mm_xor_si128(*(__m128i *)(v5 + 48), v9);
      __asm
      {
        aesdec  xmm1, xmm4
        aesdec  xmm2, xmm4
        aesdec  xmm14, xmm4
        aesdec  xmm15, xmm4
        aesdec  xmm1, xmm5
        aesdec  xmm2, xmm5
        aesdec  xmm14, xmm5
        aesdec  xmm15, xmm5
        aesdec  xmm1, xmm6
        aesdec  xmm2, xmm6
        aesdec  xmm14, xmm6
        aesdec  xmm15, xmm6
        aesdec  xmm1, xmm7
        aesdec  xmm2, xmm7
        aesdec  xmm14, xmm7
        aesdec  xmm15, xmm7
        aesdec  xmm1, xmm8
        aesdec  xmm2, xmm8
        aesdec  xmm14, xmm8
        aesdec  xmm15, xmm8
        aesdec  xmm1, xmm9
        aesdec  xmm2, xmm9
        aesdec  xmm14, xmm9
        aesdec  xmm15, xmm9
        aesdec  xmm1, xmm10
        aesdec  xmm2, xmm10
        aesdec  xmm14, xmm10
        aesdec  xmm15, xmm10
        aesdec  xmm1, xmm11
        aesdec  xmm2, xmm11
        aesdec  xmm14, xmm11
        aesdec  xmm15, xmm11
        aesdec  xmm1, xmm12
        aesdec  xmm2, xmm12
        aesdec  xmm14, xmm12
        aesdec  xmm15, xmm12
        aesdeclast xmm1, xmm13
        aesdeclast xmm2, xmm13
        aesdeclast xmm14, xmm13
        aesdeclast xmm15, xmm13
      }
      v65 = _mm_xor_si128(_XMM1, v6);
      v66 = _mm_xor_si128(_XMM2, *(__m128i *)v5);
      v67 = _mm_xor_si128(_XMM14, *(__m128i *)(v5 + 16));
      v68 = _mm_xor_si128(_XMM15, *(__m128i *)(v5 + 32));
      v6 = *(__m128i *)(v5 + 48);
      *(_OWORD *)v7 = v65;
      *(_OWORD *)(v7 + 16) = v66;
      *(_OWORD *)(v7 + 32) = v67;
      *(_OWORD *)(v7 + 48) = v68;
      v5 += 64LL;
      v7 += 64LL;
      v69 = __OFSUB__(v20, 4);
      v20 -= 4;
    }
    while ( !((v20 < 0) ^ v69) );
  }
  v70 = v20 + 4;
  JUMPOUT(v70, 0, &loc_71CB);
  if ( v70 & 2 )
  {
    _XMM1 = _mm_xor_si128(*(__m128i *)v5, v9);
    _XMM2 = _mm_xor_si128(*(__m128i *)(v5 + 16), v9);
    __asm
    {
      aesdec  xmm1, xmm4
      aesdec  xmm2, xmm4
      aesdec  xmm1, xmm5
      aesdec  xmm2, xmm5
      aesdec  xmm1, xmm6
      aesdec  xmm2, xmm6
      aesdec  xmm1, xmm7
      aesdec  xmm2, xmm7
      aesdec  xmm1, xmm8
      aesdec  xmm2, xmm8
      aesdec  xmm1, xmm9
      aesdec  xmm2, xmm9
      aesdec  xmm1, xmm10
      aesdec  xmm2, xmm10
      aesdec  xmm1, xmm11
      aesdec  xmm2, xmm11
      aesdec  xmm1, xmm12
      aesdec  xmm2, xmm12
      aesdeclast xmm1, xmm13
      aesdeclast xmm2, xmm13
    }
    v93 = _mm_xor_si128(_XMM1, v6);
    v94 = _mm_xor_si128(_XMM2, *(__m128i *)v5);
    v6 = *(__m128i *)(v5 + 16);
    *(_OWORD *)v7 = v93;
    *(_OWORD *)(v7 + 16) = v94;
    v5 += 32LL;
    v7 += 32LL;
  }
  JUMPOUT(v70 & 1, 0, &loc_71CB);
  _XMM2 = _mm_xor_si128(*(__m128i *)v5, v9);
  __asm
  {
    aesdec  xmm2, xmm4
    aesdec  xmm2, xmm5
    aesdec  xmm2, xmm6
    aesdec  xmm2, xmm7
    aesdec  xmm2, xmm8
    aesdec  xmm2, xmm9
    aesdec  xmm2, xmm10
    aesdec  xmm2, xmm11
    aesdec  xmm2, xmm12
    aesdeclast xmm2, xmm13
  }
  v106 = _mm_xor_si128(_XMM2, v6);
  v107 = *(__m128i *)v5;
  *(_OWORD *)v7 = v106;
  JUMPOUT(&loc_71CB);
}

//----- (0000000000007C9F) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_encrypt(signed __int64 a1, __int64 a2, signed __int64 a3, __int64 a4, signed __int64 a5)
{
  int v5; // er12@1
  int v6; // er13@1
  int v7; // ebx@1
  __int64 result; // rax@1
  signed __int64 v9; // rcx@2
  int v10; // eax@4
  int v11; // er12@4
  int v12; // er13@4
  int v13; // ebx@4
  int v14; // eax@6
  int v15; // edx@6
  signed __int64 v16; // r8@7
  int v17; // er14@9
  int v18; // esi@9
  int v19; // er9@9
  int v20; // er10@9
  int v21; // er10@10
  int v22; // esi@10
  int v23; // er11@10
  int v24; // eax@10
  signed __int64 v25; // r14@10
  int v26; // er10@12
  int v27; // er11@12
  int v28; // edi@12
  int v29; // esi@12
  int v30; // er12@12
  int v31; // er9@12
  int v32; // eax@12
  int v33; // ebx@12
  int v34; // er10@12
  int v35; // er8@12
  int v36; // edi@12
  int v37; // ebx@12
  int v38; // er12@12
  int v39; // er11@12
  int v40; // esi@12
  int v41; // ebx@12
  int v42; // er10@12
  int v43; // er8@12
  int v44; // edi@12
  int v45; // ebx@12
  int v46; // er12@12
  int v47; // er11@12
  int v48; // esi@12
  int v49; // edx@12
  int v50; // er13@12
  int v51; // er9@12
  int v52; // edi@12
  int v53; // ebx@12
  int v54; // er10@12
  int v55; // er12@12
  int v56; // er8@12
  int v57; // ebx@12
  int v58; // er11@12
  int v59; // esi@12
  int v60; // er9@12
  int v61; // edi@12
  __int64 v62; // [sp+8h] [bp-58h]@1
  signed __int64 v63; // [sp+10h] [bp-50h]@2
  signed __int64 v64; // [sp+18h] [bp-48h]@1
  int v65; // [sp+24h] [bp-3Ch]@1
  signed __int64 v66; // [sp+28h] [bp-38h]@4
  signed __int64 v67; // [sp+30h] [bp-30h]@7

  v62 = a2;
  v64 = a1;
  v65 = *(_DWORD *)(a1 + 260);
  v5 = *(_DWORD *)a2;
  v6 = *(_DWORD *)(a2 + 4);
  v7 = *(_DWORD *)(a2 + 8);
  result = *(_DWORD *)(a2 + 12);
  if ( a3 )
  {
    v63 = a1 + 32;
    v9 = a4 + 12;
    do
    {
      if ( v65 )
      {
        v66 = a3;
        v10 = *(_DWORD *)(a1 + 12) ^ result;
        v11 = *(_DWORD *)(v9 - 12) ^ *(_DWORD *)a1 ^ v5;
        v12 = *(_DWORD *)(v9 - 8) ^ *(_DWORD *)(a1 + 4) ^ v6;
        v13 = *(_DWORD *)(v9 - 4) ^ *(_DWORD *)(a1 + 8) ^ v7;
      }
      else
      {
        v66 = a3;
        v11 = *(_DWORD *)(v9 - 12) ^ *(_DWORD *)a1;
        v12 = *(_DWORD *)(v9 - 8) ^ *(_DWORD *)(a1 + 4);
        v13 = *(_DWORD *)(v9 - 4) ^ *(_DWORD *)(a1 + 8);
        v10 = *(_DWORD *)(a1 + 12);
      }
      v14 = *(_DWORD *)v9 ^ v10;
      v15 = *(_DWORD *)(a1 + 240);
      if ( v15 == 12 )
      {
        v67 = a5;
        v16 = a1;
      }
      else
      {
        v67 = a5;
        if ( v15 != 14 )
        {
          v25 = a1;
          goto LABEL_12;
        }
        v17 = *((_DWORD *)&t_fn[384] + ((unsigned int)v14 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                    + (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v12)) ^ *(_DWORD *)(a1 + 16) ^ *((_DWORD *)t_fn + (unsigned __int8)v11);
        v18 = *((_DWORD *)&t_fn[384] + ((unsigned int)v11 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                    + (unsigned __int8)((unsigned int)v14 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v13)) ^ *(_DWORD *)(a1 + 20) ^ *((_DWORD *)t_fn + (unsigned __int8)v12);
        v19 = *((_DWORD *)&t_fn[384] + ((unsigned int)v12 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                    + (unsigned __int8)((unsigned int)v11 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v14)) ^ *(_DWORD *)(a1 + 24) ^ *((_DWORD *)t_fn + (unsigned __int8)v13);
        v20 = *((_DWORD *)&t_fn[384] + ((unsigned int)v13 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                    + (unsigned __int8)((unsigned int)v12 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v11)) ^ *(_DWORD *)(a1 + 28) ^ *((_DWORD *)t_fn + (unsigned __int8)v14);
        v11 = *((_DWORD *)&t_fn[384] + ((unsigned int)v20 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                    + (unsigned __int8)((unsigned int)v19 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v18)) ^ *(_DWORD *)(a1 + 32) ^ *((_DWORD *)t_fn + (unsigned __int8)(*((_BYTE *)&t_fn[384] + 4 * ((unsigned int)v14 >> 24)) ^ *((_BYTE *)&t_fn[256] + 4 * (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_BYTE *)&t_fn[128] + 4 * BYTE1(v12)) ^ *(_BYTE *)(a1 + 16) ^ *((_BYTE *)t_fn + 4 * (unsigned __int8)v11)));
        v12 = *((_DWORD *)&t_fn[384] + ((unsigned int)v17 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                    + (unsigned __int8)((unsigned int)v20 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v19)) ^ *(_DWORD *)(a1 + 36) ^ *((_DWORD *)t_fn + (unsigned __int8)v18);
        v13 = *((_DWORD *)&t_fn[384] + ((unsigned int)v18 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                    + (unsigned __int8)((unsigned int)v17 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v20)) ^ *(_DWORD *)(a1 + 40) ^ *((_DWORD *)t_fn + (unsigned __int8)v19);
        v14 = *((_DWORD *)&t_fn[384] + ((unsigned int)v19 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                    + (unsigned __int8)((unsigned int)v18 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v17)) ^ *(_DWORD *)(a1 + 44) ^ *((_DWORD *)t_fn + (unsigned __int8)v20);
        v16 = v63;
      }
      v21 = *((_DWORD *)&t_fn[384] + ((unsigned int)v14 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v12)) ^ *(_DWORD *)(v16 + 16) ^ *((_DWORD *)t_fn + (unsigned __int8)v11);
      v22 = *((_DWORD *)&t_fn[384] + ((unsigned int)v11 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v14 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v13)) ^ *(_DWORD *)(v16 + 20) ^ *((_DWORD *)t_fn + (unsigned __int8)v12);
      v23 = *((_DWORD *)&t_fn[384] + ((unsigned int)v12 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v11 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v14)) ^ *(_DWORD *)(v16 + 24) ^ *((_DWORD *)t_fn + (unsigned __int8)v13);
      v24 = *((_DWORD *)&t_fn[384] + ((unsigned int)v13 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v12 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v11)) ^ *(_DWORD *)(v16 + 28) ^ *((_DWORD *)t_fn + (unsigned __int8)v14);
      v25 = v16 + 32;
      v11 = *((_DWORD *)&t_fn[384] + ((unsigned int)v24 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v23 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v22)) ^ *(_DWORD *)(v16 + 32) ^ *((_DWORD *)t_fn + (unsigned __int8)v21);
      v12 = *((_DWORD *)&t_fn[384] + ((unsigned int)v21 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v24 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v23)) ^ *(_DWORD *)(v16 + 36) ^ *((_DWORD *)t_fn + (unsigned __int8)v22);
      v13 = *((_DWORD *)&t_fn[384] + ((unsigned int)v22 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v21 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v24)) ^ *(_DWORD *)(v16 + 40) ^ *((_DWORD *)t_fn + (unsigned __int8)v23);
      v14 = *((_DWORD *)&t_fn[384] + ((unsigned int)v23 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v22 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v21)) ^ *(_DWORD *)(v16 + 44) ^ *((_DWORD *)t_fn + (unsigned __int8)v24);
LABEL_12:
      v26 = *((_DWORD *)&t_fn[384] + ((unsigned int)v14 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v12)) ^ *(_DWORD *)(v25 + 16) ^ *((_DWORD *)t_fn + (unsigned __int8)v11);
      v27 = *((_DWORD *)&t_fn[384] + ((unsigned int)v11 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v14 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v13)) ^ *(_DWORD *)(v25 + 20) ^ *((_DWORD *)t_fn + (unsigned __int8)v12);
      v28 = *((_DWORD *)&t_fn[384] + ((unsigned int)v12 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v11 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v14)) ^ *(_DWORD *)(v25 + 24) ^ *((_DWORD *)t_fn + (unsigned __int8)v13);
      v29 = *((_DWORD *)&t_fn[384] + ((unsigned int)v13 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v12 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v11)) ^ *(_DWORD *)(v25 + 28) ^ *((_DWORD *)t_fn + (unsigned __int8)v14);
      v30 = *((_DWORD *)&t_fn[384] + ((unsigned int)v29 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v28 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v27)) ^ *(_DWORD *)(v25 + 32) ^ *((_DWORD *)t_fn + (unsigned __int8)(*((_BYTE *)&t_fn[384] + 4 * ((unsigned int)v14 >> 24)) ^ *((_BYTE *)&t_fn[256] + 4 * (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_BYTE *)&t_fn[128] + 4 * BYTE1(v12)) ^ *(_BYTE *)(v25 + 16) ^ *((_BYTE *)t_fn + 4 * (unsigned __int8)v11)));
      v31 = *((_DWORD *)&t_fn[384] + ((unsigned int)v26 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v29 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v28)) ^ *(_DWORD *)(v25 + 36) ^ *((_DWORD *)t_fn + (unsigned __int8)v27);
      v32 = *((_DWORD *)&t_fn[384] + ((unsigned int)v27 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v26 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v29)) ^ *(_DWORD *)(v25 + 40) ^ *((_DWORD *)t_fn + (unsigned __int8)v28);
      v33 = *((_DWORD *)&t_fn[384] + ((unsigned int)v28 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v27 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v26)) ^ *(_DWORD *)(v25 + 44) ^ *((_DWORD *)t_fn + (unsigned __int8)v29);
      v34 = *((_DWORD *)&t_fn[384] + ((unsigned int)v33 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v32 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v31)) ^ *(_DWORD *)(v25 + 48) ^ *((_DWORD *)t_fn + (unsigned __int8)v30);
      v35 = *((_DWORD *)&t_fn[384] + ((unsigned int)v30 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v33 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v32)) ^ *(_DWORD *)(v25 + 52) ^ *((_DWORD *)t_fn + (unsigned __int8)v31);
      v36 = *((_DWORD *)&t_fn[384] + ((unsigned int)v31 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v30 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v33)) ^ *(_DWORD *)(v25 + 56) ^ *((_DWORD *)t_fn + (unsigned __int8)v32);
      v37 = *((_DWORD *)&t_fn[384] + ((unsigned int)v32 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v31 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v30)) ^ *(_DWORD *)(v25 + 60) ^ *((_DWORD *)t_fn + (unsigned __int8)v33);
      v38 = *((_DWORD *)&t_fn[384] + ((unsigned int)v37 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v36 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v35)) ^ *(_DWORD *)(v25 + 64) ^ *((_DWORD *)t_fn + (unsigned __int8)v34);
      v39 = *((_DWORD *)&t_fn[384] + ((unsigned int)v34 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v37 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v36)) ^ *(_DWORD *)(v25 + 68) ^ *((_DWORD *)t_fn + (unsigned __int8)v35);
      v40 = *((_DWORD *)&t_fn[384] + ((unsigned int)v35 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v34 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v37)) ^ *(_DWORD *)(v25 + 72) ^ *((_DWORD *)t_fn + (unsigned __int8)v36);
      v41 = *((_DWORD *)&t_fn[384] + ((unsigned int)v36 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v35 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v34)) ^ *(_DWORD *)(v25 + 76) ^ *((_DWORD *)t_fn + (unsigned __int8)v37);
      v42 = *((_DWORD *)&t_fn[384] + ((unsigned int)v41 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v40 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v39)) ^ *(_DWORD *)(v25 + 80) ^ *((_DWORD *)t_fn + (unsigned __int8)v38);
      v43 = *((_DWORD *)&t_fn[384] + ((unsigned int)v38 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v41 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v40)) ^ *(_DWORD *)(v25 + 84) ^ *((_DWORD *)t_fn + (unsigned __int8)v39);
      v44 = *((_DWORD *)&t_fn[384] + ((unsigned int)v39 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v38 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v41)) ^ *(_DWORD *)(v25 + 88) ^ *((_DWORD *)t_fn + (unsigned __int8)v40);
      v45 = *((_DWORD *)&t_fn[384] + ((unsigned int)v40 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v39 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v38)) ^ *(_DWORD *)(v25 + 92) ^ *((_DWORD *)t_fn + (unsigned __int8)v41);
      v46 = *((_DWORD *)&t_fn[384] + ((unsigned int)v45 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v44 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v43)) ^ *(_DWORD *)(v25 + 96) ^ *((_DWORD *)t_fn + (unsigned __int8)v42);
      v47 = *((_DWORD *)&t_fn[384] + ((unsigned int)v42 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v45 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v44)) ^ *(_DWORD *)(v25 + 100) ^ *((_DWORD *)t_fn + (unsigned __int8)v43);
      v48 = *((_DWORD *)&t_fn[384] + ((unsigned int)v43 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v42 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v45)) ^ *(_DWORD *)(v25 + 104) ^ *((_DWORD *)t_fn + (unsigned __int8)v44);
      v49 = *((_DWORD *)&t_fn[384] + ((unsigned int)v44 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v43 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v42)) ^ *(_DWORD *)(v25 + 108) ^ *((_DWORD *)t_fn + (unsigned __int8)v45);
      v50 = *((_DWORD *)&t_fn[384] + ((unsigned int)v49 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v48 >> 16)) ^ *((_DWORD *)&t_fn[128] + (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_fn[384] + 2 * ((unsigned int)v42 >> 24)) ^ *((_WORD *)&t_fn[256] + 2 * (unsigned __int8)((unsigned int)v45 >> 16)) ^ *((_WORD *)&t_fn[128] + 2 * BYTE1(v44)) ^ *(_WORD *)(v25 + 100) ^ *((_WORD *)t_fn + 2 * (unsigned __int8)v43)) >> 8)) ^ *(_DWORD *)(v25 + 112) ^ *((_DWORD *)t_fn + (unsigned __int8)(*((_BYTE *)&t_fn[384] + 4 * ((unsigned int)v45 >> 24)) ^ *((_BYTE *)&t_fn[256] + 4 * (unsigned __int8)((unsigned int)v44 >> 16)) ^ *((_BYTE *)&t_fn[128] + 4 * BYTE1(v43)) ^ *(_BYTE *)(v25 + 96) ^ *((_BYTE *)t_fn + 4 * (unsigned __int8)v42)));
      v51 = *((_DWORD *)&t_fn[384] + ((unsigned int)v46 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v49 >> 16)) ^ *((_DWORD *)&t_fn[128] + (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_fn[384] + 2 * ((unsigned int)v43 >> 24)) ^ *((_WORD *)&t_fn[256] + 2 * (unsigned __int8)((unsigned int)v42 >> 16)) ^ *((_WORD *)&t_fn[128] + 2 * BYTE1(v45)) ^ *(_WORD *)(v25 + 104) ^ *((_WORD *)t_fn + 2 * (unsigned __int8)v44)) >> 8)) ^ *(_DWORD *)(v25 + 116) ^ *((_DWORD *)t_fn + (unsigned __int8)(*((_BYTE *)&t_fn[384] + 4 * ((unsigned int)v42 >> 24)) ^ *((_BYTE *)&t_fn[256] + 4 * (unsigned __int8)((unsigned int)v45 >> 16)) ^ *((_BYTE *)&t_fn[128] + 4 * BYTE1(v44)) ^ *(_BYTE *)(v25 + 100) ^ *((_BYTE *)t_fn + 4 * (unsigned __int8)v43)));
      v52 = *((_DWORD *)&t_fn[384] + ((unsigned int)v47 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v46 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v49)) ^ *(_DWORD *)(v25 + 120) ^ *((_DWORD *)t_fn + (unsigned __int8)(*((_BYTE *)&t_fn[384] + 4 * ((unsigned int)v43 >> 24)) ^ *((_BYTE *)&t_fn[256] + 4 * (unsigned __int8)((unsigned int)v42 >> 16)) ^ *((_BYTE *)&t_fn[128] + 4 * BYTE1(v45)) ^ *(_BYTE *)(v25 + 104) ^ *((_BYTE *)t_fn + 4 * (unsigned __int8)v44)));
      v53 = *((_DWORD *)&t_fn[384] + ((unsigned int)v48 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v47 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v46)) ^ *(_DWORD *)(v25 + 124) ^ *((_DWORD *)t_fn + (unsigned __int8)v49);
      v54 = *((_DWORD *)&t_fn[384] + ((unsigned int)v53 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v52 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v51)) ^ *(_DWORD *)(v25 + 128) ^ *((_DWORD *)t_fn + (unsigned __int8)v50);
      v55 = *((_DWORD *)&t_fn[384] + ((unsigned int)v50 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v53 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v52)) ^ *(_DWORD *)(v25 + 132) ^ *((_DWORD *)t_fn + (unsigned __int8)v51);
      v56 = *((_DWORD *)&t_fn[384] + ((unsigned int)v51 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v50 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v53)) ^ *(_DWORD *)(v25 + 136) ^ *((_DWORD *)t_fn + (unsigned __int8)v52);
      v57 = *((_DWORD *)&t_fn[384] + ((unsigned int)v52 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v51 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v50)) ^ *(_DWORD *)(v25 + 140) ^ *((_DWORD *)t_fn + (unsigned __int8)v53);
      v58 = *((_DWORD *)&t_fn[384] + ((unsigned int)v57 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v56 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v55)) ^ *(_DWORD *)(v25 + 144) ^ *((_DWORD *)t_fn + (unsigned __int8)v54);
      v59 = *((_DWORD *)&t_fn[384] + ((unsigned int)v54 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v57 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v56)) ^ *(_DWORD *)(v25 + 148) ^ *((_DWORD *)t_fn + (unsigned __int8)v55);
      v60 = *((_DWORD *)&t_fn[384] + ((unsigned int)v55 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v54 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v57)) ^ *(_DWORD *)(v25 + 152) ^ *((_DWORD *)t_fn + (unsigned __int8)v56);
      v61 = *((_DWORD *)&t_fn[384] + ((unsigned int)v56 >> 24)) ^ *((_DWORD *)&t_fn[256]
                                                                  + (unsigned __int8)((unsigned int)v55 >> 16)) ^ *((_DWORD *)&t_fn[128] + BYTE1(v54)) ^ *(_DWORD *)(v25 + 156) ^ *((_DWORD *)t_fn + (unsigned __int8)v57);
      v5 = *((_DWORD *)&t_fl[384] + ((unsigned int)v61 >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                 + (unsigned __int8)((unsigned int)v60 >> 16)) ^ *((_DWORD *)&t_fl[128] + BYTE1(v59)) ^ *(_DWORD *)(v25 + 160) ^ *((_DWORD *)t_fl + (unsigned __int8)v58);
      v6 = *((_DWORD *)&t_fl[384] + ((unsigned int)v58 >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                 + (unsigned __int8)((unsigned int)v61 >> 16)) ^ *((_DWORD *)&t_fl[128] + BYTE1(v60)) ^ *(_DWORD *)(v25 + 164) ^ *((_DWORD *)t_fl + (unsigned __int8)v59);
      v7 = *((_DWORD *)&t_fl[384] + ((unsigned int)v59 >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                 + (unsigned __int8)((unsigned int)v58 >> 16)) ^ *((_DWORD *)&t_fl[128] + BYTE1(v61)) ^ *(_DWORD *)(v25 + 168) ^ *((_DWORD *)t_fl + (unsigned __int8)v60);
      result = *((_DWORD *)&t_fl[384] + ((unsigned int)v60 >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                     + (unsigned __int8)((unsigned int)v59 >> 16)) ^ *((_DWORD *)&t_fl[128] + BYTE1(v58)) ^ (unsigned int)(*(_DWORD *)(v25 + 172) ^ *((_DWORD *)t_fl + (unsigned __int8)v61));
      *(_DWORD *)v67 = v5;
      *(_DWORD *)(v67 + 4) = v6;
      *(_DWORD *)(v67 + 8) = v7;
      *(_DWORD *)(v67 + 12) = result;
      v9 += 16LL;
      a5 = v67 + 16;
      a3 = v66 - 1;
      a1 = v64;
    }
    while ( v66 != 1 );
  }
  if ( v65 )
  {
    *(_DWORD *)v62 = v5;
    *(_DWORD *)(v62 + 4) = v6;
    *(_DWORD *)(v62 + 8) = v7;
    *(_DWORD *)(v62 + 12) = result;
  }
  return result;
}
// 509A0: using guessed type __int64 t_fn[512];
// 519A0: using guessed type __int64 t_fl[512];

//----- (0000000000008A81) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_decrypt(__int64 a1, void *a2, signed __int64 a3, __int64 a4, __int64 a5)
{
  signed __int64 v5; // rbx@1
  __int64 v6; // r14@1
  signed __int64 v7; // rsi@3
  unsigned int v8; // er14@3
  int v9; // er13@3
  int v10; // er12@3
  int v11; // er15@3
  __int64 v12; // ST88_8@5
  signed __int64 v13; // r9@7
  signed __int64 v14; // rsi@7
  int v15; // er14@8
  int v16; // er13@8
  int v17; // er12@8
  int v18; // er15@8
  int v19; // eax@8
  signed __int64 v20; // r8@9
  unsigned int v21; // edx@11
  unsigned int v22; // esi@11
  __int64 v23; // ST40_8@11
  int v24; // er15@11
  int v25; // er11@11
  int v26; // eax@11
  int v27; // esi@11
  int v28; // er9@12
  int v29; // eax@12
  int v30; // ecx@12
  int v31; // edx@12
  signed __int64 v32; // r11@12
  __int64 v33; // rbx@14
  int v34; // er9@14
  __int64 v35; // rax@14
  int v36; // edx@14
  int v37; // edx@14
  __int64 v38; // r8@14
  int v39; // esi@14
  int v40; // er14@14
  int v41; // er14@14
  int v42; // ecx@14
  int v43; // esi@14
  int v44; // er9@14
  int v45; // edi@14
  int v46; // edx@14
  int v47; // esi@14
  int v48; // er14@14
  int v49; // ecx@14
  int v50; // esi@14
  int v51; // er9@14
  int v52; // edi@14
  int v53; // edx@14
  int v54; // esi@14
  int v55; // er14@14
  int v56; // ecx@14
  int v57; // esi@14
  int v58; // er9@14
  int v59; // edi@14
  int v60; // edx@14
  int v61; // esi@14
  int v62; // esi@14
  int v63; // ecx@14
  int v64; // edi@14
  int v65; // eax@14
  signed __int64 v66; // rbx@15
  signed __int64 v67; // r11@16
  signed __int64 v68; // r8@16
  signed __int64 v69; // r9@16
  signed __int64 v70; // r14@16
  signed __int64 v72; // [sp+10h] [bp-C0h]@7
  __int64 v73; // [sp+30h] [bp-A0h]@1
  void *v74; // [sp+38h] [bp-98h]@1
  signed __int64 v75; // [sp+60h] [bp-70h]@7
  const void *v76; // [sp+68h] [bp-68h]@3
  int v77; // [sp+74h] [bp-5Ch]@1
  __int64 v78; // [sp+78h] [bp-58h]@1
  signed __int64 v79; // [sp+80h] [bp-50h]@9
  signed __int64 v80; // [sp+88h] [bp-48h]@9
  char v81; // [sp+90h] [bp-40h]@5
  __int64 v82; // [sp+A0h] [bp-30h]@1

  v5 = a3;
  v74 = a2;
  v78 = a1;
  v6 = off_69010[0];
  v82 = *(_QWORD *)off_69010[0];
  v77 = *(_DWORD *)(a1 + 260);
  v73 = (unsigned int)(4 * *(_DWORD *)(a1 + 240));
  if ( !v77 || a3 )
  {
    v7 = 16 * a3;
    v76 = (const void *)(a4 + 16 * a3 - 16);
    v8 = *(_DWORD *)v76;
    v9 = *(_DWORD *)(16 * a3 + a4 - 12);
    v10 = *(_DWORD *)(16 * a3 + a4 - 8);
    v11 = *(_DWORD *)(16 * a3 + a4 - 4);
    if ( v77 && a3 )
    {
      v12 = a5;
      memmove(&v81, v76, 0x10uLL);
      v7 = 16 * v5;
      a5 = v12;
    }
    if ( v5 )
    {
      v13 = a1 + 4 * v73;
      v75 = a1 + 4 * v73;
      v14 = a5 + v7 - 16;
      v72 = a1 + 4 * v73 - 32;
      do
      {
        v15 = *(_DWORD *)v13 ^ v8;
        v16 = *(_DWORD *)(v78 + 4 * (v73 | 1)) ^ v9;
        v17 = *(_DWORD *)(v78 + 4 * (v73 | 2)) ^ v10;
        v18 = *(_DWORD *)(v78 + 4 * (v73 | 3)) ^ v11;
        v19 = *(_DWORD *)(v78 + 240);
        if ( v19 == 12 )
        {
          v79 = v14;
          v80 = v5;
          v20 = v13;
        }
        else
        {
          v80 = v5;
          if ( v19 != 14 )
          {
            v79 = v14;
            v32 = v13;
            goto LABEL_14;
          }
          v79 = v14;
          v21 = v18;
          v22 = v18;
          v23 = (unsigned __int8)v18;
          v24 = *((_DWORD *)&t_in[384] + ((unsigned int)v16 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                      + (unsigned __int8)((unsigned int)v17 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v18)) ^ *(_DWORD *)(v78 + 4 * v73 - 16) ^ *((_DWORD *)t_in + (unsigned __int8)v15);
          v25 = *((_DWORD *)&t_in[384] + ((unsigned int)v17 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                      + (unsigned __int8)(v21 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v15)) ^ *(_DWORD *)(v78 + 4 * v73 - 12) ^ *((_DWORD *)t_in + (unsigned __int8)v16);
          v26 = *((_DWORD *)&t_in[384] + (v22 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                        + (unsigned __int8)((unsigned int)v15 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v16)) ^ *(_DWORD *)(v78 + 4 * v73 - 8) ^ *((_DWORD *)t_in + (unsigned __int8)v17);
          v27 = *((_DWORD *)&t_in[384] + ((unsigned int)v15 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                      + (unsigned __int8)((unsigned int)v16 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v17)) ^ *(_DWORD *)(v78 + 4 * v73 - 4) ^ *((_DWORD *)t_in + v23);
          v15 = *((_DWORD *)&t_in[384] + ((unsigned int)v25 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                      + (unsigned __int8)((unsigned int)v26 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v27)) ^ *(_DWORD *)(v78 + 4 * v73 - 32) ^ *((_DWORD *)t_in + (unsigned __int8)v24);
          v16 = *((_DWORD *)&t_in[384] + ((unsigned int)v26 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                      + (unsigned __int8)((unsigned int)v27 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v24)) ^ *(_DWORD *)(v78 + 4 * v73 - 28) ^ *((_DWORD *)t_in + (unsigned __int8)v25);
          v17 = *((_DWORD *)&t_in[384] + ((unsigned int)v27 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                      + (unsigned __int8)((unsigned int)v24 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v25)) ^ *(_DWORD *)(v78 + 4 * v73 - 24) ^ *((_DWORD *)t_in + (unsigned __int8)v26);
          v18 = *((_DWORD *)&t_in[384] + ((unsigned int)v24 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                      + (unsigned __int8)((unsigned int)v25 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v26)) ^ *(_DWORD *)(v78 + 4 * v73 - 20) ^ *((_DWORD *)t_in + (unsigned __int8)v27);
          v20 = v72;
        }
        v28 = *((_DWORD *)&t_in[384] + ((unsigned int)v16 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v17 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v18)) ^ *(_DWORD *)(v20 - 16) ^ *((_DWORD *)t_in + (unsigned __int8)v15);
        v29 = *((_DWORD *)&t_in[384] + ((unsigned int)v17 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v18 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v15)) ^ *(_DWORD *)(v20 - 12) ^ *((_DWORD *)t_in + (unsigned __int8)v16);
        v30 = *((_DWORD *)&t_in[384] + ((unsigned int)v18 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v15 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v16)) ^ *(_DWORD *)(v20 - 8) ^ *((_DWORD *)t_in + (unsigned __int8)v17);
        v31 = *((_DWORD *)&t_in[384] + ((unsigned int)v15 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v16 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v17)) ^ *(_DWORD *)(v20 - 4) ^ *((_DWORD *)t_in + (unsigned __int8)v18);
        v32 = v20 - 32;
        v15 = *((_DWORD *)&t_in[384] + ((unsigned int)v29 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v30 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v31)) ^ *(_DWORD *)(v20 - 32) ^ *((_DWORD *)t_in + (unsigned __int8)(*((_BYTE *)&t_in[384] + 4 * ((unsigned int)v16 >> 24)) ^ *((_BYTE *)&t_in[256] + 4 * (unsigned __int8)((unsigned int)v17 >> 16)) ^ *((_BYTE *)&t_in[128] + 4 * BYTE1(v18)) ^ *(_BYTE *)(v20 - 16) ^ *((_BYTE *)t_in + 4 * (unsigned __int8)v15)));
        v16 = *((_DWORD *)&t_in[384] + ((unsigned int)v30 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v31 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v28)) ^ *(_DWORD *)(v20 - 28) ^ *((_DWORD *)t_in + (unsigned __int8)v29);
        v17 = *((_DWORD *)&t_in[384] + ((unsigned int)v31 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v28 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v29)) ^ *(_DWORD *)(v20 - 24) ^ *((_DWORD *)t_in + (unsigned __int8)v30);
        v18 = *((_DWORD *)&t_in[384] + ((unsigned int)v28 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v29 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v30)) ^ *(_DWORD *)(v20 - 20) ^ *((_DWORD *)t_in + (unsigned __int8)v31);
LABEL_14:
        v33 = (*((_DWORD *)&t_in[384] + ((unsigned int)v17 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                     + (unsigned __int8)((unsigned int)v18 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v15)) ^ (unsigned int)(*(_DWORD *)(v32 - 12) ^ *((_DWORD *)t_in + (unsigned __int8)v16))) >> 24;
        v34 = *((_DWORD *)&t_in[384] + v33) ^ *((_DWORD *)&t_in[256]
                                              + (unsigned __int8)((*((_DWORD *)&t_in[384] + ((unsigned int)v18 >> 24)) ^ *((_DWORD *)&t_in[256] + (unsigned __int8)((unsigned int)v15 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v16)) ^ (unsigned int)(*(_DWORD *)(v32 - 8) ^ *((_DWORD *)t_in + (unsigned __int8)v17))) >> 16)) ^ *((_DWORD *)&t_in[128] + (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_in[384] + 2 * ((unsigned int)v15 >> 24)) ^ *((_WORD *)&t_in[256] + 2 * (unsigned __int8)((unsigned int)v16 >> 16)) ^ *((_WORD *)&t_in[128] + 2 * BYTE1(v17)) ^ *(_WORD *)(v32 - 4) ^ *((_WORD *)t_in + 2 * (unsigned __int8)v18)) >> 8)) ^ *(_DWORD *)(v32 - 32) ^ *((_DWORD *)t_in + (unsigned __int8)(*((_BYTE *)&t_in[384] + 4 * ((unsigned int)v16 >> 24)) ^ *((_BYTE *)&t_in[256] + 4 * (unsigned __int8)((unsigned int)v17 >> 16)) ^ *((_BYTE *)&t_in[128] + 4 * BYTE1(v18)) ^ *(_BYTE *)(v32 - 16) ^ *((_BYTE *)t_in + 4 * (unsigned __int8)v15)));
        v35 = (*((_DWORD *)&t_in[384] + ((unsigned int)v18 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                     + (unsigned __int8)((unsigned int)v15 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v16)) ^ (unsigned int)(*(_DWORD *)(v32 - 8) ^ *((_DWORD *)t_in + (unsigned __int8)v17))) >> 24;
        LODWORD(v33) = *((_DWORD *)&t_in[384] + v35) ^ *((_DWORD *)&t_in[256]
                                                       + (unsigned __int8)((*((_DWORD *)&t_in[384]
                                                                            + ((unsigned int)v15 >> 24)) ^ *((_DWORD *)&t_in[256] + (unsigned __int8)((unsigned int)v16 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v17)) ^ (unsigned int)(*(_DWORD *)(v32 - 4) ^ *((_DWORD *)t_in + (unsigned __int8)v18))) >> 16)) ^ *((_DWORD *)&t_in[128] + (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_in[384] + 2 * ((unsigned int)v16 >> 24)) ^ *((_WORD *)&t_in[256] + 2 * (unsigned __int8)((unsigned int)v17 >> 16)) ^ *((_WORD *)&t_in[128] + 2 * BYTE1(v18)) ^ *(_WORD *)(v32 - 16) ^ *((_WORD *)t_in + 2 * (unsigned __int8)v15)) >> 8)) ^ *(_DWORD *)(v32 - 28) ^ *((_DWORD *)t_in + (unsigned __int8)(*((_BYTE *)&t_in[384] + 4 * ((unsigned int)v17 >> 24)) ^ *((_BYTE *)&t_in[256] + 4 * (unsigned __int8)((unsigned int)v18 >> 16)) ^ *((_BYTE *)&t_in[128] + 4 * BYTE1(v15)) ^ *(_BYTE *)(v32 - 12) ^ *((_BYTE *)t_in + 4 * (unsigned __int8)v16)));
        v36 = *((_DWORD *)&t_in[256]
              + (unsigned __int8)((*((_DWORD *)&t_in[384] + ((unsigned int)v16 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                                         + (unsigned __int8)((unsigned int)v17 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v18)) ^ (unsigned int)(*(_DWORD *)(v32 - 16) ^ *((_DWORD *)t_in + (unsigned __int8)v15))) >> 16)) ^ *((_DWORD *)&t_in[128] + (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_in[384] + 2 * ((unsigned int)v17 >> 24)) ^ *((_WORD *)&t_in[256] + 2 * (unsigned __int8)((unsigned int)v18 >> 16)) ^ *((_WORD *)&t_in[128] + 2 * BYTE1(v15)) ^ *(_WORD *)(v32 - 12) ^ *((_WORD *)t_in + 2 * (unsigned __int8)v16)) >> 8)) ^ *(_DWORD *)(v32 - 24) ^ *((_DWORD *)t_in + (unsigned __int8)(*((_BYTE *)&t_in[384] + 4 * ((unsigned int)v18 >> 24)) ^ *((_BYTE *)&t_in[256] + 4 * (unsigned __int8)((unsigned int)v15 >> 16)) ^ *((_BYTE *)&t_in[128] + 4 * BYTE1(v16)) ^ *(_BYTE *)(v32 - 8) ^ *((_BYTE *)t_in + 4 * (unsigned __int8)v17)));
        v37 = *((_DWORD *)&t_in[384]
              + ((*((_DWORD *)&t_in[384] + ((unsigned int)v15 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                        + (unsigned __int8)((unsigned int)v16 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v17)) ^ (unsigned int)(*(_DWORD *)(v32 - 4) ^ *((_DWORD *)t_in + (unsigned __int8)v18))) >> 24)) ^ v36;
        v38 = (*((_DWORD *)&t_in[384] + ((unsigned int)v16 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                     + (unsigned __int8)((unsigned int)v17 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v18)) ^ (unsigned int)(*(_DWORD *)(v32 - 16) ^ *((_DWORD *)t_in + (unsigned __int8)v15))) >> 24;
        v39 = *((_DWORD *)&t_in[384] + v38) ^ *((_DWORD *)&t_in[256]
                                              + (unsigned __int8)((*((_DWORD *)&t_in[384] + ((unsigned int)v17 >> 24)) ^ *((_DWORD *)&t_in[256] + (unsigned __int8)((unsigned int)v18 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v15)) ^ (unsigned int)(*(_DWORD *)(v32 - 12) ^ *((_DWORD *)t_in + (unsigned __int8)v16))) >> 16)) ^ *((_DWORD *)&t_in[128] + (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_in[384] + 2 * ((unsigned int)v18 >> 24)) ^ *((_WORD *)&t_in[256] + 2 * (unsigned __int8)((unsigned int)v15 >> 16)) ^ *((_WORD *)&t_in[128] + 2 * BYTE1(v16)) ^ *(_WORD *)(v32 - 8) ^ *((_WORD *)t_in + 2 * (unsigned __int8)v17)) >> 8)) ^ *(_DWORD *)(v32 - 20) ^ *((_DWORD *)t_in + (unsigned __int8)(*((_BYTE *)&t_in[384] + 4 * ((unsigned int)v15 >> 24)) ^ *((_BYTE *)&t_in[256] + 4 * (unsigned __int8)((unsigned int)v16 >> 16)) ^ *((_BYTE *)&t_in[128] + 4 * BYTE1(v17)) ^ *(_BYTE *)(v32 - 4) ^ *((_BYTE *)t_in + 4 * (unsigned __int8)v18)));
        LODWORD(v38) = *((_DWORD *)&t_in[384] + ((unsigned int)v33 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                             + (unsigned __int8)((unsigned int)v37 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v39)) ^ *(_DWORD *)(v32 - 48) ^ *((_DWORD *)t_in + (unsigned __int8)v34);
        v40 = *((_DWORD *)&t_in[128] + BYTE1(v34)) ^ *(_DWORD *)(v32 - 44) ^ *((_DWORD *)t_in
                                                                             + (unsigned __int8)(*((_BYTE *)&t_in[384]
                                                                                                 + 4 * v35) ^ *((_BYTE *)&t_in[256] + 4 * (unsigned __int8)((*((_DWORD *)&t_in[384] + ((unsigned int)v15 >> 24)) ^ *((_DWORD *)&t_in[256] + (unsigned __int8)((unsigned int)v16 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v17)) ^ (unsigned int)(*(_DWORD *)(v32 - 4) ^ *((_DWORD *)t_in + (unsigned __int8)v18))) >> 16)) ^ *((_BYTE *)&t_in[128] + 4 * (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_in[384] + 2 * ((unsigned int)v16 >> 24)) ^ *((_WORD *)&t_in[256] + 2 * (unsigned __int8)((unsigned int)v17 >> 16)) ^ *((_WORD *)&t_in[128] + 2 * BYTE1(v18)) ^ *(_WORD *)(v32 - 16) ^ *((_WORD *)t_in + 2 * (unsigned __int8)v15)) >> 8)) ^ *(_BYTE *)(v32 - 28) ^ *((_BYTE *)t_in + 4 * (unsigned __int8)(*((_BYTE *)&t_in[384] + 4 * ((unsigned int)v17 >> 24)) ^ *((_BYTE *)&t_in[256] + 4 * (unsigned __int8)((unsigned int)v18 >> 16)) ^ *((_BYTE *)&t_in[128] + 4 * BYTE1(v15)) ^ *(_BYTE *)(v32 - 12) ^ *((_BYTE *)t_in + 4 * (unsigned __int8)v16)))));
        v41 = *((_DWORD *)&t_in[384] + ((unsigned int)v37 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v39 >> 16)) ^ v40;
        v42 = *((_DWORD *)&t_in[384] + ((unsigned int)v39 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v34 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v33)) ^ *(_DWORD *)(v32 - 40) ^ *((_DWORD *)t_in + (unsigned __int8)v37);
        v43 = *((_DWORD *)&t_in[384] + ((unsigned int)v34 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v33 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v37)) ^ *(_DWORD *)(v32 - 36) ^ *((_DWORD *)t_in + (unsigned __int8)v39);
        v44 = *((_DWORD *)&t_in[384] + ((unsigned int)v41 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v42 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v43)) ^ *(_DWORD *)(v32 - 64) ^ *((_DWORD *)t_in + (unsigned __int8)v38);
        v45 = *((_DWORD *)&t_in[384] + ((unsigned int)v42 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v43 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v38)) ^ *(_DWORD *)(v32 - 60) ^ *((_DWORD *)t_in + (unsigned __int8)v41);
        v46 = *((_DWORD *)&t_in[384] + ((unsigned int)v43 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v38 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v41)) ^ *(_DWORD *)(v32 - 56) ^ *((_DWORD *)t_in + (unsigned __int8)v42);
        v47 = *((_DWORD *)&t_in[384] + ((unsigned int)v38 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v41 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v42)) ^ *(_DWORD *)(v32 - 52) ^ *((_DWORD *)t_in + (unsigned __int8)v43);
        LODWORD(v38) = *((_DWORD *)&t_in[384] + ((unsigned int)v45 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                             + (unsigned __int8)((unsigned int)v46 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v47)) ^ *(_DWORD *)(v32 - 80) ^ *((_DWORD *)t_in + (unsigned __int8)v44);
        v48 = *((_DWORD *)&t_in[384] + ((unsigned int)v46 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v47 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v44)) ^ *(_DWORD *)(v32 - 76) ^ *((_DWORD *)t_in + (unsigned __int8)v45);
        v49 = *((_DWORD *)&t_in[384] + ((unsigned int)v47 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v44 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v45)) ^ *(_DWORD *)(v32 - 72) ^ *((_DWORD *)t_in + (unsigned __int8)v46);
        v50 = *((_DWORD *)&t_in[384] + ((unsigned int)v44 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v45 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v46)) ^ *(_DWORD *)(v32 - 68) ^ *((_DWORD *)t_in + (unsigned __int8)v47);
        v51 = *((_DWORD *)&t_in[384] + ((unsigned int)v48 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v49 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v50)) ^ *(_DWORD *)(v32 - 96) ^ *((_DWORD *)t_in + (unsigned __int8)v38);
        v52 = *((_DWORD *)&t_in[384] + ((unsigned int)v49 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v50 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v38)) ^ *(_DWORD *)(v32 - 92) ^ *((_DWORD *)t_in + (unsigned __int8)v48);
        v53 = *((_DWORD *)&t_in[384] + ((unsigned int)v50 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v38 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v48)) ^ *(_DWORD *)(v32 - 88) ^ *((_DWORD *)t_in + (unsigned __int8)v49);
        v54 = *((_DWORD *)&t_in[384] + ((unsigned int)v38 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v48 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v49)) ^ *(_DWORD *)(v32 - 84) ^ *((_DWORD *)t_in + (unsigned __int8)v50);
        LODWORD(v38) = *((_DWORD *)&t_in[384] + ((unsigned int)v52 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                             + (unsigned __int8)((unsigned int)v53 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v54)) ^ *(_DWORD *)(v32 - 112) ^ *((_DWORD *)t_in + (unsigned __int8)v51);
        v55 = *((_DWORD *)&t_in[384] + ((unsigned int)v53 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v54 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v51)) ^ *(_DWORD *)(v32 - 108) ^ *((_DWORD *)t_in + (unsigned __int8)v52);
        v56 = *((_DWORD *)&t_in[384] + ((unsigned int)v54 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v51 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v52)) ^ *(_DWORD *)(v32 - 104) ^ *((_DWORD *)t_in + (unsigned __int8)v53);
        v57 = *((_DWORD *)&t_in[384] + ((unsigned int)v51 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v52 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v53)) ^ *(_DWORD *)(v32 - 100) ^ *((_DWORD *)t_in + (unsigned __int8)v54);
        v58 = *((_DWORD *)&t_in[384] + ((unsigned int)v55 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v56 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v57)) ^ *(_DWORD *)(v32 - 128) ^ *((_DWORD *)t_in + (unsigned __int8)v38);
        v59 = *((_DWORD *)&t_in[384] + ((unsigned int)v56 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v57 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v38)) ^ *(_DWORD *)(v32 - 124) ^ *((_DWORD *)t_in + (unsigned __int8)v55);
        v60 = *((_DWORD *)&t_in[384] + ((unsigned int)v57 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v38 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v55)) ^ *(_DWORD *)(v32 - 120) ^ *((_DWORD *)t_in + (unsigned __int8)v56);
        v61 = *((_DWORD *)&t_in[384] + ((unsigned int)v38 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v55 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v56)) ^ *(_DWORD *)(v32 - 116) ^ *((_DWORD *)t_in + (unsigned __int8)v57);
        v8 = *((_DWORD *)&t_in[384] + ((unsigned int)v59 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                   + (unsigned __int8)((unsigned int)v60 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v61)) ^ *(_DWORD *)(v32 - 144) ^ *((_DWORD *)t_in + (unsigned __int8)v58);
        v9 = *((_DWORD *)&t_in[384] + ((unsigned int)v60 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                   + (unsigned __int8)((unsigned int)v61 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v58)) ^ *(_DWORD *)(v32 - 140) ^ *((_DWORD *)t_in + (unsigned __int8)v59);
        v10 = *((_DWORD *)&t_in[384] + ((unsigned int)v61 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v58 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v59)) ^ *(_DWORD *)(v32 - 136) ^ *((_DWORD *)t_in + (unsigned __int8)v60);
        v11 = *((_DWORD *)&t_in[384] + ((unsigned int)v58 >> 24)) ^ *((_DWORD *)&t_in[256]
                                                                    + (unsigned __int8)((unsigned int)v59 >> 16)) ^ *((_DWORD *)&t_in[128] + BYTE1(v60)) ^ *(_DWORD *)(v32 - 132) ^ *((_DWORD *)t_in + (unsigned __int8)v61);
        v62 = *((_DWORD *)&t_il[384] + ((unsigned int)v9 >> 24)) ^ *((_DWORD *)&t_il[256]
                                                                   + (unsigned __int8)((unsigned int)v10 >> 16)) ^ *((_DWORD *)&t_il[128] + BYTE1(v11)) ^ *(_DWORD *)(v32 - 160) ^ *((_DWORD *)t_il + (unsigned __int8)(*((_BYTE *)&t_in[384] + 4 * ((unsigned int)v59 >> 24)) ^ *((_BYTE *)&t_in[256] + 4 * (unsigned __int8)((unsigned int)v60 >> 16)) ^ *((_BYTE *)&t_in[128] + 4 * BYTE1(v61)) ^ *(_BYTE *)(v32 - 144) ^ *((_BYTE *)t_in + 4 * (unsigned __int8)v58)));
        v63 = *((_DWORD *)&t_il[384] + ((unsigned int)v10 >> 24)) ^ *((_DWORD *)&t_il[256]
                                                                    + (unsigned __int8)((unsigned int)v11 >> 16)) ^ *((_DWORD *)&t_il[128] + BYTE1(v8)) ^ *(_DWORD *)(v32 - 156) ^ *((_DWORD *)t_il + (unsigned __int8)v9);
        v64 = *((_DWORD *)&t_il[384] + ((unsigned int)v11 >> 24)) ^ *((_DWORD *)&t_il[256] + (unsigned __int8)(v8 >> 16)) ^ *((_DWORD *)&t_il[128] + BYTE1(v9)) ^ *(_DWORD *)(v32 - 152) ^ *((_DWORD *)t_il + (unsigned __int8)v10);
        v65 = *((_DWORD *)&t_il[384] + (v8 >> 24)) ^ *((_DWORD *)&t_il[256] + (unsigned __int8)((unsigned int)v9 >> 16)) ^ *((_DWORD *)&t_il[128] + BYTE1(v10)) ^ *(_DWORD *)(v32 - 148) ^ *((_DWORD *)t_il + (unsigned __int8)v11);
        if ( v77 )
        {
          v66 = v80;
          if ( v80 == 1 )
          {
            v67 = (signed __int64)((char *)v74 + 12);
            v68 = (signed __int64)((char *)v74 + 8);
            v69 = (signed __int64)((char *)v74 + 4);
            v70 = (signed __int64)v74;
          }
          else
          {
            v70 = (signed __int64)((char *)v76 - 16);
            v69 = (signed __int64)((char *)v76 - 12);
            v68 = (signed __int64)((char *)v76 - 8);
            v67 = (signed __int64)((char *)v76 - 4);
            v76 = (char *)v76 - 16;
          }
          v8 = *(_DWORD *)v70;
          v9 = *(_DWORD *)v69;
          v10 = *(_DWORD *)v68;
          v11 = *(_DWORD *)v67;
          v62 ^= v8;
          v63 ^= *(_DWORD *)v69;
          v64 ^= *(_DWORD *)v68;
          v65 ^= *(_DWORD *)v67;
          v13 = v75;
        }
        else
        {
          v66 = v80;
          v13 = v75;
          if ( v80 != 1 )
          {
            v8 = *((_DWORD *)v76 - 4);
            v9 = *((_DWORD *)v76 - 3);
            v10 = *((_DWORD *)v76 - 2);
            v11 = *((_DWORD *)v76 - 1);
            v76 = (char *)v76 - 16;
          }
        }
        *(_DWORD *)v79 = v62;
        *(_DWORD *)(v79 + 4) = v63;
        *(_DWORD *)(v79 + 8) = v64;
        *(_DWORD *)(v79 + 12) = v65;
        v14 = v79 - 16;
        v5 = v66 - 1;
      }
      while ( v5 );
    }
    v6 = off_69010[0];
    if ( v77 )
      memmove(v74, &v81, 0x10uLL);
  }
  return *(_QWORD *)v6;
}
// 529A0: using guessed type __int64 t_in[512];
// 539A0: using guessed type __int64 t_il[512];
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000099C8) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_rsa_priv(__int64 *a1, __int64 a2, unsigned __int64 a3)
{
  __int64 v3; // ST38_8@1
  __int64 v4; // ST30_8@1
  signed __int64 v5; // r14@1
  __int64 v6; // r12@1
  signed __int64 v7; // r13@1
  __int64 v8; // ST08_8@1
  signed __int64 v9; // rax@1
  signed __int64 v10; // rax@1
  signed __int64 v11; // rax@1
  signed __int64 v12; // rax@1
  signed __int64 v13; // rax@1
  signed __int64 v14; // rax@1
  signed __int64 v15; // rax@1
  signed __int64 v16; // rax@1
  signed __int64 v17; // rax@1
  signed __int64 result; // rax@1
  __int64 v19; // rcx@1
  __int64 v20; // [sp+48h] [bp-38h]@1
  __int64 v21; // [sp+50h] [bp-30h]@1

  v3 = a3;
  v21 = *(_QWORD *)off_69010[0];
  v4 = *a1;
  v5 = (signed __int64)&a1[4 * *a1 + 3];
  v20 = 0LL;
  v6 = a1[4 * v4 + 3];
  v7 = 16 * a1[4 * v4 + 3];
  v8 = *(_QWORD *)(v7 + v5 + 24);
  v9 = ccder_encode_integer(a1[4 * v4 + 3], v5 + 48 * v6 + 48, a2, a3);
  v10 = ccder_encode_integer(v8, v5 + 40 * v6 + 48, a2, v9);
  v11 = ccder_encode_integer(v6, 32 * v6 + v5 + 48, a2, v10);
  v12 = ccder_encode_integer(*(_QWORD *)(v7 + v5 + 24), v7 + v5 + 40, a2, v11);
  v13 = ccder_encode_integer(a1[4 * v4 + 3], (__int64)&a1[4 * v4 + 5], a2, v12);
  v14 = ccder_encode_integer(v4, (__int64)&a1[3 * v4 + 3], a2, v13);
  v15 = ccder_encode_integer(v4, (__int64)&a1[2 * v4 + 3], a2, v14);
  v16 = ccder_encode_integer(v4, (__int64)(a1 + 2), a2, v15);
  v17 = ccder_encode_integer(1LL, (__int64)&v20, a2, v16);
  result = ccder_encode_constructed_tl(0x2000000000000010uLL, v3, a2, v17);
  v19 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000009B6C) ----------------------------------------------------
__int64 __fastcall ccecies_encrypt_gcm_setup(__int64 a1, __int64 a2, __int64 a3, __int64 a4, int a5, int a6, unsigned int a7)
{
  __int64 result; // rax@1

  result = a7;
  *(_QWORD *)(a1 + 8) = a3;
  *(_QWORD *)a1 = a2;
  *(_QWORD *)(a1 + 16) = a4;
  *(_DWORD *)(a1 + 32) = a7;
  *(_DWORD *)(a1 + 24) = a5;
  *(_DWORD *)(a1 + 28) = a6;
  return result;
}

//----- (0000000000009B8B) ----------------------------------------------------
__int64 __fastcall ccec_sign_composite(__int64 a1, unsigned __int64 a2, unsigned __int64 a3, void *a4, void *a5, __int64 a6)
{
  unsigned __int64 v6; // rax@1
  __int64 v7; // r15@1
  char *v8; // rbx@1
  __int64 v9; // r14@2
  unsigned __int64 v10; // r13@2
  unsigned __int64 v11; // rax@2
  size_t v12; // r14@2
  void *v13; // r12@4
  __int64 v14; // r14@4
  unsigned __int64 v15; // r15@4
  __int64 v16; // r13@4
  unsigned __int64 v17; // rax@4
  size_t v18; // rbx@4
  void *v19; // r12@6
  __int64 result; // rax@7
  __int64 v21; // [sp+0h] [bp-60h]@1
  unsigned int v22; // [sp+Ch] [bp-54h]@1
  __int64 v23; // [sp+10h] [bp-50h]@2
  __int64 v24; // [sp+18h] [bp-48h]@4
  void *v25; // [sp+20h] [bp-40h]@1
  void *v26; // [sp+28h] [bp-38h]@1
  __int64 v27; // [sp+30h] [bp-30h]@1

  v25 = a5;
  v26 = a4;
  v27 = *(_QWORD *)off_69010[0];
  v6 = (8LL * **(_QWORD **)a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v7 = (__int64)((char *)&v21 - v6);
  v8 = (char *)&v21 - v6;
  v22 = ccec_sign_internal(a1, a2, a3, (__int64)((char *)&v21 - v6), (__int64)((char *)&v21 - v6), a6);
  if ( !v22 )
  {
    v23 = **(_QWORD **)a1;
    v9 = v23;
    v10 = ccec_signature_r_s_size((__int64 **)a1);
    v11 = ccn_write_uint_size(v9, v7);
    v12 = v10 - v11;
    if ( v10 <= v11 )
      v12 = 0LL;
    v24 = (__int64)v8;
    v13 = v26;
    bzero(v26, v12);
    ccn_write_uint(v23, v7, v10 - v12, (__int64)((char *)v13 + v12));
    v14 = **(_QWORD **)a1;
    v15 = ccec_signature_r_s_size((__int64 **)a1);
    v16 = v24;
    v17 = ccn_write_uint_size(v14, v24);
    v18 = v15 - v17;
    if ( v15 <= v17 )
      v18 = 0LL;
    v19 = v25;
    bzero(v25, v18);
    ccn_write_uint(v14, v16, v15 - v18, (__int64)((char *)v19 + v18));
  }
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v27 )
    result = v22;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000009CC7) ----------------------------------------------------
signed __int64 __fastcall ccec_der_export_priv_size(__int64 **a1, __int64 a2, int a3)
{
  int v3; // er13@1
  unsigned __int64 v4; // r12@1
  unsigned __int64 v5; // rbx@1
  signed __int64 v6; // r13@3
  unsigned __int64 v7; // r12@3
  signed __int64 v8; // rax@4
  signed __int64 v9; // rax@6

  v3 = a3;
  v4 = (unsigned __int64)(ccn_bitlen(**a1, (__int64)(*a1 + 2)) + 7) >> 3;
  v5 = 0LL;
  if ( v3 )
    v5 = ((unsigned __int64)(ccn_bitlen(**a1, (__int64)(*a1 + 2)) + 7) >> 2) | 1;
  v6 = ccder_sizeof_uint64(1LL);
  v7 = v6 + ccder_sizeof(4LL, v4);
  if ( a2 )
  {
    v8 = ccder_sizeof_oid(a2);
    v7 += ccder_sizeof(-6917529027641081856LL, v8);
  }
  if ( v5 )
  {
    v9 = ccder_sizeof(3LL, v5 + 1);
    v7 += ccder_sizeof(-6917529027641081855LL, v9);
  }
  return ccder_sizeof(2305843009213693968LL, v7);
}

//----- (0000000000009DA8) ----------------------------------------------------
signed __int64 __fastcall ccec_der_export_priv(__int64 **a1, __int64 a2, int a3, __int64 a4, __int64 a5)
{
  __int64 *v5; // r14@1
  unsigned __int64 v6; // r12@1
  __int64 v7; // r14@1
  __int64 v8; // r15@1
  unsigned __int64 v9; // rax@1
  size_t v10; // rbx@1
  signed __int64 v11; // rdx@3
  __int64 v12; // rcx@3
  __int64 v13; // rbx@3
  signed __int64 v14; // r14@3
  __int64 **v15; // r14@4
  unsigned __int64 v16; // r15@4
  char *v17; // r12@4
  __int64 v18; // r13@4
  size_t v19; // r14@5
  __int64 v20; // rbx@5
  __int64 v21; // rax@5
  __int64 v22; // rax@5
  signed __int64 v23; // rax@5
  __int64 v24; // rdx@5
  __int64 v25; // rax@6
  __int64 v26; // rbx@6
  signed __int64 v27; // r15@8
  __int64 v28; // rdi@8
  __int64 v29; // rbx@8
  __int64 v30; // rax@8
  __int64 v31; // rbx@10
  signed __int64 v32; // rax@10
  signed __int64 v33; // rax@10
  bool v34; // zf@10
  signed __int64 result; // rax@10
  __int64 v36; // rcx@12
  __int64 v37; // [sp+0h] [bp-70h]@1
  __int64 **v38; // [sp+8h] [bp-68h]@1
  __int64 v39; // [sp+10h] [bp-60h]@1
  __int64 v40; // [sp+18h] [bp-58h]@1
  __int64 v41; // [sp+20h] [bp-50h]@1
  const void *v42; // [sp+28h] [bp-48h]@1
  size_t v43; // [sp+30h] [bp-40h]@1
  __int64 v44; // [sp+38h] [bp-38h]@1
  __int64 v45; // [sp+40h] [bp-30h]@1
  __int64 v46; // [sp+48h] [bp-28h]@4

  v44 = a5;
  LODWORD(v41) = a3;
  v39 = a2;
  v38 = a1;
  v45 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v40 = a5 + a4;
  v6 = (unsigned __int64)(ccn_bitlen(*v5, (__int64)(v5 + 2)) + 7) >> 3;
  v43 = v6;
  v42 = (char *)&v37 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v7 = *v5;
  v8 = (__int64)&a1[3 * **a1 + 2];
  v9 = ccn_write_uint_size(v7, v8);
  v10 = 0LL;
  if ( v6 > v9 )
    v10 = v6 - v9;
  bzero((char *)&v37 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0LL), v10);
  v11 = v6 - v10;
  v12 = (__int64)((char *)&v37 + v10 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v13 = v40;
  ccn_write_uint(v7, v8, v11, v12);
  v14 = v13;
  if ( (_DWORD)v41 )
  {
    v15 = v38;
    v16 = ((unsigned __int64)(ccn_bitlen(**v38, (__int64)(*v38 + 2)) + 7) >> 2) | 1;
    v17 = (char *)&v46 - ((v16 + 16) & 0xFFFFFFFFFFFFFFF0LL);
    ccec_export_pub(v15, (__int64)v17);
    v14 = 0LL;
    v18 = v13;
    if ( !v13 )
      goto LABEL_10;
    v19 = v13;
    v41 = ccder_sizeof(3LL, v16 + 1);
    v20 = v44;
    v21 = ccder_encode_body(v16, v17, v44, v19);
    v22 = ccder_encode_body(1uLL, &dword_5094C, v20, v21);
    v23 = ccder_encode_tl(3uLL, v16 + 1, v20, v22);
    v24 = v20;
    v13 = v19;
    v14 = ccder_encode_tl(0xA000000000000001LL, v41, v24, v23);
  }
  v25 = v13;
  v26 = v39;
  if ( v39 )
  {
    v18 = v25;
    if ( v14 )
    {
      v27 = ccder_sizeof_oid(v39);
      v28 = v26;
      v29 = v44;
      v30 = ccder_encode_oid(v28, v44, v14);
      v14 = ccder_encode_tl(0xA000000000000000LL, v27, v29, v30);
    }
  }
  else
  {
    v18 = v25;
  }
LABEL_10:
  v31 = v44;
  v32 = ccder_encode_implicit_raw_octet_string(4uLL, v43, v42, v44, v14);
  v33 = ccder_encode_uint64(1LL, v31, v32);
  v34 = ccder_encode_constructed_tl(0x2000000000000010uLL, v18, v31, v33) == v31;
  result = 0xFFFFFFFFLL;
  if ( v34 )
    result = 0LL;
  v36 = *(_QWORD *)off_69010[0];
  return result;
}
// 5094C: using guessed type int dword_5094C;
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000009FFA) ----------------------------------------------------
#error "9FFA: too complex function (funcsize=5916)"

//----- (0000000000010D69) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_encrypt_key128(__int64 a1, __int64 a2)
{
  int v2; // edx@1
  int v3; // er14@1
  int v4; // er15@1
  unsigned int v5; // er11@1
  int v6; // edx@1
  int v7; // er10@1
  int v8; // eax@1
  int v9; // ecx@1
  int v10; // edx@1
  int v11; // ecx@1
  int v12; // eax@1
  int v13; // er10@1
  int v14; // er14@1
  int v15; // eax@1
  int v16; // ecx@1
  int v17; // er14@1
  int v18; // edx@1
  int v19; // er10@1
  int v20; // er15@1
  int v21; // edx@1
  int v22; // er15@1
  int v23; // edi@1
  int v24; // er10@1
  int v25; // ecx@1
  int v26; // edi@1
  int v27; // er15@1
  int v28; // ecx@1
  int v29; // edx@1
  int v30; // er10@1
  __int64 result; // rax@1
  int v32; // ebx@1
  int v33; // ebx@1

  v2 = *(_DWORD *)a1;
  *(_DWORD *)a2 = *(_DWORD *)a1;
  v3 = *(_DWORD *)(a1 + 4);
  *(_DWORD *)(a2 + 4) = v3;
  v4 = *(_DWORD *)(a1 + 8);
  *(_DWORD *)(a2 + 8) = v4;
  v5 = *(_DWORD *)(a1 + 12);
  *(_DWORD *)(a2 + 12) = v5;
  v6 = LODWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v5) ^ *((_DWORD *)&t_fl[256] + (v5 >> 24)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(v5 >> 16)) ^ *((_DWORD *)t_fl + BYTE1(v5)) ^ v2;
  *(_DWORD *)(a2 + 16) = v6;
  *(_DWORD *)(a2 + 20) = v3 ^ v6;
  v7 = v4 ^ v3 ^ v6;
  *(_DWORD *)(a2 + 24) = v7;
  v8 = v5 ^ v4 ^ v3 ^ v6;
  *(_DWORD *)(a2 + 28) = v8;
  v9 = HIDWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v8) ^ *((_DWORD *)&t_fl[256]
                                                                          + ((v5 ^ v4 ^ v3 ^ v6) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v5 ^ v4 ^ v3 ^ v6) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v5 ^ v4 ^ v3 ^ v6) >> 16));
  v10 = v9 ^ v6;
  *(_DWORD *)(a2 + 32) = v10;
  v11 = v3 ^ v9;
  *(_DWORD *)(a2 + 36) = v11;
  *(_DWORD *)(a2 + 40) = v7 ^ v11;
  *(_DWORD *)(a2 + 44) = v5 ^ v11;
  v12 = LODWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v5 ^ v11)) ^ *((_DWORD *)&t_fl[256]
                                                                                   + ((v5 ^ v11) >> 24)) ^ v10 ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v5 ^ v11) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v5 ^ v11) >> 16));
  *(_DWORD *)(a2 + 48) = v12;
  *(_DWORD *)(a2 + 52) = v11 ^ v12;
  v13 = v12 ^ v7;
  *(_DWORD *)(a2 + 56) = v13;
  *(_DWORD *)(a2 + 60) = v13 ^ v5 ^ v11;
  v14 = HIDWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v13 ^ v5 ^ v11)) ^ *((_DWORD *)&t_fl[256]
                                                                                         + ((v13 ^ v5 ^ v11) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v13 ^ v5 ^ v11) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v13 ^ v5 ^ v11) >> 16));
  v15 = v14 ^ v12;
  *(_DWORD *)(a2 + 64) = v15;
  v16 = v14 ^ v11;
  *(_DWORD *)(a2 + 68) = v16;
  *(_DWORD *)(a2 + 72) = v13 ^ v16;
  v17 = v5 ^ v14;
  *(_DWORD *)(a2 + 76) = v17;
  v18 = LODWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v17) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v17 >> 24)) ^ v15 ^ *((_DWORD *)t_fl + BYTE1(v17)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v17 >> 16));
  *(_DWORD *)(a2 + 80) = v18;
  *(_DWORD *)(a2 + 84) = v16 ^ v18;
  v19 = v18 ^ v13;
  *(_DWORD *)(a2 + 88) = v19;
  *(_DWORD *)(a2 + 92) = v17 ^ v19;
  v20 = HIDWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v17 ^ v19)) ^ *((_DWORD *)&t_fl[256]
                                                                                    + ((v17 ^ (unsigned int)v19) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v17 ^ v19) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v17 ^ (unsigned int)v19) >> 16));
  v21 = v20 ^ v18;
  *(_DWORD *)(a2 + 96) = v21;
  v22 = v16 ^ v20;
  *(_DWORD *)(a2 + 100) = v22;
  *(_DWORD *)(a2 + 104) = v19 ^ v22;
  *(_DWORD *)(a2 + 108) = v17 ^ v22;
  v23 = LODWORD(t_rc[3]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v17 ^ v22)) ^ *((_DWORD *)&t_fl[256]
                                                                                    + ((v17 ^ (unsigned int)v22) >> 24)) ^ v21 ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v17 ^ v22) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v17 ^ (unsigned int)v22) >> 16));
  *(_DWORD *)(a2 + 112) = v23;
  *(_DWORD *)(a2 + 116) = v22 ^ v23;
  v24 = v23 ^ v19;
  *(_DWORD *)(a2 + 120) = v24;
  *(_DWORD *)(a2 + 124) = v24 ^ v17 ^ v22;
  v25 = HIDWORD(t_rc[3]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v24 ^ v17 ^ v22)) ^ *((_DWORD *)&t_fl[256]
                                                                                          + ((v24 ^ v17 ^ (unsigned int)v22) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v24 ^ v17 ^ v22) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v24 ^ v17 ^ (unsigned int)v22) >> 16));
  v26 = v25 ^ v23;
  *(_DWORD *)(a2 + 128) = v26;
  v27 = v25 ^ v22;
  *(_DWORD *)(a2 + 132) = v27;
  *(_DWORD *)(a2 + 136) = v24 ^ v27;
  v28 = v17 ^ v25;
  *(_DWORD *)(a2 + 140) = v28;
  v29 = LODWORD(t_rc[4]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v28) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v28 >> 24)) ^ v26 ^ *((_DWORD *)t_fl + BYTE1(v28)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v28 >> 16));
  *(_DWORD *)(a2 + 144) = v29;
  *(_DWORD *)(a2 + 148) = v27 ^ v29;
  v30 = v29 ^ v24;
  *(_DWORD *)(a2 + 152) = v30;
  *(_DWORD *)(a2 + 156) = v28 ^ v30;
  result = (unsigned __int8)(v28 ^ v30);
  v32 = HIDWORD(t_rc[4]) ^ *((_DWORD *)&t_fl[384] + result) ^ *((_DWORD *)&t_fl[256] + ((v28 ^ (unsigned int)v30) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v28 ^ v30) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v28 ^ (unsigned int)v30) >> 16));
  *(_DWORD *)(a2 + 160) = v32 ^ v29;
  v33 = v27 ^ v32;
  *(_DWORD *)(a2 + 164) = v33;
  *(_DWORD *)(a2 + 168) = v33 ^ v30;
  *(_DWORD *)(a2 + 172) = v28 ^ v33;
  *(_DWORD *)(a2 + 240) = 10;
  return result;
}
// 50970: using guessed type __int64 t_rc[6];
// 519A0: using guessed type __int64 t_fl[512];

//----- (000000000001111E) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_encrypt_key192(__int64 a1, __int64 a2)
{
  int v2; // eax@1
  int v3; // er10@1
  int v4; // er15@1
  int v5; // er11@1
  int v6; // er12@1
  unsigned int v7; // er13@1
  int v8; // eax@1
  int v9; // er14@1
  int v10; // er15@1
  int v11; // er15@1
  int v12; // ecx@1
  int v13; // eax@1
  int v14; // ecx@1
  int v15; // er10@1
  int v16; // edx@1
  int v17; // er14@1
  int v18; // edi@1
  int v19; // eax@1
  int v20; // edx@1
  int v21; // ecx@1
  __int64 result; // rax@1
  int v23; // edi@1
  int v24; // er14@1
  int v25; // er15@1
  int v26; // edx@1
  int v27; // edi@1
  int v28; // edx@1
  int v29; // er10@1
  int v30; // ecx@1
  int v31; // er14@1
  int v32; // er15@1
  int v33; // er15@1
  int v34; // ebx@1
  int v35; // edx@1

  v2 = *(_DWORD *)a1;
  *(_DWORD *)a2 = *(_DWORD *)a1;
  v3 = *(_DWORD *)(a1 + 4);
  *(_DWORD *)(a2 + 4) = v3;
  v4 = *(_DWORD *)(a1 + 8);
  *(_DWORD *)(a2 + 8) = v4;
  v5 = *(_DWORD *)(a1 + 12);
  *(_DWORD *)(a2 + 12) = v5;
  v6 = *(_DWORD *)(a1 + 16);
  *(_DWORD *)(a2 + 16) = v6;
  v7 = *(_DWORD *)(a1 + 20);
  *(_DWORD *)(a2 + 20) = v7;
  v8 = LODWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v7) ^ *((_DWORD *)&t_fl[256] + (v7 >> 24)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(v7 >> 16)) ^ *((_DWORD *)t_fl + BYTE1(v7)) ^ v2;
  *(_DWORD *)(a2 + 24) = v8;
  *(_DWORD *)(a2 + 28) = v3 ^ v8;
  v9 = v4 ^ v3 ^ v8;
  *(_DWORD *)(a2 + 32) = v9;
  v10 = v5 ^ v4 ^ v3 ^ v8;
  *(_DWORD *)(a2 + 36) = v10;
  v11 = v6 ^ v10;
  *(_DWORD *)(a2 + 40) = v11;
  *(_DWORD *)(a2 + 44) = v7 ^ v11;
  v12 = HIDWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v7 ^ v11)) ^ *((_DWORD *)&t_fl[256]
                                                                                   + ((v7 ^ v11) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v7 ^ v11) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v7 ^ v11) >> 16));
  v13 = v12 ^ v8;
  *(_DWORD *)(a2 + 48) = v13;
  v14 = v3 ^ v12;
  *(_DWORD *)(a2 + 52) = v14;
  *(_DWORD *)(a2 + 56) = v9 ^ v14;
  *(_DWORD *)(a2 + 60) = v5 ^ v14;
  *(_DWORD *)(a2 + 64) = v11 ^ v5 ^ v14;
  v15 = v7 ^ v5 ^ v14;
  *(_DWORD *)(a2 + 68) = v15;
  v16 = LODWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v7 ^ v5 ^ v14)) ^ *((_DWORD *)&t_fl[256]
                                                                                        + ((v7 ^ v5 ^ v14) >> 24)) ^ v13 ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v7 ^ v5 ^ v14) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v7 ^ v5 ^ v14) >> 16));
  *(_DWORD *)(a2 + 72) = v16;
  *(_DWORD *)(a2 + 76) = v14 ^ v16;
  v17 = v16 ^ v9;
  *(_DWORD *)(a2 + 80) = v17;
  *(_DWORD *)(a2 + 84) = v17 ^ v5 ^ v14;
  *(_DWORD *)(a2 + 88) = v11 ^ v17;
  v18 = v15 ^ v11 ^ v17;
  *(_DWORD *)(a2 + 92) = v18;
  v19 = HIDWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v18) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((v15 ^ v11 ^ (unsigned int)v17) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v15 ^ v11 ^ v17) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v15 ^ v11 ^ (unsigned int)v17) >> 16));
  v20 = v19 ^ v16;
  *(_DWORD *)(a2 + 96) = v20;
  v21 = v19 ^ v14;
  *(_DWORD *)(a2 + 100) = v21;
  *(_DWORD *)(a2 + 104) = v17 ^ v21;
  result = v5 ^ (unsigned int)v19;
  *(_DWORD *)(a2 + 108) = result;
  *(_DWORD *)(a2 + 112) = result ^ v11 ^ v17;
  *(_DWORD *)(a2 + 116) = v15 ^ result;
  v23 = LODWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v15 ^ result)) ^ *((_DWORD *)&t_fl[256]
                                                                                       + ((v15 ^ (unsigned int)result) >> 24)) ^ v20 ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v15 ^ result) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v15 ^ (unsigned int)result) >> 16));
  *(_DWORD *)(a2 + 120) = v23;
  *(_DWORD *)(a2 + 124) = v21 ^ v23;
  v24 = v23 ^ v17;
  *(_DWORD *)(a2 + 128) = v24;
  *(_DWORD *)(a2 + 132) = result ^ v24;
  v25 = v23 ^ v11;
  *(_DWORD *)(a2 + 136) = v25;
  *(_DWORD *)(a2 + 140) = v25 ^ v15 ^ result;
  v26 = HIDWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v25 ^ v15 ^ result)) ^ *((_DWORD *)&t_fl[256]
                                                                                             + ((v25 ^ v15 ^ (unsigned int)result) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v25 ^ v15 ^ result) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v25 ^ v15 ^ (unsigned int)result) >> 16));
  v27 = v26 ^ v23;
  *(_DWORD *)(a2 + 144) = v27;
  v28 = v21 ^ v26;
  *(_DWORD *)(a2 + 148) = v28;
  *(_DWORD *)(a2 + 152) = v24 ^ v28;
  *(_DWORD *)(a2 + 156) = result ^ v28;
  *(_DWORD *)(a2 + 160) = v25 ^ result ^ v28;
  v29 = v28 ^ v15;
  *(_DWORD *)(a2 + 164) = v29;
  v30 = LODWORD(t_rc[3]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v29) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v29 >> 24)) ^ v27 ^ *((_DWORD *)t_fl + BYTE1(v29)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v29 >> 16));
  *(_DWORD *)(a2 + 168) = v30;
  *(_DWORD *)(a2 + 172) = v28 ^ v30;
  v31 = v30 ^ v24;
  *(_DWORD *)(a2 + 176) = v31;
  *(_DWORD *)(a2 + 180) = v31 ^ result ^ v28;
  v32 = v31 ^ v25;
  *(_DWORD *)(a2 + 184) = v32;
  v33 = v29 ^ v32;
  *(_DWORD *)(a2 + 188) = v33;
  v34 = HIDWORD(t_rc[3]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v33) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v33 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v33)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v33 >> 16));
  *(_DWORD *)(a2 + 192) = v34 ^ v30;
  v35 = v34 ^ v28;
  *(_DWORD *)(a2 + 196) = v35;
  *(_DWORD *)(a2 + 200) = v31 ^ v35;
  *(_DWORD *)(a2 + 204) = result ^ v34;
  *(_DWORD *)(a2 + 240) = 12;
  return result;
}
// 50970: using guessed type __int64 t_rc[6];
// 519A0: using guessed type __int64 t_fl[512];

//----- (00000000000114D8) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_encrypt_key256(__int64 a1, __int64 a2)
{
  int v2; // eax@1
  int v3; // ecx@1
  int v4; // er14@1
  int v5; // ST0C_4@1
  int v6; // er10@1
  int v7; // ST08_4@1
  int v8; // ST04_4@1
  unsigned int v9; // ST10_4@1
  int v10; // eax@1
  int v11; // ST1C_4@1
  int v12; // er10@1
  int v13; // edx@1
  int v14; // er13@1
  int v15; // eax@1
  int v16; // er13@1
  int v17; // er15@1
  int v18; // er12@1
  int v19; // er10@1
  int v20; // er12@1
  int v21; // eax@1
  int v22; // ST1C_4@1
  int v23; // ebx@1
  int v24; // er15@1
  int v25; // er10@1
  int v26; // edx@1
  int v27; // er8@1
  int v28; // er9@1
  int v29; // eax@1
  int v30; // er13@1
  int v31; // er9@1
  int v32; // er14@1
  int v33; // er10@1
  int v34; // er12@1
  int v35; // er14@1
  int v36; // eax@1
  int v37; // ST1C_4@1
  int v38; // er10@1
  int v39; // edi@1
  int v40; // edi@1
  int v41; // eax@1
  int v42; // edi@1
  int v43; // er9@1
  int v44; // ecx@1
  int v45; // ecx@1
  int v46; // ebx@1
  int v47; // ecx@1
  int v48; // eax@1
  int v49; // eax@1
  int v50; // eax@1
  __int64 result; // rax@1

  v2 = *(_DWORD *)a1;
  *(_DWORD *)a2 = *(_DWORD *)a1;
  v3 = *(_DWORD *)(a1 + 4);
  *(_DWORD *)(a2 + 4) = v3;
  v4 = *(_DWORD *)(a1 + 8);
  *(_DWORD *)(a2 + 8) = v4;
  v5 = *(_DWORD *)(a1 + 12);
  *(_DWORD *)(a2 + 12) = v5;
  v6 = *(_DWORD *)(a1 + 16);
  *(_DWORD *)(a2 + 16) = v6;
  v7 = *(_DWORD *)(a1 + 20);
  *(_DWORD *)(a2 + 20) = v7;
  v8 = *(_DWORD *)(a1 + 24);
  *(_DWORD *)(a2 + 24) = v8;
  v9 = *(_DWORD *)(a1 + 28);
  *(_DWORD *)(a2 + 28) = v9;
  v10 = LODWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v9) ^ *((_DWORD *)&t_fl[256] + (v9 >> 24)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(v9 >> 16)) ^ *((_DWORD *)t_fl + BYTE1(v9)) ^ v2;
  *(_DWORD *)(a2 + 32) = v10;
  *(_DWORD *)(a2 + 36) = v3 ^ v10;
  v11 = v4 ^ v3 ^ v10;
  *(_DWORD *)(a2 + 40) = v11;
  *(_DWORD *)(a2 + 44) = v5 ^ v11;
  v12 = *((_DWORD *)&t_fl[384] + ((v5 ^ (unsigned int)v11) >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                     + (unsigned __int8)((v5 ^ (unsigned int)v11) >> 16)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(BYTE1(v5) ^ BYTE1(v11))) ^ *((_DWORD *)t_fl + (unsigned __int8)(v5 ^ v11)) ^ v6;
  *(_DWORD *)(a2 + 48) = v12;
  *(_DWORD *)(a2 + 52) = v7 ^ v12;
  v13 = v8 ^ v7 ^ v12;
  *(_DWORD *)(a2 + 56) = v13;
  *(_DWORD *)(a2 + 60) = v9 ^ v13;
  v14 = HIDWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v9 ^ v13)) ^ *((_DWORD *)&t_fl[256]
                                                                                   + ((v9 ^ v13) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v9 ^ v13) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v9 ^ v13) >> 16));
  v15 = v14 ^ v10;
  *(_DWORD *)(a2 + 64) = v15;
  v16 = v3 ^ v14;
  *(_DWORD *)(a2 + 68) = v16;
  v17 = v11;
  *(_DWORD *)(a2 + 72) = v11 ^ v16;
  *(_DWORD *)(a2 + 76) = v5 ^ v16;
  v18 = *((_DWORD *)&t_fl[384] + ((v5 ^ (unsigned int)v16) >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                     + (unsigned __int8)((v5 ^ (unsigned int)v16) >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)(v5 ^ v16)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned __int16)(v5 ^ v16) >> 8));
  v19 = v18 ^ v12;
  *(_DWORD *)(a2 + 80) = v19;
  v20 = v7 ^ v18;
  *(_DWORD *)(a2 + 84) = v20;
  *(_DWORD *)(a2 + 88) = v13 ^ v20;
  *(_DWORD *)(a2 + 92) = v9 ^ v20;
  v21 = LODWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v9 ^ v20)) ^ *((_DWORD *)&t_fl[256]
                                                                                   + ((v9 ^ v20) >> 24)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v9 ^ v20) >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)(BYTE1(v9) ^ BYTE1(v20))) ^ v15;
  *(_DWORD *)(a2 + 96) = v21;
  *(_DWORD *)(a2 + 100) = v16 ^ v21;
  v22 = v21 ^ v11;
  *(_DWORD *)(a2 + 104) = v22;
  v23 = v22 ^ v5 ^ v16;
  v24 = v21 ^ v17;
  *(_DWORD *)(a2 + 108) = v23;
  v25 = *((_DWORD *)&t_fl[384] + ((unsigned int)v23 >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                              + (unsigned __int8)((unsigned int)v23 >> 16)) ^ *((_DWORD *)&t_fl[128] + BYTE1(v23)) ^ *((_DWORD *)t_fl + (unsigned __int8)v23) ^ v19;
  *(_DWORD *)(a2 + 112) = v25;
  *(_DWORD *)(a2 + 116) = v20 ^ v25;
  v26 = v25 ^ v13;
  *(_DWORD *)(a2 + 120) = v26;
  v27 = v26 ^ v9 ^ v20;
  *(_DWORD *)(a2 + 124) = v27;
  v28 = HIDWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v27) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v27 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v27)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v27 >> 16));
  v29 = v28 ^ v21;
  *(_DWORD *)(a2 + 128) = v29;
  v30 = v28 ^ v16;
  *(_DWORD *)(a2 + 132) = v30;
  *(_DWORD *)(a2 + 136) = v24 ^ v30;
  v31 = v5 ^ v28;
  *(_DWORD *)(a2 + 140) = v31;
  v32 = *((_DWORD *)&t_fl[384] + ((unsigned int)v31 >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                              + (unsigned __int8)((unsigned int)v31 >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)v31) ^ *((_DWORD *)&t_fl[128] + BYTE1(v31));
  v33 = v32 ^ v25;
  *(_DWORD *)(a2 + 144) = v33;
  v34 = v32 ^ v20;
  *(_DWORD *)(a2 + 148) = v34;
  *(_DWORD *)(a2 + 152) = v26 ^ v34;
  v35 = v9 ^ v32;
  *(_DWORD *)(a2 + 156) = v35;
  v36 = LODWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v35) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v35 >> 24)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v35 >> 16)) ^ *((_DWORD *)t_fl + BYTE1(v35)) ^ v29;
  *(_DWORD *)(a2 + 160) = v36;
  *(_DWORD *)(a2 + 164) = v30 ^ v36;
  v37 = v36 ^ v22;
  *(_DWORD *)(a2 + 168) = v37;
  *(_DWORD *)(a2 + 172) = v31 ^ v37;
  v38 = *((_DWORD *)&t_fl[384] + ((v31 ^ (unsigned int)v37) >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                      + (unsigned __int8)((v31 ^ (unsigned int)v37) >> 16)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned __int16)(v31 ^ v37) >> 8)) ^ *((_DWORD *)t_fl + (unsigned __int8)(v31 ^ v37)) ^ v33;
  *(_DWORD *)(a2 + 176) = v38;
  *(_DWORD *)(a2 + 180) = v34 ^ v38;
  v39 = *((_DWORD *)&t_fl[256] + ((v35 ^ v38 ^ (unsigned int)v26) >> 24)) ^ *((_DWORD *)t_fl
                                                                            + (unsigned __int8)((unsigned __int16)(v35 ^ v38 ^ v26) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v35 ^ v38 ^ (unsigned int)v26) >> 16));
  *(_DWORD *)(a2 + 184) = v38 ^ v26;
  *(_DWORD *)(a2 + 188) = v35 ^ v38 ^ v26;
  v40 = HIDWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v35 ^ v38 ^ v26)) ^ v39;
  v41 = v40 ^ v36;
  *(_DWORD *)(a2 + 192) = v41;
  v42 = v30 ^ v40;
  *(_DWORD *)(a2 + 196) = v42;
  *(_DWORD *)(a2 + 200) = v37 ^ v42;
  v43 = v42 ^ v31;
  v44 = *((_DWORD *)&t_fl[384] + ((unsigned int)v43 >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                              + (unsigned __int8)((unsigned int)v43 >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)v43) ^ *((_DWORD *)&t_fl[128] + BYTE1(v43));
  *(_DWORD *)(a2 + 204) = v43;
  *(_DWORD *)(a2 + 208) = v44 ^ v38;
  v45 = v34 ^ v44;
  *(_DWORD *)(a2 + 212) = v45;
  v46 = v45 ^ v38 ^ v26;
  v47 = v35 ^ v45;
  v48 = *((_DWORD *)&t_fl[384] + (unsigned __int8)v47) ^ *((_DWORD *)&t_fl[256] + ((unsigned int)v47 >> 24)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v47 >> 16)) ^ *((_DWORD *)t_fl + BYTE1(v47)) ^ v41;
  *(_DWORD *)(a2 + 216) = v46;
  *(_DWORD *)(a2 + 220) = v47;
  v49 = LODWORD(t_rc[3]) ^ v48;
  *(_DWORD *)(a2 + 224) = v49;
  *(_DWORD *)(a2 + 228) = v49 ^ v42;
  v50 = v37 ^ v49;
  *(_DWORD *)(a2 + 232) = v50;
  result = v43 ^ (unsigned int)v50;
  *(_DWORD *)(a2 + 236) = result;
  *(_DWORD *)(a2 + 240) = 14;
  return result;
}
// 50970: using guessed type __int64 t_rc[6];
// 519A0: using guessed type __int64 t_fl[512];

//----- (0000000000011A26) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_encrypt_key(__int64 a1, signed __int64 a2, __int64 a3)
{
  __int64 result; // rax@9

  if ( a2 > 255 )
  {
    if ( a2 != 256 )
      return result;
    return ccaes_gladman_encrypt_key256(a1, a3);
  }
  if ( a2 > 191 )
  {
    if ( a2 != 192 )
      return result;
    return ccaes_gladman_encrypt_key192(a1, a3);
  }
  if ( a2 > 127 )
  {
    if ( a2 != 128 )
      return result;
  }
  else if ( a2 != 16 )
  {
    if ( a2 != 24 )
    {
      if ( a2 != 32 )
        return result;
      return ccaes_gladman_encrypt_key256(a1, a3);
    }
    return ccaes_gladman_encrypt_key192(a1, a3);
  }
  return ccaes_gladman_encrypt_key128(a1, a3);
}

//----- (0000000000011A8E) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_decrypt_key128(__int64 a1, __int64 a2)
{
  __int64 v2; // r13@1
  int v3; // ebx@1
  int v4; // edx@1
  int v5; // ecx@1
  unsigned int v6; // er8@1
  int v7; // er11@1
  int v8; // er12@1
  int v9; // eax@1
  int v10; // er10@1
  int v11; // eax@1
  int v12; // edx@1
  int v13; // ecx@1
  int v14; // ST2C_4@1
  int v15; // ST28_4@1
  int v16; // ecx@1
  int v17; // ST24_4@1
  int v18; // ecx@1
  int v19; // ST20_4@1
  int v20; // edi@1
  int v21; // ecx@1
  int v22; // er14@1
  int v23; // esi@1
  int v24; // edx@1
  int v25; // ST1C_4@1
  int v26; // er8@1
  int v27; // eax@1
  int v28; // ST30_4@1
  int v29; // er12@1
  int v30; // eax@1
  int v31; // ST10_4@1
  int v32; // er11@1
  int v33; // eax@1
  int v34; // ST08_4@1
  int v35; // er10@1
  int v36; // eax@1
  int v37; // ST00_4@1
  int v38; // er9@1
  int v39; // eax@1
  int v40; // ST30_4@1
  int v41; // er8@1
  int v42; // esi@1
  int v43; // edx@1
  int v44; // eax@1
  int v45; // edx@1
  int v46; // er14@1
  int v47; // eax@1
  int v48; // er14@1
  int v49; // ecx@1
  int v50; // er12@1
  int v51; // er11@1
  int v52; // eax@1
  int v53; // er11@1
  int v54; // er10@1
  int v55; // er9@1
  int v56; // eax@1
  int v57; // er10@1
  int v58; // esi@1
  __int64 result; // rax@1

  v2 = a2;
  v3 = *(_DWORD *)a1;
  *(_DWORD *)a2 = *(_DWORD *)a1;
  v4 = *(_DWORD *)(a1 + 4);
  *(_DWORD *)(a2 + 4) = v4;
  v5 = *(_DWORD *)(a1 + 8);
  *(_DWORD *)(a2 + 8) = v5;
  v6 = *(_DWORD *)(a1 + 12);
  *(_DWORD *)(a2 + 12) = v6;
  v7 = v4 ^ v6;
  v8 = v5 ^ v6;
  v9 = LODWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v6) ^ *((_DWORD *)&t_fl[256] + (v6 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v6)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(v6 >> 16));
  v10 = v9 ^ v6 ^ v5 ^ v3 ^ v4;
  v11 = v3 ^ v9;
  v12 = v11 ^ v4;
  *(_DWORD *)(a2 + 16) = *((_DWORD *)&t_im[384] + ((unsigned int)v11 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                               + (unsigned __int8)((unsigned int)v11 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v11) ^ *((_DWORD *)&t_im[128] + BYTE1(v11));
  v13 = v12 ^ v5;
  v14 = *((_DWORD *)&t_im[384] + ((unsigned int)v12 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v12 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v12) ^ *((_DWORD *)&t_im[128] + BYTE1(v12));
  *(_DWORD *)(a2 + 20) = v14;
  v15 = *((_DWORD *)&t_im[384] + ((unsigned int)v13 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v13) ^ *((_DWORD *)&t_im[128] + BYTE1(v13));
  *(_DWORD *)(a2 + 24) = v15;
  v16 = *(_DWORD *)(a2 + 12) ^ v13;
  v17 = *((_DWORD *)&t_im[384] + ((unsigned int)v16 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v16 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v16) ^ *((_DWORD *)&t_im[128] + BYTE1(v16));
  v18 = HIDWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v10) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v10 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v10)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v10 >> 16));
  v19 = *((_DWORD *)&t_im[384] + ((unsigned int)v18 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v18 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(BYTE4(t_rc[0]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v10) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v10 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v10)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v10 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(WORD2(t_rc[0]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v10) ^ *((_WORD *)&t_fl[256] + 2 * ((unsigned int)v10 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v10)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)((unsigned int)v10 >> 16))) >> 8));
  v20 = v18 ^ v7;
  v21 = LODWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v20) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v20 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v20)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v20 >> 16));
  v22 = *((_DWORD *)&t_im[384] + ((unsigned int)v21 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v21 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(LOBYTE(t_rc[1]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v20) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v20 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v20)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v20 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(LOWORD(t_rc[1]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v20) ^ *((_WORD *)&t_fl[256] + 2 * ((unsigned int)v20 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v20)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)((unsigned int)v20 >> 16))) >> 8));
  v23 = v21 ^ v8;
  v24 = *((_DWORD *)&t_im[256]
        + (unsigned __int8)((HIDWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v23) ^ *((_DWORD *)&t_fl[256]
                                                                                                 + ((unsigned int)v23 >> 24)) ^ (unsigned int)(*((_DWORD *)t_fl + BYTE1(v23)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v23 >> 16)))) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(BYTE4(t_rc[1]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v23) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v23 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v23)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v23 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(WORD2(t_rc[1]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v23) ^ *((_WORD *)&t_fl[256] + 2 * ((unsigned int)v23 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v23)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)((unsigned int)v23 >> 16))) >> 8));
  v25 = *((_DWORD *)&t_im[384]
        + ((HIDWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v23) ^ *((_DWORD *)&t_fl[256]
                                                                                + ((unsigned int)v23 >> 24)) ^ (unsigned int)(*((_DWORD *)t_fl + BYTE1(v23)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v23 >> 16)))) >> 24)) ^ v24;
  v26 = v6 ^ HIDWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v23) ^ *((_DWORD *)&t_fl[256]
                                                                                 + ((unsigned int)v23 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v23)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v23 >> 16));
  v27 = LODWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v26) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v26 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v26)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v26 >> 16));
  v28 = v27 ^ v10;
  v29 = *((_DWORD *)&t_im[384] + ((unsigned int)v27 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v27 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(LOBYTE(t_rc[2]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v26) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v26 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v26)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v26 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(LOWORD(t_rc[2]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v26) ^ *((_WORD *)&t_fl[256] + 2 * ((unsigned int)v26 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v26)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)((unsigned int)v26 >> 16))) >> 8));
  v30 = HIDWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v28) ^ *((_DWORD *)&t_fl[256]
                                                                            + ((unsigned int)v28 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v28)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v28 >> 16));
  v31 = v30 ^ v20;
  v32 = *((_DWORD *)&t_im[384] + ((unsigned int)v30 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v30 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(BYTE4(t_rc[2]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v28) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v28 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v28)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v28 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(WORD2(t_rc[2]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v28) ^ *((_WORD *)&t_fl[256] + 2 * ((unsigned int)v28 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v28)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)((unsigned int)v28 >> 16))) >> 8));
  v33 = LODWORD(t_rc[3]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v30 ^ v20)) ^ *((_DWORD *)&t_fl[256]
                                                                                    + ((v30 ^ (unsigned int)v20) >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v31)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v30 ^ (unsigned int)v20) >> 16));
  v34 = v33 ^ v23;
  v35 = *((_DWORD *)&t_im[384] + ((unsigned int)v33 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((LODWORD(t_rc[3]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v31) ^ *((_DWORD *)&t_fl[256] + ((unsigned int)v31 >> 24)) ^ (unsigned int)(*((_DWORD *)t_fl + BYTE1(v31)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v31 >> 16)))) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(LOBYTE(t_rc[3]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v31) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v31 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v31)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v31 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(LOWORD(t_rc[3]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v31) ^ *((_WORD *)&t_fl[256] + 2 * ((unsigned int)v31 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v31)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)((unsigned int)v31 >> 16))) >> 8));
  v36 = HIDWORD(t_rc[3]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v33 ^ v23)) ^ *((_DWORD *)&t_fl[256]
                                                                                    + ((v33 ^ (unsigned int)v23) >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v34)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v33 ^ (unsigned int)v23) >> 16));
  v37 = v36 ^ v26;
  v38 = *((_DWORD *)&t_im[384] + ((unsigned int)v36 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v36 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v36) ^ *((_DWORD *)&t_im[128] + BYTE1(v36));
  v39 = LODWORD(t_rc[4]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v36 ^ v26)) ^ *((_DWORD *)&t_fl[256]
                                                                                    + ((v36 ^ (unsigned int)v26) >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v37)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v36 ^ (unsigned int)v26) >> 16));
  v40 = v39 ^ v28;
  v41 = *((_DWORD *)&t_im[384] + ((unsigned int)v39 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v39 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(LOBYTE(t_rc[4]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v37) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v37 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v37)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v37 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(LOWORD(t_rc[4]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v37) ^ *((_WORD *)&t_fl[256] + 2 * ((unsigned int)v37 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v37)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)((unsigned int)v37 >> 16))) >> 8));
  v42 = *((_DWORD *)&t_fl[384] + (unsigned __int8)v40) ^ *((_DWORD *)&t_fl[256] + ((unsigned int)v40 >> 24)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v40 >> 16)) ^ *((_DWORD *)t_fl + BYTE1(v40)) ^ v31;
  *(_DWORD *)(v2 + 28) = v17;
  v43 = *(_DWORD *)(v2 + 16) ^ v19;
  *(_DWORD *)(v2 + 32) = v43;
  *(_DWORD *)(v2 + 36) = v14 ^ v43;
  v44 = v15 ^ v14 ^ v43;
  *(_DWORD *)(v2 + 40) = v44;
  *(_DWORD *)(v2 + 44) = v17 ^ v44;
  v45 = v22 ^ v43;
  v46 = v14 ^ v22;
  *(_DWORD *)(v2 + 48) = v45;
  *(_DWORD *)(v2 + 52) = v46;
  *(_DWORD *)(v2 + 56) = v44 ^ v46;
  *(_DWORD *)(v2 + 60) = v17 ^ v46;
  *(_DWORD *)(v2 + 64) = v45 ^ v25;
  *(_DWORD *)(v2 + 68) = v46 ^ v45 ^ v25;
  v47 = v45 ^ v25 ^ v44;
  *(_DWORD *)(v2 + 72) = v47;
  *(_DWORD *)(v2 + 76) = v47 ^ v17 ^ v46;
  v48 = v29 ^ v46;
  v49 = v29 ^ v45 ^ v25;
  *(_DWORD *)(v2 + 80) = v49;
  *(_DWORD *)(v2 + 84) = v48;
  *(_DWORD *)(v2 + 88) = v47 ^ v48;
  v50 = v17 ^ v29;
  *(_DWORD *)(v2 + 92) = v50;
  v51 = v49 ^ v32;
  *(_DWORD *)(v2 + 96) = v51;
  *(_DWORD *)(v2 + 100) = v48 ^ v51;
  v52 = v51 ^ v47;
  *(_DWORD *)(v2 + 104) = v52;
  *(_DWORD *)(v2 + 108) = v50 ^ v52;
  v53 = v35 ^ v51;
  *(_DWORD *)(v2 + 112) = v53;
  v54 = v48 ^ v35;
  *(_DWORD *)(v2 + 116) = v54;
  *(_DWORD *)(v2 + 120) = v52 ^ v54;
  *(_DWORD *)(v2 + 124) = v50 ^ v54;
  v55 = v53 ^ v38;
  *(_DWORD *)(v2 + 128) = v55;
  *(_DWORD *)(v2 + 132) = v54 ^ v55;
  v56 = v55 ^ v52;
  *(_DWORD *)(v2 + 136) = v56;
  *(_DWORD *)(v2 + 140) = v56 ^ v50 ^ v54;
  *(_DWORD *)(v2 + 144) = v41 ^ v55;
  v57 = v41 ^ v54;
  *(_DWORD *)(v2 + 148) = v57;
  *(_DWORD *)(v2 + 152) = v56 ^ v57;
  *(_DWORD *)(v2 + 156) = v50 ^ v41;
  v58 = HIDWORD(t_rc[4]) ^ v42;
  result = v58 ^ v40 ^ v37 ^ (unsigned int)v34;
  *(_DWORD *)(v2 + 160) = result;
  *(_DWORD *)(v2 + 164) = v58 ^ v37;
  *(_DWORD *)(v2 + 168) = v58 ^ v40;
  *(_DWORD *)(v2 + 172) = v58;
  *(_DWORD *)(v2 + 240) = 10;
  return result;
}
// 50970: using guessed type __int64 t_rc[6];
// 519A0: using guessed type __int64 t_fl[512];
// 549A0: using guessed type __int64 t_im[512];

//----- (0000000000012159) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_decrypt_key192(__int64 a1, __int64 a2)
{
  __int64 v2; // ST20_8@1
  int v3; // ebx@1
  int v4; // er12@1
  int v5; // er8@1
  int v6; // eax@1
  int v7; // er11@1
  int v8; // er9@1
  unsigned int v9; // er13@1
  __int64 v10; // rsi@1
  __int64 v11; // rcx@1
  __int64 v12; // rdi@1
  __int64 v13; // rax@1
  int v14; // ST44_4@1
  int v15; // er10@1
  int v16; // ST40_4@1
  int v17; // ST3C_4@1
  int v18; // er8@1
  int v19; // ST38_4@1
  int v20; // ST34_4@1
  int v21; // ST30_4@1
  int v22; // ebx@1
  __int64 v23; // r9@1
  __int64 v24; // rsi@1
  int v25; // edx@1
  __int64 v26; // rcx@1
  int v27; // ST2C_4@1
  int v28; // ST1C_4@1
  int v29; // er10@1
  unsigned int v30; // er13@1
  int v31; // er10@1
  int v32; // ST18_4@1
  int v33; // er8@1
  int v34; // er11@1
  int v35; // edx@1
  int v36; // er12@1
  int v37; // er10@1
  __int64 v38; // rdx@1
  __int64 v39; // ST10_8@1
  int v40; // er10@1
  int v41; // ebx@1
  int v42; // er11@1
  int v43; // er10@1
  int v44; // ST08_4@1
  int v45; // er11@1
  unsigned int v46; // er13@1
  int v47; // ST00_4@1
  int v48; // ST48_4@1
  unsigned int v49; // er13@1
  int v50; // er14@1
  int v51; // er13@1
  int v52; // er15@1
  int v53; // er12@1
  int v54; // er12@1
  int v55; // ebx@1
  int v56; // er11@1
  int v57; // er15@1
  int v58; // ebx@1
  int v59; // er14@1
  __int64 result; // rax@1

  v2 = a2;
  v3 = *(_DWORD *)a1;
  *(_DWORD *)a2 = *(_DWORD *)a1;
  v4 = *(_DWORD *)(a1 + 4);
  *(_DWORD *)(a2 + 4) = v4;
  v5 = *(_DWORD *)(a1 + 8);
  *(_DWORD *)(a2 + 8) = v5;
  v6 = *(_DWORD *)(a1 + 12);
  *(_DWORD *)(a2 + 12) = v6;
  v7 = v6;
  v8 = *(_DWORD *)(a1 + 16);
  *(_DWORD *)(a2 + 16) = *((_DWORD *)&t_im[384] + (*(_DWORD *)(a1 + 16) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                                  + (unsigned __int8)(*(_DWORD *)(a1 + 16) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)*(_DWORD *)(a1 + 16)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)(*(_WORD *)(a1 + 16) >> 8));
  v9 = *(_DWORD *)(a1 + 20);
  v10 = (unsigned __int8)*(_DWORD *)(a1 + 20);
  v11 = (unsigned __int8)(*(_WORD *)(a1 + 20) >> 8);
  v12 = (unsigned __int8)(*(_DWORD *)(a1 + 20) >> 16);
  v13 = v9 >> 24;
  v14 = *((_DWORD *)&t_im[384] + v13) ^ *((_DWORD *)&t_im[256] + v12) ^ *((_DWORD *)t_im + v10) ^ *((_DWORD *)&t_im[128]
                                                                                                  + v11);
  v15 = LODWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + v10) ^ *((_DWORD *)&t_fl[256] + v13) ^ *((_DWORD *)&t_fl[128] + v12) ^ *((_DWORD *)t_fl + v11) ^ v3;
  v16 = *((_DWORD *)&t_im[384] + ((unsigned int)v15 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v15 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v15) ^ *((_DWORD *)&t_im[128] + BYTE1(v15));
  v17 = *((_DWORD *)&t_im[384] + ((v4 ^ (unsigned int)v15) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                     + (unsigned __int8)((v4 ^ (unsigned int)v15) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(v4 ^ v15)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(v4 ^ v15) >> 8));
  v18 = v5 ^ v4 ^ v15;
  v19 = *((_DWORD *)&t_im[384] + ((unsigned int)v18 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v18 >> 16)) ^ *((_DWORD *)t_im + (unsigned int)v18) ^ *((_DWORD *)&t_im[128] + BYTE1(v18));
  v20 = *((_DWORD *)&t_im[384] + ((v7 ^ (unsigned int)v18) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                     + (unsigned __int8)((v7 ^ (unsigned int)v18) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(v7 ^ v18)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(v7 ^ v18) >> 8));
  v21 = *((_DWORD *)&t_im[384] + ((v8 ^ v7 ^ (unsigned int)v18) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                          + (unsigned __int8)((v8 ^ v7 ^ (unsigned int)v18) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(v8 ^ v7 ^ v18)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(v8 ^ v7 ^ v18) >> 8));
  v22 = v8 ^ v7 ^ v18;
  LODWORD(v11) = v9 ^ v8 ^ v7 ^ v18;
  v23 = (unsigned __int8)(v9 ^ v8 ^ v7 ^ v18);
  v24 = (unsigned __int8)((unsigned int)v11 >> 16);
  v25 = *((_DWORD *)&t_im[256] + v24) ^ *((_DWORD *)t_im + v23) ^ *((_DWORD *)&t_im[128] + BYTE1(v11));
  LODWORD(v24) = *((_DWORD *)t_fl + BYTE1(v11)) ^ *((_DWORD *)&t_fl[128] + v24);
  v26 = (unsigned int)v11 >> 24;
  v27 = *((_DWORD *)&t_im[384] + v26) ^ v25;
  LODWORD(v24) = HIDWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + v23) ^ *((_DWORD *)&t_fl[256] + v26) ^ v24;
  v28 = *((_DWORD *)&t_im[384] + ((unsigned int)v24 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v24 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v24) ^ *((_DWORD *)&t_im[128] + BYTE1(v24));
  v29 = v24 ^ v15;
  LODWORD(v23) = v4 ^ v24;
  LODWORD(v24) = v7;
  v30 = v23 ^ v7 ^ v9;
  LODWORD(v13) = LODWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v30) ^ *((_DWORD *)&t_fl[256] + (v30 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v30)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(v30 >> 16));
  v31 = v13 ^ v29;
  v32 = *((_DWORD *)&t_im[384] + ((unsigned int)v13 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(LOBYTE(t_rc[1]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v30) ^ *((_BYTE *)&t_fl[256] + 4 * (v30 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v30)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)(v30 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(LOWORD(t_rc[1]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v30) ^ *((_WORD *)&t_fl[256] + 2 * (v30 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v30)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)(v30 >> 16))) >> 8));
  v33 = v31 ^ v18;
  v34 = v22;
  LODWORD(v13) = v33 ^ v22 ^ v30;
  v35 = HIDWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v33 ^ v22 ^ v30)) ^ *((_DWORD *)&t_fl[256]
                                                                                          + ((v33 ^ v22 ^ v30) >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v13)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v33 ^ v22 ^ v30) >> 16));
  v36 = *((_DWORD *)&t_im[384] + ((unsigned int)v35 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v35 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(BYTE4(t_rc[1]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)(v33 ^ v22 ^ v30)) ^ *((_BYTE *)&t_fl[256] + 4 * ((v33 ^ v22 ^ v30) >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v13)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((v33 ^ v22 ^ v30) >> 16)))) ^ *((_DWORD *)&t_im[128] + BYTE1(v35));
  v37 = v35 ^ v31;
  LODWORD(v23) = v35 ^ v23;
  v38 = (unsigned int)v24 ^ v35;
  v39 = v38;
  LODWORD(v38) = v30 ^ v38;
  LODWORD(v13) = LODWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v38) ^ *((_DWORD *)&t_fl[256]
                                                                                     + ((unsigned int)v38 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v38)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v38 >> 16));
  v40 = v13 ^ v37;
  v41 = *((_DWORD *)&t_im[384] + ((unsigned int)v13 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(LOBYTE(t_rc[2]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v38) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v38 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v38)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v38 >> 16)))) ^ *((_DWORD *)&t_im[128] + BYTE1(v13));
  LODWORD(v24) = v40 ^ v34;
  LODWORD(v38) = v40 ^ v34 ^ v38;
  LODWORD(v13) = HIDWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v38) ^ *((_DWORD *)&t_fl[256]
                                                                                     + ((unsigned int)v38 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v38)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v38 >> 16));
  v42 = *((_DWORD *)&t_im[256] + (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_DWORD *)t_im
                                                                               + (unsigned __int8)(BYTE4(t_rc[2]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v38) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v38 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v38)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v38 >> 16)))) ^ *((_DWORD *)&t_im[128] + BYTE1(v13));
  LODWORD(v38) = v40 ^ v33;
  v43 = v13 ^ v40;
  v44 = v13 ^ v23;
  v45 = *((_DWORD *)&t_im[384] + ((unsigned int)v13 >> 24)) ^ v42;
  v46 = v13 ^ v23 ^ v30;
  LODWORD(v13) = LODWORD(t_rc[3]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v46) ^ *((_DWORD *)&t_fl[256] + (v46 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v46)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(v46 >> 16));
  v47 = v13 ^ v43;
  LODWORD(v23) = *((_DWORD *)&t_im[384] + ((unsigned int)v13 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                       + (unsigned __int8)((unsigned int)v13 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(LOBYTE(t_rc[3]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v46) ^ *((_BYTE *)&t_fl[256] + 4 * (v46 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v46)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)(v46 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(LOWORD(t_rc[3]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v46) ^ *((_WORD *)&t_fl[256] + 2 * (v46 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v46)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)(v46 >> 16))) >> 8));
  LODWORD(v13) = v13 ^ v43 ^ v38;
  v48 = v13;
  v49 = v13 ^ v24 ^ v46;
  v50 = *((_DWORD *)&t_fl[384] + (unsigned __int8)v49) ^ *((_DWORD *)&t_fl[256] + (v49 >> 24)) ^ *((_DWORD *)t_fl
                                                                                                 + BYTE1(v49)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(v49 >> 16));
  *(_DWORD *)(v2 + 20) = v14;
  *(_DWORD *)(v2 + 24) = v16;
  *(_DWORD *)(v2 + 28) = v17;
  *(_DWORD *)(v2 + 32) = v19;
  *(_DWORD *)(v2 + 36) = v20;
  *(_DWORD *)(v2 + 40) = v21;
  *(_DWORD *)(v2 + 44) = v27;
  LODWORD(v38) = *(_DWORD *)(v2 + 24) ^ v28;
  *(_DWORD *)(v2 + 48) = v38;
  v51 = *(_DWORD *)(v2 + 28);
  *(_DWORD *)(v2 + 52) = v51 ^ v38;
  LODWORD(v13) = *(_DWORD *)(v2 + 32) ^ v51 ^ v38;
  *(_DWORD *)(v2 + 56) = v13;
  *(_DWORD *)(v2 + 60) = v20 ^ v13;
  v52 = v21 ^ v20 ^ v13;
  *(_DWORD *)(v2 + 64) = v52;
  *(_DWORD *)(v2 + 68) = v27 ^ v52;
  LODWORD(v38) = v32 ^ v38;
  *(_DWORD *)(v2 + 72) = v38;
  *(_DWORD *)(v2 + 76) = v51 ^ v32;
  *(_DWORD *)(v2 + 80) = v13 ^ v51 ^ v32;
  *(_DWORD *)(v2 + 84) = v20 ^ v51 ^ v32;
  *(_DWORD *)(v2 + 88) = v52 ^ v20 ^ v51 ^ v32;
  v53 = v38 ^ v36;
  LODWORD(v24) = v20 ^ v51 ^ v32 ^ v27;
  *(_DWORD *)(v2 + 92) = v24;
  *(_DWORD *)(v2 + 96) = v53;
  *(_DWORD *)(v2 + 100) = v51 ^ v32 ^ v53;
  LODWORD(v13) = v53 ^ v13;
  *(_DWORD *)(v2 + 104) = v13;
  *(_DWORD *)(v2 + 108) = v13 ^ v20 ^ v51 ^ v32;
  *(_DWORD *)(v2 + 112) = v52 ^ v13;
  *(_DWORD *)(v2 + 116) = v24 ^ v52 ^ v13;
  LODWORD(v26) = v41 ^ v51 ^ v32;
  v54 = v41 ^ v53;
  *(_DWORD *)(v2 + 120) = v54;
  *(_DWORD *)(v2 + 124) = v26;
  *(_DWORD *)(v2 + 128) = v13 ^ v26;
  v55 = v20 ^ v41;
  *(_DWORD *)(v2 + 132) = v55;
  *(_DWORD *)(v2 + 136) = v55 ^ v52 ^ v13;
  v56 = v54 ^ v45;
  *(_DWORD *)(v2 + 140) = v24 ^ v55;
  *(_DWORD *)(v2 + 144) = v56;
  *(_DWORD *)(v2 + 148) = v26 ^ v56;
  LODWORD(v13) = v56 ^ v13;
  *(_DWORD *)(v2 + 152) = v13;
  *(_DWORD *)(v2 + 156) = v55 ^ v13;
  v57 = v56 ^ v52;
  *(_DWORD *)(v2 + 160) = v57;
  *(_DWORD *)(v2 + 164) = v57 ^ v24 ^ v55;
  *(_DWORD *)(v2 + 168) = v23 ^ v56;
  LODWORD(v23) = v26 ^ v23;
  *(_DWORD *)(v2 + 172) = v23;
  *(_DWORD *)(v2 + 176) = v23 ^ v13;
  v58 = v23 ^ v55;
  *(_DWORD *)(v2 + 180) = v58;
  *(_DWORD *)(v2 + 184) = v57 ^ v58;
  *(_DWORD *)(v2 + 188) = v24 ^ v23;
  v59 = HIDWORD(t_rc[3]) ^ v50;
  *(_DWORD *)(v2 + 192) = v59 ^ v47;
  *(_DWORD *)(v2 + 196) = v59 ^ v44;
  *(_DWORD *)(v2 + 200) = v48 ^ v59 ^ v44;
  result = v39;
  *(_DWORD *)(v2 + 204) = v39 ^ v59;
  *(_DWORD *)(v2 + 240) = 12;
  return result;
}
// 50970: using guessed type __int64 t_rc[6];
// 519A0: using guessed type __int64 t_fl[512];
// 549A0: using guessed type __int64 t_im[512];

//----- (00000000000128A0) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_decrypt_key256(__int64 a1, __int64 a2)
{
  __int64 v2; // r8@1
  int v3; // ebx@1
  int v4; // eax@1
  int v5; // ST60_4@1
  int v6; // edx@1
  int v7; // er9@1
  int v8; // eax@1
  int v9; // ST4C_4@1
  int v10; // er10@1
  int v11; // esi@1
  int v12; // er12@1
  int v13; // er11@1
  unsigned int v14; // ST40_4@1
  __int64 v15; // rax@1
  int v16; // ecx@1
  int v17; // ebx@1
  __int64 v18; // rax@1
  int v19; // ST3C_4@1
  int v20; // ST50_4@1
  int v21; // ST38_4@1
  int v22; // ST34_4@1
  int v23; // edi@1
  int v24; // ST58_4@1
  int v25; // ST30_4@1
  int v26; // edi@1
  __int64 v27; // rax@1
  int v28; // ebx@1
  int v29; // edx@1
  __int64 v30; // rdi@1
  int v31; // ST2C_4@1
  int v32; // edx@1
  int v33; // ST28_4@1
  int v34; // ST24_4@1
  int v35; // er10@1
  int v36; // ST20_4@1
  int v37; // ST08_4@1
  __int64 v38; // r9@1
  __int64 v39; // rsi@1
  __int64 v40; // rdi@1
  int v41; // ebx@1
  int v42; // ecx@1
  __int64 v43; // rsi@1
  int v44; // ST1C_4@1
  int v45; // ecx@1
  int v46; // ST18_4@1
  int v47; // ecx@1
  int v48; // er13@1
  int v49; // ST14_4@1
  int v50; // edx@1
  int v51; // er13@1
  int v52; // er14@1
  int v53; // ST10_4@1
  unsigned int v54; // ST58_4@1
  int v55; // ST04_4@1
  unsigned int v56; // edx@1
  int v57; // er10@1
  int v58; // er12@1
  int v59; // er12@1
  int v60; // er11@1
  int v61; // ST60_4@1
  int v62; // er10@1
  int v63; // er14@1
  int v64; // ecx@1
  int v65; // ST50_4@1
  __int64 v66; // rax@1
  __int64 v67; // ST58_8@1
  int v68; // ecx@1
  int v69; // ST60_4@1
  unsigned int v70; // er10@1
  int v71; // er11@1
  int v72; // er13@1
  int v73; // edx@1
  int v74; // ecx@1
  int v75; // er12@1
  int v76; // er12@1
  int v77; // er12@1
  int v78; // er14@1
  int v79; // er14@1
  int v80; // er14@1
  int v81; // edx@1
  int v82; // edx@1
  int v83; // edx@1
  int v84; // ecx@1
  __int64 result; // rax@1
  int v86; // ecx@1

  v2 = a2;
  v3 = *(_DWORD *)a1;
  *(_DWORD *)a2 = *(_DWORD *)a1;
  v4 = *(_DWORD *)(a1 + 4);
  v5 = v4;
  *(_DWORD *)(a2 + 4) = v4;
  v6 = v4;
  v7 = *(_DWORD *)(a1 + 8);
  *(_DWORD *)(a2 + 8) = v7;
  v8 = *(_DWORD *)(a1 + 12);
  v9 = v8;
  *(_DWORD *)(a2 + 12) = v8;
  v10 = v8;
  v11 = *(_DWORD *)(a1 + 16);
  *(_DWORD *)(v2 + 16) = *((_DWORD *)&t_im[384] + (*(_DWORD *)(a1 + 16) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                                  + (unsigned __int8)(*(_DWORD *)(a1 + 16) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)*(_DWORD *)(a1 + 16)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)(*(_WORD *)(a1 + 16) >> 8));
  v12 = *(_DWORD *)(a1 + 20);
  *(_DWORD *)(v2 + 20) = *((_DWORD *)&t_im[384] + (*(_DWORD *)(a1 + 20) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                                  + (unsigned __int8)(*(_DWORD *)(a1 + 20) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)*(_DWORD *)(a1 + 20)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)(*(_WORD *)(a1 + 20) >> 8));
  v13 = *(_DWORD *)(a1 + 24);
  *(_DWORD *)(v2 + 24) = *((_DWORD *)&t_im[384] + (*(_DWORD *)(a1 + 24) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                                  + (unsigned __int8)(*(_DWORD *)(a1 + 24) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)*(_DWORD *)(a1 + 24)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)(*(_WORD *)(a1 + 24) >> 8));
  v14 = *(_DWORD *)(a1 + 28);
  v15 = (unsigned __int8)(v14 >> 16);
  v16 = *((_DWORD *)&t_im[256] + v15) ^ *((_DWORD *)t_im + (unsigned __int8)v14) ^ *((_DWORD *)&t_im[128] + BYTE1(v14));
  v17 = *((_DWORD *)&t_fl[128] + v15) ^ *((_DWORD *)t_fl + BYTE1(v14)) ^ v3;
  v18 = *(_DWORD *)(a1 + 28) >> 24;
  v19 = *((_DWORD *)&t_im[384] + v18) ^ v16;
  v20 = LODWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v14) ^ *((_DWORD *)&t_fl[256] + v18) ^ v17;
  v21 = *((_DWORD *)&t_im[384] + ((unsigned int)v20 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v20 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v20) ^ *((_DWORD *)&t_im[128] + BYTE1(v20));
  v22 = *((_DWORD *)&t_im[384] + ((v6 ^ (unsigned int)v20) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                     + (unsigned __int8)((v6 ^ (unsigned int)v20) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(v6 ^ v20)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(v6 ^ v20) >> 8));
  v23 = v7 ^ v6 ^ v20;
  v24 = v23;
  v25 = *((_DWORD *)&t_im[384] + ((v7 ^ v6 ^ (unsigned int)v20) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                          + (unsigned __int8)((v7 ^ v6 ^ (unsigned int)v20) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(v7 ^ v6 ^ v20)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(v7 ^ v6 ^ v20) >> 8));
  v26 = v10 ^ v23;
  v27 = (unsigned __int8)((unsigned int)v26 >> 16);
  v28 = *((_DWORD *)&t_im[256] + v27) ^ *((_DWORD *)t_im + (unsigned __int8)v26) ^ *((_DWORD *)&t_im[128] + BYTE1(v26));
  v29 = *((_DWORD *)&t_fl[256] + v27) ^ *((_DWORD *)&t_fl[128] + BYTE1(v26)) ^ v11 ^ *((_DWORD *)t_fl
                                                                                     + (unsigned __int8)v26);
  v30 = (unsigned int)v26 >> 24;
  v31 = *((_DWORD *)&t_im[384] + v30) ^ v28;
  v32 = *((_DWORD *)&t_fl[384] + v30) ^ v29;
  v33 = *((_DWORD *)&t_im[384] + ((unsigned int)v32 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v32 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v32) ^ *((_DWORD *)&t_im[128] + BYTE1(v32));
  v34 = *((_DWORD *)&t_im[384] + ((v12 ^ (unsigned int)v32) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                      + (unsigned __int8)((v12 ^ (unsigned int)v32) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(v12 ^ v32)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(v12 ^ v32) >> 8));
  v35 = v13 ^ v12 ^ v32;
  v36 = *((_DWORD *)&t_im[384] + ((v13 ^ v12 ^ (unsigned int)v32) >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                            + (unsigned __int8)((v13 ^ v12 ^ (unsigned int)v32) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(v13 ^ v12 ^ v32)) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(v13 ^ v12 ^ v32) >> 8));
  v37 = v14 ^ v13 ^ v12 ^ v32;
  v38 = (unsigned __int8)(v14 ^ v13 ^ v12 ^ v32);
  v39 = (unsigned __int8)((unsigned __int16)(v14 ^ v13 ^ v12 ^ v32) >> 8);
  v40 = (unsigned __int8)((v14 ^ v13 ^ v12 ^ v32) >> 16);
  v41 = *((_DWORD *)&t_im[256] + v40) ^ *((_DWORD *)t_im + v38) ^ *((_DWORD *)&t_im[128] + v39);
  v42 = *((_DWORD *)t_fl + v39) ^ *((_DWORD *)&t_fl[128] + v40);
  v43 = (v14 ^ v13 ^ v12 ^ v32) >> 24;
  v44 = *((_DWORD *)&t_im[384] + v43) ^ v41;
  v45 = HIDWORD(t_rc[0]) ^ *((_DWORD *)&t_fl[384] + v38) ^ *((_DWORD *)&t_fl[256] + v43) ^ v42;
  v46 = *((_DWORD *)&t_im[384] + ((unsigned int)v45 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v45 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v45) ^ *((_DWORD *)&t_im[128] + BYTE1(v45));
  LODWORD(v38) = v45 ^ v20;
  v47 = v5 ^ v45;
  v48 = *((_DWORD *)&t_fl[384] + ((v9 ^ (unsigned int)v47) >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                     + (unsigned __int8)((v9 ^ (unsigned int)v47) >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)(v9 ^ v47)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned __int16)(v9 ^ v47) >> 8));
  v49 = *((_DWORD *)&t_im[384] + ((unsigned int)v48 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v48 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(*((_BYTE *)&t_fl[384] + 4 * ((v9 ^ (unsigned int)v47) >> 24)) ^ *((_BYTE *)&t_fl[256] + 4 * (unsigned __int8)((v9 ^ (unsigned int)v47) >> 16)) ^ *((_BYTE *)t_fl + 4 * (unsigned __int8)(v9 ^ v47)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned __int16)(v9 ^ v47) >> 8)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_fl[384] + 2 * ((v9 ^ (unsigned int)v47) >> 24)) ^ *((_WORD *)&t_fl[256] + 2 * (unsigned __int8)((v9 ^ (unsigned int)v47) >> 16)) ^ *((_WORD *)t_fl + 2 * (unsigned __int8)(v9 ^ v47)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)((unsigned __int16)(v9 ^ v47) >> 8))) >> 8));
  v50 = v48 ^ v32;
  v51 = v14 ^ v12 ^ v48;
  LODWORD(v27) = LODWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v51) ^ *((_DWORD *)&t_fl[256]
                                                                                     + ((unsigned int)v51 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v51)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v51 >> 16));
  v52 = v27 ^ v38;
  v53 = *((_DWORD *)&t_im[384] + ((unsigned int)v27 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v27 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(LOBYTE(t_rc[1]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v51) ^ *((_BYTE *)&t_fl[256] + 4 * ((unsigned int)v51 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v51)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((unsigned int)v51 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(LOWORD(t_rc[1]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v51) ^ *((_WORD *)&t_fl[256] + 2 * ((unsigned int)v51 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v51)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)((unsigned int)v51 >> 16))) >> 8));
  LODWORD(v27) = v27 ^ v38 ^ v24;
  v54 = v27;
  LODWORD(v40) = v27 ^ v9 ^ v47;
  LODWORD(v27) = *((_DWORD *)&t_fl[384] + (((unsigned int)v27 ^ v9 ^ v47) >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                                    + (unsigned __int8)(((unsigned int)v27 ^ v9 ^ v47) >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)(v27 ^ v9 ^ v47)) ^ *((_DWORD *)&t_fl[128] + BYTE1(v40));
  v55 = *((_DWORD *)&t_im[384] + ((unsigned int)v27 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v27 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v27) ^ *((_DWORD *)&t_im[128] + BYTE1(v27));
  v56 = v51 ^ v35 ^ v27 ^ v50;
  v57 = HIDWORD(t_rc[1]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)v56) ^ *((_DWORD *)&t_fl[256] + (v56 >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v56)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(v56 >> 16));
  v58 = *((_DWORD *)&t_im[256] + (unsigned __int8)((unsigned int)v57 >> 16)) ^ *((_DWORD *)t_im
                                                                               + (unsigned __int8)(BYTE4(t_rc[1]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)v56) ^ *((_BYTE *)&t_fl[256] + 4 * (v56 >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v56)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)(v56 >> 16)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(WORD2(t_rc[1]) ^ *((_WORD *)&t_fl[384] + 2 * (unsigned __int8)v56) ^ *((_WORD *)&t_fl[256] + 2 * (v56 >> 24)) ^ *((_WORD *)t_fl + 2 * BYTE1(v56)) ^ *((_WORD *)&t_fl[128] + 2 * (unsigned __int8)(v56 >> 16))) >> 8));
  v59 = *((_DWORD *)&t_im[384] + ((unsigned int)v57 >> 24)) ^ v58;
  v60 = v57 ^ v52;
  v61 = v57 ^ v47;
  v62 = v9 ^ v57;
  v63 = *((_DWORD *)&t_im[384]
        + ((*((_DWORD *)&t_fl[384] + ((unsigned int)v62 >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                  + (unsigned __int8)((unsigned int)v62 >> 16)) ^ (unsigned int)(*((_DWORD *)t_fl + (unsigned __int8)v62) ^ *((_DWORD *)&t_fl[128] + BYTE1(v62)))) >> 24)) ^ *((_DWORD *)&t_im[256] + (unsigned __int8)((*((_DWORD *)&t_fl[384] + ((unsigned int)v62 >> 24)) ^ *((_DWORD *)&t_fl[256] + (unsigned __int8)((unsigned int)v62 >> 16)) ^ (unsigned int)(*((_DWORD *)t_fl + (unsigned __int8)v62) ^ *((_DWORD *)&t_fl[128] + BYTE1(v62)))) >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(*((_BYTE *)&t_fl[384] + 4 * ((unsigned int)v62 >> 24)) ^ *((_BYTE *)&t_fl[256] + 4 * (unsigned __int8)((unsigned int)v62 >> 16)) ^ *((_BYTE *)t_fl + 4 * (unsigned __int8)v62) ^ *((_BYTE *)&t_fl[128] + 4 * BYTE1(v62)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_fl[384] + 2 * ((unsigned int)v62 >> 24)) ^ *((_WORD *)&t_fl[256] + 2 * (unsigned __int8)((unsigned int)v62 >> 16)) ^ *((_WORD *)t_fl + 2 * (unsigned __int8)v62) ^ *((_WORD *)&t_fl[128] + 2 * BYTE1(v62))) >> 8));
  LODWORD(v27) = v14 ^ *((_DWORD *)&t_fl[384] + ((unsigned int)v62 >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                             + (unsigned __int8)((unsigned int)v62 >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)v62) ^ *((_DWORD *)&t_fl[128] + BYTE1(v62));
  v64 = LODWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384]
                           + (unsigned __int8)(v14 ^ *((_BYTE *)&t_fl[384] + 4 * ((unsigned int)v62 >> 24)) ^ *((_BYTE *)&t_fl[256] + 4 * (unsigned __int8)((unsigned int)v62 >> 16)) ^ *((_BYTE *)t_fl + 4 * (unsigned __int8)v62) ^ *((_BYTE *)&t_fl[128] + 4 * BYTE1(v62)))) ^ *((_DWORD *)&t_fl[256] + ((v14 ^ *((_DWORD *)&t_fl[384] + ((unsigned int)v62 >> 24)) ^ *((_DWORD *)&t_fl[256] + (unsigned __int8)((unsigned int)v62 >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)v62) ^ *((_DWORD *)&t_fl[128] + BYTE1(v62))) >> 24)) ^ *((_DWORD *)t_fl + BYTE1(v27)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((v14 ^ *((_DWORD *)&t_fl[384] + ((unsigned int)v62 >> 24)) ^ *((_DWORD *)&t_fl[256] + (unsigned __int8)((unsigned int)v62 >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)v62) ^ *((_DWORD *)&t_fl[128] + BYTE1(v62))) >> 16));
  LODWORD(v38) = *((_DWORD *)t_im
                 + (unsigned __int8)(LOBYTE(t_rc[2]) ^ *((_BYTE *)&t_fl[384]
                                                       + 4
                                                       * (unsigned __int8)(v14 ^ *((_BYTE *)&t_fl[384]
                                                                                 + 4 * ((unsigned int)v62 >> 24)) ^ *((_BYTE *)&t_fl[256] + 4 * (unsigned __int8)((unsigned int)v62 >> 16)) ^ *((_BYTE *)t_fl + 4 * (unsigned __int8)v62) ^ *((_BYTE *)&t_fl[128] + 4 * BYTE1(v62)))) ^ *((_BYTE *)&t_fl[256] + 4 * ((v14 ^ *((_DWORD *)&t_fl[384] + ((unsigned int)v62 >> 24)) ^ *((_DWORD *)&t_fl[256] + (unsigned __int8)((unsigned int)v62 >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)v62) ^ *((_DWORD *)&t_fl[128] + BYTE1(v62))) >> 24)) ^ *((_BYTE *)t_fl + 4 * BYTE1(v27)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)((v14 ^ *((_DWORD *)&t_fl[384] + ((unsigned int)v62 >> 24)) ^ *((_DWORD *)&t_fl[256] + (unsigned __int8)((unsigned int)v62 >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)v62) ^ *((_DWORD *)&t_fl[128] + BYTE1(v62))) >> 16)))) ^ *((_DWORD *)&t_im[128] + BYTE1(v64));
  v65 = v64 ^ v60;
  LODWORD(v38) = *((_DWORD *)&t_im[384] + ((unsigned int)v64 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                       + (unsigned __int8)((unsigned int)v64 >> 16)) ^ v38;
  v66 = v64 ^ v60 ^ v54;
  v67 = v66;
  LODWORD(v66) = *((_DWORD *)&t_fl[384] + ((v62 ^ (unsigned int)v66) >> 24)) ^ *((_DWORD *)&t_fl[256]
                                                                               + (unsigned __int8)((v62 ^ (unsigned int)v66) >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)(v62 ^ v66)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned __int16)(v62 ^ v66) >> 8));
  LODWORD(v40) = *((_DWORD *)&t_im[384] + ((unsigned int)v66 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                       + (unsigned __int8)((unsigned int)v66 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)v66) ^ *((_DWORD *)&t_im[128] + BYTE1(v66));
  v68 = HIDWORD(t_rc[2]) ^ *((_DWORD *)&t_fl[384] + (unsigned __int8)(v66 ^ v37)) ^ *((_DWORD *)&t_fl[256]
                                                                                    + (((unsigned int)v66 ^ v37) >> 24)) ^ *((_DWORD *)t_fl + (unsigned __int8)((unsigned __int16)(v66 ^ v37) >> 8)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)(((unsigned int)v66 ^ v37) >> 16));
  LODWORD(v43) = *((_DWORD *)&t_im[384] + ((unsigned int)v68 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                                       + (unsigned __int8)((unsigned int)v68 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(BYTE4(t_rc[2]) ^ *((_BYTE *)&t_fl[384] + 4 * (unsigned __int8)(v66 ^ v37)) ^ *((_BYTE *)&t_fl[256] + 4 * (((unsigned int)v66 ^ v37) >> 24)) ^ *((_BYTE *)t_fl + 4 * (unsigned __int8)((unsigned __int16)(v66 ^ v37) >> 8)) ^ *((_BYTE *)&t_fl[128] + 4 * (unsigned __int8)(((unsigned int)v66 ^ v37) >> 16)))) ^ *((_DWORD *)&t_im[128] + BYTE1(v68));
  LODWORD(v66) = v68 ^ v61;
  v69 = v66;
  v70 = v66 ^ v62;
  v71 = *((_DWORD *)&t_fl[384] + (v70 >> 24)) ^ *((_DWORD *)&t_fl[256] + (unsigned __int8)(v70 >> 16)) ^ *((_DWORD *)t_fl + (unsigned __int8)v70) ^ *((_DWORD *)&t_fl[128] + BYTE1(v70));
  v72 = v71 ^ v51;
  v73 = *((_DWORD *)&t_im[384] + ((unsigned int)v71 >> 24)) ^ *((_DWORD *)&t_im[256]
                                                              + (unsigned __int8)((unsigned int)v71 >> 16)) ^ *((_DWORD *)t_im + (unsigned __int8)(*((_BYTE *)&t_fl[384] + 4 * (v70 >> 24)) ^ *((_BYTE *)&t_fl[256] + 4 * (unsigned __int8)(v70 >> 16)) ^ *((_BYTE *)t_fl + 4 * (unsigned __int8)v70) ^ *((_BYTE *)&t_fl[128] + 4 * BYTE1(v70)))) ^ *((_DWORD *)&t_im[128] + (unsigned __int8)((unsigned __int16)(*((_WORD *)&t_fl[384] + 2 * (v70 >> 24)) ^ *((_WORD *)&t_fl[256] + 2 * (unsigned __int8)(v70 >> 16)) ^ *((_WORD *)t_fl + 2 * (unsigned __int8)v70) ^ *((_WORD *)&t_fl[128] + 2 * BYTE1(v70))) >> 8));
  v74 = *((_DWORD *)&t_fl[384] + (unsigned __int8)v72) ^ *((_DWORD *)&t_fl[256] + ((unsigned int)v72 >> 24)) ^ *((_DWORD *)&t_fl[128] + (unsigned __int8)((unsigned int)v72 >> 16)) ^ *((_DWORD *)t_fl + BYTE1(v72)) ^ v65 ^ v68;
  *(_DWORD *)(v2 + 28) = v19;
  *(_DWORD *)(v2 + 32) = v21;
  *(_DWORD *)(v2 + 36) = v22;
  *(_DWORD *)(v2 + 40) = v25;
  *(_DWORD *)(v2 + 44) = v31;
  *(_DWORD *)(v2 + 48) = v33;
  *(_DWORD *)(v2 + 52) = v34;
  *(_DWORD *)(v2 + 56) = v36;
  *(_DWORD *)(v2 + 60) = v44;
  LODWORD(v66) = *(_DWORD *)(v2 + 32) ^ v46;
  *(_DWORD *)(v2 + 64) = v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 36) ^ v66;
  *(_DWORD *)(v2 + 68) = v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 40) ^ v66;
  *(_DWORD *)(v2 + 72) = v66;
  *(_DWORD *)(v2 + 76) = *(_DWORD *)(v2 + 44) ^ v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 48) ^ v49;
  *(_DWORD *)(v2 + 80) = v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 52) ^ v66;
  *(_DWORD *)(v2 + 84) = v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 56) ^ v66;
  *(_DWORD *)(v2 + 88) = v66;
  *(_DWORD *)(v2 + 92) = *(_DWORD *)(v2 + 60) ^ v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 64) ^ v53;
  *(_DWORD *)(v2 + 96) = v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 68) ^ v66;
  *(_DWORD *)(v2 + 100) = v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 72) ^ v66;
  *(_DWORD *)(v2 + 104) = v66;
  *(_DWORD *)(v2 + 108) = *(_DWORD *)(v2 + 76) ^ v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 80) ^ v55;
  *(_DWORD *)(v2 + 112) = v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 84) ^ v66;
  *(_DWORD *)(v2 + 116) = v66;
  LODWORD(v66) = *(_DWORD *)(v2 + 88) ^ v66;
  *(_DWORD *)(v2 + 120) = v66;
  *(_DWORD *)(v2 + 124) = *(_DWORD *)(v2 + 92) ^ v66;
  v75 = *(_DWORD *)(v2 + 96) ^ v59;
  *(_DWORD *)(v2 + 128) = v75;
  v76 = *(_DWORD *)(v2 + 100) ^ v75;
  *(_DWORD *)(v2 + 132) = v76;
  v77 = *(_DWORD *)(v2 + 104) ^ v76;
  *(_DWORD *)(v2 + 136) = v77;
  *(_DWORD *)(v2 + 140) = *(_DWORD *)(v2 + 108) ^ v77;
  v78 = *(_DWORD *)(v2 + 112) ^ v63;
  *(_DWORD *)(v2 + 144) = v78;
  v79 = *(_DWORD *)(v2 + 116) ^ v78;
  *(_DWORD *)(v2 + 148) = v79;
  v80 = *(_DWORD *)(v2 + 120) ^ v79;
  *(_DWORD *)(v2 + 152) = v80;
  *(_DWORD *)(v2 + 156) = *(_DWORD *)(v2 + 124) ^ v80;
  LODWORD(v38) = *(_DWORD *)(v2 + 128) ^ v38;
  *(_DWORD *)(v2 + 160) = v38;
  LODWORD(v38) = *(_DWORD *)(v2 + 132) ^ v38;
  *(_DWORD *)(v2 + 164) = v38;
  LODWORD(v38) = *(_DWORD *)(v2 + 136) ^ v38;
  *(_DWORD *)(v2 + 168) = v38;
  *(_DWORD *)(v2 + 172) = *(_DWORD *)(v2 + 140) ^ v38;
  LODWORD(v40) = *(_DWORD *)(v2 + 144) ^ v40;
  *(_DWORD *)(v2 + 176) = v40;
  LODWORD(v40) = *(_DWORD *)(v2 + 148) ^ v40;
  *(_DWORD *)(v2 + 180) = v40;
  LODWORD(v40) = *(_DWORD *)(v2 + 152) ^ v40;
  *(_DWORD *)(v2 + 184) = v40;
  *(_DWORD *)(v2 + 188) = *(_DWORD *)(v2 + 156) ^ v40;
  LODWORD(v43) = *(_DWORD *)(v2 + 160) ^ v43;
  *(_DWORD *)(v2 + 192) = v43;
  LODWORD(v43) = *(_DWORD *)(v2 + 164) ^ v43;
  *(_DWORD *)(v2 + 196) = v43;
  LODWORD(v43) = *(_DWORD *)(v2 + 168) ^ v43;
  *(_DWORD *)(v2 + 200) = v43;
  *(_DWORD *)(v2 + 204) = *(_DWORD *)(v2 + 172) ^ v43;
  v81 = *(_DWORD *)(v2 + 176) ^ v73;
  *(_DWORD *)(v2 + 208) = v81;
  v82 = *(_DWORD *)(v2 + 180) ^ v81;
  *(_DWORD *)(v2 + 212) = v82;
  v83 = *(_DWORD *)(v2 + 184) ^ v82;
  *(_DWORD *)(v2 + 216) = v83;
  *(_DWORD *)(v2 + 220) = *(_DWORD *)(v2 + 188) ^ v83;
  v84 = LODWORD(t_rc[3]) ^ v74;
  *(_DWORD *)(v2 + 224) = v84;
  *(_DWORD *)(v2 + 228) = v84 ^ v69;
  result = v67;
  v86 = v67 ^ v84;
  *(_DWORD *)(v2 + 232) = v86;
  *(_DWORD *)(v2 + 236) = v70 ^ v86;
  *(_DWORD *)(v2 + 240) = 14;
  return result;
}
// 50970: using guessed type __int64 t_rc[6];
// 519A0: using guessed type __int64 t_fl[512];
// 549A0: using guessed type __int64 t_im[512];

//----- (0000000000013360) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_decrypt_key(__int64 a1, signed __int64 a2, __int64 a3)
{
  __int64 result; // rax@9

  if ( a2 > 255 )
  {
    if ( a2 != 256 )
      return result;
    return ccaes_gladman_decrypt_key256(a1, a3);
  }
  if ( a2 > 191 )
  {
    if ( a2 != 192 )
      return result;
    return ccaes_gladman_decrypt_key192(a1, a3);
  }
  if ( a2 > 127 )
  {
    if ( a2 != 128 )
      return result;
  }
  else if ( a2 != 16 )
  {
    if ( a2 != 24 )
    {
      if ( a2 != 32 )
        return result;
      return ccaes_gladman_decrypt_key256(a1, a3);
    }
    return ccaes_gladman_decrypt_key192(a1, a3);
  }
  return ccaes_gladman_decrypt_key128(a1, a3);
}

//----- (00000000000133C8) ----------------------------------------------------
void *__fastcall ccrsa_init_pub(__int64 a1, const void *a2, const void *a3)
{
  const void *v3; // r14@1

  v3 = a3;
  ccn_set(*(_QWORD *)a1, (void *)(a1 + 16), a2);
  cczp_init(a1);
  return ccn_set(*(_QWORD *)a1, (void *)(16LL * *(_QWORD *)a1 + a1 + 24), v3);
}

//----- (000000000001340A) ----------------------------------------------------
void gen_tabs()
{
  ;
}

//----- (0000000000013410) ----------------------------------------------------
void __fastcall aesxts_mult_x(__m128i *a1)
{
  *(_OWORD *)&a1->m128i_i64[0] = _mm_xor_si128(
                                   _mm_or_si128(
                                     _mm_sll_epi64(*a1, 1u),
                                     _mm_shuffle_epi32(_mm_srl_epi64(*a1, 0x3Fu), -58)),
                                   _mm_and_si128(
                                     _mm_cvtsi32_si128(0x86u),
                                     _mm_shuffle_epi32(_mm_sra_epi32(*a1, 0x1Fu), 3)));
}

//----- (0000000000013470) ----------------------------------------------------
signed __int64 __fastcall aesxts_tweak_crypt_opt(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4)
{
  __m128i *v4; // r14@1
  __m128i v5; // xmm7@1
  signed __int64 result; // rax@1

  v4 = a3;
  v5 = *(__m128i *)&a3->m128i_i64[0];
  *(_OWORD *)&a2->m128i_i64[0] = _mm_xor_si128(*a1, *a3);
  result = vng_aes_encrypt_opt((__int64)a2, (__int64)a2, a4);
  if ( !(_DWORD)result )
  {
    *(_OWORD *)&a2->m128i_i64[0] = _mm_xor_si128(*a2, v5);
    *(_OWORD *)&v4->m128i_i64[0] = _mm_xor_si128(
                                     _mm_or_si128(
                                       _mm_sll_epi64(v5, 1u),
                                       _mm_shuffle_epi32(_mm_srl_epi64(v5, 0x3Fu), -58)),
                                     _mm_and_si128(
                                       _mm_cvtsi32_si128(0x86u),
                                       _mm_shuffle_epi32(_mm_sra_epi32(v5, 0x1Fu), 3)));
  }
  return result;
}

//----- (0000000000013520) ----------------------------------------------------
signed __int64 __fastcall aesxts_tweak_crypt_aesni(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4)
{
  __m128i *v4; // r14@1
  __m128i v5; // xmm7@1
  signed __int64 result; // rax@1

  v4 = a3;
  v5 = *(__m128i *)&a3->m128i_i64[0];
  *(_OWORD *)&a2->m128i_i64[0] = _mm_xor_si128(*a1, *a3);
  result = vng_aes_encrypt_aesni(a2, (__int64)a2, a4);
  if ( !(_DWORD)result )
  {
    *(_OWORD *)&a2->m128i_i64[0] = _mm_xor_si128(*a2, v5);
    *(_OWORD *)&v4->m128i_i64[0] = _mm_xor_si128(
                                     _mm_or_si128(
                                       _mm_sll_epi64(v5, 1u),
                                       _mm_shuffle_epi32(_mm_srl_epi64(v5, 0x3Fu), -58)),
                                     _mm_and_si128(
                                       _mm_cvtsi32_si128(0x86u),
                                       _mm_shuffle_epi32(_mm_sra_epi32(v5, 0x1Fu), 3)));
  }
  return result;
}

//----- (00000000000135D0) ----------------------------------------------------
signed __int64 __fastcall aesxts_tweak_crypt_group_aesni(__int64 a1, __int64 a2, __m128i *a3, __int64 a4, int a5)
{
  __int64 v5; // r12@1
  __int64 v6; // r13@1
  __m128i *v7; // r14@1
  __int64 v8; // r15@1
  int v9; // ebx@1
  __m128i v10; // xmm7@2
  __m128i v11; // xmm7@3
  __m128i v12; // xmm0@3
  __m128i v13; // xmm1@3
  __m128i v14; // xmm2@3
  __m128i v15; // xmm3@3
  int v16; // eax@3
  signed __int64 result; // rax@6
  unsigned __int8 v202; // of@10
  int v203; // ebx@12
  __m128i v204; // xmm7@13
  int v205; // ebx@13
  __m128i v206; // [sp+10h] [bp-F0h]@3
  __m128i v207; // [sp+20h] [bp-E0h]@3
  __m128i v208; // [sp+30h] [bp-D0h]@3
  __m128i v209; // [sp+40h] [bp-C0h]@3

  v5 = a1;
  v6 = a2;
  v7 = a3;
  v8 = a4;
  v9 = a5 - 4;
  if ( a5 >= 4 )
  {
    v10 = *(__m128i *)&a3->m128i_i64[0];
    do
    {
      v206 = v10;
      v11 = _mm_xor_si128(
              _mm_or_si128(_mm_sll_epi64(v10, 1u), _mm_shuffle_epi32(_mm_srl_epi64(v10, 0x3Fu), -58)),
              _mm_and_si128(_mm_cvtsi32_si128(0x86u), _mm_shuffle_epi32(_mm_sra_epi32(v10, 0x1Fu), 3)));
      v207 = v11;
      v208 = v11;
      v209 = v11;
      v10 = _mm_xor_si128(
              _mm_or_si128(_mm_sll_epi64(v209, 1u), _mm_shuffle_epi32(_mm_srl_epi64(v209, 0x3Fu), -58)),
              _mm_and_si128(_mm_cvtsi32_si128(0x86u), _mm_shuffle_epi32(_mm_sra_epi32(v209, 0x1Fu), 3)));
      v12 = _mm_xor_si128(*(__m128i *)v5, v206);
      v13 = _mm_xor_si128(*(__m128i *)(v5 + 16), v207);
      v14 = _mm_xor_si128(*(__m128i *)(v5 + 32), v208);
      v15 = _mm_xor_si128(*(__m128i *)(v5 + 48), v209);
      v16 = *(_DWORD *)(a4 + 240);
      if ( v16 == 160 )
      {
        _XMM5 = *(_OWORD *)(a4 + 16);
        _XMM0 = _mm_xor_si128(v12, *(__m128i *)a4);
        _XMM1 = _mm_xor_si128(v13, *(__m128i *)a4);
        _XMM2 = _mm_xor_si128(v14, *(__m128i *)a4);
        _XMM3 = _mm_xor_si128(v15, *(__m128i *)a4);
        _XMM4 = *(_OWORD *)(a4 + 32);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 48);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 64);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 80);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 96);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 112);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 128);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 144);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 160);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
          aesenclast xmm0, xmm4
          aesenclast xmm1, xmm4
          aesenclast xmm2, xmm4
          aesenclast xmm3, xmm4
        }
      }
      else if ( v16 == 192 )
      {
        _XMM5 = *(_OWORD *)(a4 + 16);
        _XMM0 = _mm_xor_si128(v12, *(__m128i *)a4);
        _XMM1 = _mm_xor_si128(v13, *(__m128i *)a4);
        _XMM2 = _mm_xor_si128(v14, *(__m128i *)a4);
        _XMM3 = _mm_xor_si128(v15, *(__m128i *)a4);
        _XMM4 = *(_OWORD *)(a4 + 32);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 48);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 64);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 80);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 96);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 112);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 128);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 144);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 160);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 176);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 192);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
          aesenclast xmm0, xmm4
          aesenclast xmm1, xmm4
          aesenclast xmm2, xmm4
          aesenclast xmm3, xmm4
        }
      }
      else
      {
        if ( v16 != 224 )
          return 0xFFFFFFFFLL;
        _XMM5 = *(_OWORD *)(a4 + 16);
        _XMM0 = _mm_xor_si128(v12, *(__m128i *)a4);
        _XMM1 = _mm_xor_si128(v13, *(__m128i *)a4);
        _XMM2 = _mm_xor_si128(v14, *(__m128i *)a4);
        _XMM3 = _mm_xor_si128(v15, *(__m128i *)a4);
        _XMM4 = *(_OWORD *)(a4 + 32);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 48);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 64);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 80);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 96);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 112);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 128);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 144);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 160);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 176);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 192);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 208);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 224);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
          aesenclast xmm0, xmm4
          aesenclast xmm1, xmm4
          aesenclast xmm2, xmm4
          aesenclast xmm3, xmm4
        }
      }
      *(_OWORD *)v6 = _mm_xor_si128(_XMM0, v206);
      *(_OWORD *)(v6 + 16) = _mm_xor_si128(_XMM1, v207);
      *(_OWORD *)(v6 + 32) = _mm_xor_si128(_XMM2, v208);
      *(_OWORD *)(v6 + 48) = _mm_xor_si128(_XMM3, v209);
      v5 += 64LL;
      v6 += 64LL;
      v202 = __OFSUB__(v9, 4);
      v9 -= 4;
    }
    while ( !((v9 < 0) ^ v202) );
    *(_OWORD *)&a3->m128i_i64[0] = v10;
  }
  result = 0LL;
  v203 = v9 + 4;
  if ( v203 )
  {
    v204 = *(__m128i *)&a3->m128i_i64[0];
    v202 = __OFSUB__(v203, 1);
    v205 = v203 - 1;
    if ( (v205 < 0) ^ v202 )
    {
LABEL_16:
      *(_OWORD *)&v7->m128i_i64[0] = v204;
    }
    else
    {
      while ( 1 )
      {
        *(_OWORD *)v6 = _mm_xor_si128(*(__m128i *)v5, v204);
        result = aes_encrypt_xmm_no_save(v6, v6, v8);
        if ( (_DWORD)result )
          break;
        *(_OWORD *)v6 = _mm_xor_si128(*(__m128i *)v6, v204);
        v204 = _mm_xor_si128(
                 _mm_or_si128(_mm_sll_epi64(v204, 1u), _mm_shuffle_epi32(_mm_srl_epi64(v204, 0x3Fu), -58)),
                 _mm_and_si128(_mm_cvtsi32_si128(0x86u), _mm_shuffle_epi32(_mm_sra_epi32(v204, 0x1Fu), 3)));
        v6 += 16LL;
        v5 += 16LL;
        v202 = __OFSUB__(v205--, 1);
        if ( (v205 < 0) ^ v202 )
          goto LABEL_16;
      }
    }
  }
  return result;
}

//----- (0000000000013C80) ----------------------------------------------------
void __fastcall aesxts_tweak_crypt_group_opt(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4, int a5)
{
  __m128i *v5; // r12@1
  __m128i *v6; // r13@1
  __m128i *v7; // r14@1
  __int64 v8; // r15@1
  __m128i v9; // xmm7@1
  int v10; // ebx@1
  unsigned __int8 v11; // of@3

  v5 = a1;
  v6 = a2;
  v7 = a3;
  v8 = a4;
  v9 = *(__m128i *)&a3->m128i_i64[0];
  v10 = a5 - 1;
  if ( a5 < 1 )
  {
LABEL_4:
    *(_OWORD *)&v7->m128i_i64[0] = v9;
  }
  else
  {
    while ( 1 )
    {
      *(_OWORD *)&v6->m128i_i64[0] = _mm_xor_si128(*v5, v9);
      if ( (unsigned int)aes_encrypt_xmm_no_save((__int64)v6, (__int64)v6, v8) )
        break;
      *(_OWORD *)&v6->m128i_i64[0] = _mm_xor_si128(*v6, v9);
      v9 = _mm_xor_si128(
             _mm_or_si128(_mm_sll_epi64(v9, 1u), _mm_shuffle_epi32(_mm_srl_epi64(v9, 0x3Fu), -58)),
             _mm_and_si128(_mm_cvtsi32_si128(0x86u), _mm_shuffle_epi32(_mm_sra_epi32(v9, 0x1Fu), 3)));
      ++v6;
      ++v5;
      v11 = __OFSUB__(v10--, 1);
      if ( (v10 < 0) ^ v11 )
        goto LABEL_4;
    }
  }
}

//----- (0000000000013D80) ----------------------------------------------------
signed __int64 __fastcall aesxts_tweak_uncrypt_opt(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4)
{
  __m128i *v4; // r14@1
  __m128i v5; // xmm7@1
  signed __int64 result; // rax@1

  v4 = a3;
  v5 = *(__m128i *)&a3->m128i_i64[0];
  *(_OWORD *)&a2->m128i_i64[0] = _mm_xor_si128(*a1, *a3);
  result = vng_aes_decrypt_opt((__int64)a2, (__int64)a2, a4);
  if ( !(_DWORD)result )
  {
    *(_OWORD *)&a2->m128i_i64[0] = _mm_xor_si128(*a2, v5);
    *(_OWORD *)&v4->m128i_i64[0] = _mm_xor_si128(
                                     _mm_or_si128(
                                       _mm_sll_epi64(v5, 1u),
                                       _mm_shuffle_epi32(_mm_srl_epi64(v5, 0x3Fu), -58)),
                                     _mm_and_si128(
                                       _mm_cvtsi32_si128(0x86u),
                                       _mm_shuffle_epi32(_mm_sra_epi32(v5, 0x1Fu), 3)));
  }
  return result;
}

//----- (0000000000013E30) ----------------------------------------------------
signed __int64 __fastcall aesxts_tweak_uncrypt_aesni(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4)
{
  __m128i *v4; // r14@1
  __m128i v5; // xmm7@1
  signed __int64 result; // rax@1

  v4 = a3;
  v5 = *(__m128i *)&a3->m128i_i64[0];
  *(_OWORD *)&a2->m128i_i64[0] = _mm_xor_si128(*a1, *a3);
  result = vng_aes_decrypt_aesni(a2, (__int64)a2, a4);
  if ( !(_DWORD)result )
  {
    *(_OWORD *)&a2->m128i_i64[0] = _mm_xor_si128(*a2, v5);
    *(_OWORD *)&v4->m128i_i64[0] = _mm_xor_si128(
                                     _mm_or_si128(
                                       _mm_sll_epi64(v5, 1u),
                                       _mm_shuffle_epi32(_mm_srl_epi64(v5, 0x3Fu), -58)),
                                     _mm_and_si128(
                                       _mm_cvtsi32_si128(0x86u),
                                       _mm_shuffle_epi32(_mm_sra_epi32(v5, 0x1Fu), 3)));
  }
  return result;
}

//----- (0000000000013EE0) ----------------------------------------------------
signed __int64 __fastcall aesxts_tweak_uncrypt_group_aesni(__int64 a1, __int64 a2, __m128i *a3, __int64 a4, int a5)
{
  __int64 v5; // r12@1
  __int64 v6; // r13@1
  __m128i *v7; // r14@1
  __int64 v8; // r15@1
  int v9; // ebx@1
  __m128i v10; // xmm7@2
  __m128i v11; // xmm7@3
  __m128i v12; // xmm0@3
  __m128i v13; // xmm1@3
  __m128i v14; // xmm2@3
  __m128i v15; // xmm3@3
  int v16; // eax@3
  signed __int64 result; // rax@6
  __m128i v18; // xmm4@7
  __m128i v73; // xmm4@8
  __m128i v134; // xmm4@9
  unsigned __int8 v205; // of@10
  int v206; // ebx@12
  __m128i v207; // xmm7@13
  int v208; // ebx@13
  __m128i v209; // [sp+10h] [bp-F0h]@3
  __m128i v210; // [sp+20h] [bp-E0h]@3
  __m128i v211; // [sp+30h] [bp-D0h]@3
  __m128i v212; // [sp+40h] [bp-C0h]@3

  v5 = a1;
  v6 = a2;
  v7 = a3;
  v8 = a4;
  v9 = a5 - 4;
  if ( a5 >= 4 )
  {
    v10 = *(__m128i *)&a3->m128i_i64[0];
    do
    {
      v209 = v10;
      v11 = _mm_xor_si128(
              _mm_or_si128(_mm_sll_epi64(v10, 1u), _mm_shuffle_epi32(_mm_srl_epi64(v10, 0x3Fu), -58)),
              _mm_and_si128(_mm_cvtsi32_si128(0x86u), _mm_shuffle_epi32(_mm_sra_epi32(v10, 0x1Fu), 3)));
      v210 = v11;
      v211 = v11;
      v212 = v11;
      v10 = _mm_xor_si128(
              _mm_or_si128(_mm_sll_epi64(v212, 1u), _mm_shuffle_epi32(_mm_srl_epi64(v212, 0x3Fu), -58)),
              _mm_and_si128(_mm_cvtsi32_si128(0x86u), _mm_shuffle_epi32(_mm_sra_epi32(v212, 0x1Fu), 3)));
      v12 = _mm_xor_si128(*(__m128i *)v5, v209);
      v13 = _mm_xor_si128(*(__m128i *)(v5 + 16), v210);
      v14 = _mm_xor_si128(*(__m128i *)(v5 + 32), v211);
      v15 = _mm_xor_si128(*(__m128i *)(v5 + 48), v212);
      v16 = *(_DWORD *)(a4 + 240);
      if ( v16 == 160 )
      {
        v18 = *(__m128i *)(a4 + 160);
        _XMM5 = *(_OWORD *)(a4 + 144);
        _XMM0 = _mm_xor_si128(v12, v18);
        _XMM1 = _mm_xor_si128(v13, v18);
        _XMM2 = _mm_xor_si128(v14, v18);
        _XMM3 = _mm_xor_si128(v15, v18);
        _XMM4 = *(_OWORD *)(a4 + 128);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 112);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 96);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 80);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 64);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 48);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 32);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 16);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)a4;
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
          aesdeclast xmm0, xmm4
          aesdeclast xmm1, xmm4
          aesdeclast xmm2, xmm4
          aesdeclast xmm3, xmm4
        }
      }
      else if ( v16 == 192 )
      {
        v73 = *(__m128i *)(a4 + 192);
        _XMM5 = *(_OWORD *)(a4 + 176);
        _XMM0 = _mm_xor_si128(v12, v73);
        _XMM1 = _mm_xor_si128(v13, v73);
        _XMM2 = _mm_xor_si128(v14, v73);
        _XMM3 = _mm_xor_si128(v15, v73);
        _XMM4 = *(_OWORD *)(a4 + 160);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 144);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 128);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 112);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 96);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 80);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 64);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 48);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 32);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 16);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)a4;
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
          aesdeclast xmm0, xmm4
          aesdeclast xmm1, xmm4
          aesdeclast xmm2, xmm4
          aesdeclast xmm3, xmm4
        }
      }
      else
      {
        if ( v16 != 224 )
          return 0xFFFFFFFFLL;
        v134 = *(__m128i *)(a4 + 224);
        _XMM5 = *(_OWORD *)(a4 + 208);
        _XMM0 = _mm_xor_si128(v12, v134);
        _XMM1 = _mm_xor_si128(v13, v134);
        _XMM2 = _mm_xor_si128(v14, v134);
        _XMM3 = _mm_xor_si128(v15, v134);
        _XMM4 = *(_OWORD *)(a4 + 192);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 176);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 160);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 144);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 128);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 112);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 96);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 80);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 64);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 48);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a4 + 32);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a4 + 16);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)a4;
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
          aesdeclast xmm0, xmm4
          aesdeclast xmm1, xmm4
          aesdeclast xmm2, xmm4
          aesdeclast xmm3, xmm4
        }
      }
      *(_OWORD *)v6 = _mm_xor_si128(_XMM0, v209);
      *(_OWORD *)(v6 + 16) = _mm_xor_si128(_XMM1, v210);
      *(_OWORD *)(v6 + 32) = _mm_xor_si128(_XMM2, v211);
      *(_OWORD *)(v6 + 48) = _mm_xor_si128(_XMM3, v212);
      v5 += 64LL;
      v6 += 64LL;
      v205 = __OFSUB__(v9, 4);
      v9 -= 4;
    }
    while ( !((v9 < 0) ^ v205) );
    *(_OWORD *)&a3->m128i_i64[0] = v10;
  }
  result = 0LL;
  v206 = v9 + 4;
  if ( v206 )
  {
    v207 = *(__m128i *)&a3->m128i_i64[0];
    v205 = __OFSUB__(v206, 1);
    v208 = v206 - 1;
    if ( (v208 < 0) ^ v205 )
    {
LABEL_16:
      *(_OWORD *)&v7->m128i_i64[0] = v207;
    }
    else
    {
      while ( 1 )
      {
        *(_OWORD *)v6 = _mm_xor_si128(*(__m128i *)v5, v207);
        result = aes_decrypt_xmm_no_save(v6, v6, v8);
        if ( (_DWORD)result )
          break;
        *(_OWORD *)v6 = _mm_xor_si128(*(__m128i *)v6, v207);
        v207 = _mm_xor_si128(
                 _mm_or_si128(_mm_sll_epi64(v207, 1u), _mm_shuffle_epi32(_mm_srl_epi64(v207, 0x3Fu), -58)),
                 _mm_and_si128(_mm_cvtsi32_si128(0x86u), _mm_shuffle_epi32(_mm_sra_epi32(v207, 0x1Fu), 3)));
        v5 += 16LL;
        v6 += 16LL;
        v205 = __OFSUB__(v208--, 1);
        if ( (v208 < 0) ^ v205 )
          goto LABEL_16;
      }
    }
  }
  return result;
}

//----- (0000000000014585) ----------------------------------------------------
void __fastcall aesxts_tweak_uncrypt_group_opt(__m128i *a1, __m128i *a2, __m128i *a3, __int64 a4, int a5)
{
  __m128i *v5; // r12@1
  __m128i *v6; // r13@1
  __m128i *v7; // r14@1
  __int64 v8; // r15@1
  __m128i v9; // xmm7@1
  int v10; // ebx@1
  unsigned __int8 v11; // of@3

  v5 = a1;
  v6 = a2;
  v7 = a3;
  v8 = a4;
  v9 = *(__m128i *)&a3->m128i_i64[0];
  v10 = a5 - 1;
  if ( a5 < 1 )
  {
LABEL_4:
    *(_OWORD *)&v7->m128i_i64[0] = v9;
  }
  else
  {
    while ( 1 )
    {
      *(_OWORD *)&v6->m128i_i64[0] = _mm_xor_si128(*v5, v9);
      if ( (unsigned int)aes_decrypt_xmm_no_save((__int64)v6, (__int64)v6, v8) )
        break;
      *(_OWORD *)&v6->m128i_i64[0] = _mm_xor_si128(*v6, v9);
      v9 = _mm_xor_si128(
             _mm_or_si128(_mm_sll_epi64(v9, 1u), _mm_shuffle_epi32(_mm_srl_epi64(v9, 0x3Fu), -58)),
             _mm_and_si128(_mm_cvtsi32_si128(0x86u), _mm_shuffle_epi32(_mm_sra_epi32(v9, 0x1Fu), 3)));
      ++v5;
      ++v6;
      v11 = __OFSUB__(v10--, 1);
      if ( (v10 < 0) ^ v11 )
        goto LABEL_4;
    }
  }
}

//----- (0000000000014685) ----------------------------------------------------
signed __int64 __fastcall ccrsa_sign(__int64 *a1, __int64 a2, void *a3, __int64 a4, __int64 a5)
{
  __int64 v5; // rbx@1
  signed __int64 v6; // r12@1
  __int64 v7; // r12@1
  char *v8; // r14@1
  signed __int64 v9; // r13@1
  size_t v10; // r15@1
  __int64 v11; // rsi@1
  __int64 v12; // rdx@1
  signed __int64 v13; // r15@1
  signed __int64 v14; // r15@1
  signed __int64 v15; // rdx@1
  signed __int64 v16; // rbx@1
  char *v17; // rax@1
  signed __int64 v18; // rcx@2
  bool v23; // cf@3
  signed __int64 v26; // rax@6
  signed __int64 result; // rax@7
  __int64 v28; // rcx@9
  __int64 v29; // [sp+0h] [bp-50h]@1
  void *v30; // [sp+8h] [bp-48h]@1
  __int64 *v31; // [sp+10h] [bp-40h]@1
  __int64 v32; // [sp+18h] [bp-38h]@1
  __int64 v33; // [sp+20h] [bp-30h]@1

  v29 = a5;
  v32 = a4;
  v30 = a3;
  v31 = a1;
  v33 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v6 = ccn_bitlen(*a1, (__int64)(a1 + 2));
  v7 = (unsigned __int64)(v6 + ccn_bitlen(a1[2 * v5 + 3], (__int64)&a1[2 * v5 + 5]) + 63) >> 6;
  v8 = (char *)&v29 - ((8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v9 = (signed __int64)&v8[8 * v7];
  v10 = *(_QWORD *)a2;
  memcpy(&v8[8 * v7] - v10, v30, v10);
  v11 = *(_QWORD *)(a2 + 32);
  v12 = *(_BYTE *)(v11 + 1);
  v13 = v12 + v10 + 2;
  memcpy((void *)(v9 - v13), (const void *)v11, v12 + 2);
  v14 = ~v13;
  *(_BYTE *)(v14 + v9) = 0;
  v15 = v9 + v14 - (_QWORD)v8;
  v16 = v14 + 2 - v15;
  memset((void *)(v9 + v16), 255, v15 - 2);
  *(_WORD *)(v9 + v16 - 2) = 256;
  v17 = (char *)&v29 - ((8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  if ( v7 - 1 > 0 )
  {
    v18 = (signed __int64)&v8[8 * v7 - 16];
    v17 = (char *)&v29 - ((8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL);
    do
    {
      _RDX = *(_QWORD *)v17;
      __asm { bswap   rdx }
      _RSI = *(_QWORD *)(v18 + 8);
      __asm { bswap   rsi }
      *(_QWORD *)v17 = _RSI;
      *(_QWORD *)(v18 + 8) = _RDX;
      v17 += 8;
      v23 = (unsigned __int64)v17 < v18;
      v18 -= 8LL;
    }
    while ( v23 );
  }
  if ( v7 & 1 )
  {
    _RCX = *(_QWORD *)v17;
    __asm { bswap   rcx }
    *(_QWORD *)v17 = _RCX;
  }
  ccrsa_priv_crypt(v31, (__int64)v8, v8);
  v26 = ccn_write_uint_size(v7, (__int64)v8);
  if ( *(_QWORD *)v32 >= (unsigned __int64)v26 )
  {
    *(_QWORD *)v32 = v26;
    ccn_write_uint(v7, (__int64)v8, v26, v29);
    cc_clear(8 * v7, v8);
    result = 0LL;
  }
  else
  {
    cc_clear(8 * v7, v8);
    result = 0xFFFFFFFFLL;
  }
  v28 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000014833) ----------------------------------------------------
__int64 __fastcall CC_CAST_encrypt(__int64 a1, __int64 a2)
{
  int v2; // er15@1
  unsigned int v3; // eax@1
  int v4; // edx@1
  unsigned int v5; // eax@1
  int v6; // er15@1
  unsigned int v7; // ebx@1
  int v8; // er11@1
  unsigned int v9; // ebx@1
  int v10; // edx@1
  unsigned int v11; // ebx@1
  int v12; // er11@1
  unsigned int v13; // eax@1
  int v14; // er15@1
  unsigned int v15; // eax@1
  int v16; // edx@1
  unsigned int v17; // eax@1
  int v18; // er15@1
  unsigned int v19; // eax@1
  int v20; // er12@1
  unsigned int v21; // eax@1
  int v22; // edx@1
  unsigned int v23; // eax@1
  int v24; // er12@1
  unsigned int v25; // eax@1
  __int64 v26; // rcx@1
  int v27; // er13@1
  __int64 v28; // rbx@1
  __int64 result; // rax@1
  int v30; // er13@1
  unsigned int v31; // edx@2
  int v32; // er15@2
  unsigned int v33; // eax@2
  int v34; // er13@2
  unsigned int v35; // eax@2
  unsigned int v36; // eax@2
  __int64 v37; // rcx@2
  int v38; // edx@2
  __int64 v39; // rsi@2

  v2 = *(_DWORD *)(a1 + 4);
  v3 = __ROL4__(v2 + *(_DWORD *)a2, *(_DWORD *)(a2 + 4));
  v4 = *(_DWORD *)a1 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v3 >> 16))
                      + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v3)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v3))
                      - *((_DWORD *)CC_CAST_S_table2 + (v3 >> 24)));
  v5 = __ROL4__(v4 ^ *(_DWORD *)(a2 + 8), *(_DWORD *)(a2 + 12));
  v6 = (*((_DWORD *)CC_CAST_S_table2 + (v5 >> 24))
      + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v5))
      - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v5)) ^ *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v5 >> 16)) ^ v2;
  v7 = __ROL4__(*(_DWORD *)(a2 + 16) - v6, *(_DWORD *)(a2 + 20));
  v8 = v4 ^ ((*((_DWORD *)CC_CAST_S_table2 + (v7 >> 24)) ^ (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v7))
                                                          + *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v7)))
           - *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v7 >> 16)));
  v9 = __ROL4__(v8 + *(_DWORD *)(a2 + 24), *(_DWORD *)(a2 + 28));
  v10 = v6 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v9 >> 16))
            + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v9)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v9))
            - *((_DWORD *)CC_CAST_S_table2 + (v9 >> 24)));
  v11 = __ROL4__(v10 ^ *(_DWORD *)(a2 + 32), *(_DWORD *)(a2 + 36));
  v12 = (*((_DWORD *)CC_CAST_S_table2 + (v11 >> 24))
       + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v11))
       - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v11)) ^ *((_DWORD *)CC_CAST_S_table3
                                                                 + (unsigned __int8)(v11 >> 16)) ^ v8;
  v13 = __ROL4__(*(_DWORD *)(a2 + 40) - v12, *(_DWORD *)(a2 + 44));
  v14 = v10 ^ ((*((_DWORD *)CC_CAST_S_table2 + (v13 >> 24)) ^ (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v13))
                                                             + *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v13)))
             - *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v13 >> 16)));
  v15 = __ROL4__(v14 + *(_DWORD *)(a2 + 48), *(_DWORD *)(a2 + 52));
  v16 = v12 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v15 >> 16))
             + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v15)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v15))
             - *((_DWORD *)CC_CAST_S_table2 + (v15 >> 24)));
  v17 = __ROL4__(v16 ^ *(_DWORD *)(a2 + 56), *(_DWORD *)(a2 + 60));
  v18 = (*((_DWORD *)CC_CAST_S_table2 + (v17 >> 24))
       + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v17))
       - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v17)) ^ *((_DWORD *)CC_CAST_S_table3
                                                                 + (unsigned __int8)(v17 >> 16)) ^ v14;
  v19 = __ROL4__(*(_DWORD *)(a2 + 64) - v18, *(_DWORD *)(a2 + 68));
  v20 = v16 ^ ((*((_DWORD *)CC_CAST_S_table2 + (v19 >> 24)) ^ (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v19))
                                                             + *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v19)))
             - *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v19 >> 16)));
  v21 = __ROL4__(v20 + *(_DWORD *)(a2 + 72), *(_DWORD *)(a2 + 76));
  v22 = v18 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v21 >> 16))
             + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v21)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v21))
             - *((_DWORD *)CC_CAST_S_table2 + (v21 >> 24)));
  v23 = __ROL4__(v22 ^ *(_DWORD *)(a2 + 80), *(_DWORD *)(a2 + 84));
  v24 = (*((_DWORD *)CC_CAST_S_table2 + (v23 >> 24))
       + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v23))
       - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v23)) ^ *((_DWORD *)CC_CAST_S_table3
                                                                 + (unsigned __int8)(v23 >> 16)) ^ v20;
  v25 = __ROL4__(*(_DWORD *)(a2 + 88) - v24, *(_DWORD *)(a2 + 92));
  v26 = BYTE1(v25);
  v27 = *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v25);
  v28 = v25 >> 24;
  result = (unsigned __int8)(v25 >> 16);
  v30 = v22 ^ ((*((_DWORD *)CC_CAST_S_table2 + v28) ^ (*((_DWORD *)CC_CAST_S_table0 + v26) + v27))
             - *((_DWORD *)CC_CAST_S_table3 + result));
  if ( !*(_DWORD *)(a2 + 128) )
  {
    v31 = __ROL4__(v30 + *(_DWORD *)(a2 + 96), *(_DWORD *)(a2 + 100));
    v32 = v24 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v31 >> 16))
               + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v31)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v31))
               - *((_DWORD *)CC_CAST_S_table2 + (v31 >> 24)));
    v33 = __ROL4__(v32 ^ *(_DWORD *)(a2 + 104), *(_DWORD *)(a2 + 108));
    v34 = (*((_DWORD *)CC_CAST_S_table2 + (v33 >> 24))
         + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v33))
         - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v33)) ^ *((_DWORD *)CC_CAST_S_table3
                                                                   + (unsigned __int8)(v33 >> 16)) ^ v30;
    v35 = __ROL4__(*(_DWORD *)(a2 + 112) - v34, *(_DWORD *)(a2 + 116));
    v24 = v32 ^ ((*((_DWORD *)CC_CAST_S_table2 + (v35 >> 24)) ^ (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v35))
                                                               + *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v35)))
               - *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v35 >> 16)));
    v36 = __ROL4__(v24 + *(_DWORD *)(a2 + 120), *(_DWORD *)(a2 + 124));
    v37 = BYTE1(v36);
    v38 = *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v36);
    v39 = v36 >> 24;
    result = (unsigned __int8)(v36 >> 16);
    v30 = (*((_DWORD *)CC_CAST_S_table3 + result)
         + (*((_DWORD *)CC_CAST_S_table0 + v37) ^ v38)
         - *((_DWORD *)CC_CAST_S_table2 + v39)) ^ v34;
  }
  *(_DWORD *)(a1 + 4) = v24;
  *(_DWORD *)a1 = v30;
  return result;
}
// 559A0: using guessed type __int64 CC_CAST_S_table0[128];
// 55DA0: using guessed type __int64 CC_CAST_S_table1[128];
// 561A0: using guessed type __int64 CC_CAST_S_table2[128];
// 565A0: using guessed type __int64 CC_CAST_S_table3[128];

//----- (0000000000014B75) ----------------------------------------------------
__int64 __fastcall CC_CAST_decrypt(__int64 a1, __int64 a2)
{
  int v2; // er14@1
  int v3; // er12@1
  unsigned int v4; // ebx@2
  int v5; // edx@2
  unsigned int v6; // ebx@2
  int v7; // er15@2
  unsigned int v8; // eax@2
  int v9; // edx@2
  unsigned int v10; // eax@2
  unsigned int v11; // edx@3
  int v12; // ebx@3
  unsigned int v13; // edx@3
  int v14; // er12@3
  unsigned int v15; // eax@3
  int v16; // er15@3
  unsigned int v17; // eax@3
  int v18; // ebx@3
  unsigned int v19; // eax@3
  int v20; // er15@3
  unsigned int v21; // eax@3
  int v22; // er14@3
  unsigned int v23; // eax@3
  int v24; // er12@3
  unsigned int v25; // eax@3
  int v26; // er14@3
  unsigned int v27; // eax@3
  int v28; // edx@3
  unsigned int v29; // eax@3
  int v30; // er15@3
  unsigned int v31; // eax@3
  int v32; // edx@3
  unsigned int v33; // eax@3
  __int64 v34; // rcx@3
  int v35; // esi@3
  __int64 v36; // rbx@3
  __int64 result; // rax@3
  int v38; // esi@3

  v2 = *(_DWORD *)a1;
  v3 = *(_DWORD *)(a1 + 4);
  if ( !*(_DWORD *)(a2 + 128) )
  {
    v4 = __ROL4__(v3 + *(_DWORD *)(a2 + 120), *(_DWORD *)(a2 + 124));
    v5 = v2 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v4 >> 16))
             + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v4)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v4))
             - *((_DWORD *)CC_CAST_S_table2 + (v4 >> 24)));
    v6 = __ROL4__(*(_DWORD *)(a2 + 112) - v5, *(_DWORD *)(a2 + 116));
    v7 = v3 ^ ((*((_DWORD *)CC_CAST_S_table2 + (v6 >> 24)) ^ (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v6))
                                                            + *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v6)))
             - *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v6 >> 16)));
    v8 = __ROL4__(v7 ^ *(_DWORD *)(a2 + 104), *(_DWORD *)(a2 + 108));
    v9 = (*((_DWORD *)CC_CAST_S_table2 + (v8 >> 24))
        + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v8))
        - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v8)) ^ *((_DWORD *)CC_CAST_S_table3
                                                                 + (unsigned __int8)(v8 >> 16)) ^ v5;
    v10 = __ROL4__(v9 + *(_DWORD *)(a2 + 96), *(_DWORD *)(a2 + 100));
    v3 = v7 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v10 >> 16))
             + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v10)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v10))
             - *((_DWORD *)CC_CAST_S_table2 + (v10 >> 24)));
    v2 = v9;
  }
  v11 = __ROL4__(*(_DWORD *)(a2 + 88) - v3, *(_DWORD *)(a2 + 92));
  v12 = v2 ^ ((*((_DWORD *)CC_CAST_S_table2 + (v11 >> 24)) ^ (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v11))
                                                            + *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v11)))
            - *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v11 >> 16)));
  v13 = __ROL4__(v12 ^ *(_DWORD *)(a2 + 80), *(_DWORD *)(a2 + 84));
  v14 = (*((_DWORD *)CC_CAST_S_table2 + (v13 >> 24))
       + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v13))
       - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v13)) ^ *((_DWORD *)CC_CAST_S_table3
                                                                 + (unsigned __int8)(v13 >> 16)) ^ v3;
  v15 = __ROL4__(v14 + *(_DWORD *)(a2 + 72), *(_DWORD *)(a2 + 76));
  v16 = v12 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v15 >> 16))
             + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v15)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v15))
             - *((_DWORD *)CC_CAST_S_table2 + (v15 >> 24)));
  v17 = __ROL4__(*(_DWORD *)(a2 + 64) - v16, *(_DWORD *)(a2 + 68));
  v18 = v14 ^ ((*((_DWORD *)CC_CAST_S_table2 + (v17 >> 24)) ^ (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v17))
                                                             + *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v17)))
             - *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v17 >> 16)));
  v19 = __ROL4__(v18 ^ *(_DWORD *)(a2 + 56), *(_DWORD *)(a2 + 60));
  v20 = (*((_DWORD *)CC_CAST_S_table2 + (v19 >> 24))
       + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v19))
       - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v19)) ^ *((_DWORD *)CC_CAST_S_table3
                                                                 + (unsigned __int8)(v19 >> 16)) ^ v16;
  v21 = __ROL4__(v20 + *(_DWORD *)(a2 + 48), *(_DWORD *)(a2 + 52));
  v22 = v18 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v21 >> 16))
             + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v21)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v21))
             - *((_DWORD *)CC_CAST_S_table2 + (v21 >> 24)));
  v23 = __ROL4__(*(_DWORD *)(a2 + 40) - v22, *(_DWORD *)(a2 + 44));
  v24 = v20 ^ ((*((_DWORD *)CC_CAST_S_table2 + (v23 >> 24)) ^ (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v23))
                                                             + *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v23)))
             - *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v23 >> 16)));
  v25 = __ROL4__(v24 ^ *(_DWORD *)(a2 + 32), *(_DWORD *)(a2 + 36));
  v26 = (*((_DWORD *)CC_CAST_S_table2 + (v25 >> 24))
       + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v25))
       - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v25)) ^ *((_DWORD *)CC_CAST_S_table3
                                                                 + (unsigned __int8)(v25 >> 16)) ^ v22;
  v27 = __ROL4__(v26 + *(_DWORD *)(a2 + 24), *(_DWORD *)(a2 + 28));
  v28 = v24 ^ (*((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v27 >> 16))
             + (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v27)) ^ *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v27))
             - *((_DWORD *)CC_CAST_S_table2 + (v27 >> 24)));
  v29 = __ROL4__(*(_DWORD *)(a2 + 16) - v28, *(_DWORD *)(a2 + 20));
  v30 = v26 ^ ((*((_DWORD *)CC_CAST_S_table2 + (v29 >> 24)) ^ (*((_DWORD *)CC_CAST_S_table0 + BYTE1(v29))
                                                             + *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v29)))
             - *((_DWORD *)CC_CAST_S_table3 + (unsigned __int8)(v29 >> 16)));
  v31 = __ROL4__(v30 ^ *(_DWORD *)(a2 + 8), *(_DWORD *)(a2 + 12));
  v32 = (*((_DWORD *)CC_CAST_S_table2 + (v31 >> 24))
       + *((_DWORD *)CC_CAST_S_table0 + BYTE1(v31))
       - *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v31)) ^ *((_DWORD *)CC_CAST_S_table3
                                                                 + (unsigned __int8)(v31 >> 16)) ^ v28;
  v33 = __ROL4__(v32 + *(_DWORD *)a2, *(_DWORD *)(a2 + 4));
  v34 = BYTE1(v33);
  v35 = *((_DWORD *)CC_CAST_S_table1 + (unsigned __int8)v33);
  v36 = v33 >> 24;
  result = (unsigned __int8)(v33 >> 16);
  v38 = v30 ^ (*((_DWORD *)CC_CAST_S_table3 + result)
             + (*((_DWORD *)CC_CAST_S_table0 + v34) ^ v35)
             - *((_DWORD *)CC_CAST_S_table2 + v36));
  *(_DWORD *)(a1 + 4) = v32;
  *(_DWORD *)a1 = v38;
  return result;
}
// 559A0: using guessed type __int64 CC_CAST_S_table0[128];
// 55DA0: using guessed type __int64 CC_CAST_S_table1[128];
// 561A0: using guessed type __int64 CC_CAST_S_table2[128];
// 565A0: using guessed type __int64 CC_CAST_S_table3[128];

//----- (0000000000014ECD) ----------------------------------------------------
__int64 __fastcall CC_CAST_set_key(__int64 a1, unsigned __int64 a2, __int64 a3)
{
  __int64 v3; // rax@1
  signed __int64 v4; // rax@3
  unsigned __int64 v5; // rsi@6
  signed __int64 v6; // rcx@6
  signed __int64 v7; // rcx@8
  __int64 *v8; // rsi@8
  unsigned int v9; // er13@10
  int v10; // edi@10
  unsigned __int64 v11; // r8@10
  unsigned int v12; // er9@10
  unsigned __int64 v13; // rsi@10
  unsigned int v14; // er14@10
  int v15; // ecx@10
  int v16; // ebx@10
  int v17; // edx@10
  int v18; // eax@10
  int v19; // ST144_4@12
  __int64 v20; // ST118_8@12
  __int64 v21; // ST110_8@12
  __int64 v22; // ST148_8@12
  __int64 v23; // ST120_8@12
  int v24; // STFC_4@12
  int v25; // edx@12
  __int64 v26; // ST130_8@12
  int v27; // STF8_4@12
  int v28; // STF4_4@12
  __int64 v29; // r14@12
  int v30; // STF0_4@12
  int v31; // eax@12
  __int64 v32; // STD8_8@12
  __int64 v33; // STE8_8@12
  int v34; // er8@12
  __int64 v35; // r9@12
  int v36; // STE4_4@12
  int v37; // ebx@12
  int v38; // ST138_4@12
  int v39; // eax@12
  __int64 v40; // STC8_8@12
  int v41; // STD4_4@12
  int v42; // STC4_4@12
  int v43; // ST144_4@12
  int v44; // edx@12
  int v45; // edx@12
  int v46; // edx@12
  __int64 v47; // ST88_8@12
  __int64 v48; // ST90_8@12
  unsigned int v49; // STB4_4@12
  unsigned int v50; // STB8_4@12
  int v51; // ebx@12
  __int64 v52; // STA8_8@12
  int v53; // ebx@12
  __int64 v54; // ST98_8@12
  __int64 v55; // STA0_8@12
  int v56; // eax@12
  __int64 v57; // ST108_8@12
  int v58; // ST84_4@12
  __int64 v59; // ST60_8@12
  __int64 v60; // ST38_8@12
  int v61; // ST48_4@12
  __int64 v62; // ST40_8@12
  __int64 v63; // ST30_8@12
  unsigned int v64; // ebx@12
  int v65; // ST138_4@12
  __int64 v66; // ST110_8@12
  __int64 v67; // r13@12
  int v68; // ST10_4@12
  int v69; // ST14_4@12
  int v70; // ST12C_4@12
  int v71; // ecx@12
  int v72; // er8@12
  __int64 v73; // ST20_8@12
  __int64 v74; // ST28_8@12
  unsigned __int64 v75; // ST18_8@12
  __int64 v76; // ST40_8@12
  __int64 v77; // ST48_8@12
  __int64 v78; // ST68_8@12
  __int64 v79; // ST38_8@12
  unsigned __int64 v80; // ST30_8@12
  __int64 v81; // ST58_8@12
  __int64 v82; // ST60_8@12
  unsigned __int64 v83; // ST50_8@12
  __int64 v84; // ST88_8@12
  int v85; // edx@12
  int v86; // edi@12
  int v87; // esi@12
  __int64 v88; // rcx@12
  int v89; // edi@12
  int v90; // esi@12
  __int64 v91; // r13@12
  __int64 v92; // rax@13
  __int64 v94; // [sp+0h] [bp-240h]@10
  int v95; // [sp+Ch] [bp-234h]@12
  __int64 v96; // [sp+100h] [bp-140h]@10
  unsigned int v97; // [sp+138h] [bp-108h]@10
  unsigned int v98; // [sp+144h] [bp-FCh]@12
  int v99; // [sp+148h] [bp-F8h]@12
  int v100[16]; // [sp+150h] [bp-F0h]@10
  int v101[16]; // [sp+190h] [bp-B0h]@14
  unsigned __int64 v102; // [sp+1D0h] [bp-70h]@2
  unsigned int v103; // [sp+1D8h] [bp-68h]@10
  unsigned int v104; // [sp+1DCh] [bp-64h]@10
  unsigned __int64 v105; // [sp+1E0h] [bp-60h]@10
  unsigned int v106; // [sp+1E8h] [bp-58h]@10
  unsigned int v107; // [sp+1ECh] [bp-54h]@10
  unsigned __int64 v108; // [sp+1F0h] [bp-50h]@10
  unsigned __int64 v109; // [sp+1F8h] [bp-48h]@10
  unsigned __int64 v110; // [sp+200h] [bp-40h]@10
  unsigned __int64 v111; // [sp+208h] [bp-38h]@10
  __int64 v112; // [sp+210h] [bp-30h]@1

  v112 = *(_QWORD *)off_69010[0];
  v3 = 0LL;
  do
    *((_DWORD *)&v102 + v3++) = 0;
  while ( v3 != 16 );
  v4 = 16LL;
  if ( a2 <= 0x10 )
    v4 = a2;
  if ( v4 )
  {
    v5 = ~a2;
    v6 = -17LL;
    if ( v5 > 0xFFFFFFFFFFFFFFEFLL )
      v6 = v5;
    v7 = ~v6;
    v8 = (__int64 *)&v102;
    do
    {
      *(_DWORD *)v8 = *(_BYTE *)a3++;
      v8 = (__int64 *)((char *)v8 + 4);
      --v7;
    }
    while ( v7 );
  }
  *(_DWORD *)(a1 + 128) = (unsigned __int64)v4 < 0xB ? 1 : 0;
  v94 = a1;
  v97 = v109;
  v9 = v111;
  v10 = v108;
  v11 = v108 >> 32;
  v12 = v110;
  v13 = v110 >> 32;
  v14 = HIDWORD(v111);
  v96 = (__int64)v100;
  v15 = v104 | ((_DWORD)v102 << 24) | (v102 >> 16) & 0xFFFF0000 | (v103 << 8);
  v16 = v107 | ((_DWORD)v105 << 24) | (v105 >> 16) & 0xFFFF0000 | (v106 << 8);
  v17 = HIDWORD(v109) | ((_DWORD)v108 << 24) | (HIDWORD(v108) << 16) | ((_DWORD)v109 << 8);
  v18 = HIDWORD(v111) | ((_DWORD)v110 << 24) | (HIDWORD(v110) << 16) | ((_DWORD)v111 << 8);
  while ( 1 )
  {
    v19 = *((_DWORD *)CC_CAST_S_table6 + (v10 & 0xFFFFFFFF)) ^ *((_DWORD *)CC_CAST_S_table7 + v9) ^ *((_DWORD *)CC_CAST_S_table6 + v12) ^ *((_DWORD *)CC_CAST_S_table5 + v14) ^ *((_DWORD *)CC_CAST_S_table4 + (unsigned int)v13) ^ v15;
    v20 = (unsigned __int8)v19;
    v21 = BYTE1(v19);
    v22 = (unsigned __int8)((unsigned int)v19 >> 16);
    v23 = (unsigned int)v19 >> 24;
    v24 = *((_DWORD *)CC_CAST_S_table6 + v22);
    v25 = *((_DWORD *)CC_CAST_S_table7 + v97) ^ *((_DWORD *)CC_CAST_S_table7 + (unsigned __int8)v19) ^ v24 ^ *((_DWORD *)CC_CAST_S_table5 + BYTE1(v19)) ^ *((_DWORD *)CC_CAST_S_table4 + v23) ^ v17;
    v26 = (unsigned __int8)((unsigned int)v25 >> 16);
    v27 = *((_DWORD *)CC_CAST_S_table5 + BYTE1(v25));
    v28 = *((_DWORD *)CC_CAST_S_table6 + v26);
    v29 = (unsigned int)v25 >> 24;
    v30 = *((_DWORD *)CC_CAST_S_table7 + v29);
    v31 = *((_DWORD *)CC_CAST_S_table4 + v11) ^ v30 ^ v28 ^ v27 ^ *((_DWORD *)CC_CAST_S_table4 + (unsigned __int8)v25) ^ v18;
    v32 = (unsigned __int8)v31;
    v33 = (unsigned __int8)((unsigned int)v31 >> 16);
    v34 = *((_DWORD *)CC_CAST_S_table5 + v33);
    v35 = (unsigned int)v31 >> 24;
    v36 = *((_DWORD *)CC_CAST_S_table4 + BYTE1(v31));
    v37 = *((_DWORD *)CC_CAST_S_table5 + HIDWORD(v109)) ^ *((_DWORD *)CC_CAST_S_table7 + v35) ^ *((_DWORD *)CC_CAST_S_table6
                                                                                                + (unsigned __int8)v31) ^ v34 ^ v36 ^ v16;
    v38 = *((_DWORD *)CC_CAST_S_table7 + BYTE1(v25)) ^ *((_DWORD *)CC_CAST_S_table6 + (unsigned __int8)v25) ^ *((_DWORD *)CC_CAST_S_table4 + v35) ^ v34;
    v39 = *((_DWORD *)CC_CAST_S_table6 + v23) ^ *((_DWORD *)CC_CAST_S_table7 + BYTE1(v25)) ^ *((_DWORD *)CC_CAST_S_table6
                                                                                             + v29) ^ *((_DWORD *)CC_CAST_S_table5 + (unsigned __int8)v25) ^ *((_DWORD *)CC_CAST_S_table4 + v26) ^ v31;
    v40 = (unsigned __int8)v39;
    v104 = (unsigned __int8)v39;
    v103 = BYTE1(v39);
    HIDWORD(v102) = (unsigned __int8)((unsigned int)v39 >> 16);
    LODWORD(v102) = (unsigned int)v39 >> 24;
    v41 = *((_DWORD *)CC_CAST_S_table5 + BYTE1(v39));
    v42 = *((_DWORD *)CC_CAST_S_table7 + BYTE1(v19));
    v43 = v42 ^ *((_DWORD *)CC_CAST_S_table7 + (unsigned __int8)v39) ^ *((_DWORD *)CC_CAST_S_table6
                                                                       + (unsigned __int8)((unsigned int)v39 >> 16)) ^ v41 ^ *((_DWORD *)CC_CAST_S_table4 + ((unsigned int)v39 >> 24)) ^ v19;
    v107 = (unsigned __int8)v43;
    v44 = *((_DWORD *)CC_CAST_S_table4 + (unsigned __int8)v43) ^ v25;
    v106 = BYTE1(v43);
    v45 = *((_DWORD *)CC_CAST_S_table5 + BYTE1(v43)) ^ v44;
    HIDWORD(v105) = (unsigned __int8)((unsigned int)v43 >> 16);
    v46 = *((_DWORD *)CC_CAST_S_table6 + (unsigned __int8)((unsigned int)v43 >> 16)) ^ v45;
    LODWORD(v105) = (unsigned int)v43 >> 24;
    LODWORD(v22) = *((_DWORD *)CC_CAST_S_table4 + v22) ^ *((_DWORD *)CC_CAST_S_table7 + ((unsigned int)v43 >> 24)) ^ v46;
    v47 = (unsigned __int8)v37;
    v48 = BYTE1(v37);
    v49 = v37;
    v50 = v37;
    HIDWORD(v109) = (unsigned __int8)v22;
    LODWORD(v109) = BYTE1(v22);
    v51 = *((_DWORD *)CC_CAST_S_table4 + BYTE1(v22)) ^ v37;
    HIDWORD(v108) = (unsigned __int8)((unsigned int)v22 >> 16);
    v52 = (unsigned int)v22 >> 24;
    v53 = *((_DWORD *)CC_CAST_S_table5 + v20) ^ *((_DWORD *)CC_CAST_S_table7 + v52) ^ *((_DWORD *)CC_CAST_S_table6
                                                                                      + (unsigned __int8)v22) ^ *((_DWORD *)CC_CAST_S_table5 + (unsigned __int8)((unsigned int)v22 >> 16)) ^ v51;
    LODWORD(v108) = (unsigned int)v22 >> 24;
    HIDWORD(v111) = (unsigned __int8)v53;
    LODWORD(v111) = BYTE1(v53);
    v54 = (unsigned __int8)((unsigned int)v53 >> 16);
    HIDWORD(v110) = (unsigned __int8)((unsigned int)v53 >> 16);
    v55 = (unsigned int)v53 >> 24;
    LODWORD(v110) = (unsigned int)v53 >> 24;
    v56 = *((_DWORD *)CC_CAST_S_table6 + ((unsigned int)v108 & 0xFFFFFFFF)) ^ *((_DWORD *)CC_CAST_S_table7 + BYTE1(v53)) ^ *((_DWORD *)CC_CAST_S_table6 + v55) ^ *((_DWORD *)CC_CAST_S_table5 + (unsigned __int8)v53) ^ *((_DWORD *)CC_CAST_S_table4 + v54) ^ v39;
    v57 = (unsigned __int8)v56;
    v58 = *((_DWORD *)CC_CAST_S_table5 + BYTE1(v56));
    v59 = (unsigned __int8)((unsigned int)v56 >> 16);
    v60 = (unsigned int)v56 >> 24;
    LODWORD(v22) = *((_DWORD *)CC_CAST_S_table7 + ((unsigned int)v109 & 0xFFFFFFFF)) ^ *((_DWORD *)CC_CAST_S_table7
                                                                                       + (unsigned __int8)v56) ^ *((_DWORD *)CC_CAST_S_table6 + v59) ^ v58 ^ *((_DWORD *)CC_CAST_S_table4 + v60) ^ v22;
    v61 = *((_DWORD *)CC_CAST_S_table4 + (unsigned __int8)v22);
    LODWORD(v26) = *((_DWORD *)CC_CAST_S_table5 + BYTE1(v22));
    v62 = (unsigned __int8)((unsigned int)v22 >> 16);
    v63 = (unsigned int)v22 >> 24;
    v64 = *((_DWORD *)CC_CAST_S_table4 + (v108 >> 32)) ^ *((_DWORD *)CC_CAST_S_table7 + v63) ^ *((_DWORD *)CC_CAST_S_table6
                                                                                               + v62) ^ v26 ^ v61 ^ v53;
    v65 = *((_DWORD *)CC_CAST_S_table4 + v21) ^ v38;
    LODWORD(v29) = *((_DWORD *)CC_CAST_S_table4 + v48) ^ *((_DWORD *)CC_CAST_S_table5 + v47);
    v66 = (unsigned __int8)(v64 >> 16);
    v67 = v64 >> 24;
    v68 = *((_DWORD *)CC_CAST_S_table5 + (v109 >> 32)) ^ *((_DWORD *)CC_CAST_S_table7 + v67) ^ *((_DWORD *)CC_CAST_S_table6
                                                                                               + (unsigned __int8)v64) ^ *((_DWORD *)CC_CAST_S_table5 + v66) ^ *((_DWORD *)CC_CAST_S_table4 + BYTE1(v64)) ^ v43;
    v69 = *((_DWORD *)CC_CAST_S_table4 + v59);
    v70 = *((_DWORD *)CC_CAST_S_table6 + BYTE1(v68)) ^ v69 ^ *((_DWORD *)CC_CAST_S_table5 + v60);
    v71 = *((_DWORD *)CC_CAST_S_table4 + v62);
    LODWORD(v35) = *((_DWORD *)CC_CAST_S_table6 + BYTE1(v64)) ^ v71 ^ *((_DWORD *)CC_CAST_S_table5 + v63);
    v72 = *((_DWORD *)CC_CAST_S_table6 + v67) ^ v61 ^ v26;
    LODWORD(v48) = *((_DWORD *)CC_CAST_S_table7 + BYTE1(v22));
    v98 = *((_DWORD *)CC_CAST_S_table6 + v60) ^ v48 ^ *((_DWORD *)CC_CAST_S_table6 + v63) ^ *((_DWORD *)CC_CAST_S_table5
                                                                                            + (unsigned __int8)v22) ^ v71 ^ v64;
    v73 = (unsigned int)v102;
    v74 = HIDWORD(v102);
    v75 = v111;
    v76 = HIDWORD(v110);
    v77 = v107;
    v78 = v104;
    v79 = v106;
    v80 = v108;
    v81 = (unsigned int)v105;
    v82 = HIDWORD(v105);
    v83 = v109;
    v84 = v107;
    v104 = (unsigned __int8)v98;
    v103 = BYTE1(v98);
    HIDWORD(v102) = (unsigned __int8)(v98 >> 16);
    LODWORD(v102) = v98 >> 24;
    LODWORD(v26) = *((_DWORD *)CC_CAST_S_table6 + BYTE1(v56)) ^ *((_DWORD *)CC_CAST_S_table7 + v66) ^ v72;
    v95 = *((_DWORD *)CC_CAST_S_table7 + BYTE1(v56)) ^ *((_DWORD *)CC_CAST_S_table7 + (unsigned __int8)v98) ^ *((_DWORD *)CC_CAST_S_table6 + (unsigned __int8)(v98 >> 16)) ^ *((_DWORD *)CC_CAST_S_table5 + BYTE1(v98)) ^ *((_DWORD *)CC_CAST_S_table4 + (v98 >> 24)) ^ v56;
    v107 = (unsigned __int8)v95;
    v106 = BYTE1(v95);
    HIDWORD(v105) = (unsigned __int8)((unsigned int)v95 >> 16);
    v85 = *((_DWORD *)CC_CAST_S_table6 + (unsigned __int8)((unsigned int)v95 >> 16)) ^ *((_DWORD *)CC_CAST_S_table5
                                                                                       + BYTE1(v95)) ^ *((_DWORD *)CC_CAST_S_table4 + (unsigned __int8)v95) ^ v22;
    LODWORD(v105) = (unsigned int)v95 >> 24;
    v99 = v69 ^ *((_DWORD *)CC_CAST_S_table7 + ((unsigned int)v95 >> 24)) ^ v85;
    HIDWORD(v109) = (unsigned __int8)v99;
    LODWORD(v109) = BYTE1(v99);
    v86 = *((_DWORD *)CC_CAST_S_table4 + BYTE1(v99)) ^ v68;
    HIDWORD(v108) = (unsigned __int8)((unsigned int)v99 >> 16);
    v87 = *((_DWORD *)CC_CAST_S_table5 + (unsigned __int8)((unsigned int)v99 >> 16));
    v88 = (unsigned int)v99 >> 24;
    v18 = *((_DWORD *)CC_CAST_S_table5 + (unsigned __int8)v56) ^ *((_DWORD *)CC_CAST_S_table7 + v88) ^ *((_DWORD *)CC_CAST_S_table6 + (unsigned __int8)v99) ^ v87 ^ v86;
    v89 = (unsigned __int8)(*((_BYTE *)CC_CAST_S_table5 + 4 * v57) ^ *((_BYTE *)CC_CAST_S_table7 + 4 * v88) ^ *((_BYTE *)CC_CAST_S_table6 + 4 * (unsigned __int8)v99) ^ v87 ^ v86);
    LODWORD(v108) = (unsigned int)v99 >> 24;
    HIDWORD(v111) = v89;
    LODWORD(v111) = BYTE1(v18);
    v90 = *((_DWORD *)CC_CAST_S_table7 + BYTE1(v95)) ^ *((_DWORD *)CC_CAST_S_table6 + (unsigned __int8)v95) ^ *((_DWORD *)CC_CAST_S_table4 + v88) ^ v87;
    *(_DWORD *)v96 = v65;
    *(_DWORD *)(v96 + 4) = v27 ^ v30 ^ v28 ^ *((_DWORD *)CC_CAST_S_table5 + v32) ^ v36;
    *(_DWORD *)(v96 + 8) = *((_DWORD *)CC_CAST_S_table6 + v33) ^ v42 ^ *((_DWORD *)CC_CAST_S_table6 + v20) ^ *((_DWORD *)CC_CAST_S_table4 + (v50 >> 24)) ^ *((_DWORD *)CC_CAST_S_table5 + (unsigned __int8)(v49 >> 16));
    *(_DWORD *)(v96 + 12) = *((_DWORD *)CC_CAST_S_table7 + (v50 >> 24)) ^ *((_DWORD *)CC_CAST_S_table7 + v23) ^ v24 ^ v29;
    *(_DWORD *)(v96 + 16) = *((_DWORD *)CC_CAST_S_table4 + v52) ^ *((_DWORD *)CC_CAST_S_table7 + v54) ^ *((_DWORD *)CC_CAST_S_table6 + v55) ^ *((_DWORD *)CC_CAST_S_table4 + v40) ^ v41;
    *(_DWORD *)(v96 + 20) = *((_DWORD *)CC_CAST_S_table5 + v76) ^ *((_DWORD *)CC_CAST_S_table7 + (v75 >> 32)) ^ *((_DWORD *)CC_CAST_S_table6 + ((unsigned int)v75 & 0xFFFFFFFF)) ^ *((_DWORD *)CC_CAST_S_table4 + v74) ^ *((_DWORD *)CC_CAST_S_table5 + v73);
    *(_DWORD *)(v96 + 24) = *((_DWORD *)CC_CAST_S_table6 + v78) ^ *((_DWORD *)CC_CAST_S_table7 + (v80 >> 32)) ^ *((_DWORD *)CC_CAST_S_table6 + ((unsigned int)v80 & 0xFFFFFFFF)) ^ *((_DWORD *)CC_CAST_S_table4 + v77) ^ *((_DWORD *)CC_CAST_S_table5 + v79);
    *(_DWORD *)(v96 + 28) = *((_DWORD *)CC_CAST_S_table7 + v84) ^ *((_DWORD *)CC_CAST_S_table7 + (v83 >> 32)) ^ *((_DWORD *)CC_CAST_S_table6 + ((unsigned int)v83 & 0xFFFFFFFF)) ^ *((_DWORD *)CC_CAST_S_table4 + v82) ^ *((_DWORD *)CC_CAST_S_table5 + v81);
    v91 = (unsigned int)v68 >> 24;
    *(_DWORD *)(v96 + 32) = *((_DWORD *)CC_CAST_S_table4 + v66) ^ *((_DWORD *)CC_CAST_S_table7
                                                                  + (unsigned __int8)((unsigned int)v68 >> 16)) ^ *((_DWORD *)CC_CAST_S_table6 + v91) ^ *((_DWORD *)CC_CAST_S_table4 + v57) ^ v58;
    *(_DWORD *)(v96 + 36) = *((_DWORD *)CC_CAST_S_table5 + v91) ^ *((_DWORD *)CC_CAST_S_table7 + (unsigned __int8)v68) ^ v70;
    *(_DWORD *)(v96 + 40) = v26;
    *(_DWORD *)(v96 + 44) = v48 ^ *((_DWORD *)CC_CAST_S_table7 + (unsigned __int8)v64) ^ v35;
    HIDWORD(v110) = (unsigned __int8)((unsigned int)v18 >> 16);
    LODWORD(v110) = (unsigned int)v18 >> 24;
    *(_DWORD *)(v96 + 48) = *((_DWORD *)CC_CAST_S_table4 + v104) ^ v90;
    *(_DWORD *)(v96 + 52) = *((_DWORD *)CC_CAST_S_table5 + v107) ^ *((_DWORD *)CC_CAST_S_table7 + (unsigned int)v105) ^ *((_DWORD *)CC_CAST_S_table6 + HIDWORD(v105)) ^ *((_DWORD *)CC_CAST_S_table4 + ((unsigned int)v109 & 0xFFFFFFFF)) ^ *((_DWORD *)CC_CAST_S_table5 + (v109 >> 32));
    *(_DWORD *)(v96 + 56) = *((_DWORD *)CC_CAST_S_table6 + (unsigned int)v108) ^ *((_DWORD *)CC_CAST_S_table7 + v103) ^ *((_DWORD *)CC_CAST_S_table6 + v104) ^ *((_DWORD *)CC_CAST_S_table4 + ((unsigned int)v110 & 0xFFFFFFFF)) ^ *((_DWORD *)CC_CAST_S_table5 + (v110 >> 32));
    *(_DWORD *)(v96 + 60) = *((_DWORD *)CC_CAST_S_table7 + HIDWORD(v110)) ^ *((_DWORD *)CC_CAST_S_table7
                                                                            + (unsigned int)v102) ^ *((_DWORD *)CC_CAST_S_table6 + HIDWORD(v102)) ^ *((_DWORD *)CC_CAST_S_table4 + ((unsigned int)v111 & 0xFFFFFFFF)) ^ *((_DWORD *)CC_CAST_S_table5 + (v111 >> 32));
    if ( (int *)v96 != v100 )
      break;
    LODWORD(v13) = HIDWORD(v110);
    v14 = HIDWORD(v111);
    v12 = v110;
    v9 = v111;
    v10 = v108;
    v97 = v109;
    v11 = v108 >> 32;
    v96 += 64LL;
    v15 = v98;
    v17 = v99;
    v16 = v95;
  }
  v92 = 0LL;
  do
  {
    *(_DWORD *)(v94 + 8 * v92) = v100[v92];
    *(_DWORD *)(v94 + 8 * v92 + 4) = ((unsigned __int8)v101[v92] + 16) & 0x1F;
    ++v92;
  }
  while ( v92 != 16 );
  return *(_QWORD *)off_69010[0];
}
// 569A0: using guessed type __int64 CC_CAST_S_table4[128];
// 56DA0: using guessed type __int64 CC_CAST_S_table5[128];
// 571A0: using guessed type __int64 CC_CAST_S_table6[128];
// 575A0: using guessed type __int64 CC_CAST_S_table7[128];
// 69010: using guessed type __int64 off_69010[2];
// 14ECD: using guessed type int var_F0[16];
// 14ECD: using guessed type int var_B0[16];

//----- (0000000000015987) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_eckey_size(unsigned __int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r15@1
  signed __int64 v4; // r13@1
  unsigned __int64 v5; // rbx@1
  signed __int64 v6; // rax@2
  signed __int64 v7; // rax@4

  v3 = a3;
  v4 = ccder_sizeof_uint64(1LL);
  v5 = v4 + ccder_sizeof(4LL, a1);
  if ( a2 )
  {
    v6 = ccder_sizeof_oid(a2);
    v5 += ccder_sizeof(-6917529027641081856LL, v6);
  }
  if ( v3 )
  {
    v7 = ccder_sizeof(3LL, v3 + 1);
    v5 += ccder_sizeof(-6917529027641081855LL, v7);
  }
  return ccder_sizeof(2305843009213693968LL, v5);
}

//----- (0000000000015A29) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_eckey(size_t a1, const void *a2, __int64 a3, size_t a4, const void *a5, __int64 a6, size_t a7)
{
  __int64 v7; // rbx@1
  const void *v8; // r12@1
  size_t v9; // r14@1
  __int64 v10; // r13@1
  __int64 v11; // rcx@1
  signed __int64 v12; // r15@1
  signed __int64 v13; // ST00_8@3
  __int64 v14; // rax@3
  __int64 v15; // rax@3
  signed __int64 v16; // rax@3
  signed __int64 v17; // rax@3
  __int64 v18; // r12@4
  signed __int64 v19; // r14@6
  __int64 v20; // rax@6
  signed __int64 v21; // rax@7
  signed __int64 v22; // rax@7

  v7 = a6;
  v8 = a5;
  v9 = a4;
  v10 = a3;
  v11 = a7;
  v12 = a7;
  if ( v9 )
  {
    v12 = a7;
    if ( a7 )
    {
      v13 = ccder_sizeof(3LL, v9 + 1);
      v14 = ccder_encode_body(v9, v8, v7, a7);
      v15 = ccder_encode_body(1uLL, qword_579A0, v7, v14);
      v16 = ccder_encode_tl(3uLL, v9 + 1, v7, v15);
      v17 = ccder_encode_tl(0xA000000000000001LL, v13, v7, v16);
      v11 = a7;
      v12 = v17;
    }
  }
  v18 = v11;
  if ( v10 && v12 )
  {
    v19 = ccder_sizeof_oid(v10);
    v20 = ccder_encode_oid(v10, v7, v12);
    v12 = ccder_encode_tl(0xA000000000000000LL, v19, v7, v20);
  }
  v21 = ccder_encode_implicit_raw_octet_string(4uLL, a1, a2, v7, v12);
  v22 = ccder_encode_uint64(1LL, v7, v21);
  return ccder_encode_constructed_tl(0x2000000000000010uLL, v18, v7, v22);
}
// 579A0: using guessed type __int64 qword_579A0[2];

//----- (0000000000015B66) ----------------------------------------------------
__int64 (*ccaes_cbc_decrypt_mode())[2]
{
  __int64 v0; // rax@1
  bool v1; // zf@1
  __int64 (*result)[2]; // rax@1

  LODWORD(v0) = cpuid_features();
  v1 = (v0 & 0x200000000000000uLL) >> 57 == 0;
  result = (__int64 (*)[2])ccaes_intel_cbc_decrypt_opt_mode;
  if ( !v1 )
    result = off_69020[0];
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 69020: using guessed type __int64 (*off_69020[4])[2];
// 69128: using guessed type __int64 ccaes_intel_cbc_decrypt_opt_mode[2];

//----- (0000000000015B91) ----------------------------------------------------
__int64 (*ccaes_cbc_encrypt_mode())[2]
{
  __int64 v0; // rax@1
  bool v1; // zf@1
  __int64 (*result)[2]; // rax@1

  LODWORD(v0) = cpuid_features();
  v1 = (v0 & 0x200000000000000uLL) >> 57 == 0;
  result = (__int64 (*)[2])ccaes_intel_cbc_encrypt_opt_mode;
  if ( !v1 )
    result = off_69028[0];
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 69028: using guessed type __int64 (*off_69028[3])[2];
// 69178: using guessed type __int64 ccaes_intel_cbc_encrypt_opt_mode[2];

//----- (0000000000015BBC) ----------------------------------------------------
signed __int64 __fastcall ccec_compact_import_pub_size(signed __int64 a1)
{
  signed __int64 result; // rax@1

  result = 192LL;
  if ( a1 > 65 )
  {
    if ( a1 == 66 )
      return 521LL;
  }
  else if ( a1 > 47 )
  {
    if ( a1 == 48 )
      return 384LL;
  }
  else
  {
    if ( a1 == 24 )
      return result;
    if ( a1 == 28 )
      return 224LL;
    if ( a1 == 32 )
      return 256LL;
  }
  return 0LL;
}

//----- (0000000000015C0F) ----------------------------------------------------
__int64 __fastcall ccec_compact_import_pub(__int64 *a1, __int64 a2, unsigned __int64 a3, __int64 a4)
{
  __int64 v4; // r12@1
  __int64 v5; // r15@1
  __int64 v6; // r14@1
  signed __int64 v7; // rax@1
  unsigned __int64 v8; // rdx@1
  signed int v9; // ecx@1
  __int64 v10; // rax@2
  __int64 v11; // r12@2
  __int64 v12; // rbx@2
  int v13; // eax@3
  __int64 v14; // r13@4
  __int64 v15; // rbx@4
  __int64 v16; // r12@4
  signed __int64 v17; // rbx@6
  __int64 v19; // [sp+0h] [bp-50h]@1
  __int64 v20; // [sp+8h] [bp-48h]@2
  unsigned __int64 v21; // [sp+10h] [bp-40h]@1
  __int64 v22; // [sp+18h] [bp-38h]@1
  __int64 v23; // [sp+20h] [bp-30h]@1

  v4 = a4;
  v21 = a3;
  v22 = a2;
  v23 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v6 = (__int64)(&v19 - 2 * *a1);
  v7 = ccn_bitlen(*a1, (__int64)(a1 + 2));
  v8 = v22;
  v9 = -1;
  if ( (unsigned __int64)(v7 + 7) >> 3 == v22 )
  {
    v20 = (__int64)(a1 + 2);
    *(_QWORD *)v4 = a1;
    v10 = v4;
    v11 = v4 + 16;
    v12 = v10;
    if ( (unsigned int)ccn_read_uint(v5, v11, v8, v21) )
      goto LABEL_9;
    v13 = ccec_affine_point_from_x(a1, (void *)v11, (const void *)v11, v6);
    v9 = -1;
    if ( !v13 )
    {
      ccn_sub(v5, v6, v20, v11 + 8LL * **(_QWORD **)v12);
      v14 = v12;
      v15 = **(_QWORD **)v12;
      v16 = v11 + 8 * v15;
      if ( (signed int)ccn_cmp(v5, v6, v16) < 0 )
      {
        ccn_set(v5, (void *)v16, (const void *)v6);
        v15 = **(_QWORD **)v14;
      }
      v17 = 16 * v15;
      *(_QWORD *)(v17 + v14 + 16) = 1LL;
      bzero((void *)(v17 + v14 + 24), 8 * v5 - 8);
      v9 = 0;
    }
  }
  while ( *(_QWORD *)off_69010[0] != v23 )
LABEL_9:
    v9 = -1;
  return (unsigned int)v9;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000015D57) ----------------------------------------------------
void __usercall ccmode_ccm_encrypt_x86_64(unsigned __int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __m128i a6@<xmm0>, __m128i a7@<xmm1>, __m128i a8@<xmm2>, __m128i a9@<xmm3>)
{
  __int64 v9; // r14@1
  __int64 v10; // r15@1
  unsigned __int64 v11; // r12@1
  __int64 v12; // r13@1
  int v13; // eax@1
  __int64 v14; // rax@6
  __int64 v15; // rax@7
  __m128i v16; // xmm4@7
  __m128i v17; // xmm5@7
  __int64 v18; // rax@8
  __int64 v19; // r15@8
  unsigned __int64 v20; // r14@8
  __int64 v21; // r13@11
  __int64 v22; // r13@11
  __int64 v23; // rcx@16
  unsigned __int64 v24; // r14@17
  signed __int64 v25; // ST00_8@17
  __int64 v26; // ST10_8@17
  __int64 v27; // r13@17
  __int64 v28; // r15@17
  __int64 v29; // r13@17

  v9 = a5;
  v10 = a2;
  v11 = a1;
  v12 = a3;
  v13 = *(_DWORD *)(a4 + 64);
  if ( v13 == 2 )
    goto LABEL_6;
  if ( v13 == 1 )
  {
    if ( *(_DWORD *)(a4 + 72) )
    {
      (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(*(_QWORD *)a3 + 24LL))(
        a3 + 8,
        1LL,
        a4 + 16,
        a4 + 16);
      *(_DWORD *)(a4 + 72) = 0;
    }
    *(_DWORD *)(a4 + 64) = 2;
LABEL_6:
    LODWORD(v14) = cpuid_features();
    if ( _bittest((const unsigned __int64 *)&v14, 0x39u) )
    {
      LODWORD(v15) = cpuid_features();
      if ( _bittest((const unsigned __int64 *)&v15, 0x29u) )
      {
        v18 = v10;
        v19 = v9;
        v20 = *(_DWORD *)(a4 + 68);
        if ( *(_DWORD *)(a4 + 68) )
        {
          if ( v20 > v11 )
            v20 = v11;
          v21 = v18;
          ccmode_ccm_macdata(a3, a4, 0, v20, v18);
          ccmode_ccm_crypt(a3, a4, v20, v21, v19);
          v22 = v20 + v21;
          v11 -= v20;
          v9 = v20 + v19;
          v10 = v22;
          v12 = a3;
          if ( *(_DWORD *)(a4 + 68) )
            goto LABEL_18;
        }
        else
        {
          v9 = v19;
          v10 = v18;
        }
        if ( *(_DWORD *)(v12 + 248) == 160 )
        {
          v23 = v9;
          if ( v11 >= 0x10 )
          {
            v24 = v11 >> 4;
            v25 = *(_QWORD *)(*(_QWORD *)v12 + 8LL) - 1LL - *(_QWORD *)(a4 + 80);
            v26 = v12;
            v27 = v10;
            v28 = v23;
            ccm128_encrypt(a6, a7, a8, a9, v16, v17);
            v11 &= 0xFu;
            v24 *= 16LL;
            v29 = v24 + v27;
            v9 = v24 + v28;
            v10 = v29;
            v12 = v26;
          }
        }
      }
    }
LABEL_18:
    ccmode_ccm_macdata(v12, a4, 0, v11, v10);
    ccmode_ccm_crypt(v12, a4, v11, v10, v9);
  }
}
// 4B8C8: using guessed type int cpuid_features(void);

//----- (0000000000015EF1) ----------------------------------------------------
__int64 (*ccaes_ecb_decrypt_mode())[2]
{
  __int64 v0; // rax@1
  bool v1; // zf@1
  __int64 (*result)[2]; // rax@1

  LODWORD(v0) = cpuid_features();
  v1 = (v0 & 0x200000000000000uLL) >> 57 == 0;
  result = (__int64 (*)[2])ccaes_intel_ecb_decrypt_opt_mode;
  if ( !v1 )
    result = off_69030[0];
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 69030: using guessed type __int64 (*off_69030[2])[2];
// 691C8: using guessed type __int64 ccaes_intel_ecb_decrypt_opt_mode[2];

//----- (0000000000015F1C) ----------------------------------------------------
__int64 (*ccaes_ecb_encrypt_mode())[2]
{
  __int64 v0; // rax@1
  bool v1; // zf@1
  __int64 (*result)[2]; // rax@1

  LODWORD(v0) = cpuid_features();
  v1 = (v0 & 0x200000000000000uLL) >> 57 == 0;
  result = (__int64 (*)[2])ccaes_intel_ecb_encrypt_opt_mode;
  if ( !v1 )
    result = off_69038;
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 69038: using guessed type __int64 (*off_69038)[2];
// 69208: using guessed type __int64 ccaes_intel_ecb_encrypt_opt_mode[2];

//----- (0000000000015F47) ----------------------------------------------------
__int64 __fastcall ccwrap_auth_encrypt(__int64 a1, __int64 a2, unsigned __int64 a3, void *a4, __int64 a5, void *a6)
{
  __int64 v6; // r15@1
  unsigned __int64 v7; // rsi@3
  const void *v8; // r12@5
  signed __int64 v9; // r13@7
  void *v10; // r14@7
  unsigned __int64 v11; // r15@7
  __int64 *v14; // rbx@9
  signed int v15; // er14@10
  size_t v17; // [sp+8h] [bp-88h]@3
  __int64 v18; // [sp+10h] [bp-80h]@3
  void *v19; // [sp+18h] [bp-78h]@1
  void *v20; // [sp+20h] [bp-70h]@1
  signed __int64 v21; // [sp+28h] [bp-68h]@3
  __int64 v22; // [sp+30h] [bp-60h]@3
  __int64 v23; // [sp+40h] [bp-50h]@3
  unsigned __int64 v24; // [sp+48h] [bp-48h]@3
  __int64 v25; // [sp+50h] [bp-40h]@3
  char v26; // [sp+58h] [bp-38h]@8
  __int64 v27; // [sp+60h] [bp-30h]@1

  v20 = a6;
  v19 = a4;
  v6 = off_69010[0];
  v27 = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)(a1 + 8) != 16LL || a3 <= 0xF )
    goto LABEL_13;
  v23 = a2;
  v18 = a5;
  v17 = a3;
  v7 = a3 >> 3;
  v24 = a3 >> 3;
  v25 = -6438275382588823898LL;
  v22 = 0LL;
  v21 = 1LL;
  do
  {
    if ( v7 )
    {
      v8 = v20;
      if ( !v22 )
        v8 = v19;
      v9 = v21;
      v10 = v20;
      v11 = 0LL;
      do
      {
        memcpy(&v26, v8, 8uLL);
        (*(void (__fastcall **)(__int64, signed __int64, __int64 *, __int64 *))(a1 + 24))(v23, 1LL, &v25, &v25);
        memcpy(v10, &v26, 8uLL);
        v7 = v24;
        _RAX = v9;
        __asm { bswap   rax }
        v25 ^= _RAX;
        ++v11;
        v8 = (char *)v8 + 8;
        v10 = (char *)v10 + 8;
        ++v9;
      }
      while ( v11 < v24 );
    }
    v14 = &v25;
    ++v22;
    v21 += v7;
  }
  while ( v22 != 6 );
  memmove((char *)v20 + 8, v20, v17);
  memcpy(v20, &v25, 8uLL);
  *(_QWORD *)v18 = v17 + 8;
  v15 = 0;
  v6 = off_69010[0];
  while ( 1 )
  {
    cc_clear(0x10uLL, v14);
    if ( *(_QWORD *)v6 == v27 )
      break;
LABEL_13:
    v15 = -1;
    v14 = &v25;
  }
  return (unsigned int)v15;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000160D8) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_cbc_decrypt_init(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4)
{
  __int64 result; // rax@1

  result = ccaes_gladman_decrypt_key(a4, a3, a2);
  *(_DWORD *)(a2 + 260) = 1;
  return result;
}

//----- (0000000000016100) ----------------------------------------------------
__int64 __fastcall ccaes_gladman_cbc_encrypt_init(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4)
{
  __int64 result; // rax@1

  result = ccaes_gladman_encrypt_key(a4, a3, a2);
  *(_DWORD *)(a2 + 260) = 1;
  return result;
}

//----- (0000000000016128) ----------------------------------------------------
__int64 __usercall ccsha256_vng_intel_ssse3_compress@<rax>(__int64 a1@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm4>, __m128i a9@<xmm5>, __m128i a10@<xmm6>, __m128i a11@<xmm7>)
{
  __m128i v11; // xmm0@1
  __m128i v12; // xmm1@1
  __m128i v13; // xmm2@1
  __m128i v14; // xmm3@1
  signed __int64 v15; // rdx@1
  __m128i v16; // xmm0@1
  __m128i v17; // xmm1@1
  __m128i v18; // xmm2@1
  __m128i v19; // xmm3@1
  const __m128i *v20; // rbx@1
  __m128i v21; // xmm5@1
  __m128i v22; // xmm6@1
  __m128i v23; // xmm7@1
  int v24; // er9@2
  int v25; // er10@2
  int v26; // er12@2
  int v27; // er13@2
  int v28; // er14@2
  __m128i v29; // xmm5@2
  const __m128i *v30; // rbx@2
  int v31; // eax@2
  int v32; // ecx@2
  int v33; // eax@2
  __m128i v34; // xmm7@2
  __m128i v35; // xmm4@2
  int v36; // ecx@2
  __m128i v37; // xmm6@2
  __m128i v38; // xmm7@2
  int v39; // eax@2
  int v40; // ecx@2
  int v41; // er15@2
  int v42; // er11@2
  int v43; // er15@2
  int v44; // eax@2
  int v45; // ecx@2
  int v46; // eax@2
  __m128i v47; // xmm0@2
  int v48; // ecx@2
  int v49; // eax@2
  int v50; // ecx@2
  __m128i v51; // xmm7@2
  int v52; // er14@2
  __m128i v53; // xmm4@2
  __m128i v54; // xmm6@2
  int v55; // er10@2
  __m128i v56; // xmm7@2
  int v57; // er14@2
  int v58; // eax@2
  int v59; // ecx@2
  int v60; // eax@2
  int v61; // ecx@2
  __m128i v62; // xmm0@2
  int v63; // eax@2
  int v64; // ecx@2
  int v65; // er13@2
  __m128i v66; // xmm6@2
  int v67; // er9@2
  __m128i v68; // xmm4@2
  __m128i v69; // xmm7@2
  int v70; // er13@2
  __m128i v71; // xmm6@2
  int v72; // eax@2
  int v73; // ecx@2
  int v74; // eax@2
  int v75; // ecx@2
  int v76; // eax@2
  int v77; // ecx@2
  int v78; // er12@2
  __m128i v79; // xmm0@2
  int v80; // er8@2
  int v81; // er12@2
  __m128i v82; // xmm5@2
  int v83; // eax@2
  int v84; // ecx@2
  int v85; // eax@2
  __m128i v86; // xmm7@2
  __m128i v87; // xmm4@2
  int v88; // ecx@2
  __m128i v89; // xmm6@2
  __m128i v90; // xmm7@2
  int v91; // eax@2
  int v92; // ecx@2
  int v93; // er11@2
  int v94; // er15@2
  int v95; // er11@2
  int v96; // eax@2
  int v97; // ecx@2
  int v98; // eax@2
  __m128i v99; // xmm1@2
  int v100; // ecx@2
  int v101; // eax@2
  int v102; // ecx@2
  __m128i v103; // xmm7@2
  int v104; // er10@2
  __m128i v105; // xmm4@2
  __m128i v106; // xmm6@2
  int v107; // er14@2
  __m128i v108; // xmm7@2
  int v109; // er10@2
  int v110; // eax@2
  int v111; // ecx@2
  int v112; // eax@2
  int v113; // ecx@2
  __m128i v114; // xmm1@2
  int v115; // eax@2
  int v116; // ecx@2
  int v117; // er9@2
  __m128i v118; // xmm6@2
  int v119; // er13@2
  __m128i v120; // xmm4@2
  __m128i v121; // xmm7@2
  int v122; // er9@2
  __m128i v123; // xmm6@2
  int v124; // eax@2
  int v125; // ecx@2
  int v126; // eax@2
  int v127; // ecx@2
  int v128; // eax@2
  int v129; // ecx@2
  int v130; // er8@2
  __m128i v131; // xmm1@2
  int v132; // er12@2
  int v133; // er8@2
  __m128i v134; // xmm5@2
  int v135; // eax@2
  int v136; // ecx@2
  int v137; // eax@2
  __m128i v138; // xmm7@2
  __m128i v139; // xmm4@2
  int v140; // ecx@2
  __m128i v141; // xmm6@2
  __m128i v142; // xmm7@2
  int v143; // eax@2
  int v144; // ecx@2
  int v145; // er15@2
  int v146; // er11@2
  int v147; // er15@2
  int v148; // eax@2
  int v149; // ecx@2
  int v150; // eax@2
  __m128i v151; // xmm2@2
  int v152; // ecx@2
  int v153; // eax@2
  int v154; // ecx@2
  __m128i v155; // xmm7@2
  int v156; // er14@2
  __m128i v157; // xmm4@2
  __m128i v158; // xmm6@2
  int v159; // er10@2
  __m128i v160; // xmm7@2
  int v161; // er14@2
  int v162; // eax@2
  int v163; // ecx@2
  int v164; // eax@2
  int v165; // ecx@2
  __m128i v166; // xmm2@2
  int v167; // eax@2
  int v168; // ecx@2
  int v169; // er13@2
  __m128i v170; // xmm6@2
  int v171; // er9@2
  __m128i v172; // xmm4@2
  __m128i v173; // xmm7@2
  int v174; // er13@2
  __m128i v175; // xmm6@2
  int v176; // eax@2
  int v177; // ecx@2
  int v178; // eax@2
  int v179; // ecx@2
  int v180; // eax@2
  int v181; // ecx@2
  int v182; // er12@2
  __m128i v183; // xmm2@2
  int v184; // er8@2
  int v185; // er12@2
  __m128i v186; // xmm5@2
  int v187; // eax@2
  int v188; // ecx@2
  int v189; // eax@2
  __m128i v190; // xmm7@2
  __m128i v191; // xmm4@2
  int v192; // ecx@2
  __m128i v193; // xmm6@2
  __m128i v194; // xmm7@2
  int v195; // eax@2
  int v196; // ecx@2
  int v197; // er11@2
  int v198; // er15@2
  int v199; // er11@2
  int v200; // eax@2
  int v201; // ecx@2
  int v202; // eax@2
  __m128i v203; // xmm3@2
  int v204; // ecx@2
  int v205; // eax@2
  int v206; // ecx@2
  __m128i v207; // xmm7@2
  int v208; // er10@2
  __m128i v209; // xmm4@2
  __m128i v210; // xmm6@2
  int v211; // er14@2
  __m128i v212; // xmm7@2
  int v213; // er10@2
  int v214; // eax@2
  int v215; // ecx@2
  int v216; // eax@2
  int v217; // ecx@2
  __m128i v218; // xmm3@2
  int v219; // eax@2
  int v220; // ecx@2
  int v221; // er9@2
  __m128i v222; // xmm6@2
  int v223; // er13@2
  __m128i v224; // xmm4@2
  __m128i v225; // xmm7@2
  int v226; // er9@2
  __m128i v227; // xmm6@2
  int v228; // eax@2
  int v229; // ecx@2
  int v230; // eax@2
  int v231; // ecx@2
  int v232; // eax@2
  int v233; // ecx@2
  int v234; // er8@2
  __m128i v235; // xmm3@2
  int v236; // er12@2
  int v237; // er8@2
  __m128i v238; // xmm5@2
  int v239; // eax@2
  int v240; // ecx@2
  int v241; // eax@2
  __m128i v242; // xmm7@2
  __m128i v243; // xmm4@2
  int v244; // ecx@2
  __m128i v245; // xmm6@2
  __m128i v246; // xmm7@2
  int v247; // eax@2
  int v248; // ecx@2
  int v249; // er15@2
  int v250; // er11@2
  int v251; // er15@2
  int v252; // eax@2
  int v253; // ecx@2
  int v254; // eax@2
  __m128i v255; // xmm0@2
  int v256; // ecx@2
  int v257; // eax@2
  int v258; // ecx@2
  __m128i v259; // xmm7@2
  int v260; // er14@2
  __m128i v261; // xmm4@2
  __m128i v262; // xmm6@2
  int v263; // er10@2
  __m128i v264; // xmm7@2
  int v265; // er14@2
  int v266; // eax@2
  int v267; // ecx@2
  int v268; // eax@2
  int v269; // ecx@2
  __m128i v270; // xmm0@2
  int v271; // eax@2
  int v272; // ecx@2
  int v273; // er13@2
  __m128i v274; // xmm6@2
  int v275; // er9@2
  __m128i v276; // xmm4@2
  __m128i v277; // xmm7@2
  int v278; // er13@2
  __m128i v279; // xmm6@2
  int v280; // eax@2
  int v281; // ecx@2
  int v282; // eax@2
  int v283; // ecx@2
  int v284; // eax@2
  int v285; // ecx@2
  int v286; // er12@2
  __m128i v287; // xmm0@2
  int v288; // er8@2
  int v289; // er12@2
  __m128i v290; // xmm5@2
  int v291; // eax@2
  int v292; // ecx@2
  int v293; // eax@2
  __m128i v294; // xmm7@2
  __m128i v295; // xmm4@2
  int v296; // ecx@2
  __m128i v297; // xmm6@2
  __m128i v298; // xmm7@2
  int v299; // eax@2
  int v300; // ecx@2
  int v301; // er11@2
  int v302; // er15@2
  int v303; // er11@2
  int v304; // eax@2
  int v305; // ecx@2
  int v306; // eax@2
  __m128i v307; // xmm1@2
  int v308; // ecx@2
  int v309; // eax@2
  int v310; // ecx@2
  __m128i v311; // xmm7@2
  int v312; // er10@2
  __m128i v313; // xmm4@2
  __m128i v314; // xmm6@2
  int v315; // er14@2
  __m128i v316; // xmm7@2
  int v317; // er10@2
  int v318; // eax@2
  int v319; // ecx@2
  int v320; // eax@2
  int v321; // ecx@2
  __m128i v322; // xmm1@2
  int v323; // eax@2
  int v324; // ecx@2
  int v325; // er9@2
  __m128i v326; // xmm6@2
  int v327; // er13@2
  __m128i v328; // xmm4@2
  __m128i v329; // xmm7@2
  int v330; // er9@2
  __m128i v331; // xmm6@2
  int v332; // eax@2
  int v333; // ecx@2
  int v334; // eax@2
  int v335; // ecx@2
  int v336; // eax@2
  int v337; // ecx@2
  int v338; // er8@2
  __m128i v339; // xmm1@2
  int v340; // er12@2
  int v341; // er8@2
  __m128i v342; // xmm5@2
  int v343; // eax@2
  int v344; // ecx@2
  int v345; // eax@2
  __m128i v346; // xmm7@2
  __m128i v347; // xmm4@2
  int v348; // ecx@2
  __m128i v349; // xmm6@2
  __m128i v350; // xmm7@2
  int v351; // eax@2
  int v352; // ecx@2
  int v353; // er15@2
  int v354; // er11@2
  int v355; // er15@2
  int v356; // eax@2
  int v357; // ecx@2
  int v358; // eax@2
  __m128i v359; // xmm2@2
  int v360; // ecx@2
  int v361; // eax@2
  int v362; // ecx@2
  __m128i v363; // xmm7@2
  int v364; // er14@2
  __m128i v365; // xmm4@2
  __m128i v366; // xmm6@2
  int v367; // er10@2
  __m128i v368; // xmm7@2
  int v369; // er14@2
  int v370; // eax@2
  int v371; // ecx@2
  int v372; // eax@2
  int v373; // ecx@2
  __m128i v374; // xmm2@2
  int v375; // eax@2
  int v376; // ecx@2
  int v377; // er13@2
  __m128i v378; // xmm6@2
  int v379; // er9@2
  __m128i v380; // xmm4@2
  __m128i v381; // xmm7@2
  int v382; // er13@2
  __m128i v383; // xmm6@2
  int v384; // eax@2
  int v385; // ecx@2
  int v386; // eax@2
  int v387; // ecx@2
  int v388; // eax@2
  int v389; // ecx@2
  int v390; // er12@2
  __m128i v391; // xmm2@2
  int v392; // er8@2
  int v393; // er12@2
  __m128i v394; // xmm5@2
  int v395; // eax@2
  int v396; // ecx@2
  int v397; // eax@2
  __m128i v398; // xmm7@2
  __m128i v399; // xmm4@2
  int v400; // ecx@2
  __m128i v401; // xmm6@2
  __m128i v402; // xmm7@2
  int v403; // eax@2
  int v404; // ecx@2
  int v405; // er11@2
  int v406; // er15@2
  int v407; // er11@2
  int v408; // eax@2
  int v409; // ecx@2
  int v410; // eax@2
  __m128i v411; // xmm3@2
  int v412; // ecx@2
  int v413; // eax@2
  int v414; // ecx@2
  __m128i v415; // xmm7@2
  int v416; // er10@2
  __m128i v417; // xmm4@2
  __m128i v418; // xmm6@2
  int v419; // er14@2
  __m128i v420; // xmm7@2
  int v421; // er10@2
  int v422; // eax@2
  int v423; // ecx@2
  int v424; // eax@2
  int v425; // ecx@2
  __m128i v426; // xmm3@2
  int v427; // eax@2
  int v428; // ecx@2
  int v429; // er9@2
  __m128i v430; // xmm6@2
  int v431; // er13@2
  __m128i v432; // xmm4@2
  __m128i v433; // xmm7@2
  int v434; // er9@2
  __m128i v435; // xmm6@2
  int v436; // eax@2
  int v437; // ecx@2
  int v438; // eax@2
  int v439; // ecx@2
  int v440; // eax@2
  int v441; // ecx@2
  int v442; // er8@2
  __m128i v443; // xmm3@2
  int v444; // er12@2
  int v445; // er8@2
  __m128i v446; // xmm5@2
  int v447; // eax@2
  int v448; // ecx@2
  int v449; // eax@2
  __m128i v450; // xmm7@2
  __m128i v451; // xmm4@2
  int v452; // ecx@2
  __m128i v453; // xmm6@2
  __m128i v454; // xmm7@2
  int v455; // eax@2
  int v456; // ecx@2
  int v457; // er15@2
  int v458; // er11@2
  int v459; // er15@2
  int v460; // eax@2
  int v461; // ecx@2
  int v462; // eax@2
  __m128i v463; // xmm0@2
  int v464; // ecx@2
  int v465; // eax@2
  int v466; // ecx@2
  __m128i v467; // xmm7@2
  int v468; // er14@2
  __m128i v469; // xmm4@2
  __m128i v470; // xmm6@2
  int v471; // er10@2
  __m128i v472; // xmm7@2
  int v473; // er14@2
  int v474; // eax@2
  int v475; // ecx@2
  int v476; // eax@2
  int v477; // ecx@2
  __m128i v478; // xmm0@2
  int v479; // eax@2
  int v480; // ecx@2
  int v481; // er13@2
  __m128i v482; // xmm6@2
  int v483; // er9@2
  __m128i v484; // xmm4@2
  __m128i v485; // xmm7@2
  int v486; // er13@2
  __m128i v487; // xmm6@2
  int v488; // eax@2
  int v489; // ecx@2
  int v490; // eax@2
  int v491; // ecx@2
  int v492; // eax@2
  int v493; // ecx@2
  int v494; // er12@2
  __m128i v495; // xmm0@2
  int v496; // er8@2
  int v497; // er12@2
  __m128i v498; // xmm5@2
  int v499; // eax@2
  int v500; // ecx@2
  int v501; // eax@2
  __m128i v502; // xmm7@2
  __m128i v503; // xmm4@2
  int v504; // ecx@2
  __m128i v505; // xmm6@2
  __m128i v506; // xmm7@2
  int v507; // eax@2
  int v508; // ecx@2
  int v509; // er11@2
  int v510; // er15@2
  int v511; // er11@2
  int v512; // eax@2
  int v513; // ecx@2
  int v514; // eax@2
  __m128i v515; // xmm1@2
  int v516; // ecx@2
  int v517; // eax@2
  int v518; // ecx@2
  __m128i v519; // xmm7@2
  int v520; // er10@2
  __m128i v521; // xmm4@2
  __m128i v522; // xmm6@2
  int v523; // er14@2
  __m128i v524; // xmm7@2
  int v525; // er10@2
  int v526; // eax@2
  int v527; // ecx@2
  int v528; // eax@2
  int v529; // ecx@2
  __m128i v530; // xmm1@2
  int v531; // eax@2
  int v532; // ecx@2
  int v533; // er9@2
  __m128i v534; // xmm6@2
  int v535; // er13@2
  __m128i v536; // xmm4@2
  __m128i v537; // xmm7@2
  int v538; // er9@2
  __m128i v539; // xmm6@2
  int v540; // eax@2
  int v541; // ecx@2
  int v542; // eax@2
  int v543; // ecx@2
  int v544; // eax@2
  int v545; // ecx@2
  int v546; // er8@2
  __m128i v547; // xmm1@2
  int v548; // er12@2
  int v549; // er8@2
  __m128i v550; // xmm5@2
  int v551; // eax@2
  int v552; // ecx@2
  int v553; // eax@2
  __m128i v554; // xmm7@2
  __m128i v555; // xmm4@2
  int v556; // ecx@2
  __m128i v557; // xmm6@2
  __m128i v558; // xmm7@2
  int v559; // eax@2
  int v560; // ecx@2
  int v561; // er15@2
  int v562; // er11@2
  int v563; // er15@2
  int v564; // eax@2
  int v565; // ecx@2
  int v566; // eax@2
  __m128i v567; // xmm2@2
  int v568; // ecx@2
  int v569; // eax@2
  int v570; // ecx@2
  __m128i v571; // xmm7@2
  int v572; // er14@2
  __m128i v573; // xmm4@2
  __m128i v574; // xmm6@2
  int v575; // er10@2
  __m128i v576; // xmm7@2
  int v577; // er14@2
  int v578; // eax@2
  int v579; // ecx@2
  int v580; // eax@2
  int v581; // ecx@2
  __m128i v582; // xmm2@2
  int v583; // eax@2
  int v584; // ecx@2
  int v585; // er13@2
  __m128i v586; // xmm6@2
  int v587; // er9@2
  __m128i v588; // xmm4@2
  __m128i v589; // xmm7@2
  int v590; // er13@2
  __m128i v591; // xmm6@2
  int v592; // eax@2
  int v593; // ecx@2
  int v594; // eax@2
  int v595; // ecx@2
  int v596; // eax@2
  int v597; // ecx@2
  int v598; // er12@2
  __m128i v599; // xmm2@2
  int v600; // er8@2
  int v601; // er12@2
  int v602; // eax@2
  int v603; // ecx@2
  int v604; // eax@2
  __m128i v605; // xmm7@2
  __m128i v606; // xmm4@2
  int v607; // ecx@2
  __m128i v608; // xmm6@2
  __m128i v609; // xmm7@2
  int v610; // eax@2
  int v611; // ecx@2
  int v612; // er11@2
  int v613; // er15@2
  int v614; // er11@2
  int v615; // eax@2
  int v616; // ecx@2
  int v617; // eax@2
  __m128i v618; // xmm3@2
  int v619; // ecx@2
  int v620; // eax@2
  int v621; // ecx@2
  __m128i v622; // xmm7@2
  int v623; // er10@2
  __m128i v624; // xmm4@2
  __m128i v625; // xmm6@2
  int v626; // er14@2
  __m128i v627; // xmm7@2
  int v628; // er10@2
  int v629; // eax@2
  int v630; // ecx@2
  int v631; // eax@2
  int v632; // ecx@2
  __m128i v633; // xmm3@2
  int v634; // eax@2
  int v635; // ecx@2
  int v636; // er9@2
  __m128i v637; // xmm6@2
  int v638; // er13@2
  __m128i v639; // xmm4@2
  __m128i v640; // xmm7@2
  int v641; // er9@2
  __m128i v642; // xmm6@2
  int v643; // eax@2
  int v644; // ecx@2
  int v645; // eax@2
  int v646; // ecx@2
  int v647; // eax@2
  int v648; // ecx@2
  int v649; // er8@2
  int v650; // er12@2
  int v651; // er8@2
  signed __int64 v652; // rbx@2
  int v653; // eax@3
  int v654; // ecx@3
  int v655; // eax@3
  int v656; // ecx@3
  int v657; // eax@3
  int v658; // ecx@3
  int v659; // er15@3
  int v660; // er11@3
  int v661; // er15@3
  int v662; // eax@3
  int v663; // ecx@3
  int v664; // eax@3
  int v665; // ecx@3
  int v666; // eax@3
  int v667; // ecx@3
  int v668; // er14@3
  int v669; // er10@3
  int v670; // er14@3
  int v671; // eax@3
  int v672; // ecx@3
  int v673; // eax@3
  int v674; // ecx@3
  int v675; // eax@3
  int v676; // ecx@3
  int v677; // er13@3
  int v678; // er9@3
  int v679; // er13@3
  int v680; // eax@3
  int v681; // ecx@3
  int v682; // eax@3
  int v683; // ecx@3
  int v684; // eax@3
  int v685; // ecx@3
  int v686; // er12@3
  int v687; // er8@3
  int v688; // er12@3
  int v689; // eax@3
  int v690; // ecx@3
  int v691; // eax@3
  int v692; // ecx@3
  int v693; // eax@3
  int v694; // ecx@3
  int v695; // er11@3
  int v696; // er15@3
  int v697; // er11@3
  int v698; // eax@3
  int v699; // ecx@3
  int v700; // eax@3
  int v701; // ecx@3
  int v702; // eax@3
  int v703; // ecx@3
  int v704; // er10@3
  int v705; // er14@3
  int v706; // er10@3
  int v707; // eax@3
  int v708; // ecx@3
  int v709; // eax@3
  int v710; // ecx@3
  int v711; // eax@3
  int v712; // ecx@3
  int v713; // er9@3
  int v714; // er13@3
  int v715; // er9@3
  int v716; // eax@3
  int v717; // ecx@3
  int v718; // eax@3
  int v719; // ecx@3
  int v720; // eax@3
  int v721; // ecx@3
  int v722; // er8@3
  int v723; // er12@3
  int v724; // er8@3
  int v725; // eax@3
  int v726; // ecx@3
  int v727; // eax@3
  int v728; // ecx@3
  int v729; // eax@3
  int v730; // ecx@3
  int v731; // er15@3
  int v732; // er11@3
  int v733; // er15@3
  int v734; // eax@3
  int v735; // ecx@3
  int v736; // eax@3
  int v737; // ecx@3
  int v738; // eax@3
  int v739; // ecx@3
  int v740; // er14@3
  int v741; // er10@3
  int v742; // er14@3
  int v743; // eax@3
  int v744; // ecx@3
  int v745; // eax@3
  int v746; // ecx@3
  int v747; // eax@3
  int v748; // ecx@3
  int v749; // er13@3
  int v750; // er9@3
  int v751; // er13@3
  int v752; // eax@3
  int v753; // ecx@3
  int v754; // eax@3
  int v755; // ecx@3
  int v756; // eax@3
  int v757; // ecx@3
  int v758; // er12@3
  int v759; // er8@3
  int v760; // er12@3
  int v761; // eax@3
  int v762; // ecx@3
  int v763; // eax@3
  int v764; // ecx@3
  int v765; // eax@3
  int v766; // ecx@3
  int v767; // er11@3
  int v768; // er15@3
  int v769; // er11@3
  int v770; // eax@3
  int v771; // ecx@3
  int v772; // eax@3
  int v773; // ecx@3
  int v774; // eax@3
  int v775; // ecx@3
  int v776; // er10@3
  int v777; // er14@3
  int v778; // er10@3
  int v779; // eax@3
  int v780; // ecx@3
  int v781; // eax@3
  int v782; // ecx@3
  int v783; // eax@3
  int v784; // ecx@3
  int v785; // er9@3
  int v786; // er13@3
  int v787; // er9@3
  int v788; // eax@3
  int v789; // ecx@3
  int v790; // eax@3
  int v791; // ecx@3
  int v792; // eax@3
  int v793; // ecx@3
  int v794; // er8@3
  int v795; // eax@4
  int v796; // ecx@4
  int v797; // eax@4
  int v798; // ecx@4
  int v799; // eax@4
  int v800; // ecx@4
  int v801; // er15@4
  int v802; // er11@4
  int v803; // er15@4
  int v804; // eax@4
  int v805; // ecx@4
  int v806; // eax@4
  int v807; // ecx@4
  int v808; // eax@4
  int v809; // ecx@4
  int v810; // er14@4
  int v811; // er10@4
  int v812; // er14@4
  int v813; // eax@4
  int v814; // ecx@4
  int v815; // eax@4
  int v816; // ecx@4
  int v817; // eax@4
  int v818; // ecx@4
  int v819; // er13@4
  int v820; // er9@4
  int v821; // er13@4
  int v822; // eax@4
  int v823; // ecx@4
  int v824; // eax@4
  int v825; // ecx@4
  int v826; // eax@4
  int v827; // ecx@4
  int v828; // er12@4
  int v829; // er8@4
  int v830; // er12@4
  int v831; // eax@4
  int v832; // ecx@4
  int v833; // eax@4
  int v834; // ecx@4
  int v835; // eax@4
  int v836; // ecx@4
  int v837; // er11@4
  int v838; // er15@4
  int v839; // er11@4
  int v840; // eax@4
  int v841; // ecx@4
  int v842; // eax@4
  int v843; // ecx@4
  int v844; // eax@4
  int v845; // ecx@4
  int v846; // er10@4
  int v847; // er14@4
  int v848; // er10@4
  int v849; // eax@4
  int v850; // ecx@4
  int v851; // eax@4
  int v852; // ecx@4
  int v853; // eax@4
  int v854; // ecx@4
  int v855; // er9@4
  int v856; // er13@4
  int v857; // er9@4
  int v858; // eax@4
  int v859; // ecx@4
  int v860; // eax@4
  int v861; // ecx@4
  int v862; // eax@4
  int v863; // ecx@4
  int v864; // er8@4
  int v865; // er12@4
  int v866; // er8@4
  int v867; // eax@4
  int v868; // ecx@4
  int v869; // eax@4
  int v870; // ecx@4
  int v871; // eax@4
  int v872; // ecx@4
  int v873; // er15@4
  int v874; // er11@4
  int v875; // er15@4
  int v876; // eax@4
  int v877; // ecx@4
  int v878; // eax@4
  int v879; // ecx@4
  int v880; // eax@4
  int v881; // ecx@4
  int v882; // er14@4
  int v883; // er10@4
  int v884; // er14@4
  int v885; // eax@4
  int v886; // ecx@4
  int v887; // eax@4
  int v888; // ecx@4
  int v889; // eax@4
  int v890; // ecx@4
  int v891; // er13@4
  int v892; // er9@4
  int v893; // er13@4
  int v894; // eax@4
  int v895; // ecx@4
  int v896; // eax@4
  int v897; // ecx@4
  int v898; // eax@4
  int v899; // ecx@4
  int v900; // er12@4
  int v901; // er8@4
  int v902; // er12@4
  int v903; // eax@4
  int v904; // ecx@4
  int v905; // eax@4
  int v906; // ecx@4
  int v907; // eax@4
  int v908; // ecx@4
  int v909; // er11@4
  int v910; // er15@4
  int v911; // er11@4
  int v912; // eax@4
  int v913; // ecx@4
  int v914; // eax@4
  int v915; // ecx@4
  int v916; // eax@4
  int v917; // ecx@4
  int v918; // er10@4
  int v919; // er14@4
  int v920; // er10@4
  int v921; // eax@4
  int v922; // ecx@4
  int v923; // eax@4
  int v924; // ecx@4
  int v925; // eax@4
  int v926; // ecx@4
  int v927; // er9@4
  int v928; // er13@4
  int v929; // er9@4
  int v930; // eax@4
  int v931; // ecx@4
  int v932; // eax@4
  int v933; // ecx@4
  int v934; // eax@4
  int v935; // ecx@4
  int v936; // er8@4
  __int64 result; // rax@4
  __int128 v938; // [sp+0h] [bp-108h]@1
  __int128 v939; // [sp+10h] [bp-F8h]@1
  __int128 v940; // [sp+20h] [bp-E8h]@1
  __int128 v941; // [sp+30h] [bp-D8h]@1
  __m128i v942; // [sp+40h] [bp-C8h]@1
  __int128 v943; // [sp+50h] [bp-B8h]@1
  __int128 v944; // [sp+60h] [bp-A8h]@1
  __int128 v945; // [sp+70h] [bp-98h]@1
  __int128 v946; // [sp+80h] [bp-88h]@1
  __int128 v947; // [sp+90h] [bp-78h]@1
  __int128 v948; // [sp+A0h] [bp-68h]@1
  __int128 v949; // [sp+B0h] [bp-58h]@1
  __int128 v950; // [sp+C0h] [bp-48h]@1

  _mm_store_si128((__m128i *)&v943, a4);
  _mm_store_si128((__m128i *)&v944, a5);
  _mm_store_si128((__m128i *)&v945, a6);
  _mm_store_si128((__m128i *)&v946, a7);
  _mm_store_si128((__m128i *)&v947, a8);
  _mm_store_si128((__m128i *)&v948, a9);
  _mm_store_si128((__m128i *)&v949, a10);
  _mm_store_si128((__m128i *)&v950, a11);
  _mm_store_si128(&v942, _mm_load_si128((const __m128i *)qword_579B0));
  v11 = _mm_loadu_si128((const __m128i *)a1);
  v12 = _mm_loadu_si128((const __m128i *)(a1 + 16));
  v13 = _mm_loadu_si128((const __m128i *)(a1 + 32));
  v14 = _mm_loadu_si128((const __m128i *)(a1 + 48));
  v15 = a1 + 64;
  v16 = _mm_shuffle_epi8(v11, v942);
  v17 = _mm_shuffle_epi8(v12, v942);
  v18 = _mm_shuffle_epi8(v13, v942);
  v19 = _mm_shuffle_epi8(v14, v942);
  v20 = (const __m128i *)&ccsha256_K[8];
  v21 = _mm_add_epi32(_mm_loadu_si128((const __m128i *)&ccsha256_K[2]), v17);
  v22 = _mm_add_epi32(_mm_loadu_si128((const __m128i *)&ccsha256_K[4]), v18);
  v23 = _mm_add_epi32(_mm_loadu_si128((const __m128i *)&ccsha256_K[6]), v19);
  _mm_store_si128((__m128i *)&v938, _mm_add_epi32(_mm_loadu_si128((const __m128i *)ccsha256_K), v16));
  _mm_store_si128((__m128i *)&v939, v21);
  _mm_store_si128((__m128i *)&v940, v22);
  _mm_store_si128((__m128i *)&v941, v23);
  while ( 1 )
  {
    v24 = *(_DWORD *)(a2 + 4);
    v25 = *(_DWORD *)(a2 + 8);
    v26 = *(_DWORD *)(a2 + 16);
    v27 = *(_DWORD *)(a2 + 20);
    v28 = *(_DWORD *)(a2 + 24);
    v29 = _mm_loadu_si128(v20);
    v30 = v20 + 1;
    v31 = __ROR4__(*(_DWORD *)(a2 + 16), 14);
    v32 = __ROR4__(*(_DWORD *)a2, 9);
    v33 = __ROR4__(v26 ^ v31, 5);
    v34 = _mm_alignr_epi8(v17, v16, 4);
    v35 = _mm_srl_epi32(v34, 3u);
    v36 = __ROR4__(*(_DWORD *)a2 ^ v32, 11);
    v37 = _mm_srl_epi32(v34, 7u);
    v38 = _mm_sll_epi32(v34, 0xEu);
    v39 = __ROR4__(v26 ^ v33, 6);
    v40 = __ROR4__(*(_DWORD *)a2 ^ v36, 2);
    v41 = v938 + v39 + (v28 ^ v26 & (v28 ^ *(_DWORD *)(a2 + 20))) + *(_DWORD *)(a2 + 28);
    v42 = v41 + *(_DWORD *)(a2 + 12);
    v43 = (v25 & *(_DWORD *)a2 | v24 & (v25 | *(_DWORD *)a2)) + v40 + v41;
    v44 = __ROR4__(v42, 14);
    v45 = __ROR4__(v43, 9);
    v46 = __ROR4__(v42 ^ v44, 5);
    v47 = _mm_add_epi32(
            _mm_add_epi32(
              v16,
              _mm_xor_si128(
                _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v35, v37), v38), _mm_srl_epi32(v37, 0xBu)),
                _mm_sll_epi32(v38, 0xBu))),
            _mm_alignr_epi8(v19, v18, 4));
    v48 = __ROR4__(v43 ^ v45, 11);
    v49 = __ROR4__(v42 ^ v46, 6);
    v50 = __ROR4__(v43 ^ v48, 2);
    v51 = _mm_srli_si128(v19, 8);
    v52 = DWORD1(v938) + v49 + (v27 ^ v42 & (v27 ^ *(_DWORD *)(a2 + 16))) + v28;
    v53 = _mm_srl_epi32(v51, 0xAu);
    v54 = _mm_srl_epi32(v51, 0x11u);
    v55 = v52 + v25;
    v56 = _mm_sll_epi32(v51, 0xDu);
    v57 = (v24 & v43 | *(_DWORD *)a2 & (v24 | v43)) + v50 + v52;
    v58 = __ROR4__(v55, 14);
    v59 = __ROR4__(v57, 9);
    v60 = __ROR4__(v55 ^ v58, 5);
    v61 = __ROR4__(v57 ^ v59, 11);
    v62 = _mm_add_epi32(
            v47,
            _mm_xor_si128(
              _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v53, v54), v56), _mm_srl_epi32(v54, 2u)),
              _mm_sll_epi32(v56, 2u)));
    v63 = __ROR4__(v55 ^ v60, 6);
    v64 = __ROR4__(v57 ^ v61, 2);
    v65 = DWORD2(v938) + v63 + (v26 ^ v55 & (v26 ^ v42)) + v27;
    v66 = _mm_slli_si128(v62, 8);
    v67 = v65 + v24;
    v68 = _mm_srl_epi32(v66, 0xAu);
    v69 = _mm_sll_epi32(v66, 0xDu);
    v70 = (*(_DWORD *)a2 & v57 | v43 & (*(_DWORD *)a2 | v57)) + v64 + v65;
    v71 = _mm_srl_epi32(v66, 0x11u);
    v72 = __ROR4__(v67, 14);
    v73 = __ROR4__(v70, 9);
    v74 = __ROR4__(v67 ^ v72, 5);
    v75 = __ROR4__(v70 ^ v73, 11);
    v76 = __ROR4__(v67 ^ v74, 6);
    v77 = __ROR4__(v70 ^ v75, 2);
    v78 = DWORD3(v938) + v76 + (v42 ^ v67 & (v42 ^ v55)) + v26;
    v79 = _mm_add_epi32(
            v62,
            _mm_xor_si128(
              _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v68, v69), v71), _mm_srl_epi32(v71, 2u)),
              _mm_sll_epi32(v69, 2u)));
    v80 = v78 + *(_DWORD *)a2;
    v81 = (v43 & v70 | v57 & (v43 | v70)) + v77 + v78;
    _mm_store_si128((__m128i *)&v938, _mm_add_epi32(v29, v79));
    v82 = _mm_loadu_si128(v30);
    ++v30;
    v83 = __ROR4__(v80, 14);
    v84 = __ROR4__(v81, 9);
    v85 = __ROR4__(v80 ^ v83, 5);
    v86 = _mm_alignr_epi8(v18, v17, 4);
    v87 = _mm_srl_epi32(v86, 3u);
    v88 = __ROR4__(v81 ^ v84, 11);
    v89 = _mm_srl_epi32(v86, 7u);
    v90 = _mm_sll_epi32(v86, 0xEu);
    v91 = __ROR4__(v80 ^ v85, 6);
    v92 = __ROR4__(v81 ^ v88, 2);
    v93 = v939 + v91 + (v55 ^ v80 & (v55 ^ v67)) + v42;
    v94 = v93 + v43;
    v95 = (v57 & v81 | v70 & (v57 | v81)) + v92 + v93;
    v96 = __ROR4__(v94, 14);
    v97 = __ROR4__(v95, 9);
    v98 = __ROR4__(v94 ^ v96, 5);
    v99 = _mm_add_epi32(
            _mm_add_epi32(
              v17,
              _mm_xor_si128(
                _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v87, v89), v90), _mm_srl_epi32(v89, 0xBu)),
                _mm_sll_epi32(v90, 0xBu))),
            _mm_alignr_epi8(v79, v19, 4));
    v100 = __ROR4__(v95 ^ v97, 11);
    v101 = __ROR4__(v94 ^ v98, 6);
    v102 = __ROR4__(v95 ^ v100, 2);
    v103 = _mm_srli_si128(v79, 8);
    v104 = DWORD1(v939) + v101 + (v67 ^ v94 & (v67 ^ v80)) + v55;
    v105 = _mm_srl_epi32(v103, 0xAu);
    v106 = _mm_srl_epi32(v103, 0x11u);
    v107 = v104 + v57;
    v108 = _mm_sll_epi32(v103, 0xDu);
    v109 = (v70 & v95 | v81 & (v70 | v95)) + v102 + v104;
    v110 = __ROR4__(v107, 14);
    v111 = __ROR4__(v109, 9);
    v112 = __ROR4__(v107 ^ v110, 5);
    v113 = __ROR4__(v109 ^ v111, 11);
    v114 = _mm_add_epi32(
             v99,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v105, v106), v108), _mm_srl_epi32(v106, 2u)),
               _mm_sll_epi32(v108, 2u)));
    v115 = __ROR4__(v107 ^ v112, 6);
    v116 = __ROR4__(v109 ^ v113, 2);
    v117 = DWORD2(v939) + v115 + (v80 ^ v107 & (v80 ^ v94)) + v67;
    v118 = _mm_slli_si128(v114, 8);
    v119 = v117 + v70;
    v120 = _mm_srl_epi32(v118, 0xAu);
    v121 = _mm_sll_epi32(v118, 0xDu);
    v122 = (v81 & v109 | v95 & (v81 | v109)) + v116 + v117;
    v123 = _mm_srl_epi32(v118, 0x11u);
    v124 = __ROR4__(v119, 14);
    v125 = __ROR4__(v122, 9);
    v126 = __ROR4__(v119 ^ v124, 5);
    v127 = __ROR4__(v122 ^ v125, 11);
    v128 = __ROR4__(v119 ^ v126, 6);
    v129 = __ROR4__(v122 ^ v127, 2);
    v130 = DWORD3(v939) + v128 + (v94 ^ v119 & (v94 ^ v107)) + v80;
    v131 = _mm_add_epi32(
             v114,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v120, v121), v123), _mm_srl_epi32(v123, 2u)),
               _mm_sll_epi32(v121, 2u)));
    v132 = v130 + v81;
    v133 = (v95 & v122 | v109 & (v95 | v122)) + v129 + v130;
    _mm_store_si128((__m128i *)&v939, _mm_add_epi32(v82, v131));
    v134 = _mm_loadu_si128(v30);
    ++v30;
    v135 = __ROR4__(v132, 14);
    v136 = __ROR4__(v133, 9);
    v137 = __ROR4__(v132 ^ v135, 5);
    v138 = _mm_alignr_epi8(v19, v18, 4);
    v139 = _mm_srl_epi32(v138, 3u);
    v140 = __ROR4__(v133 ^ v136, 11);
    v141 = _mm_srl_epi32(v138, 7u);
    v142 = _mm_sll_epi32(v138, 0xEu);
    v143 = __ROR4__(v132 ^ v137, 6);
    v144 = __ROR4__(v133 ^ v140, 2);
    v145 = v940 + v143 + (v107 ^ v132 & (v107 ^ v119)) + v94;
    v146 = v145 + v95;
    v147 = (v109 & v133 | v122 & (v109 | v133)) + v144 + v145;
    v148 = __ROR4__(v146, 14);
    v149 = __ROR4__(v147, 9);
    v150 = __ROR4__(v146 ^ v148, 5);
    v151 = _mm_add_epi32(
             _mm_add_epi32(
               v18,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v139, v141), v142), _mm_srl_epi32(v141, 0xBu)),
                 _mm_sll_epi32(v142, 0xBu))),
             _mm_alignr_epi8(v131, v79, 4));
    v152 = __ROR4__(v147 ^ v149, 11);
    v153 = __ROR4__(v146 ^ v150, 6);
    v154 = __ROR4__(v147 ^ v152, 2);
    v155 = _mm_srli_si128(v131, 8);
    v156 = DWORD1(v940) + v153 + (v119 ^ v146 & (v119 ^ v132)) + v107;
    v157 = _mm_srl_epi32(v155, 0xAu);
    v158 = _mm_srl_epi32(v155, 0x11u);
    v159 = v156 + v109;
    v160 = _mm_sll_epi32(v155, 0xDu);
    v161 = (v122 & v147 | v133 & (v122 | v147)) + v154 + v156;
    v162 = __ROR4__(v159, 14);
    v163 = __ROR4__(v161, 9);
    v164 = __ROR4__(v159 ^ v162, 5);
    v165 = __ROR4__(v161 ^ v163, 11);
    v166 = _mm_add_epi32(
             v151,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v157, v158), v160), _mm_srl_epi32(v158, 2u)),
               _mm_sll_epi32(v160, 2u)));
    v167 = __ROR4__(v159 ^ v164, 6);
    v168 = __ROR4__(v161 ^ v165, 2);
    v169 = DWORD2(v940) + v167 + (v132 ^ v159 & (v132 ^ v146)) + v119;
    v170 = _mm_slli_si128(v166, 8);
    v171 = v169 + v122;
    v172 = _mm_srl_epi32(v170, 0xAu);
    v173 = _mm_sll_epi32(v170, 0xDu);
    v174 = (v133 & v161 | v147 & (v133 | v161)) + v168 + v169;
    v175 = _mm_srl_epi32(v170, 0x11u);
    v176 = __ROR4__(v171, 14);
    v177 = __ROR4__(v174, 9);
    v178 = __ROR4__(v171 ^ v176, 5);
    v179 = __ROR4__(v174 ^ v177, 11);
    v180 = __ROR4__(v171 ^ v178, 6);
    v181 = __ROR4__(v174 ^ v179, 2);
    v182 = DWORD3(v940) + v180 + (v146 ^ v171 & (v146 ^ v159)) + v132;
    v183 = _mm_add_epi32(
             v166,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v172, v173), v175), _mm_srl_epi32(v175, 2u)),
               _mm_sll_epi32(v173, 2u)));
    v184 = v182 + v133;
    v185 = (v147 & v174 | v161 & (v147 | v174)) + v181 + v182;
    _mm_store_si128((__m128i *)&v940, _mm_add_epi32(v134, v183));
    v186 = _mm_loadu_si128(v30);
    ++v30;
    v187 = __ROR4__(v184, 14);
    v188 = __ROR4__(v185, 9);
    v189 = __ROR4__(v184 ^ v187, 5);
    v190 = _mm_alignr_epi8(v79, v19, 4);
    v191 = _mm_srl_epi32(v190, 3u);
    v192 = __ROR4__(v185 ^ v188, 11);
    v193 = _mm_srl_epi32(v190, 7u);
    v194 = _mm_sll_epi32(v190, 0xEu);
    v195 = __ROR4__(v184 ^ v189, 6);
    v196 = __ROR4__(v185 ^ v192, 2);
    v197 = v941 + v195 + (v159 ^ v184 & (v159 ^ v171)) + v146;
    v198 = v197 + v147;
    v199 = (v161 & v185 | v174 & (v161 | v185)) + v196 + v197;
    v200 = __ROR4__(v198, 14);
    v201 = __ROR4__(v199, 9);
    v202 = __ROR4__(v198 ^ v200, 5);
    v203 = _mm_add_epi32(
             _mm_add_epi32(
               v19,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v191, v193), v194), _mm_srl_epi32(v193, 0xBu)),
                 _mm_sll_epi32(v194, 0xBu))),
             _mm_alignr_epi8(v183, v131, 4));
    v204 = __ROR4__(v199 ^ v201, 11);
    v205 = __ROR4__(v198 ^ v202, 6);
    v206 = __ROR4__(v199 ^ v204, 2);
    v207 = _mm_srli_si128(v183, 8);
    v208 = DWORD1(v941) + v205 + (v171 ^ v198 & (v171 ^ v184)) + v159;
    v209 = _mm_srl_epi32(v207, 0xAu);
    v210 = _mm_srl_epi32(v207, 0x11u);
    v211 = v208 + v161;
    v212 = _mm_sll_epi32(v207, 0xDu);
    v213 = (v174 & v199 | v185 & (v174 | v199)) + v206 + v208;
    v214 = __ROR4__(v211, 14);
    v215 = __ROR4__(v213, 9);
    v216 = __ROR4__(v211 ^ v214, 5);
    v217 = __ROR4__(v213 ^ v215, 11);
    v218 = _mm_add_epi32(
             v203,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v209, v210), v212), _mm_srl_epi32(v210, 2u)),
               _mm_sll_epi32(v212, 2u)));
    v219 = __ROR4__(v211 ^ v216, 6);
    v220 = __ROR4__(v213 ^ v217, 2);
    v221 = DWORD2(v941) + v219 + (v184 ^ v211 & (v184 ^ v198)) + v171;
    v222 = _mm_slli_si128(v218, 8);
    v223 = v221 + v174;
    v224 = _mm_srl_epi32(v222, 0xAu);
    v225 = _mm_sll_epi32(v222, 0xDu);
    v226 = (v185 & v213 | v199 & (v185 | v213)) + v220 + v221;
    v227 = _mm_srl_epi32(v222, 0x11u);
    v228 = __ROR4__(v223, 14);
    v229 = __ROR4__(v226, 9);
    v230 = __ROR4__(v223 ^ v228, 5);
    v231 = __ROR4__(v226 ^ v229, 11);
    v232 = __ROR4__(v223 ^ v230, 6);
    v233 = __ROR4__(v226 ^ v231, 2);
    v234 = DWORD3(v941) + v232 + (v198 ^ v223 & (v198 ^ v211)) + v184;
    v235 = _mm_add_epi32(
             v218,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v224, v225), v227), _mm_srl_epi32(v227, 2u)),
               _mm_sll_epi32(v225, 2u)));
    v236 = v234 + v185;
    v237 = (v199 & v226 | v213 & (v199 | v226)) + v233 + v234;
    _mm_store_si128((__m128i *)&v941, _mm_add_epi32(v186, v235));
    v238 = _mm_loadu_si128(v30);
    ++v30;
    v239 = __ROR4__(v236, 14);
    v240 = __ROR4__(v237, 9);
    v241 = __ROR4__(v236 ^ v239, 5);
    v242 = _mm_alignr_epi8(v131, v79, 4);
    v243 = _mm_srl_epi32(v242, 3u);
    v244 = __ROR4__(v237 ^ v240, 11);
    v245 = _mm_srl_epi32(v242, 7u);
    v246 = _mm_sll_epi32(v242, 0xEu);
    v247 = __ROR4__(v236 ^ v241, 6);
    v248 = __ROR4__(v237 ^ v244, 2);
    v249 = v938 + v247 + (v211 ^ v236 & (v211 ^ v223)) + v198;
    v250 = v249 + v199;
    v251 = (v213 & v237 | v226 & (v213 | v237)) + v248 + v249;
    v252 = __ROR4__(v250, 14);
    v253 = __ROR4__(v251, 9);
    v254 = __ROR4__(v250 ^ v252, 5);
    v255 = _mm_add_epi32(
             _mm_add_epi32(
               v79,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v243, v245), v246), _mm_srl_epi32(v245, 0xBu)),
                 _mm_sll_epi32(v246, 0xBu))),
             _mm_alignr_epi8(v235, v183, 4));
    v256 = __ROR4__(v251 ^ v253, 11);
    v257 = __ROR4__(v250 ^ v254, 6);
    v258 = __ROR4__(v251 ^ v256, 2);
    v259 = _mm_srli_si128(v235, 8);
    v260 = DWORD1(v938) + v257 + (v223 ^ v250 & (v223 ^ v236)) + v211;
    v261 = _mm_srl_epi32(v259, 0xAu);
    v262 = _mm_srl_epi32(v259, 0x11u);
    v263 = v260 + v213;
    v264 = _mm_sll_epi32(v259, 0xDu);
    v265 = (v226 & v251 | v237 & (v226 | v251)) + v258 + v260;
    v266 = __ROR4__(v263, 14);
    v267 = __ROR4__(v265, 9);
    v268 = __ROR4__(v263 ^ v266, 5);
    v269 = __ROR4__(v265 ^ v267, 11);
    v270 = _mm_add_epi32(
             v255,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v261, v262), v264), _mm_srl_epi32(v262, 2u)),
               _mm_sll_epi32(v264, 2u)));
    v271 = __ROR4__(v263 ^ v268, 6);
    v272 = __ROR4__(v265 ^ v269, 2);
    v273 = DWORD2(v938) + v271 + (v236 ^ v263 & (v236 ^ v250)) + v223;
    v274 = _mm_slli_si128(v270, 8);
    v275 = v273 + v226;
    v276 = _mm_srl_epi32(v274, 0xAu);
    v277 = _mm_sll_epi32(v274, 0xDu);
    v278 = (v237 & v265 | v251 & (v237 | v265)) + v272 + v273;
    v279 = _mm_srl_epi32(v274, 0x11u);
    v280 = __ROR4__(v275, 14);
    v281 = __ROR4__(v278, 9);
    v282 = __ROR4__(v275 ^ v280, 5);
    v283 = __ROR4__(v278 ^ v281, 11);
    v284 = __ROR4__(v275 ^ v282, 6);
    v285 = __ROR4__(v278 ^ v283, 2);
    v286 = DWORD3(v938) + v284 + (v250 ^ v275 & (v250 ^ v263)) + v236;
    v287 = _mm_add_epi32(
             v270,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v276, v277), v279), _mm_srl_epi32(v279, 2u)),
               _mm_sll_epi32(v277, 2u)));
    v288 = v286 + v237;
    v289 = (v251 & v278 | v265 & (v251 | v278)) + v285 + v286;
    _mm_store_si128((__m128i *)&v938, _mm_add_epi32(v238, v287));
    v290 = _mm_loadu_si128(v30);
    ++v30;
    v291 = __ROR4__(v288, 14);
    v292 = __ROR4__(v289, 9);
    v293 = __ROR4__(v288 ^ v291, 5);
    v294 = _mm_alignr_epi8(v183, v131, 4);
    v295 = _mm_srl_epi32(v294, 3u);
    v296 = __ROR4__(v289 ^ v292, 11);
    v297 = _mm_srl_epi32(v294, 7u);
    v298 = _mm_sll_epi32(v294, 0xEu);
    v299 = __ROR4__(v288 ^ v293, 6);
    v300 = __ROR4__(v289 ^ v296, 2);
    v301 = v939 + v299 + (v263 ^ v288 & (v263 ^ v275)) + v250;
    v302 = v301 + v251;
    v303 = (v265 & v289 | v278 & (v265 | v289)) + v300 + v301;
    v304 = __ROR4__(v302, 14);
    v305 = __ROR4__(v303, 9);
    v306 = __ROR4__(v302 ^ v304, 5);
    v307 = _mm_add_epi32(
             _mm_add_epi32(
               v131,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v295, v297), v298), _mm_srl_epi32(v297, 0xBu)),
                 _mm_sll_epi32(v298, 0xBu))),
             _mm_alignr_epi8(v287, v235, 4));
    v308 = __ROR4__(v303 ^ v305, 11);
    v309 = __ROR4__(v302 ^ v306, 6);
    v310 = __ROR4__(v303 ^ v308, 2);
    v311 = _mm_srli_si128(v287, 8);
    v312 = DWORD1(v939) + v309 + (v275 ^ v302 & (v275 ^ v288)) + v263;
    v313 = _mm_srl_epi32(v311, 0xAu);
    v314 = _mm_srl_epi32(v311, 0x11u);
    v315 = v312 + v265;
    v316 = _mm_sll_epi32(v311, 0xDu);
    v317 = (v278 & v303 | v289 & (v278 | v303)) + v310 + v312;
    v318 = __ROR4__(v315, 14);
    v319 = __ROR4__(v317, 9);
    v320 = __ROR4__(v315 ^ v318, 5);
    v321 = __ROR4__(v317 ^ v319, 11);
    v322 = _mm_add_epi32(
             v307,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v313, v314), v316), _mm_srl_epi32(v314, 2u)),
               _mm_sll_epi32(v316, 2u)));
    v323 = __ROR4__(v315 ^ v320, 6);
    v324 = __ROR4__(v317 ^ v321, 2);
    v325 = DWORD2(v939) + v323 + (v288 ^ v315 & (v288 ^ v302)) + v275;
    v326 = _mm_slli_si128(v322, 8);
    v327 = v325 + v278;
    v328 = _mm_srl_epi32(v326, 0xAu);
    v329 = _mm_sll_epi32(v326, 0xDu);
    v330 = (v289 & v317 | v303 & (v289 | v317)) + v324 + v325;
    v331 = _mm_srl_epi32(v326, 0x11u);
    v332 = __ROR4__(v327, 14);
    v333 = __ROR4__(v330, 9);
    v334 = __ROR4__(v327 ^ v332, 5);
    v335 = __ROR4__(v330 ^ v333, 11);
    v336 = __ROR4__(v327 ^ v334, 6);
    v337 = __ROR4__(v330 ^ v335, 2);
    v338 = DWORD3(v939) + v336 + (v302 ^ v327 & (v302 ^ v315)) + v288;
    v339 = _mm_add_epi32(
             v322,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v328, v329), v331), _mm_srl_epi32(v331, 2u)),
               _mm_sll_epi32(v329, 2u)));
    v340 = v338 + v289;
    v341 = (v303 & v330 | v317 & (v303 | v330)) + v337 + v338;
    _mm_store_si128((__m128i *)&v939, _mm_add_epi32(v290, v339));
    v342 = _mm_loadu_si128(v30);
    ++v30;
    v343 = __ROR4__(v340, 14);
    v344 = __ROR4__(v341, 9);
    v345 = __ROR4__(v340 ^ v343, 5);
    v346 = _mm_alignr_epi8(v235, v183, 4);
    v347 = _mm_srl_epi32(v346, 3u);
    v348 = __ROR4__(v341 ^ v344, 11);
    v349 = _mm_srl_epi32(v346, 7u);
    v350 = _mm_sll_epi32(v346, 0xEu);
    v351 = __ROR4__(v340 ^ v345, 6);
    v352 = __ROR4__(v341 ^ v348, 2);
    v353 = v940 + v351 + (v315 ^ v340 & (v315 ^ v327)) + v302;
    v354 = v353 + v303;
    v355 = (v317 & v341 | v330 & (v317 | v341)) + v352 + v353;
    v356 = __ROR4__(v354, 14);
    v357 = __ROR4__(v355, 9);
    v358 = __ROR4__(v354 ^ v356, 5);
    v359 = _mm_add_epi32(
             _mm_add_epi32(
               v183,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v347, v349), v350), _mm_srl_epi32(v349, 0xBu)),
                 _mm_sll_epi32(v350, 0xBu))),
             _mm_alignr_epi8(v339, v287, 4));
    v360 = __ROR4__(v355 ^ v357, 11);
    v361 = __ROR4__(v354 ^ v358, 6);
    v362 = __ROR4__(v355 ^ v360, 2);
    v363 = _mm_srli_si128(v339, 8);
    v364 = DWORD1(v940) + v361 + (v327 ^ v354 & (v327 ^ v340)) + v315;
    v365 = _mm_srl_epi32(v363, 0xAu);
    v366 = _mm_srl_epi32(v363, 0x11u);
    v367 = v364 + v317;
    v368 = _mm_sll_epi32(v363, 0xDu);
    v369 = (v330 & v355 | v341 & (v330 | v355)) + v362 + v364;
    v370 = __ROR4__(v367, 14);
    v371 = __ROR4__(v369, 9);
    v372 = __ROR4__(v367 ^ v370, 5);
    v373 = __ROR4__(v369 ^ v371, 11);
    v374 = _mm_add_epi32(
             v359,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v365, v366), v368), _mm_srl_epi32(v366, 2u)),
               _mm_sll_epi32(v368, 2u)));
    v375 = __ROR4__(v367 ^ v372, 6);
    v376 = __ROR4__(v369 ^ v373, 2);
    v377 = DWORD2(v940) + v375 + (v340 ^ v367 & (v340 ^ v354)) + v327;
    v378 = _mm_slli_si128(v374, 8);
    v379 = v377 + v330;
    v380 = _mm_srl_epi32(v378, 0xAu);
    v381 = _mm_sll_epi32(v378, 0xDu);
    v382 = (v341 & v369 | v355 & (v341 | v369)) + v376 + v377;
    v383 = _mm_srl_epi32(v378, 0x11u);
    v384 = __ROR4__(v379, 14);
    v385 = __ROR4__(v382, 9);
    v386 = __ROR4__(v379 ^ v384, 5);
    v387 = __ROR4__(v382 ^ v385, 11);
    v388 = __ROR4__(v379 ^ v386, 6);
    v389 = __ROR4__(v382 ^ v387, 2);
    v390 = DWORD3(v940) + v388 + (v354 ^ v379 & (v354 ^ v367)) + v340;
    v391 = _mm_add_epi32(
             v374,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v380, v381), v383), _mm_srl_epi32(v383, 2u)),
               _mm_sll_epi32(v381, 2u)));
    v392 = v390 + v341;
    v393 = (v355 & v382 | v369 & (v355 | v382)) + v389 + v390;
    _mm_store_si128((__m128i *)&v940, _mm_add_epi32(v342, v391));
    v394 = _mm_loadu_si128(v30);
    ++v30;
    v395 = __ROR4__(v392, 14);
    v396 = __ROR4__(v393, 9);
    v397 = __ROR4__(v392 ^ v395, 5);
    v398 = _mm_alignr_epi8(v287, v235, 4);
    v399 = _mm_srl_epi32(v398, 3u);
    v400 = __ROR4__(v393 ^ v396, 11);
    v401 = _mm_srl_epi32(v398, 7u);
    v402 = _mm_sll_epi32(v398, 0xEu);
    v403 = __ROR4__(v392 ^ v397, 6);
    v404 = __ROR4__(v393 ^ v400, 2);
    v405 = v941 + v403 + (v367 ^ v392 & (v367 ^ v379)) + v354;
    v406 = v405 + v355;
    v407 = (v369 & v393 | v382 & (v369 | v393)) + v404 + v405;
    v408 = __ROR4__(v406, 14);
    v409 = __ROR4__(v407, 9);
    v410 = __ROR4__(v406 ^ v408, 5);
    v411 = _mm_add_epi32(
             _mm_add_epi32(
               v235,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v399, v401), v402), _mm_srl_epi32(v401, 0xBu)),
                 _mm_sll_epi32(v402, 0xBu))),
             _mm_alignr_epi8(v391, v339, 4));
    v412 = __ROR4__(v407 ^ v409, 11);
    v413 = __ROR4__(v406 ^ v410, 6);
    v414 = __ROR4__(v407 ^ v412, 2);
    v415 = _mm_srli_si128(v391, 8);
    v416 = DWORD1(v941) + v413 + (v379 ^ v406 & (v379 ^ v392)) + v367;
    v417 = _mm_srl_epi32(v415, 0xAu);
    v418 = _mm_srl_epi32(v415, 0x11u);
    v419 = v416 + v369;
    v420 = _mm_sll_epi32(v415, 0xDu);
    v421 = (v382 & v407 | v393 & (v382 | v407)) + v414 + v416;
    v422 = __ROR4__(v419, 14);
    v423 = __ROR4__(v421, 9);
    v424 = __ROR4__(v419 ^ v422, 5);
    v425 = __ROR4__(v421 ^ v423, 11);
    v426 = _mm_add_epi32(
             v411,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v417, v418), v420), _mm_srl_epi32(v418, 2u)),
               _mm_sll_epi32(v420, 2u)));
    v427 = __ROR4__(v419 ^ v424, 6);
    v428 = __ROR4__(v421 ^ v425, 2);
    v429 = DWORD2(v941) + v427 + (v392 ^ v419 & (v392 ^ v406)) + v379;
    v430 = _mm_slli_si128(v426, 8);
    v431 = v429 + v382;
    v432 = _mm_srl_epi32(v430, 0xAu);
    v433 = _mm_sll_epi32(v430, 0xDu);
    v434 = (v393 & v421 | v407 & (v393 | v421)) + v428 + v429;
    v435 = _mm_srl_epi32(v430, 0x11u);
    v436 = __ROR4__(v431, 14);
    v437 = __ROR4__(v434, 9);
    v438 = __ROR4__(v431 ^ v436, 5);
    v439 = __ROR4__(v434 ^ v437, 11);
    v440 = __ROR4__(v431 ^ v438, 6);
    v441 = __ROR4__(v434 ^ v439, 2);
    v442 = DWORD3(v941) + v440 + (v406 ^ v431 & (v406 ^ v419)) + v392;
    v443 = _mm_add_epi32(
             v426,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v432, v433), v435), _mm_srl_epi32(v435, 2u)),
               _mm_sll_epi32(v433, 2u)));
    v444 = v442 + v393;
    v445 = (v407 & v434 | v421 & (v407 | v434)) + v441 + v442;
    _mm_store_si128((__m128i *)&v941, _mm_add_epi32(v394, v443));
    v446 = _mm_loadu_si128(v30);
    ++v30;
    v447 = __ROR4__(v444, 14);
    v448 = __ROR4__(v445, 9);
    v449 = __ROR4__(v444 ^ v447, 5);
    v450 = _mm_alignr_epi8(v339, v287, 4);
    v451 = _mm_srl_epi32(v450, 3u);
    v452 = __ROR4__(v445 ^ v448, 11);
    v453 = _mm_srl_epi32(v450, 7u);
    v454 = _mm_sll_epi32(v450, 0xEu);
    v455 = __ROR4__(v444 ^ v449, 6);
    v456 = __ROR4__(v445 ^ v452, 2);
    v457 = v938 + v455 + (v419 ^ v444 & (v419 ^ v431)) + v406;
    v458 = v457 + v407;
    v459 = (v421 & v445 | v434 & (v421 | v445)) + v456 + v457;
    v460 = __ROR4__(v458, 14);
    v461 = __ROR4__(v459, 9);
    v462 = __ROR4__(v458 ^ v460, 5);
    v463 = _mm_add_epi32(
             _mm_add_epi32(
               v287,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v451, v453), v454), _mm_srl_epi32(v453, 0xBu)),
                 _mm_sll_epi32(v454, 0xBu))),
             _mm_alignr_epi8(v443, v391, 4));
    v464 = __ROR4__(v459 ^ v461, 11);
    v465 = __ROR4__(v458 ^ v462, 6);
    v466 = __ROR4__(v459 ^ v464, 2);
    v467 = _mm_srli_si128(v443, 8);
    v468 = DWORD1(v938) + v465 + (v431 ^ v458 & (v431 ^ v444)) + v419;
    v469 = _mm_srl_epi32(v467, 0xAu);
    v470 = _mm_srl_epi32(v467, 0x11u);
    v471 = v468 + v421;
    v472 = _mm_sll_epi32(v467, 0xDu);
    v473 = (v434 & v459 | v445 & (v434 | v459)) + v466 + v468;
    v474 = __ROR4__(v471, 14);
    v475 = __ROR4__(v473, 9);
    v476 = __ROR4__(v471 ^ v474, 5);
    v477 = __ROR4__(v473 ^ v475, 11);
    v478 = _mm_add_epi32(
             v463,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v469, v470), v472), _mm_srl_epi32(v470, 2u)),
               _mm_sll_epi32(v472, 2u)));
    v479 = __ROR4__(v471 ^ v476, 6);
    v480 = __ROR4__(v473 ^ v477, 2);
    v481 = DWORD2(v938) + v479 + (v444 ^ v471 & (v444 ^ v458)) + v431;
    v482 = _mm_slli_si128(v478, 8);
    v483 = v481 + v434;
    v484 = _mm_srl_epi32(v482, 0xAu);
    v485 = _mm_sll_epi32(v482, 0xDu);
    v486 = (v445 & v473 | v459 & (v445 | v473)) + v480 + v481;
    v487 = _mm_srl_epi32(v482, 0x11u);
    v488 = __ROR4__(v483, 14);
    v489 = __ROR4__(v486, 9);
    v490 = __ROR4__(v483 ^ v488, 5);
    v491 = __ROR4__(v486 ^ v489, 11);
    v492 = __ROR4__(v483 ^ v490, 6);
    v493 = __ROR4__(v486 ^ v491, 2);
    v494 = DWORD3(v938) + v492 + (v458 ^ v483 & (v458 ^ v471)) + v444;
    v495 = _mm_add_epi32(
             v478,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v484, v485), v487), _mm_srl_epi32(v487, 2u)),
               _mm_sll_epi32(v485, 2u)));
    v496 = v494 + v445;
    v497 = (v459 & v486 | v473 & (v459 | v486)) + v493 + v494;
    _mm_store_si128((__m128i *)&v938, _mm_add_epi32(v446, v495));
    v498 = _mm_loadu_si128(v30);
    ++v30;
    v499 = __ROR4__(v496, 14);
    v500 = __ROR4__(v497, 9);
    v501 = __ROR4__(v496 ^ v499, 5);
    v502 = _mm_alignr_epi8(v391, v339, 4);
    v503 = _mm_srl_epi32(v502, 3u);
    v504 = __ROR4__(v497 ^ v500, 11);
    v505 = _mm_srl_epi32(v502, 7u);
    v506 = _mm_sll_epi32(v502, 0xEu);
    v507 = __ROR4__(v496 ^ v501, 6);
    v508 = __ROR4__(v497 ^ v504, 2);
    v509 = v939 + v507 + (v471 ^ v496 & (v471 ^ v483)) + v458;
    v510 = v509 + v459;
    v511 = (v473 & v497 | v486 & (v473 | v497)) + v508 + v509;
    v512 = __ROR4__(v510, 14);
    v513 = __ROR4__(v511, 9);
    v514 = __ROR4__(v510 ^ v512, 5);
    v515 = _mm_add_epi32(
             _mm_add_epi32(
               v339,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v503, v505), v506), _mm_srl_epi32(v505, 0xBu)),
                 _mm_sll_epi32(v506, 0xBu))),
             _mm_alignr_epi8(v495, v443, 4));
    v516 = __ROR4__(v511 ^ v513, 11);
    v517 = __ROR4__(v510 ^ v514, 6);
    v518 = __ROR4__(v511 ^ v516, 2);
    v519 = _mm_srli_si128(v495, 8);
    v520 = DWORD1(v939) + v517 + (v483 ^ v510 & (v483 ^ v496)) + v471;
    v521 = _mm_srl_epi32(v519, 0xAu);
    v522 = _mm_srl_epi32(v519, 0x11u);
    v523 = v520 + v473;
    v524 = _mm_sll_epi32(v519, 0xDu);
    v525 = (v486 & v511 | v497 & (v486 | v511)) + v518 + v520;
    v526 = __ROR4__(v523, 14);
    v527 = __ROR4__(v525, 9);
    v528 = __ROR4__(v523 ^ v526, 5);
    v529 = __ROR4__(v525 ^ v527, 11);
    v530 = _mm_add_epi32(
             v515,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v521, v522), v524), _mm_srl_epi32(v522, 2u)),
               _mm_sll_epi32(v524, 2u)));
    v531 = __ROR4__(v523 ^ v528, 6);
    v532 = __ROR4__(v525 ^ v529, 2);
    v533 = DWORD2(v939) + v531 + (v496 ^ v523 & (v496 ^ v510)) + v483;
    v534 = _mm_slli_si128(v530, 8);
    v535 = v533 + v486;
    v536 = _mm_srl_epi32(v534, 0xAu);
    v537 = _mm_sll_epi32(v534, 0xDu);
    v538 = (v497 & v525 | v511 & (v497 | v525)) + v532 + v533;
    v539 = _mm_srl_epi32(v534, 0x11u);
    v540 = __ROR4__(v535, 14);
    v541 = __ROR4__(v538, 9);
    v542 = __ROR4__(v535 ^ v540, 5);
    v543 = __ROR4__(v538 ^ v541, 11);
    v544 = __ROR4__(v535 ^ v542, 6);
    v545 = __ROR4__(v538 ^ v543, 2);
    v546 = DWORD3(v939) + v544 + (v510 ^ v535 & (v510 ^ v523)) + v496;
    v547 = _mm_add_epi32(
             v530,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v536, v537), v539), _mm_srl_epi32(v539, 2u)),
               _mm_sll_epi32(v537, 2u)));
    v548 = v546 + v497;
    v549 = (v511 & v538 | v525 & (v511 | v538)) + v545 + v546;
    _mm_store_si128((__m128i *)&v939, _mm_add_epi32(v498, v547));
    v550 = _mm_loadu_si128(v30);
    ++v30;
    v551 = __ROR4__(v548, 14);
    v552 = __ROR4__(v549, 9);
    v553 = __ROR4__(v548 ^ v551, 5);
    v554 = _mm_alignr_epi8(v443, v391, 4);
    v555 = _mm_srl_epi32(v554, 3u);
    v556 = __ROR4__(v549 ^ v552, 11);
    v557 = _mm_srl_epi32(v554, 7u);
    v558 = _mm_sll_epi32(v554, 0xEu);
    v559 = __ROR4__(v548 ^ v553, 6);
    v560 = __ROR4__(v549 ^ v556, 2);
    v561 = v940 + v559 + (v523 ^ v548 & (v523 ^ v535)) + v510;
    v562 = v561 + v511;
    v563 = (v525 & v549 | v538 & (v525 | v549)) + v560 + v561;
    v564 = __ROR4__(v562, 14);
    v565 = __ROR4__(v563, 9);
    v566 = __ROR4__(v562 ^ v564, 5);
    v567 = _mm_add_epi32(
             _mm_add_epi32(
               v391,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v555, v557), v558), _mm_srl_epi32(v557, 0xBu)),
                 _mm_sll_epi32(v558, 0xBu))),
             _mm_alignr_epi8(v547, v495, 4));
    v568 = __ROR4__(v563 ^ v565, 11);
    v569 = __ROR4__(v562 ^ v566, 6);
    v570 = __ROR4__(v563 ^ v568, 2);
    v571 = _mm_srli_si128(v547, 8);
    v572 = DWORD1(v940) + v569 + (v535 ^ v562 & (v535 ^ v548)) + v523;
    v573 = _mm_srl_epi32(v571, 0xAu);
    v574 = _mm_srl_epi32(v571, 0x11u);
    v575 = v572 + v525;
    v576 = _mm_sll_epi32(v571, 0xDu);
    v577 = (v538 & v563 | v549 & (v538 | v563)) + v570 + v572;
    v578 = __ROR4__(v575, 14);
    v579 = __ROR4__(v577, 9);
    v580 = __ROR4__(v575 ^ v578, 5);
    v581 = __ROR4__(v577 ^ v579, 11);
    v582 = _mm_add_epi32(
             v567,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v573, v574), v576), _mm_srl_epi32(v574, 2u)),
               _mm_sll_epi32(v576, 2u)));
    v583 = __ROR4__(v575 ^ v580, 6);
    v584 = __ROR4__(v577 ^ v581, 2);
    v585 = DWORD2(v940) + v583 + (v548 ^ v575 & (v548 ^ v562)) + v535;
    v586 = _mm_slli_si128(v582, 8);
    v587 = v585 + v538;
    v588 = _mm_srl_epi32(v586, 0xAu);
    v589 = _mm_sll_epi32(v586, 0xDu);
    v590 = (v549 & v577 | v563 & (v549 | v577)) + v584 + v585;
    v591 = _mm_srl_epi32(v586, 0x11u);
    v592 = __ROR4__(v587, 14);
    v593 = __ROR4__(v590, 9);
    v594 = __ROR4__(v587 ^ v592, 5);
    v595 = __ROR4__(v590 ^ v593, 11);
    v596 = __ROR4__(v587 ^ v594, 6);
    v597 = __ROR4__(v590 ^ v595, 2);
    v598 = DWORD3(v940) + v596 + (v562 ^ v587 & (v562 ^ v575)) + v548;
    v599 = _mm_add_epi32(
             v582,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v588, v589), v591), _mm_srl_epi32(v591, 2u)),
               _mm_sll_epi32(v589, 2u)));
    v600 = v598 + v549;
    v601 = (v563 & v590 | v577 & (v563 | v590)) + v597 + v598;
    _mm_store_si128((__m128i *)&v940, _mm_add_epi32(v550, v599));
    v602 = __ROR4__(v600, 14);
    v603 = __ROR4__(v601, 9);
    v604 = __ROR4__(v600 ^ v602, 5);
    v605 = _mm_alignr_epi8(v495, v443, 4);
    v606 = _mm_srl_epi32(v605, 3u);
    v607 = __ROR4__(v601 ^ v603, 11);
    v608 = _mm_srl_epi32(v605, 7u);
    v609 = _mm_sll_epi32(v605, 0xEu);
    v610 = __ROR4__(v600 ^ v604, 6);
    v611 = __ROR4__(v601 ^ v607, 2);
    v612 = v941 + v610 + (v575 ^ v600 & (v575 ^ v587)) + v562;
    v613 = v612 + v563;
    v614 = (v577 & v601 | v590 & (v577 | v601)) + v611 + v612;
    v615 = __ROR4__(v613, 14);
    v616 = __ROR4__(v614, 9);
    v617 = __ROR4__(v613 ^ v615, 5);
    v618 = _mm_add_epi32(
             _mm_add_epi32(
               v443,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v606, v608), v609), _mm_srl_epi32(v608, 0xBu)),
                 _mm_sll_epi32(v609, 0xBu))),
             _mm_alignr_epi8(v599, v547, 4));
    v619 = __ROR4__(v614 ^ v616, 11);
    v620 = __ROR4__(v613 ^ v617, 6);
    v621 = __ROR4__(v614 ^ v619, 2);
    v622 = _mm_srli_si128(v599, 8);
    v623 = DWORD1(v941) + v620 + (v587 ^ v613 & (v587 ^ v600)) + v575;
    v624 = _mm_srl_epi32(v622, 0xAu);
    v625 = _mm_srl_epi32(v622, 0x11u);
    v626 = v623 + v577;
    v627 = _mm_sll_epi32(v622, 0xDu);
    v628 = (v590 & v614 | v601 & (v590 | v614)) + v621 + v623;
    v629 = __ROR4__(v626, 14);
    v630 = __ROR4__(v628, 9);
    v631 = __ROR4__(v626 ^ v629, 5);
    v632 = __ROR4__(v628 ^ v630, 11);
    v633 = _mm_add_epi32(
             v618,
             _mm_xor_si128(
               _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v624, v625), v627), _mm_srl_epi32(v625, 2u)),
               _mm_sll_epi32(v627, 2u)));
    v634 = __ROR4__(v626 ^ v631, 6);
    v635 = __ROR4__(v628 ^ v632, 2);
    v636 = DWORD2(v941) + v634 + (v600 ^ v626 & (v600 ^ v613)) + v587;
    v637 = _mm_slli_si128(v633, 8);
    v638 = v636 + v590;
    v639 = _mm_srl_epi32(v637, 0xAu);
    v640 = _mm_sll_epi32(v637, 0xDu);
    v641 = (v601 & v628 | v614 & (v601 | v628)) + v635 + v636;
    v642 = _mm_srl_epi32(v637, 0x11u);
    v643 = __ROR4__(v638, 14);
    v644 = __ROR4__(v641, 9);
    v645 = __ROR4__(v638 ^ v643, 5);
    v646 = __ROR4__(v641 ^ v644, 11);
    v647 = __ROR4__(v638 ^ v645, 6);
    v648 = __ROR4__(v641 ^ v646, 2);
    v649 = DWORD3(v941) + v647 + (v613 ^ v638 & (v613 ^ v626)) + v600;
    v650 = v649 + v601;
    v651 = (v614 & v641 | v628 & (v614 | v641)) + v648 + v649;
    _mm_store_si128(
      (__m128i *)&v941,
      _mm_add_epi32(
        _mm_loadu_si128(v30),
        _mm_add_epi32(
          v633,
          _mm_xor_si128(
            _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v639, v640), v642), _mm_srl_epi32(v642, 2u)),
            _mm_sll_epi32(v640, 2u)))));
    v652 = (signed __int64)&v30[-15];
    --a3;
    if ( !a3 )
      break;
    v653 = __ROR4__(v650, 14);
    v654 = __ROR4__(v651, 9);
    v655 = __ROR4__(v650 ^ v653, 5);
    v656 = __ROR4__(v651 ^ v654, 11);
    v657 = __ROR4__(v650 ^ v655, 6);
    v658 = __ROR4__(v651 ^ v656, 2);
    v659 = v938 + v657 + (v626 ^ v650 & (v626 ^ v638)) + v613;
    v660 = v659 + v614;
    v661 = (v628 & v651 | v641 & (v628 | v651)) + v658 + v659;
    v16 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)v15), v942);
    v662 = __ROR4__(v660, 14);
    v663 = __ROR4__(v661, 9);
    v664 = __ROR4__(v660 ^ v662, 5);
    v665 = __ROR4__(v661 ^ v663, 11);
    v666 = __ROR4__(v660 ^ v664, 6);
    v667 = __ROR4__(v661 ^ v665, 2);
    v668 = DWORD1(v938) + v666 + (v638 ^ v660 & (v638 ^ v650)) + v626;
    v669 = v668 + v628;
    v670 = (v641 & v661 | v651 & (v641 | v661)) + v667 + v668;
    v671 = __ROR4__(v669, 14);
    v672 = __ROR4__(v670, 9);
    v673 = __ROR4__(v669 ^ v671, 5);
    v674 = __ROR4__(v670 ^ v672, 11);
    v675 = __ROR4__(v669 ^ v673, 6);
    v676 = __ROR4__(v670 ^ v674, 2);
    v677 = DWORD2(v938) + v675 + (v650 ^ v669 & (v650 ^ v660)) + v638;
    v678 = v677 + v641;
    v679 = (v651 & v670 | v661 & (v651 | v670)) + v676 + v677;
    v680 = __ROR4__(v678, 14);
    v681 = __ROR4__(v679, 9);
    v682 = __ROR4__(v678 ^ v680, 5);
    v683 = __ROR4__(v679 ^ v681, 11);
    v684 = __ROR4__(v678 ^ v682, 6);
    v685 = __ROR4__(v679 ^ v683, 2);
    v686 = DWORD3(v938) + v684 + (v660 ^ v678 & (v660 ^ v669)) + v650;
    v687 = v686 + v651;
    v688 = (v661 & v679 | v670 & (v661 | v679)) + v685 + v686;
    _mm_store_si128((__m128i *)&v938, _mm_add_epi32(_mm_loadu_si128((const __m128i *)v652), v16));
    v689 = __ROR4__(v687, 14);
    v690 = __ROR4__(v688, 9);
    v691 = __ROR4__(v687 ^ v689, 5);
    v692 = __ROR4__(v688 ^ v690, 11);
    v693 = __ROR4__(v687 ^ v691, 6);
    v694 = __ROR4__(v688 ^ v692, 2);
    v695 = v939 + v693 + (v669 ^ v687 & (v669 ^ v678)) + v660;
    v696 = v695 + v661;
    v697 = (v670 & v688 | v679 & (v670 | v688)) + v694 + v695;
    v17 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(v15 + 16)), v942);
    v698 = __ROR4__(v696, 14);
    v699 = __ROR4__(v697, 9);
    v700 = __ROR4__(v696 ^ v698, 5);
    v701 = __ROR4__(v697 ^ v699, 11);
    v702 = __ROR4__(v696 ^ v700, 6);
    v703 = __ROR4__(v697 ^ v701, 2);
    v704 = DWORD1(v939) + v702 + (v678 ^ v696 & (v678 ^ v687)) + v669;
    v705 = v704 + v670;
    v706 = (v679 & v697 | v688 & (v679 | v697)) + v703 + v704;
    v707 = __ROR4__(v705, 14);
    v708 = __ROR4__(v706, 9);
    v709 = __ROR4__(v705 ^ v707, 5);
    v710 = __ROR4__(v706 ^ v708, 11);
    v711 = __ROR4__(v705 ^ v709, 6);
    v712 = __ROR4__(v706 ^ v710, 2);
    v713 = DWORD2(v939) + v711 + (v687 ^ v705 & (v687 ^ v696)) + v678;
    v714 = v713 + v679;
    v715 = (v688 & v706 | v697 & (v688 | v706)) + v712 + v713;
    v716 = __ROR4__(v714, 14);
    v717 = __ROR4__(v715, 9);
    v718 = __ROR4__(v714 ^ v716, 5);
    v719 = __ROR4__(v715 ^ v717, 11);
    v720 = __ROR4__(v714 ^ v718, 6);
    v721 = __ROR4__(v715 ^ v719, 2);
    v722 = DWORD3(v939) + v720 + (v696 ^ v714 & (v696 ^ v705)) + v687;
    v723 = v722 + v688;
    v724 = (v697 & v715 | v706 & (v697 | v715)) + v721 + v722;
    _mm_store_si128((__m128i *)&v939, _mm_add_epi32(_mm_loadu_si128((const __m128i *)(v652 + 16)), v17));
    v725 = __ROR4__(v723, 14);
    v726 = __ROR4__(v724, 9);
    v727 = __ROR4__(v723 ^ v725, 5);
    v728 = __ROR4__(v724 ^ v726, 11);
    v729 = __ROR4__(v723 ^ v727, 6);
    v730 = __ROR4__(v724 ^ v728, 2);
    v731 = v940 + v729 + (v705 ^ v723 & (v705 ^ v714)) + v696;
    v732 = v731 + v697;
    v733 = (v706 & v724 | v715 & (v706 | v724)) + v730 + v731;
    v18 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(v15 + 32)), v942);
    v734 = __ROR4__(v732, 14);
    v735 = __ROR4__(v733, 9);
    v736 = __ROR4__(v732 ^ v734, 5);
    v737 = __ROR4__(v733 ^ v735, 11);
    v738 = __ROR4__(v732 ^ v736, 6);
    v739 = __ROR4__(v733 ^ v737, 2);
    v740 = DWORD1(v940) + v738 + (v714 ^ v732 & (v714 ^ v723)) + v705;
    v741 = v740 + v706;
    v742 = (v715 & v733 | v724 & (v715 | v733)) + v739 + v740;
    v743 = __ROR4__(v741, 14);
    v744 = __ROR4__(v742, 9);
    v745 = __ROR4__(v741 ^ v743, 5);
    v746 = __ROR4__(v742 ^ v744, 11);
    v747 = __ROR4__(v741 ^ v745, 6);
    v748 = __ROR4__(v742 ^ v746, 2);
    v749 = DWORD2(v940) + v747 + (v723 ^ v741 & (v723 ^ v732)) + v714;
    v750 = v749 + v715;
    v751 = (v724 & v742 | v733 & (v724 | v742)) + v748 + v749;
    v752 = __ROR4__(v750, 14);
    v753 = __ROR4__(v751, 9);
    v754 = __ROR4__(v750 ^ v752, 5);
    v755 = __ROR4__(v751 ^ v753, 11);
    v756 = __ROR4__(v750 ^ v754, 6);
    v757 = __ROR4__(v751 ^ v755, 2);
    v758 = DWORD3(v940) + v756 + (v732 ^ v750 & (v732 ^ v741)) + v723;
    v759 = v758 + v724;
    v760 = (v733 & v751 | v742 & (v733 | v751)) + v757 + v758;
    _mm_store_si128((__m128i *)&v940, _mm_add_epi32(_mm_loadu_si128((const __m128i *)(v652 + 32)), v18));
    v761 = __ROR4__(v759, 14);
    v762 = __ROR4__(v760, 9);
    v763 = __ROR4__(v759 ^ v761, 5);
    v764 = __ROR4__(v760 ^ v762, 11);
    v765 = __ROR4__(v759 ^ v763, 6);
    v766 = __ROR4__(v760 ^ v764, 2);
    v767 = v941 + v765 + (v741 ^ v759 & (v741 ^ v750)) + v732;
    v768 = v767 + v733;
    v769 = (v742 & v760 | v751 & (v742 | v760)) + v766 + v767;
    v19 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(v15 + 48)), v942);
    v770 = __ROR4__(v768, 14);
    v771 = __ROR4__(v769, 9);
    v772 = __ROR4__(v768 ^ v770, 5);
    v773 = __ROR4__(v769 ^ v771, 11);
    v774 = __ROR4__(v768 ^ v772, 6);
    v775 = __ROR4__(v769 ^ v773, 2);
    v776 = DWORD1(v941) + v774 + (v750 ^ v768 & (v750 ^ v759)) + v741;
    v777 = v776 + v742;
    v778 = (v751 & v769 | v760 & (v751 | v769)) + v775 + v776;
    v779 = __ROR4__(v777, 14);
    v780 = __ROR4__(v778, 9);
    v781 = __ROR4__(v777 ^ v779, 5);
    v782 = __ROR4__(v778 ^ v780, 11);
    v783 = __ROR4__(v777 ^ v781, 6);
    v784 = __ROR4__(v778 ^ v782, 2);
    v785 = DWORD2(v941) + v783 + (v759 ^ v777 & (v759 ^ v768)) + v750;
    v786 = v785 + v751;
    v787 = (v760 & v778 | v769 & (v760 | v778)) + v784 + v785;
    v788 = __ROR4__(v786, 14);
    v789 = __ROR4__(v787, 9);
    v790 = __ROR4__(v786 ^ v788, 5);
    v791 = __ROR4__(v787 ^ v789, 11);
    v792 = __ROR4__(v786 ^ v790, 6);
    v793 = __ROR4__(v787 ^ v791, 2);
    v794 = DWORD3(v941) + v792 + (v768 ^ v786 & (v768 ^ v777)) + v759;
    _mm_store_si128((__m128i *)&v941, _mm_add_epi32(_mm_loadu_si128((const __m128i *)(v652 + 48)), v19));
    v20 = (const __m128i *)(v652 + 64);
    v15 += 64LL;
    *(_DWORD *)a2 += (v769 & v787 | v778 & (v769 | v787)) + v793 + v794;
    *(_DWORD *)(a2 + 4) += v787;
    *(_DWORD *)(a2 + 8) += v778;
    *(_DWORD *)(a2 + 12) += v769;
    *(_DWORD *)(a2 + 16) += v794 + v760;
    *(_DWORD *)(a2 + 20) += v786;
    *(_DWORD *)(a2 + 24) += v777;
    *(_DWORD *)(a2 + 28) += v768;
  }
  v795 = __ROR4__(v650, 14);
  v796 = __ROR4__(v651, 9);
  v797 = __ROR4__(v650 ^ v795, 5);
  v798 = __ROR4__(v651 ^ v796, 11);
  v799 = __ROR4__(v650 ^ v797, 6);
  v800 = __ROR4__(v651 ^ v798, 2);
  v801 = v938 + v799 + (v626 ^ v650 & (v626 ^ v638)) + v613;
  v802 = v801 + v614;
  v803 = (v628 & v651 | v641 & (v628 | v651)) + v800 + v801;
  v804 = __ROR4__(v802, 14);
  v805 = __ROR4__(v803, 9);
  v806 = __ROR4__(v802 ^ v804, 5);
  v807 = __ROR4__(v803 ^ v805, 11);
  v808 = __ROR4__(v802 ^ v806, 6);
  v809 = __ROR4__(v803 ^ v807, 2);
  v810 = DWORD1(v938) + v808 + (v638 ^ v802 & (v638 ^ v650)) + v626;
  v811 = v810 + v628;
  v812 = (v641 & v803 | v651 & (v641 | v803)) + v809 + v810;
  v813 = __ROR4__(v811, 14);
  v814 = __ROR4__(v812, 9);
  v815 = __ROR4__(v811 ^ v813, 5);
  v816 = __ROR4__(v812 ^ v814, 11);
  v817 = __ROR4__(v811 ^ v815, 6);
  v818 = __ROR4__(v812 ^ v816, 2);
  v819 = DWORD2(v938) + v817 + (v650 ^ v811 & (v650 ^ v802)) + v638;
  v820 = v819 + v641;
  v821 = (v651 & v812 | v803 & (v651 | v812)) + v818 + v819;
  v822 = __ROR4__(v820, 14);
  v823 = __ROR4__(v821, 9);
  v824 = __ROR4__(v820 ^ v822, 5);
  v825 = __ROR4__(v821 ^ v823, 11);
  v826 = __ROR4__(v820 ^ v824, 6);
  v827 = __ROR4__(v821 ^ v825, 2);
  v828 = DWORD3(v938) + v826 + (v802 ^ v820 & (v802 ^ v811)) + v650;
  v829 = v828 + v651;
  v830 = (v803 & v821 | v812 & (v803 | v821)) + v827 + v828;
  v831 = __ROR4__(v829, 14);
  v832 = __ROR4__(v830, 9);
  v833 = __ROR4__(v829 ^ v831, 5);
  v834 = __ROR4__(v830 ^ v832, 11);
  v835 = __ROR4__(v829 ^ v833, 6);
  v836 = __ROR4__(v830 ^ v834, 2);
  v837 = v939 + v835 + (v811 ^ v829 & (v811 ^ v820)) + v802;
  v838 = v837 + v803;
  v839 = (v812 & v830 | v821 & (v812 | v830)) + v836 + v837;
  v840 = __ROR4__(v838, 14);
  v841 = __ROR4__(v839, 9);
  v842 = __ROR4__(v838 ^ v840, 5);
  v843 = __ROR4__(v839 ^ v841, 11);
  v844 = __ROR4__(v838 ^ v842, 6);
  v845 = __ROR4__(v839 ^ v843, 2);
  v846 = DWORD1(v939) + v844 + (v820 ^ v838 & (v820 ^ v829)) + v811;
  v847 = v846 + v812;
  v848 = (v821 & v839 | v830 & (v821 | v839)) + v845 + v846;
  v849 = __ROR4__(v847, 14);
  v850 = __ROR4__(v848, 9);
  v851 = __ROR4__(v847 ^ v849, 5);
  v852 = __ROR4__(v848 ^ v850, 11);
  v853 = __ROR4__(v847 ^ v851, 6);
  v854 = __ROR4__(v848 ^ v852, 2);
  v855 = DWORD2(v939) + v853 + (v829 ^ v847 & (v829 ^ v838)) + v820;
  v856 = v855 + v821;
  v857 = (v830 & v848 | v839 & (v830 | v848)) + v854 + v855;
  v858 = __ROR4__(v856, 14);
  v859 = __ROR4__(v857, 9);
  v860 = __ROR4__(v856 ^ v858, 5);
  v861 = __ROR4__(v857 ^ v859, 11);
  v862 = __ROR4__(v856 ^ v860, 6);
  v863 = __ROR4__(v857 ^ v861, 2);
  v864 = DWORD3(v939) + v862 + (v838 ^ v856 & (v838 ^ v847)) + v829;
  v865 = v864 + v830;
  v866 = (v839 & v857 | v848 & (v839 | v857)) + v863 + v864;
  v867 = __ROR4__(v865, 14);
  v868 = __ROR4__(v866, 9);
  v869 = __ROR4__(v865 ^ v867, 5);
  v870 = __ROR4__(v866 ^ v868, 11);
  v871 = __ROR4__(v865 ^ v869, 6);
  v872 = __ROR4__(v866 ^ v870, 2);
  v873 = v940 + v871 + (v847 ^ v865 & (v847 ^ v856)) + v838;
  v874 = v873 + v839;
  v875 = (v848 & v866 | v857 & (v848 | v866)) + v872 + v873;
  v876 = __ROR4__(v874, 14);
  v877 = __ROR4__(v875, 9);
  v878 = __ROR4__(v874 ^ v876, 5);
  v879 = __ROR4__(v875 ^ v877, 11);
  v880 = __ROR4__(v874 ^ v878, 6);
  v881 = __ROR4__(v875 ^ v879, 2);
  v882 = DWORD1(v940) + v880 + (v856 ^ v874 & (v856 ^ v865)) + v847;
  v883 = v882 + v848;
  v884 = (v857 & v875 | v866 & (v857 | v875)) + v881 + v882;
  v885 = __ROR4__(v883, 14);
  v886 = __ROR4__(v884, 9);
  v887 = __ROR4__(v883 ^ v885, 5);
  v888 = __ROR4__(v884 ^ v886, 11);
  v889 = __ROR4__(v883 ^ v887, 6);
  v890 = __ROR4__(v884 ^ v888, 2);
  v891 = DWORD2(v940) + v889 + (v865 ^ v883 & (v865 ^ v874)) + v856;
  v892 = v891 + v857;
  v893 = (v866 & v884 | v875 & (v866 | v884)) + v890 + v891;
  v894 = __ROR4__(v892, 14);
  v895 = __ROR4__(v893, 9);
  v896 = __ROR4__(v892 ^ v894, 5);
  v897 = __ROR4__(v893 ^ v895, 11);
  v898 = __ROR4__(v892 ^ v896, 6);
  v899 = __ROR4__(v893 ^ v897, 2);
  v900 = DWORD3(v940) + v898 + (v874 ^ v892 & (v874 ^ v883)) + v865;
  v901 = v900 + v866;
  v902 = (v875 & v893 | v884 & (v875 | v893)) + v899 + v900;
  v903 = __ROR4__(v901, 14);
  v904 = __ROR4__(v902, 9);
  v905 = __ROR4__(v901 ^ v903, 5);
  v906 = __ROR4__(v902 ^ v904, 11);
  v907 = __ROR4__(v901 ^ v905, 6);
  v908 = __ROR4__(v902 ^ v906, 2);
  v909 = v941 + v907 + (v883 ^ v901 & (v883 ^ v892)) + v874;
  v910 = v909 + v875;
  v911 = (v884 & v902 | v893 & (v884 | v902)) + v908 + v909;
  v912 = __ROR4__(v910, 14);
  v913 = __ROR4__(v911, 9);
  v914 = __ROR4__(v910 ^ v912, 5);
  v915 = __ROR4__(v911 ^ v913, 11);
  v916 = __ROR4__(v910 ^ v914, 6);
  v917 = __ROR4__(v911 ^ v915, 2);
  v918 = DWORD1(v941) + v916 + (v892 ^ v910 & (v892 ^ v901)) + v883;
  v919 = v918 + v884;
  v920 = (v893 & v911 | v902 & (v893 | v911)) + v917 + v918;
  v921 = __ROR4__(v919, 14);
  v922 = __ROR4__(v920, 9);
  v923 = __ROR4__(v919 ^ v921, 5);
  v924 = __ROR4__(v920 ^ v922, 11);
  v925 = __ROR4__(v919 ^ v923, 6);
  v926 = __ROR4__(v920 ^ v924, 2);
  v927 = DWORD2(v941) + v925 + (v901 ^ v919 & (v901 ^ v910)) + v892;
  v928 = v927 + v893;
  v929 = (v902 & v920 | v911 & (v902 | v920)) + v926 + v927;
  v930 = __ROR4__(v928, 14);
  v931 = __ROR4__(v929, 9);
  v932 = __ROR4__(v928 ^ v930, 5);
  v933 = __ROR4__(v929 ^ v931, 11);
  v934 = __ROR4__(v928 ^ v932, 6);
  v935 = __ROR4__(v929 ^ v933, 2);
  v936 = DWORD3(v941) + v934 + (v910 ^ v928 & (v910 ^ v919)) + v901;
  result = v911 & v929 | v920 & (v911 | (unsigned int)v929);
  *(_DWORD *)a2 += result + v935 + v936;
  *(_DWORD *)(a2 + 4) += v929;
  *(_DWORD *)(a2 + 8) += v920;
  *(_DWORD *)(a2 + 12) += v911;
  *(_DWORD *)(a2 + 16) += v936 + v902;
  *(_DWORD *)(a2 + 20) += v928;
  *(_DWORD *)(a2 + 24) += v919;
  *(_DWORD *)(a2 + 28) += v910;
  return result;
}
// 579B0: using guessed type __int64 qword_579B0[2];
// 66F00: using guessed type __int64 ccsha256_K[32];

//----- (00000000000188C0) ----------------------------------------------------
void __usercall ONE_0(__int64 a1@<rax>)
{
  *(_DWORD *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  JUMPOUT(Lbswap_mask_0);
}

//----- (0000000000018940) ----------------------------------------------------
int __usercall ccm128_encrypt@<eax>(__m128i a1@<xmm0>, __m128i a2@<xmm1>, __m128i a3@<xmm2>, __m128i a4@<xmm3>, __m128i a5@<xmm4>, __m128i a6@<xmm5>)
{
  __int128 v7; // [sp+0h] [bp-70h]@1
  __int128 v8; // [sp+10h] [bp-60h]@1
  __int128 v9; // [sp+20h] [bp-50h]@1
  __int128 v10; // [sp+30h] [bp-40h]@1
  __int128 v11; // [sp+40h] [bp-30h]@1
  __int128 v12; // [sp+50h] [bp-20h]@1

  _mm_store_si128((__m128i *)&v7, a1);
  _mm_store_si128((__m128i *)&v8, a2);
  _mm_store_si128((__m128i *)&v9, a3);
  _mm_store_si128((__m128i *)&v10, a4);
  _mm_store_si128((__m128i *)&v11, a5);
  _mm_store_si128((__m128i *)&v12, a6);
  return Main_Loop();
}
// 18994: using guessed type int Main_Loop(void);

//----- (0000000000018994) ----------------------------------------------------
#error "18ACD: positive sp value has been found (funcsize=0)"

//----- (0000000000018ACE) ----------------------------------------------------
void __fastcall init_wrapper_opt(__int64 a1, __int64 a2, signed int a3, __int64 a4)
{
  vng_aes_decrypt_opt_key(a4, a3);
}

//----- (0000000000018AE3) ----------------------------------------------------
__int64 __fastcall cbc_wrapper_opt(__int64 a1, __m128i *a2, __int64 a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r13@1
  int v6; // ebx@1
  __int64 v7; // rcx@1
  __int64 v8; // r14@2
  char v10; // [sp+0h] [bp-40h]@2
  __int64 v11; // [sp+10h] [bp-30h]@1

  v5 = a4;
  v6 = a3;
  v7 = off_69010[0];
  v11 = *(_QWORD *)off_69010[0];
  if ( a3 )
  {
    v8 = a5;
    memcpy(&v10, (const void *)(16 * a3 + v5 - 16), 0x10uLL);
    vng_aes_decrypt_opt_cbc(v5, a2, v6, v8, a1);
    memcpy(a2, &v10, 0x10uLL);
    v7 = off_69010[0];
  }
  return *(_QWORD *)v7;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000018B78) ----------------------------------------------------
signed __int64 __fastcall init_wrapper_aesni(__int64 a1, __int64 a2, unsigned int a3, __int64 a4)
{
  return vng_aes_decrypt_aesni_key(a4, a3, a2);
}

//----- (0000000000018B8D) ----------------------------------------------------
__int64 __fastcall cbc_wrapper_aesni(__int64 a1, __m128i *a2, __int64 a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r13@1
  int v6; // ebx@1
  __int64 v7; // rcx@1
  __int64 v8; // r14@2
  char v10; // [sp+0h] [bp-40h]@2
  __int64 v11; // [sp+10h] [bp-30h]@1

  v5 = a4;
  v6 = a3;
  v7 = off_69010[0];
  v11 = *(_QWORD *)off_69010[0];
  if ( a3 )
  {
    v8 = a5;
    memcpy(&v10, (const void *)(16 * a3 + v5 - 16), 0x10uLL);
    vng_aes_decrypt_cbc_hw(v5, a2, v6, v8, a1);
    memcpy(a2, &v10, 0x10uLL);
    v7 = off_69010[0];
  }
  return *(_QWORD *)v7;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000018C22) ----------------------------------------------------
void __fastcall init_wrapper_opt_0(__int64 a1, __int64 a2, signed int a3, __int64 a4)
{
  vng_aes_encrypt_opt_key(a4, a3);
}

//----- (0000000000018C37) ----------------------------------------------------
void *__fastcall cbc_wrapper_opt_0(__int64 a1, __int64 a2, __int64 a3, __m128i *a4, __int64 a5)
{
  __int64 v5; // r15@1
  __int64 v6; // rbx@1
  void *result; // rax@1

  v5 = a5;
  v6 = a3;
  result = (void *)a1;
  if ( a3 )
  {
    vng_aes_encrypt_opt_cbc(a4, a2, a3, a5, a1);
    result = memcpy((void *)a2, (const void *)(16 * v6 + v5 - 16), 0x10uLL);
  }
  return result;
}

//----- (0000000000018C90) ----------------------------------------------------
signed __int64 __fastcall init_wrapper_aesni_0(__int64 a1, __int64 a2, unsigned int a3, __int64 a4)
{
  return vng_aes_encrypt_aesni_key(a4, a3, a2);
}

//----- (0000000000018CA5) ----------------------------------------------------
void *__fastcall cbc_wrapper_aesni_0(__int64 a1, __m128i *a2, __int64 a3, __m128i *a4, __int64 a5)
{
  __int64 v5; // r15@1
  __int64 v6; // rbx@1
  void *result; // rax@1

  v5 = a5;
  v6 = a3;
  result = (void *)a1;
  if ( a3 )
  {
    vng_aes_encrypt_cbc_hw(a4, a2, a3, a5, a1);
    result = memcpy(a2, (const void *)(16 * v6 + v5 - 16), 0x10uLL);
  }
  return result;
}

//----- (0000000000018CFE) ----------------------------------------------------
void __fastcall init_wrapper_opt_1(__int64 a1, __int64 a2, signed int a3, __int64 a4)
{
  vng_aes_decrypt_opt_key(a4, a3);
}

//----- (0000000000018D13) ----------------------------------------------------
signed __int64 __fastcall ecb_wrapper_opt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 i; // r15@1
  signed __int64 result; // rax@2

  v4 = a4;
  v5 = a3;
  for ( i = a2; i; --i )
  {
    result = vng_aes_decrypt_opt(v5, v4, a1);
    v5 += 16LL;
    v4 += 16LL;
  }
  return result;
}

//----- (0000000000018D53) ----------------------------------------------------
signed __int64 __fastcall init_wrapper_aesni_1(__int64 a1, __int64 a2, unsigned int a3, __int64 a4)
{
  return vng_aes_decrypt_aesni_key(a4, a3, a2);
}

//----- (0000000000018D68) ----------------------------------------------------
__int64 __fastcall ecb_wrapper_aesni(__int64 a1, int a2, __int64 a3, __int64 a4)
{
  __int64 result; // rax@1
  unsigned __int8 v5; // of@2
  int v6; // esi@2
  __m128i v7; // xmm4@3
  bool v72; // zf@5
  bool v73; // sf@5
  int v74; // esi@5
  int v100; // esi@9
  __m128i v101; // xmm4@10
  int v156; // esi@12
  int v178; // esi@15
  __m128i v179; // xmm4@16
  int v254; // esi@18

  result = *(_DWORD *)(a1 + 240);
  if ( (_DWORD)result == 192 )
  {
    v5 = __OFSUB__(a2, 4);
    v6 = a2 - 4;
    if ( !((v6 < 0) ^ v5) )
    {
      v7 = *(__m128i *)(a1 + 192);
      _XMM5 = *(_OWORD *)(a1 + 176);
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, v7);
        _XMM1 = _mm_xor_si128(*(__m128i *)(a3 + 16), v7);
        _XMM2 = _mm_xor_si128(*(__m128i *)(a3 + 32), v7);
        _XMM3 = _mm_xor_si128(*(__m128i *)(a3 + 48), v7);
        _XMM4 = *(_OWORD *)(a1 + 160);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 144);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 128);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 112);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 96);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 80);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 64);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 48);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 32);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 16);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)a1;
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 176);
        __asm
        {
          aesdeclast xmm0, xmm4
          aesdeclast xmm1, xmm4
          aesdeclast xmm2, xmm4
          aesdeclast xmm3, xmm4
        }
        v7 = *(__m128i *)(a1 + 192);
        *(_OWORD *)a4 = _XMM0;
        *(_OWORD *)(a4 + 16) = _XMM1;
        *(_OWORD *)(a4 + 32) = _XMM2;
        *(_OWORD *)(a4 + 48) = _XMM3;
        a4 += 64LL;
        a3 += 64LL;
        v5 = __OFSUB__(v6, 4);
        v6 -= 4;
      }
      while ( !((unsigned __int8)((v6 < 0) ^ v5) | (v6 == 0)) );
    }
    v5 = __OFADD__(4, v6);
    v72 = v6 == -4;
    v73 = v6 + 4 < 0;
    v74 = v6 + 4;
    if ( !((unsigned __int8)(v73 ^ v5) | v72) )
    {
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, *(__m128i *)(a1 + 192));
        _XMM1 = *(_OWORD *)(a1 + 176);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 160);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 144);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 128);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 112);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 96);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 80);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 64);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 48);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 32);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 16);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)a1;
        __asm { aesdeclast xmm0, xmm1 }
        *(_OWORD *)a4 = _XMM0;
        a4 += 16LL;
        a3 += 16LL;
        v5 = __OFSUB__(v74--, 1);
      }
      while ( !((unsigned __int8)((v74 < 0) ^ v5) | (v74 == 0)) );
    }
  }
  else if ( (_DWORD)result == 160 )
  {
    v5 = __OFSUB__(a2, 4);
    v100 = a2 - 4;
    if ( !((v100 < 0) ^ v5) )
    {
      v101 = *(__m128i *)(a1 + 160);
      _XMM5 = *(_OWORD *)(a1 + 144);
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, v101);
        _XMM1 = _mm_xor_si128(*(__m128i *)(a3 + 16), v101);
        _XMM2 = _mm_xor_si128(*(__m128i *)(a3 + 32), v101);
        _XMM3 = _mm_xor_si128(*(__m128i *)(a3 + 48), v101);
        _XMM4 = *(_OWORD *)(a1 + 128);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 112);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 96);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 80);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 64);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 48);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 32);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 16);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)a1;
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 144);
        __asm
        {
          aesdeclast xmm0, xmm4
          aesdeclast xmm1, xmm4
          aesdeclast xmm2, xmm4
          aesdeclast xmm3, xmm4
        }
        v101 = *(__m128i *)(a1 + 160);
        *(_OWORD *)a4 = _XMM0;
        *(_OWORD *)(a4 + 16) = _XMM1;
        *(_OWORD *)(a4 + 32) = _XMM2;
        *(_OWORD *)(a4 + 48) = _XMM3;
        a4 += 64LL;
        a3 += 64LL;
        v5 = __OFSUB__(v100, 4);
        v100 -= 4;
      }
      while ( !((unsigned __int8)((v100 < 0) ^ v5) | (v100 == 0)) );
    }
    v5 = __OFADD__(4, v100);
    v72 = v100 == -4;
    v73 = v100 + 4 < 0;
    v156 = v100 + 4;
    if ( !((unsigned __int8)(v73 ^ v5) | v72) )
    {
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, *(__m128i *)(a1 + 160));
        _XMM1 = *(_OWORD *)(a1 + 144);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 128);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 112);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 96);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 80);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 64);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 48);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 32);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 16);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)a1;
        __asm { aesdeclast xmm0, xmm1 }
        *(_OWORD *)a4 = _XMM0;
        a4 += 16LL;
        a3 += 16LL;
        v5 = __OFSUB__(v156--, 1);
      }
      while ( !((unsigned __int8)((v156 < 0) ^ v5) | (v156 == 0)) );
    }
  }
  else
  {
    v5 = __OFSUB__(a2, 4);
    v178 = a2 - 4;
    if ( !((v178 < 0) ^ v5) )
    {
      v179 = *(__m128i *)(a1 + 224);
      _XMM5 = *(_OWORD *)(a1 + 208);
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, v179);
        _XMM1 = _mm_xor_si128(*(__m128i *)(a3 + 16), v179);
        _XMM2 = _mm_xor_si128(*(__m128i *)(a3 + 32), v179);
        _XMM3 = _mm_xor_si128(*(__m128i *)(a3 + 48), v179);
        _XMM4 = *(_OWORD *)(a1 + 192);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 176);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 160);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 144);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 128);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 112);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 96);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 80);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 64);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 48);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 32);
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 16);
        __asm
        {
          aesdec  xmm0, xmm4
          aesdec  xmm1, xmm4
          aesdec  xmm2, xmm4
          aesdec  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)a1;
        __asm
        {
          aesdec  xmm0, xmm5
          aesdec  xmm1, xmm5
          aesdec  xmm2, xmm5
          aesdec  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 208);
        __asm
        {
          aesdeclast xmm0, xmm4
          aesdeclast xmm1, xmm4
          aesdeclast xmm2, xmm4
          aesdeclast xmm3, xmm4
        }
        v179 = *(__m128i *)(a1 + 224);
        *(_OWORD *)a4 = _XMM0;
        *(_OWORD *)(a4 + 16) = _XMM1;
        *(_OWORD *)(a4 + 32) = _XMM2;
        *(_OWORD *)(a4 + 48) = _XMM3;
        a4 += 64LL;
        a3 += 64LL;
        v5 = __OFSUB__(v178, 4);
        v178 -= 4;
      }
      while ( !((unsigned __int8)((v178 < 0) ^ v5) | (v178 == 0)) );
    }
    v5 = __OFADD__(4, v178);
    v72 = v178 == -4;
    v73 = v178 + 4 < 0;
    v254 = v178 + 4;
    if ( !((unsigned __int8)(v73 ^ v5) | v72) )
    {
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, *(__m128i *)(a1 + 224));
        _XMM1 = *(_OWORD *)(a1 + 208);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 192);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 176);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 160);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 144);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 128);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 112);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 96);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 80);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 64);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 48);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 32);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 16);
        __asm { aesdec  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)a1;
        __asm { aesdeclast xmm0, xmm1 }
        *(_OWORD *)a4 = _XMM0;
        a4 += 16LL;
        a3 += 16LL;
        v5 = __OFSUB__(v254--, 1);
      }
      while ( !((unsigned __int8)((v254 < 0) ^ v5) | (v254 == 0)) );
    }
  }
  return result;
}

//----- (0000000000019404) ----------------------------------------------------
int __fastcall ccmode_ccm_init(__int64 a1, __int64 a2)
{
  __int64 v2; // rdi@1

  v2 = *(_QWORD *)(a1 + 72);
  *(_QWORD *)a2 = v2;
  return (*(int (__fastcall **)(__int64, __int64))(v2 + 16))(v2, a2 + 8);
}

//----- (000000000001941A) ----------------------------------------------------
void __fastcall init_wrapper_opt_2(__int64 a1, __int64 a2, signed int a3, __int64 a4)
{
  vng_aes_encrypt_opt_key(a4, a3);
}

//----- (000000000001942F) ----------------------------------------------------
signed __int64 __fastcall ecb_wrapper_opt_0(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 i; // r15@1
  signed __int64 result; // rax@2

  v4 = a4;
  v5 = a3;
  for ( i = a2; i; --i )
  {
    result = vng_aes_encrypt_opt(v5, v4, a1);
    v5 += 16LL;
    v4 += 16LL;
  }
  return result;
}

//----- (000000000001946F) ----------------------------------------------------
signed __int64 __fastcall init_wrapper_aesni_2(__int64 a1, __int64 a2, unsigned int a3, __int64 a4)
{
  return vng_aes_encrypt_aesni_key(a4, a3, a2);
}

//----- (0000000000019484) ----------------------------------------------------
__int64 __fastcall ecb_wrapper_aesni_0(__int64 a1, int a2, __int64 a3, __int64 a4)
{
  __int64 result; // rax@1
  unsigned __int8 v5; // of@2
  int v6; // esi@2
  __m128i v7; // xmm4@3
  bool v72; // zf@5
  bool v73; // sf@5
  int v74; // esi@5
  int v100; // esi@9
  __m128i v101; // xmm4@10
  int v156; // esi@12
  int v178; // esi@15
  __m128i v179; // xmm4@16
  int v254; // esi@18

  result = *(_DWORD *)(a1 + 240);
  if ( (_DWORD)result == 192 )
  {
    v5 = __OFSUB__(a2, 4);
    v6 = a2 - 4;
    if ( !((v6 < 0) ^ v5) )
    {
      v7 = *(__m128i *)a1;
      _XMM5 = *(_OWORD *)(a1 + 16);
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, v7);
        _XMM1 = _mm_xor_si128(*(__m128i *)(a3 + 16), v7);
        _XMM2 = _mm_xor_si128(*(__m128i *)(a3 + 32), v7);
        _XMM3 = _mm_xor_si128(*(__m128i *)(a3 + 48), v7);
        _XMM4 = *(_OWORD *)(a1 + 32);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 48);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 64);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 80);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 96);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 112);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 128);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 144);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 160);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 176);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 192);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 16);
        __asm
        {
          aesenclast xmm0, xmm4
          aesenclast xmm1, xmm4
          aesenclast xmm2, xmm4
          aesenclast xmm3, xmm4
        }
        v7 = *(__m128i *)a1;
        *(_OWORD *)a4 = _XMM0;
        *(_OWORD *)(a4 + 16) = _XMM1;
        *(_OWORD *)(a4 + 32) = _XMM2;
        *(_OWORD *)(a4 + 48) = _XMM3;
        a4 += 64LL;
        a3 += 64LL;
        v5 = __OFSUB__(v6, 4);
        v6 -= 4;
      }
      while ( !((unsigned __int8)((v6 < 0) ^ v5) | (v6 == 0)) );
    }
    v5 = __OFADD__(4, v6);
    v72 = v6 == -4;
    v73 = v6 + 4 < 0;
    v74 = v6 + 4;
    if ( !((unsigned __int8)(v73 ^ v5) | v72) )
    {
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, *(__m128i *)a1);
        _XMM1 = *(_OWORD *)(a1 + 16);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 32);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 48);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 64);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 80);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 96);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 112);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 128);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 144);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 160);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 176);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 192);
        __asm { aesenclast xmm0, xmm1 }
        *(_OWORD *)a4 = _XMM0;
        a4 += 16LL;
        a3 += 16LL;
        v5 = __OFSUB__(v74--, 1);
      }
      while ( !((unsigned __int8)((v74 < 0) ^ v5) | (v74 == 0)) );
    }
  }
  else if ( (_DWORD)result == 160 )
  {
    v5 = __OFSUB__(a2, 4);
    v100 = a2 - 4;
    if ( !((v100 < 0) ^ v5) )
    {
      v101 = *(__m128i *)a1;
      _XMM5 = *(_OWORD *)(a1 + 16);
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, v101);
        _XMM1 = _mm_xor_si128(*(__m128i *)(a3 + 16), v101);
        _XMM2 = _mm_xor_si128(*(__m128i *)(a3 + 32), v101);
        _XMM3 = _mm_xor_si128(*(__m128i *)(a3 + 48), v101);
        _XMM4 = *(_OWORD *)(a1 + 32);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 48);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 64);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 80);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 96);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 112);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 128);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 144);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 160);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 16);
        __asm
        {
          aesenclast xmm0, xmm4
          aesenclast xmm1, xmm4
          aesenclast xmm2, xmm4
          aesenclast xmm3, xmm4
        }
        v101 = *(__m128i *)a1;
        *(_OWORD *)a4 = _XMM0;
        *(_OWORD *)(a4 + 16) = _XMM1;
        *(_OWORD *)(a4 + 32) = _XMM2;
        *(_OWORD *)(a4 + 48) = _XMM3;
        a4 += 64LL;
        a3 += 64LL;
        v5 = __OFSUB__(v100, 4);
        v100 -= 4;
      }
      while ( !((unsigned __int8)((v100 < 0) ^ v5) | (v100 == 0)) );
    }
    v5 = __OFADD__(4, v100);
    v72 = v100 == -4;
    v73 = v100 + 4 < 0;
    v156 = v100 + 4;
    if ( !((unsigned __int8)(v73 ^ v5) | v72) )
    {
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, *(__m128i *)a1);
        _XMM1 = *(_OWORD *)(a1 + 16);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 32);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 48);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 64);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 80);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 96);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 112);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 128);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 144);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 160);
        __asm { aesenclast xmm0, xmm1 }
        *(_OWORD *)a4 = _XMM0;
        a4 += 16LL;
        a3 += 16LL;
        v5 = __OFSUB__(v156--, 1);
      }
      while ( !((unsigned __int8)((v156 < 0) ^ v5) | (v156 == 0)) );
    }
  }
  else
  {
    v5 = __OFSUB__(a2, 4);
    v178 = a2 - 4;
    if ( !((v178 < 0) ^ v5) )
    {
      v179 = *(__m128i *)a1;
      _XMM5 = *(_OWORD *)(a1 + 16);
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, v179);
        _XMM1 = _mm_xor_si128(*(__m128i *)(a3 + 16), v179);
        _XMM2 = _mm_xor_si128(*(__m128i *)(a3 + 32), v179);
        _XMM3 = _mm_xor_si128(*(__m128i *)(a3 + 48), v179);
        _XMM4 = *(_OWORD *)(a1 + 32);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 48);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 64);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 80);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 96);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 112);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 128);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 144);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 160);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 176);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 192);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 208);
        __asm
        {
          aesenc  xmm0, xmm4
          aesenc  xmm1, xmm4
          aesenc  xmm2, xmm4
          aesenc  xmm3, xmm4
        }
        _XMM4 = *(_OWORD *)(a1 + 224);
        __asm
        {
          aesenc  xmm0, xmm5
          aesenc  xmm1, xmm5
          aesenc  xmm2, xmm5
          aesenc  xmm3, xmm5
        }
        _XMM5 = *(_OWORD *)(a1 + 16);
        __asm
        {
          aesenclast xmm0, xmm4
          aesenclast xmm1, xmm4
          aesenclast xmm2, xmm4
          aesenclast xmm3, xmm4
        }
        v179 = *(__m128i *)a1;
        *(_OWORD *)a4 = _XMM0;
        *(_OWORD *)(a4 + 16) = _XMM1;
        *(_OWORD *)(a4 + 32) = _XMM2;
        *(_OWORD *)(a4 + 48) = _XMM3;
        a4 += 64LL;
        a3 += 64LL;
        v5 = __OFSUB__(v178, 4);
        v178 -= 4;
      }
      while ( !((unsigned __int8)((v178 < 0) ^ v5) | (v178 == 0)) );
    }
    v5 = __OFADD__(4, v178);
    v72 = v178 == -4;
    v73 = v178 + 4 < 0;
    v254 = v178 + 4;
    if ( !((unsigned __int8)(v73 ^ v5) | v72) )
    {
      do
      {
        _XMM0 = _mm_xor_si128(*(__m128i *)a3, *(__m128i *)a1);
        _XMM1 = *(_OWORD *)(a1 + 16);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 32);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 48);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 64);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 80);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 96);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 112);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 128);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 144);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 160);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 176);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 192);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 208);
        __asm { aesenc  xmm0, xmm1 }
        _XMM1 = *(_OWORD *)(a1 + 224);
        __asm { aesenclast xmm0, xmm1 }
        *(_OWORD *)a4 = _XMM0;
        a4 += 16LL;
        a3 += 16LL;
        v5 = __OFSUB__(v254--, 1);
      }
      while ( !((unsigned __int8)((v254 < 0) ^ v5) | (v254 == 0)) );
    }
  }
  return result;
}

//----- (0000000000019B0B) ----------------------------------------------------
void __fastcall init_wrapper_opt_3(__int64 a1, __int64 a2, signed int a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  signed int v6; // er15@1

  v5 = a5;
  v6 = a3;
  vng_aes_decrypt_opt_key(a4, a3);
  vng_aes_encrypt_opt_key(v5, v6);
}

//----- (0000000000019B48) ----------------------------------------------------
signed __int64 __fastcall set_tweak_wrapper_opt(__int64 a1, __int64 a2, __int64 a3)
{
  return vng_aes_encrypt_opt(a3, a2, a1 + 244);
}

//----- (0000000000019B5F) ----------------------------------------------------
__m128i *__fastcall xts_wrapper_opt(__int64 a1, __m128i *a2, __int64 a3, __m128i *a4, __m128i *a5)
{
  vng_aes_xts_decrypt_opt(a4, 16 * a3, a5, a2, a1);
  return a2;
}

//----- (0000000000019B8D) ----------------------------------------------------
signed __int64 __fastcall init_wrapper_aesni_3(__int64 a1, __int64 a2, unsigned int a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  unsigned int v6; // er15@1

  v5 = a5;
  v6 = a3;
  vng_aes_decrypt_aesni_key(a4, a3, a2);
  return vng_aes_encrypt_aesni_key(v5, v6, a2 + 244);
}

//----- (0000000000019BCA) ----------------------------------------------------
signed __int64 __fastcall set_tweak_wrapper_aesni(__int64 a1, __int64 a2, __m128i *a3)
{
  return vng_aes_encrypt_aesni(a3, a2, a1 + 244);
}

//----- (0000000000019BE1) ----------------------------------------------------
__m128i *__fastcall xts_wrapper_aesni(__int64 a1, __m128i *a2, __int64 a3, __int64 a4, __int64 a5)
{
  vng_aes_xts_decrypt_aesni(a4, 16 * a3, a5, a2, a1);
  return a2;
}

//----- (0000000000019C0F) ----------------------------------------------------
void __usercall ccmode_gcm_init(__int64 a1@<rdi>, __int64 a2@<rsi>, __m128i a3@<xmm0>, __m128i a4@<xmm1>, __m128i a5@<xmm2>, __m128i a6@<xmm3>)
{
  __int64 v6; // r14@1
  __int64 v7; // rax@1
  __int64 v8; // rax@2
  __m128i v9; // xmm4@2

  v6 = *(_QWORD *)(a1 + 64);
  *(_QWORD *)(a2 + 80) = v6;
  (*(void (__fastcall **)(__int64, signed __int64))(v6 + 16))(v6, a2 + 384);
  bzero((void *)(a2 + 16), 0x10uLL);
  bzero((void *)(a2 + 64), 0x10uLL);
  (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, __int64))(v6 + 24))(
    a2 + 384,
    1LL,
    a2 + 16,
    a2);
  *(_DWORD *)(a2 + 96) = 0;
  *(_DWORD *)(a2 + 92) = 0;
  *(_DWORD *)(a2 + 88) = 0;
  *(_QWORD *)(a2 + 104) = 0LL;
  *(_QWORD *)(a2 + 112) = 0LL;
  LODWORD(v7) = cpuid_features();
  if ( _bittest((const unsigned __int64 *)&v7, 0x39u) )
  {
    LODWORD(v8) = cpuid_features();
    if ( _bittest((const unsigned __int64 *)&v8, 0x29u) )
      gcm_init(a2 + 128, (const __m128i *)a2, a3, a4, a5, a6, v9);
  }
}
// 4B8C8: using guessed type int cpuid_features(void);

//----- (0000000000019CC4) ----------------------------------------------------
__int64 __fastcall ccrsa_pub_crypt(__int64 *a1, __int64 a2, __int64 a3)
{
  cczp_power(a1, a2, a3, (__int64)&a1[2 * *a1 + 3]);
  return 0LL;
}

//----- (0000000000019CDD) ----------------------------------------------------
signed __int64 __fastcall ccec_der_import_priv_keytype(__int64 a1, unsigned __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 v6; // rcx@1
  signed __int64 result; // rax@1
  __int64 v8; // [sp+10h] [bp-40h]@1
  __int64 v9; // [sp+18h] [bp-38h]@1
  __int64 v10; // [sp+20h] [bp-30h]@1
  __int64 v11; // [sp+28h] [bp-28h]@1
  __int64 v12; // [sp+30h] [bp-20h]@1
  char v13; // [sp+38h] [bp-18h]@1

  v4 = a4;
  v5 = a3;
  v12 = 0LL;
  v11 = 0LL;
  v10 = 0LL;
  v9 = 0LL;
  v6 = ccder_decode_eckey(
         (__int64)&v13,
         (__int64)&v12,
         (__int64)&v10,
         (__int64)&v8,
         (__int64)&v11,
         (__int64)&v9,
         a2,
         a2 + a1);
  result = 0xFFFFFFFFLL;
  if ( v6 )
  {
    *(_QWORD *)v5 = v8;
    *(_QWORD *)v4 = v12;
    result = 0LL;
  }
  return result;
}

//----- (0000000000019D5D) ----------------------------------------------------
signed __int64 __fastcall ccec_der_import_priv(unsigned __int64 *a1, __int64 a2, unsigned __int64 a3, __int64 a4)
{
  __int64 v4; // r13@1
  __int64 v5; // r15@1
  unsigned __int64 v6; // rbx@2
  unsigned __int64 v7; // rbx@5
  __int64 v8; // rbx@6
  __int64 v9; // r13@6
  char *v10; // r15@6
  signed __int64 result; // rax@9
  int v12; // ecx@10
  signed __int64 v13; // rax@12
  __int64 v14; // rcx@13
  __int64 *v15; // [sp+10h] [bp-70h]@6
  unsigned __int64 v16; // [sp+18h] [bp-68h]@3
  char v17; // [sp+20h] [bp-60h]@1
  unsigned int *v18; // [sp+28h] [bp-58h]@1
  unsigned __int64 v19; // [sp+30h] [bp-50h]@1
  unsigned __int64 v20; // [sp+38h] [bp-48h]@1
  unsigned __int64 v21; // [sp+40h] [bp-40h]@1
  char v22; // [sp+48h] [bp-38h]@1
  __int64 v23; // [sp+50h] [bp-30h]@1

  v4 = a4;
  v5 = off_69010[0];
  v23 = *(_QWORD *)off_69010[0];
  v21 = 0LL;
  v20 = 0LL;
  v19 = 0LL;
  v18 = 0LL;
  if ( ccder_decode_eckey(
         (__int64)&v22,
         (__int64)&v21,
         (__int64)&v19,
         (__int64)&v17,
         (__int64)&v20,
         (__int64)&v18,
         a3,
         a3 + a2) )
  {
    v6 = v21;
    if ( v6 == (unsigned __int64)(ccn_bitlen(*a1, (__int64)(a1 + 2)) + 7) >> 3 )
    {
      v16 = *a1;
      if ( !(unsigned int)ccn_read_uint(v16, v4 + 24LL * **(_QWORD **)v4 + 16, v21, v19) )
      {
        if ( v18 && (v7 = v20 >> 3, v7 >= (((unsigned __int64)(ccn_bitlen(*a1, (__int64)(a1 + 2)) + 7) >> 2) | 1)) )
        {
          v12 = ccec_import_pub(a1, v20 >> 3, v18, v4);
          result = 0LL;
          if ( !v12 )
            goto LABEL_13;
        }
        else
        {
          v15 = (__int64 *)&v15;
          v8 = v4;
          v9 = v4 + 16;
          v10 = (char *)&v15 - ((24 * *a1 + 15) & 0xFFFFFFFFFFFFFFF0LL);
          ccec_projectify((__int64 *)a1, (char *)&v15 - ((24 * *a1 + 15) & 0xFFFFFFFFFFFFFFF0LL), &a1[2 * v16 + 2]);
          if ( !(unsigned int)ccec_mult((signed __int64 *)a1, v9, v9 + 24LL * **(_QWORD **)v8, v10)
            && !(unsigned int)ccec_affinify((signed __int64 *)a1, v9, (unsigned __int64 *)v9) )
          {
            v13 = 16LL * **(_QWORD **)v8;
            *(_QWORD *)(v13 + v8 + 16) = 1LL;
            bzero((void *)(v13 + v8 + 24), 8 * v16 - 8);
            result = 0LL;
            v5 = off_69010[0];
            goto LABEL_13;
          }
          v5 = off_69010[0];
        }
      }
    }
  }
  result = 0xFFFFFFFFLL;
LABEL_13:
  v14 = *(_QWORD *)v5;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000019F53) ----------------------------------------------------
__int64 __fastcall ccrsa_verify(__int64 *a1, size_t *a2, void *a3, unsigned __int64 a4, unsigned __int64 a5, __int64 a6)
{
  __int64 v6; // r12@1
  size_t *v7; // r15@1
  __int64 *v8; // r13@1
  __int64 v9; // r14@1
  unsigned __int64 v10; // rdi@1
  __int64 v11; // rbx@1
  __int64 v12; // rsi@2
  char *v13; // rcx@2
  signed __int64 v14; // rdx@3
  bool v19; // cf@4
  signed __int64 v22; // r9@7
  __int64 v23; // rax@8
  signed __int64 v24; // rdi@9
  __int64 v25; // rdx@10
  signed __int64 v26; // rcx@10
  signed __int64 v27; // rsi@14
  signed __int64 v28; // r8@14
  signed __int64 v29; // rdx@14
  signed __int64 v30; // rbx@15
  signed __int64 v31; // rax@15
  int v32; // ecx@16
  int v33; // er9@20
  int v34; // eax@22
  char v35; // cl@22
  __int64 result; // rax@24
  __int64 v37; // [sp+0h] [bp-40h]@1
  void *v38; // [sp+8h] [bp-38h]@1
  __int64 v39; // [sp+10h] [bp-30h]@1

  v6 = a6;
  v38 = a3;
  v7 = a2;
  v8 = a1;
  v9 = off_69010[0];
  v39 = *(_QWORD *)off_69010[0];
  v10 = *a1;
  v11 = (__int64)((char *)&v37 - ((8 * v10 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  ccn_read_uint(v10, v11, a4, a5);
  if ( (signed int)ccrsa_pub_crypt(v8, v11, v11) < 0 )
    goto LABEL_28;
  v12 = *v8;
  v13 = (char *)&v37 - ((8 * v10 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  if ( *v8 - 1 > 0 )
  {
    v14 = v11 + 8 * v12 - 16;
    v13 = (char *)&v37 - ((8 * v10 + 15) & 0xFFFFFFFFFFFFFFF0LL);
    do
    {
      _RAX = *(_QWORD *)v13;
      __asm { bswap   rax }
      _RDI = *(_QWORD *)(v14 + 8);
      __asm { bswap   rdi }
      *(_QWORD *)v13 = _RDI;
      *(_QWORD *)(v14 + 8) = _RAX;
      v13 += 8;
      v19 = (unsigned __int64)v13 < v14;
      v14 -= 8LL;
    }
    while ( v19 );
  }
  if ( v12 & 1 )
  {
    _RAX = *(_QWORD *)v13;
    __asm { bswap   rax }
    *(_QWORD *)v13 = _RAX;
  }
  v22 = v11 + 8 * v12 - 1;
  if ( v11 == v22 )
    goto LABEL_28;
  v23 = 0LL;
  if ( *(_BYTE *)v11 )
    goto LABEL_28;
  v24 = v11 + 8 * v12;
  do
  {
    v25 = v23;
    v26 = v11 + v23 + 1;
    if ( v26 >= (unsigned __int64)v22 )
      break;
    ++v23;
  }
  while ( !*(_BYTE *)v26 );
  if ( 8 * v12 - 2 == v25 || *(_BYTE *)(v11 + v25 + 1) != 1 )
    goto LABEL_28;
  v27 = 8 * v12 - v25 - 3;
  v28 = v11 + v25 + 2;
  v29 = v11 + v25 + 2;
  do
  {
    v30 = v29;
    v31 = v27;
    if ( v29 >= (unsigned __int64)v22 )
      break;
    v32 = *(_BYTE *)v29++;
    --v27;
  }
  while ( v32 == 255 );
  if ( v28 - v30 < 8
    || !v31
    || *(_BYTE *)v30
    || (v33 = v22 - v30, v33 < 0)
    || v33 < *v7
    || (v34 = memcmp((const void *)(v24 - *v7), v38, *v7), v35 = 1, v34) )
LABEL_28:
    v35 = 0;
  *(_BYTE *)v6 = v35;
  result = *(_QWORD *)v9;
  if ( *(_QWORD *)v9 == v39 )
    result = 0LL;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001A0DF) ----------------------------------------------------
void __fastcall init_wrapper_opt_4(__int64 a1, __int64 a2, signed int a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  signed int v6; // er15@1

  v5 = a5;
  v6 = a3;
  vng_aes_encrypt_opt_key(a4, a3);
  vng_aes_encrypt_opt_key(v5, v6);
}

//----- (000000000001A11C) ----------------------------------------------------
signed __int64 __fastcall set_tweak_wrapper_opt_0(__int64 a1, __int64 a2, __int64 a3)
{
  return vng_aes_encrypt_opt(a3, a2, a1 + 244);
}

//----- (000000000001A133) ----------------------------------------------------
__m128i *__usercall xts_wrapper_opt_0@<rax>(__int64 a1@<rdx>, __m128i *a2@<rcx>, __int64 a3@<rdi>, __m128i *a4@<rsi>, __m128i *a5@<r8>, __m128i *a6@<r15>)
{
  vng_aes_xts_encrypt_opt(a5, a4, a2, 16 * a1, a3, a6);
  return a4;
}

//----- (000000000001A161) ----------------------------------------------------
signed __int64 __fastcall init_wrapper_aesni_4(__int64 a1, __int64 a2, unsigned int a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  unsigned int v6; // er15@1

  v5 = a5;
  v6 = a3;
  vng_aes_encrypt_aesni_key(a4, a3, a2);
  return vng_aes_encrypt_aesni_key(v5, v6, a2 + 244);
}

//----- (000000000001A19E) ----------------------------------------------------
signed __int64 __fastcall set_tweak_wrapper_aesni_0(__int64 a1, __int64 a2, __m128i *a3)
{
  return vng_aes_encrypt_aesni(a3, a2, a1 + 244);
}

//----- (000000000001A1B5) ----------------------------------------------------
__m128i *__usercall xts_wrapper_aesni_0@<rax>(__int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __m128i *a4@<rsi>, __int64 a5@<r8>, __m128i *a6@<r15>)
{
  vng_aes_xts_encrypt_aesni(a5, a4, a2, 16 * a1, a3, a6);
  return a4;
}

//----- (000000000001A1E3) ----------------------------------------------------
bool __fastcall ccec_is_point(signed __int64 *a1, __int64 a2)
{
  signed __int64 *v2; // r15@1
  __int64 v3; // rdi@1
  bool result; // al@3

  v2 = a1;
  v3 = *a1;
  if ( ccn_n(v3, a2 + 16 * v3) != 1 || *(_QWORD *)(a2 + 16 * v3) != 1LL )
    result = ccec_is_point_projective(v2, a2);
  else
    result = ccec_is_point_affine(1LL, v2, a2);
  return result;
}

//----- (000000000001A237) ----------------------------------------------------
bool __fastcall ccec_is_point_projective(signed __int64 *a1, __int64 a2)
{
  unsigned __int64 v2; // rax@1
  __int64 v3; // r12@1
  __int64 v4; // rbx@1
  __int64 v5; // r14@1
  __int64 v6; // r15@1
  __int64 v7; // r15@1
  __int64 v8; // r14@1
  bool result; // al@1
  __int64 v10; // rcx@1
  __int64 v11; // [sp+0h] [bp-40h]@1
  __int64 v12; // [sp+8h] [bp-38h]@1
  __int64 v13; // [sp+10h] [bp-30h]@1

  v11 = a2;
  v13 = *(_QWORD *)off_69010[0];
  v12 = *a1;
  v2 = (8 * v12 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v3 = (__int64)((char *)&v11 - v2);
  v4 = (__int64)((char *)&v11 - v2);
  v5 = (__int64)((char *)&v11 - v2);
  v6 = v12;
  cczp_sqr(a1, (__int64)((char *)&v11 - v2), a2 + 16 * v12);
  cczp_mul(a1, v3, v4, (unsigned __int64 *)&a1[v6 + 2]);
  cczp_sqr(a1, v5, v4);
  v7 = v11;
  cczp_add(a1, v4, v11, v11);
  cczp_add(a1, v4, v4, v7);
  cczp_sub(a1, v3, v3, v4);
  cczp_mul(a1, v3, v3, (unsigned __int64 *)v5);
  cczp_sqr(a1, v4, v7);
  cczp_mul(a1, v4, v4, (unsigned __int64 *)v7);
  cczp_add(a1, v3, v3, v4);
  v8 = v12;
  cczp_sqr(a1, v4, v7 + 8 * v12);
  result = (unsigned int)ccn_cmp(v8, v4, v3) == 0;
  v10 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001A384) ----------------------------------------------------
__int64 __fastcall ccaes_ecb_decrypt_init(__int64 a1, __int64 a2, signed int a3, __int64 a4)
{
  __int64 result; // rax@1
  signed __int64 v5; // rdi@2
  int v6; // eax@5
  signed __int64 v7; // rcx@5
  __int64 v8; // r12@5
  int v9; // edi@6
  int v10; // edx@7
  int v11; // edi@7
  int v12; // edi@7
  int v13; // edi@7
  __int64 v14; // rcx@9
  unsigned int v15; // ebx@10
  int v16; // eax@10
  int v17; // eax@10
  int v18; // eax@10
  int v19; // eax@11
  int v20; // ebx@11
  int v21; // ebx@11
  int v22; // ebx@11
  int v23; // ebx@11
  int v24; // eax@14
  __int64 v25; // rcx@14
  int v26; // ebx@15
  int v27; // edx@15
  int v28; // edx@15
  signed __int64 v29; // r14@17
  signed int v30; // er15@17
  __int64 v31; // rcx@17
  signed __int64 v32; // rdi@17
  int v33; // ebx@18
  int v34; // ebx@18
  int v35; // ebx@18
  int v36; // ebx@18
  __int64 v37; // rsi@19
  signed __int64 v38; // rdx@19
  signed __int64 v39; // rsi@19
  signed __int64 v40; // rdi@19
  signed __int64 v41; // rax@19
  signed __int64 v42; // [sp+0h] [bp-30h]@5

  result = (unsigned int)a3;
  if ( (unsigned int)a3 <= 0x20 )
  {
    v5 = 4311810048LL;
    if ( _bittest((const unsigned __int64 *)&v5, (unsigned int)a3) )
    {
      *(_DWORD *)(a2 + 480) = 2 * (a3 / 8) + 6;
      *(_DWORD *)a2 = _byteswap_ulong(*(_DWORD *)a4);
      *(_DWORD *)(a2 + 4) = _byteswap_ulong(*(_DWORD *)(a4 + 4));
      *(_DWORD *)(a2 + 8) = _byteswap_ulong(*(_DWORD *)(a4 + 8));
      result = _byteswap_ulong(*(_DWORD *)(a4 + 12));
      *(_DWORD *)(a2 + 12) = result;
      if ( a3 == 32 )
      {
        *(_DWORD *)(a2 + 16) = _byteswap_ulong(*(_DWORD *)(a4 + 16));
        *(_DWORD *)(a2 + 20) = _byteswap_ulong(*(_DWORD *)(a4 + 20));
        *(_DWORD *)(a2 + 24) = _byteswap_ulong(*(_DWORD *)(a4 + 24));
        *(_DWORD *)(a2 + 28) = _byteswap_ulong(*(_DWORD *)(a4 + 28));
        v14 = 0LL;
        v42 = 60LL;
        while ( 1 )
        {
          v19 = *(_DWORD *)(a2 + 8 * v14 + 28);
          v20 = *(_DWORD *)((char *)rcon + v14) ^ *((_DWORD *)Te4_0 + (*(_DWORD *)(a2 + 8 * v14 + 28) >> 24)) ^ *((_DWORD *)Te4_1 + (unsigned __int8)*(_DWORD *)(a2 + 8 * v14 + 28)) ^ *((_DWORD *)Te4_2 + BYTE1(v19)) ^ *(_DWORD *)(a2 + 8 * v14) ^ *((_DWORD *)Te4_3 + (unsigned __int8)(*(_DWORD *)(a2 + 8 * v14 + 28) >> 16));
          *(_DWORD *)(a2 + 8 * v14 + 32) = v20;
          v21 = *(_DWORD *)(a2 + 8 * v14 + 4) ^ v20;
          *(_DWORD *)(a2 + 8 * v14 + 36) = v21;
          v22 = *(_DWORD *)(a2 + 8 * v14 + 8) ^ v21;
          *(_DWORD *)(a2 + 8 * v14 + 40) = v22;
          v23 = *(_DWORD *)(a2 + 8 * v14 + 12) ^ v22;
          *(_DWORD *)(a2 + 8 * v14 + 44) = v23;
          if ( v14 == 24 )
            break;
          v15 = __ROR4__(v23, 8);
          v16 = *((_DWORD *)Te4_0 + (v15 >> 24)) ^ *((_DWORD *)Te4_1 + (unsigned __int8)v15) ^ *((_DWORD *)Te4_2
                                                                                               + BYTE1(v15)) ^ *((_DWORD *)Te4_3 + (unsigned __int8)(v15 >> 16)) ^ *(_DWORD *)(a2 + 8 * v14 + 16);
          *(_DWORD *)(a2 + 8 * v14 + 48) = v16;
          v17 = *(_DWORD *)(a2 + 8 * v14 + 20) ^ v16;
          *(_DWORD *)(a2 + 8 * v14 + 52) = v17;
          v18 = *(_DWORD *)(a2 + 8 * v14 + 24) ^ v17;
          *(_DWORD *)(a2 + 8 * v14 + 56) = v18;
          *(_DWORD *)(a2 + 8 * v14 + 60) = *(_DWORD *)(a2 + 8 * v14 + 28) ^ v18;
          v14 += 4LL;
        }
      }
      else if ( a3 == 24 )
      {
        *(_DWORD *)(a2 + 16) = _byteswap_ulong(*(_DWORD *)(a4 + 16));
        *(_DWORD *)(a2 + 20) = _byteswap_ulong(*(_DWORD *)(a4 + 20));
        v6 = *(_DWORD *)a2;
        v7 = a2 + 44;
        v8 = 0LL;
        v42 = 52LL;
        while ( 1 )
        {
          v10 = *(_DWORD *)(v7 - 24);
          v6 ^= *(_DWORD *)((char *)rcon + v8) ^ *((_DWORD *)Te4_0 + (*(_DWORD *)(v7 - 24) >> 24)) ^ *((_DWORD *)Te4_1 + (unsigned __int8)*(_DWORD *)(v7 - 24)) ^ *((_DWORD *)Te4_2 + BYTE1(v10)) ^ *((_DWORD *)Te4_3 + (unsigned __int8)(*(_DWORD *)(v7 - 24) >> 16));
          *(_DWORD *)(v7 - 20) = v6;
          v11 = v6 ^ *(_DWORD *)(v7 - 40);
          *(_DWORD *)(v7 - 16) = v11;
          v12 = *(_DWORD *)(v7 - 36) ^ v11;
          *(_DWORD *)(v7 - 12) = v12;
          v13 = *(_DWORD *)(v7 - 32) ^ v12;
          *(_DWORD *)(v7 - 8) = v13;
          if ( v8 == 28 )
            break;
          v9 = *(_DWORD *)(v7 - 28) ^ v13;
          *(_DWORD *)(v7 - 4) = v9;
          *(_DWORD *)v7 = v10 ^ v9;
          v8 += 4LL;
          v7 += 24LL;
        }
      }
      else
      {
        if ( a3 != 16 )
          return result;
        v24 = *(_DWORD *)a2;
        v25 = 0LL;
        v42 = 44LL;
        do
        {
          v26 = *(_DWORD *)(a2 + 4 * v25 + 12);
          v24 ^= *(_DWORD *)((char *)rcon + v25) ^ *((_DWORD *)Te4_0 + (*(_DWORD *)(a2 + 4 * v25 + 12) >> 24)) ^ *((_DWORD *)Te4_1 + (unsigned __int8)*(_DWORD *)(a2 + 4 * v25 + 12)) ^ *((_DWORD *)Te4_2 + BYTE1(v26)) ^ *((_DWORD *)Te4_3 + (unsigned __int8)(*(_DWORD *)(a2 + 4 * v25 + 12) >> 16));
          *(_DWORD *)(a2 + 4 * v25 + 16) = v24;
          v27 = v24 ^ *(_DWORD *)(a2 + 4 * v25 + 4);
          *(_DWORD *)(a2 + 4 * v25 + 20) = v27;
          v28 = *(_DWORD *)(a2 + 4 * v25 + 8) ^ v27;
          *(_DWORD *)(a2 + 4 * v25 + 24) = v28;
          *(_DWORD *)(a2 + 4 * v25 + 28) = v26 ^ v28;
          v25 += 4LL;
        }
        while ( v25 != 40 );
      }
      *(_DWORD *)(a2 + 240) = *(_DWORD *)(a2 + 4 * v42 - 16);
      *(_DWORD *)(a2 + 244) = *(_DWORD *)(a2 + 4 * v42 - 12);
      *(_DWORD *)(a2 + 248) = *(_DWORD *)(a2 + 4 * v42 - 8);
      *(_DWORD *)(a2 + 252) = *(_DWORD *)(a2 + 4 * v42 - 4);
      if ( *(_DWORD *)(a2 + 480) < 2 )
      {
        v40 = a2 + 4 * v42 - 16;
        v41 = a2 + 4 * v42 - 32;
        v38 = a2 + 240;
        v39 = a2 + 256;
      }
      else
      {
        v29 = a2 + 4 * v42;
        v30 = 1;
        v31 = 0LL;
        v32 = 268LL;
        do
        {
          v33 = *(_DWORD *)(v29 + v31 - 32);
          *(_DWORD *)(a2 + v32 - 12) = *((_DWORD *)Tks3 + (unsigned __int8)*(_DWORD *)(v29 + v31 - 32)) ^ *((_DWORD *)Tks2 + BYTE1(v33)) ^ *((_DWORD *)Tks0 + (*(_DWORD *)(v29 + v31 - 32) >> 24)) ^ *((_DWORD *)Tks1 + (unsigned __int8)(*(_DWORD *)(v29 + v31 - 32) >> 16));
          v34 = *(_DWORD *)(v29 + v31 - 28);
          *(_DWORD *)(a2 + v32 - 8) = *((_DWORD *)Tks3 + (unsigned __int8)*(_DWORD *)(v29 + v31 - 28)) ^ *((_DWORD *)Tks2 + BYTE1(v34)) ^ *((_DWORD *)Tks0 + (*(_DWORD *)(v29 + v31 - 28) >> 24)) ^ *((_DWORD *)Tks1 + (unsigned __int8)(*(_DWORD *)(v29 + v31 - 28) >> 16));
          v35 = *(_DWORD *)(v29 + v31 - 24);
          *(_DWORD *)(a2 + v32 - 4) = *((_DWORD *)Tks3 + (unsigned __int8)*(_DWORD *)(v29 + v31 - 24)) ^ *((_DWORD *)Tks2 + BYTE1(v35)) ^ *((_DWORD *)Tks0 + (*(_DWORD *)(v29 + v31 - 24) >> 24)) ^ *((_DWORD *)Tks1 + (unsigned __int8)(*(_DWORD *)(v29 + v31 - 24) >> 16));
          v36 = *(_DWORD *)(v29 + v31 - 20);
          *(_DWORD *)(a2 + v32) = *((_DWORD *)Tks3 + (unsigned __int8)*(_DWORD *)(v29 + v31 - 20)) ^ *((_DWORD *)Tks2 + BYTE1(v36)) ^ *((_DWORD *)Tks0 + (*(_DWORD *)(v29 + v31 - 20) >> 24)) ^ *((_DWORD *)Tks1 + (unsigned __int8)(*(_DWORD *)(v29 + v31 - 20) >> 16));
          ++v30;
          v31 -= 16LL;
          v32 += 16LL;
        }
        while ( v30 < *(_DWORD *)(a2 + 480) );
        v37 = a2 - v31;
        v38 = v37 + 240;
        v39 = v37 + 256;
        v40 = v31 + v29 - 16;
        v41 = v31 + v29 - 32;
      }
      *(_DWORD *)v39 = *(_DWORD *)v41;
      *(_DWORD *)(v38 + 20) = *(_DWORD *)(v40 - 12);
      *(_DWORD *)(v38 + 24) = *(_DWORD *)(v40 - 8);
      result = *(_DWORD *)(v40 - 4);
      *(_DWORD *)(v38 + 28) = result;
    }
  }
  return result;
}
// 58DC0: using guessed type __int64 rcon[6];
// 58DF0: using guessed type __int64 Tks0[128];
// 591F0: using guessed type __int64 Tks1[128];
// 595F0: using guessed type __int64 Tks2[128];
// 599F0: using guessed type __int64 Tks3[128];
// 59DF0: using guessed type __int64 Te4_3[128];
// 5A1F0: using guessed type __int64 Te4_2[128];
// 5A5F0: using guessed type __int64 Te4_1[128];
// 5A9F0: using guessed type __int64 Te4_0[128];

//----- (000000000001A7C5) ----------------------------------------------------
void __fastcall ccaes_ecb_decrypt(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4)
{
  __int64 v4; // r11@1
  unsigned int v5; // edi@4
  unsigned int v6; // eax@4
  unsigned int v7; // ebx@4
  unsigned int v8; // edx@4
  signed __int64 i; // r13@4
  __int64 v10; // ST88_8@5
  signed __int64 v11; // r15@5
  __int64 v12; // ST68_8@5
  unsigned int v13; // er13@5
  unsigned int v14; // er14@5
  int v15; // edx@5
  __int64 v16; // r10@5
  __int64 v17; // ST88_8@6
  __int64 v18; // ST78_8@6
  __int64 v19; // ST70_8@6
  unsigned int v20; // er12@6
  __int64 v21; // ST68_8@6
  int v22; // ebx@6
  int v23; // ecx@6
  int v24; // edx@6
  int v25; // ebx@6
  int v26; // ecx@6
  int v27; // eax@6
  int v28; // edx@6
  int v29; // ebx@6
  __int64 v30; // r12@6
  int v31; // er14@7
  signed __int64 v32; // [sp+20h] [bp-A0h]@2
  __int64 v33; // [sp+28h] [bp-98h]@1
  signed __int64 v34; // [sp+30h] [bp-90h]@4
  __int64 v35; // [sp+38h] [bp-88h]@2
  signed __int64 v36; // [sp+40h] [bp-80h]@4
  __int64 v37; // [sp+48h] [bp-78h]@2
  int v38; // [sp+90h] [bp-30h]@4

  v4 = a1;
  v33 = a1;
  if ( a2 )
  {
    v35 = a2;
    v37 = a4;
    v32 = a1 + 284;
    while ( 1 )
    {
      v36 = a3;
      v5 = *(_DWORD *)(v4 + 240) ^ _byteswap_ulong(*(_DWORD *)a3);
      v6 = *(_DWORD *)(v4 + 244) ^ _byteswap_ulong(*(_DWORD *)(a3 + 4));
      v7 = *(_DWORD *)(v4 + 248) ^ _byteswap_ulong(*(_DWORD *)(a3 + 8));
      v8 = *(_DWORD *)(v4 + 252) ^ _byteswap_ulong(*(_DWORD *)(a3 + 12));
      v38 = *(_DWORD *)(v4 + 480) >> 1;
      v34 = 2LL * (unsigned int)(v38 - 1) + 17;
      for ( i = v32; ; i = v11 + 32 )
      {
        v17 = BYTE1(v7);
        v18 = BYTE1(v8);
        v19 = (unsigned __int8)v7;
        v20 = v7;
        v21 = (unsigned __int8)v8;
        v22 = *((_DWORD *)TD0 + (v8 >> 24)) ^ *((_DWORD *)TD1 + (unsigned __int8)(v7 >> 16));
        v23 = *((_DWORD *)TD1 + (unsigned __int8)(v8 >> 16));
        v24 = *((_DWORD *)TD2 + BYTE1(v5)) ^ *((_DWORD *)TD0 + (v20 >> 24)) ^ *((_DWORD *)TD1
                                                                              + (unsigned __int8)(v6 >> 16));
        v25 = *((_DWORD *)TD2 + BYTE1(v6)) ^ v22;
        v26 = *(_DWORD *)(i - 28) ^ *((_DWORD *)TD3 + (unsigned __int8)v6) ^ *((_DWORD *)TD2 + v17) ^ *((_DWORD *)TD0 + (v5 >> 24)) ^ v23;
        v27 = *(_DWORD *)(i - 24) ^ *((_DWORD *)TD3 + v19) ^ *((_DWORD *)TD2 + v18) ^ *((_DWORD *)TD0 + (v6 >> 24)) ^ *((_DWORD *)TD1 + (unsigned __int8)(v5 >> 16));
        v28 = *(_DWORD *)(i - 20) ^ *((_DWORD *)TD3 + v21) ^ v24;
        v29 = *(_DWORD *)(i - 16) ^ *((_DWORD *)TD3 + (unsigned __int8)v5) ^ v25;
        v30 = (unsigned int)v26 >> 24;
        if ( v38 == 1 )
          break;
        --v38;
        v10 = BYTE1(v28);
        v11 = i;
        v12 = (unsigned __int8)v28;
        v13 = v28;
        v14 = v27;
        v15 = *((_DWORD *)TD0 + ((unsigned int)v29 >> 24)) ^ *((_DWORD *)TD1 + (unsigned __int8)((unsigned int)v28 >> 16));
        v16 = BYTE1(v27);
        v5 = *(_DWORD *)(v11 - 12) ^ *((_DWORD *)TD3 + (unsigned __int8)v27) ^ *((_DWORD *)TD2 + v10) ^ *((_DWORD *)TD0 + v30) ^ *((_DWORD *)TD1 + (unsigned __int8)((unsigned int)v29 >> 16));
        v6 = *(_DWORD *)(v11 - 8) ^ *((_DWORD *)TD3 + v12) ^ *((_DWORD *)TD2 + BYTE1(v29)) ^ *((_DWORD *)TD0
                                                                                             + ((unsigned int)v27 >> 24)) ^ *((_DWORD *)TD1 + (unsigned __int8)((unsigned int)v26 >> 16));
        v7 = *(_DWORD *)(v11 - 4) ^ *((_DWORD *)TD3 + (unsigned __int8)v29) ^ *((_DWORD *)TD2 + BYTE1(v26)) ^ *((_DWORD *)TD0 + (v13 >> 24)) ^ *((_DWORD *)TD1 + (unsigned __int8)(v14 >> 16));
        v8 = *(_DWORD *)v11 ^ *((_DWORD *)TD3 + (unsigned __int8)v26) ^ *((_DWORD *)TD2 + v16) ^ v15;
      }
      v31 = *((_DWORD *)Td4 + ((unsigned int)v29 >> 24));
      v4 = v33;
      *(_DWORD *)v37 = _byteswap_ulong(*(_DWORD *)(v33 + 16 * v34) ^ (*((_DWORD *)Td4 + v30) & 0xFF000000 | *((_DWORD *)Td4 + (unsigned __int8)((unsigned int)v29 >> 16)) & 0xFF0000 | *((_DWORD *)Td4 + BYTE1(v28)) & 0xFF00 | *((_BYTE *)Td4 + 4 * (unsigned __int8)v27)));
      *(_DWORD *)(v37 + 4) = _byteswap_ulong(*(_DWORD *)(v33 + 16 * v34 + 4) ^ (*((_DWORD *)Td4
                                                                                + ((unsigned int)v27 >> 24)) & 0xFF000000 | *((_DWORD *)Td4 + (unsigned __int8)((unsigned int)v26 >> 16)) & 0xFF0000 | *((_DWORD *)Td4 + BYTE1(v29)) & 0xFF00 | *((_BYTE *)Td4 + 4 * (unsigned __int8)v28)));
      *(_DWORD *)(v37 + 8) = _byteswap_ulong(*(_DWORD *)(v33 + 16 * v34 + 8) ^ (*((_DWORD *)Td4
                                                                                + ((unsigned int)v28 >> 24)) & 0xFF000000 | *((_DWORD *)Td4 + (unsigned __int8)((unsigned int)v27 >> 16)) & 0xFF0000 | *((_DWORD *)Td4 + BYTE1(v26)) & 0xFF00 | *((_BYTE *)Td4 + 4 * (unsigned __int8)v29)));
      *(_DWORD *)(v37 + 12) = _byteswap_ulong(*(_DWORD *)(v33 + 16 * v34 + 12) ^ (v31 & 0xFF000000 | *((_DWORD *)Td4 + (unsigned __int8)((unsigned int)v28 >> 16)) & 0xFF0000 | *((_DWORD *)Td4 + BYTE1(v27)) & 0xFF00 | *((_BYTE *)Td4 + 4 * (unsigned __int8)v26)));
      if ( v35 == 1 )
        break;
      --v35;
      v37 += 16LL;
      a3 = v36 + 16;
    }
  }
}
// 579C0: using guessed type __int64 TD0[128];
// 57DC0: using guessed type __int64 TD1[128];
// 581C0: using guessed type __int64 TD2[128];
// 585C0: using guessed type __int64 TD3[128];
// 589C0: using guessed type __int64 Td4[128];

//----- (000000000001AC10) ----------------------------------------------------
__int64 __fastcall ccaes_ecb_encrypt_init(__int64 a1, __int64 a2, signed int a3, __int64 a4)
{
  __int64 result; // rax@1
  signed __int64 v5; // rdi@2
  int v6; // eax@5
  signed __int64 v7; // rcx@5
  __int64 v8; // r12@5
  int v9; // edi@6
  int v10; // edx@7
  int v11; // edi@7
  int v12; // edi@7
  int v13; // edi@7
  __int64 v14; // rcx@9
  unsigned int v15; // ebx@10
  int v16; // eax@10
  int v17; // eax@10
  int v18; // eax@10
  int v19; // eax@11
  int v20; // ebx@11
  int v21; // ebx@11
  int v22; // ebx@11
  int v23; // ebx@11
  int v24; // eax@14
  __int64 v25; // rcx@14
  int v26; // ebx@15
  int v27; // edx@15
  int v28; // edx@15
  signed __int64 v29; // r14@17
  signed int v30; // er15@17
  __int64 v31; // rcx@17
  signed __int64 v32; // rdi@17
  int v33; // ebx@18
  int v34; // ebx@18
  int v35; // ebx@18
  int v36; // ebx@18
  __int64 v37; // rsi@19
  signed __int64 v38; // rdx@19
  signed __int64 v39; // rsi@19
  signed __int64 v40; // rdi@19
  signed __int64 v41; // rax@19
  signed __int64 v42; // [sp+0h] [bp-30h]@5

  result = (unsigned int)a3;
  if ( (unsigned int)a3 <= 0x20 )
  {
    v5 = 4311810048LL;
    if ( _bittest((const unsigned __int64 *)&v5, (unsigned int)a3) )
    {
      *(_DWORD *)(a2 + 480) = 2 * (a3 / 8) + 6;
      *(_DWORD *)a2 = _byteswap_ulong(*(_DWORD *)a4);
      *(_DWORD *)(a2 + 4) = _byteswap_ulong(*(_DWORD *)(a4 + 4));
      *(_DWORD *)(a2 + 8) = _byteswap_ulong(*(_DWORD *)(a4 + 8));
      result = _byteswap_ulong(*(_DWORD *)(a4 + 12));
      *(_DWORD *)(a2 + 12) = result;
      if ( a3 == 32 )
      {
        *(_DWORD *)(a2 + 16) = _byteswap_ulong(*(_DWORD *)(a4 + 16));
        *(_DWORD *)(a2 + 20) = _byteswap_ulong(*(_DWORD *)(a4 + 20));
        *(_DWORD *)(a2 + 24) = _byteswap_ulong(*(_DWORD *)(a4 + 24));
        *(_DWORD *)(a2 + 28) = _byteswap_ulong(*(_DWORD *)(a4 + 28));
        v14 = 0LL;
        v42 = 60LL;
        while ( 1 )
        {
          v19 = *(_DWORD *)(a2 + 8 * v14 + 28);
          v20 = *(_DWORD *)((char *)rcon_0 + v14) ^ *((_DWORD *)Te4_0_0 + (*(_DWORD *)(a2 + 8 * v14 + 28) >> 24)) ^ *((_DWORD *)Te4_1_0 + (unsigned __int8)*(_DWORD *)(a2 + 8 * v14 + 28)) ^ *((_DWORD *)Te4_2_0 + BYTE1(v19)) ^ *(_DWORD *)(a2 + 8 * v14) ^ *((_DWORD *)Te4_3_0 + (unsigned __int8)(*(_DWORD *)(a2 + 8 * v14 + 28) >> 16));
          *(_DWORD *)(a2 + 8 * v14 + 32) = v20;
          v21 = *(_DWORD *)(a2 + 8 * v14 + 4) ^ v20;
          *(_DWORD *)(a2 + 8 * v14 + 36) = v21;
          v22 = *(_DWORD *)(a2 + 8 * v14 + 8) ^ v21;
          *(_DWORD *)(a2 + 8 * v14 + 40) = v22;
          v23 = *(_DWORD *)(a2 + 8 * v14 + 12) ^ v22;
          *(_DWORD *)(a2 + 8 * v14 + 44) = v23;
          if ( v14 == 24 )
            break;
          v15 = __ROR4__(v23, 8);
          v16 = *((_DWORD *)Te4_0_0 + (v15 >> 24)) ^ *((_DWORD *)Te4_1_0 + (unsigned __int8)v15) ^ *((_DWORD *)Te4_2_0 + BYTE1(v15)) ^ *((_DWORD *)Te4_3_0 + (unsigned __int8)(v15 >> 16)) ^ *(_DWORD *)(a2 + 8 * v14 + 16);
          *(_DWORD *)(a2 + 8 * v14 + 48) = v16;
          v17 = *(_DWORD *)(a2 + 8 * v14 + 20) ^ v16;
          *(_DWORD *)(a2 + 8 * v14 + 52) = v17;
          v18 = *(_DWORD *)(a2 + 8 * v14 + 24) ^ v17;
          *(_DWORD *)(a2 + 8 * v14 + 56) = v18;
          *(_DWORD *)(a2 + 8 * v14 + 60) = *(_DWORD *)(a2 + 8 * v14 + 28) ^ v18;
          v14 += 4LL;
        }
      }
      else if ( a3 == 24 )
      {
        *(_DWORD *)(a2 + 16) = _byteswap_ulong(*(_DWORD *)(a4 + 16));
        *(_DWORD *)(a2 + 20) = _byteswap_ulong(*(_DWORD *)(a4 + 20));
        v6 = *(_DWORD *)a2;
        v7 = a2 + 44;
        v8 = 0LL;
        v42 = 52LL;
        while ( 1 )
        {
          v10 = *(_DWORD *)(v7 - 24);
          v6 ^= *(_DWORD *)((char *)rcon_0 + v8) ^ *((_DWORD *)Te4_0_0 + (*(_DWORD *)(v7 - 24) >> 24)) ^ *((_DWORD *)Te4_1_0 + (unsigned __int8)*(_DWORD *)(v7 - 24)) ^ *((_DWORD *)Te4_2_0 + BYTE1(v10)) ^ *((_DWORD *)Te4_3_0 + (unsigned __int8)(*(_DWORD *)(v7 - 24) >> 16));
          *(_DWORD *)(v7 - 20) = v6;
          v11 = v6 ^ *(_DWORD *)(v7 - 40);
          *(_DWORD *)(v7 - 16) = v11;
          v12 = *(_DWORD *)(v7 - 36) ^ v11;
          *(_DWORD *)(v7 - 12) = v12;
          v13 = *(_DWORD *)(v7 - 32) ^ v12;
          *(_DWORD *)(v7 - 8) = v13;
          if ( v8 == 28 )
            break;
          v9 = *(_DWORD *)(v7 - 28) ^ v13;
          *(_DWORD *)(v7 - 4) = v9;
          *(_DWORD *)v7 = v10 ^ v9;
          v8 += 4LL;
          v7 += 24LL;
        }
      }
      else
      {
        if ( a3 != 16 )
          return result;
        v24 = *(_DWORD *)a2;
        v25 = 0LL;
        v42 = 44LL;
        do
        {
          v26 = *(_DWORD *)(a2 + 4 * v25 + 12);
          v24 ^= *(_DWORD *)((char *)rcon_0 + v25) ^ *((_DWORD *)Te4_0_0 + (*(_DWORD *)(a2 + 4 * v25 + 12) >> 24)) ^ *((_DWORD *)Te4_1_0 + (unsigned __int8)*(_DWORD *)(a2 + 4 * v25 + 12)) ^ *((_DWORD *)Te4_2_0 + BYTE1(v26)) ^ *((_DWORD *)Te4_3_0 + (unsigned __int8)(*(_DWORD *)(a2 + 4 * v25 + 12) >> 16));
          *(_DWORD *)(a2 + 4 * v25 + 16) = v24;
          v27 = v24 ^ *(_DWORD *)(a2 + 4 * v25 + 4);
          *(_DWORD *)(a2 + 4 * v25 + 20) = v27;
          v28 = *(_DWORD *)(a2 + 4 * v25 + 8) ^ v27;
          *(_DWORD *)(a2 + 4 * v25 + 24) = v28;
          *(_DWORD *)(a2 + 4 * v25 + 28) = v26 ^ v28;
          v25 += 4LL;
        }
        while ( v25 != 40 );
      }
      *(_DWORD *)(a2 + 240) = *(_DWORD *)(a2 + 4 * v42 - 16);
      *(_DWORD *)(a2 + 244) = *(_DWORD *)(a2 + 4 * v42 - 12);
      *(_DWORD *)(a2 + 248) = *(_DWORD *)(a2 + 4 * v42 - 8);
      *(_DWORD *)(a2 + 252) = *(_DWORD *)(a2 + 4 * v42 - 4);
      if ( *(_DWORD *)(a2 + 480) < 2 )
      {
        v40 = a2 + 4 * v42 - 16;
        v41 = a2 + 4 * v42 - 32;
        v38 = a2 + 240;
        v39 = a2 + 256;
      }
      else
      {
        v29 = a2 + 4 * v42;
        v30 = 1;
        v31 = 0LL;
        v32 = 268LL;
        do
        {
          v33 = *(_DWORD *)(v29 + v31 - 32);
          *(_DWORD *)(a2 + v32 - 12) = *((_DWORD *)Tks3_0 + (unsigned __int8)*(_DWORD *)(v29 + v31 - 32)) ^ *((_DWORD *)Tks2_0 + BYTE1(v33)) ^ *((_DWORD *)Tks0_0 + (*(_DWORD *)(v29 + v31 - 32) >> 24)) ^ *((_DWORD *)Tks1_0 + (unsigned __int8)(*(_DWORD *)(v29 + v31 - 32) >> 16));
          v34 = *(_DWORD *)(v29 + v31 - 28);
          *(_DWORD *)(a2 + v32 - 8) = *((_DWORD *)Tks3_0 + (unsigned __int8)*(_DWORD *)(v29 + v31 - 28)) ^ *((_DWORD *)Tks2_0 + BYTE1(v34)) ^ *((_DWORD *)Tks0_0 + (*(_DWORD *)(v29 + v31 - 28) >> 24)) ^ *((_DWORD *)Tks1_0 + (unsigned __int8)(*(_DWORD *)(v29 + v31 - 28) >> 16));
          v35 = *(_DWORD *)(v29 + v31 - 24);
          *(_DWORD *)(a2 + v32 - 4) = *((_DWORD *)Tks3_0 + (unsigned __int8)*(_DWORD *)(v29 + v31 - 24)) ^ *((_DWORD *)Tks2_0 + BYTE1(v35)) ^ *((_DWORD *)Tks0_0 + (*(_DWORD *)(v29 + v31 - 24) >> 24)) ^ *((_DWORD *)Tks1_0 + (unsigned __int8)(*(_DWORD *)(v29 + v31 - 24) >> 16));
          v36 = *(_DWORD *)(v29 + v31 - 20);
          *(_DWORD *)(a2 + v32) = *((_DWORD *)Tks3_0 + (unsigned __int8)*(_DWORD *)(v29 + v31 - 20)) ^ *((_DWORD *)Tks2_0 + BYTE1(v36)) ^ *((_DWORD *)Tks0_0 + (*(_DWORD *)(v29 + v31 - 20) >> 24)) ^ *((_DWORD *)Tks1_0 + (unsigned __int8)(*(_DWORD *)(v29 + v31 - 20) >> 16));
          ++v30;
          v31 -= 16LL;
          v32 += 16LL;
        }
        while ( v30 < *(_DWORD *)(a2 + 480) );
        v37 = a2 - v31;
        v38 = v37 + 240;
        v39 = v37 + 256;
        v40 = v31 + v29 - 16;
        v41 = v31 + v29 - 32;
      }
      *(_DWORD *)v39 = *(_DWORD *)v41;
      *(_DWORD *)(v38 + 20) = *(_DWORD *)(v40 - 12);
      *(_DWORD *)(v38 + 24) = *(_DWORD *)(v40 - 8);
      result = *(_DWORD *)(v40 - 4);
      *(_DWORD *)(v38 + 28) = result;
    }
  }
  return result;
}
// 5BDF0: using guessed type __int64 Te4_3_0[128];
// 5C1F0: using guessed type __int64 Te4_2_0[128];
// 5C5F0: using guessed type __int64 Te4_1_0[128];
// 5C9F0: using guessed type __int64 Te4_0_0[128];
// 5CDF0: using guessed type __int64 rcon_0[6];
// 5CE20: using guessed type __int64 Tks0_0[128];
// 5D220: using guessed type __int64 Tks1_0[128];
// 5D620: using guessed type __int64 Tks2_0[128];
// 5DA20: using guessed type __int64 Tks3_0[128];

//----- (000000000001B051) ----------------------------------------------------
void __fastcall ccaes_ecb_encrypt(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4)
{
  unsigned int v4; // eax@4
  unsigned __int32 v5; // er8@4
  unsigned int v6; // ebx@4
  unsigned int v7; // ecx@4
  signed __int64 i; // r13@4
  signed __int64 v9; // r14@5
  __int64 v10; // ST78_8@5
  unsigned int v11; // er9@5
  int v12; // er13@5
  unsigned int v13; // ST68_4@6
  unsigned int v14; // er14@6
  __int64 v15; // ST80_8@6
  __int64 v16; // ST78_8@6
  unsigned int v17; // er12@6
  __int64 v18; // ST70_8@6
  __int64 v19; // ST60_8@6
  int v20; // eax@6
  __int64 v21; // r15@6
  unsigned int v22; // er8@6
  int v23; // ecx@6
  int v24; // ebx@6
  int v25; // edx@6
  int v26; // eax@6
  __int64 v27; // r12@6
  int v28; // esi@7
  signed __int64 v29; // [sp+28h] [bp-A0h]@2
  __int64 v30; // [sp+30h] [bp-98h]@1
  signed __int64 v31; // [sp+38h] [bp-90h]@4
  __int64 v32; // [sp+40h] [bp-88h]@2
  signed __int64 v33; // [sp+48h] [bp-80h]@4
  __int64 v34; // [sp+50h] [bp-78h]@2
  int v35; // [sp+98h] [bp-30h]@4

  v30 = a1;
  if ( a2 )
  {
    v32 = a2;
    v34 = a4;
    v29 = a1 + 44;
    while ( 1 )
    {
      v33 = a3;
      v4 = *(_DWORD *)a1 ^ _byteswap_ulong(*(_DWORD *)a3);
      v5 = *(_DWORD *)(a1 + 4) ^ _byteswap_ulong(*(_DWORD *)(a3 + 4));
      v6 = *(_DWORD *)(a1 + 8) ^ _byteswap_ulong(*(_DWORD *)(a3 + 8));
      v7 = *(_DWORD *)(a1 + 12) ^ _byteswap_ulong(*(_DWORD *)(a3 + 12));
      v35 = *(_DWORD *)(a1 + 480) >> 1;
      v31 = 2LL * (unsigned int)(v35 - 1) + 2;
      for ( i = v29; ; i = v9 + 32 )
      {
        v13 = v4;
        v14 = v5;
        v15 = BYTE1(v7);
        v16 = (unsigned __int8)v4;
        v17 = v6;
        v18 = (unsigned __int8)v5;
        v19 = BYTE1(v4);
        v20 = *((_DWORD *)TE0 + (v7 >> 24)) ^ *((_DWORD *)TE1 + (unsigned __int8)(v4 >> 16));
        v21 = (unsigned __int8)v6;
        v22 = v7 >> 16;
        v23 = *(_DWORD *)(i - 28) ^ *((_DWORD *)TE3 + (unsigned __int8)v7) ^ *((_DWORD *)TE2 + BYTE1(v6)) ^ *((_DWORD *)TE0 + (v13 >> 24)) ^ *((_DWORD *)TE1 + (unsigned __int8)(v14 >> 16));
        v24 = *(_DWORD *)(i - 24) ^ *((_DWORD *)TE3 + v16) ^ *((_DWORD *)TE2 + v15) ^ *((_DWORD *)TE0 + (v14 >> 24)) ^ *((_DWORD *)TE1 + (unsigned __int8)(v6 >> 16));
        v25 = *(_DWORD *)(i - 20) ^ *((_DWORD *)TE3 + v18) ^ *((_DWORD *)TE2 + v19) ^ *((_DWORD *)TE0 + (v17 >> 24)) ^ *((_DWORD *)TE1 + (unsigned __int8)v22);
        v26 = *(_DWORD *)(i - 16) ^ *((_DWORD *)TE3 + v21) ^ *((_DWORD *)TE2 + BYTE1(v14)) ^ v20;
        v27 = (unsigned int)v23 >> 24;
        if ( v35 == 1 )
          break;
        --v35;
        v9 = i;
        v10 = BYTE1(v26);
        v11 = (unsigned int)v26 >> 16;
        v12 = *((_DWORD *)TE2 + BYTE1(v24)) ^ *((_DWORD *)TE0 + ((unsigned int)v26 >> 24)) ^ *((_DWORD *)TE1
                                                                                             + (unsigned __int8)((unsigned int)v23 >> 16));
        v4 = *(_DWORD *)(v9 - 12) ^ *((_DWORD *)TE3 + (unsigned __int8)v26) ^ *((_DWORD *)TE2 + BYTE1(v25)) ^ *((_DWORD *)TE0 + v27) ^ *((_DWORD *)TE1 + (unsigned __int8)((unsigned int)v24 >> 16));
        v5 = *(_DWORD *)(v9 - 8) ^ *((_DWORD *)TE3 + (unsigned __int8)v23) ^ *((_DWORD *)TE2 + v10) ^ *((_DWORD *)TE0 + ((unsigned int)v24 >> 24)) ^ *((_DWORD *)TE1 + (unsigned __int8)((unsigned int)v25 >> 16));
        v6 = *(_DWORD *)(v9 - 4) ^ *((_DWORD *)TE3 + (unsigned __int8)v24) ^ *((_DWORD *)TE2 + BYTE1(v23)) ^ *((_DWORD *)TE0 + ((unsigned int)v25 >> 24)) ^ *((_DWORD *)TE1 + (unsigned __int8)v11);
        v7 = *(_DWORD *)v9 ^ *((_DWORD *)TE3 + (unsigned __int8)v25) ^ v12;
      }
      v28 = *((_DWORD *)Te4_3_0 + ((unsigned int)v26 >> 24)) ^ *((_DWORD *)Te4_2_0
                                                               + (unsigned __int8)((unsigned int)v23 >> 16));
      a1 = v30;
      *(_DWORD *)v34 = _byteswap_ulong(*(_DWORD *)(v30 + 16 * v31) ^ *((_DWORD *)Te4_0_0 + (unsigned __int8)v26) ^ *((_DWORD *)Te4_1_0 + BYTE1(v25)) ^ *((_DWORD *)Te4_3_0 + v27) ^ *((_DWORD *)Te4_2_0 + (unsigned __int8)((unsigned int)v24 >> 16)));
      *(_DWORD *)(v34 + 4) = _byteswap_ulong(*(_DWORD *)(v30 + 16 * v31 + 4) ^ *((_DWORD *)Te4_0_0 + (unsigned __int8)v23) ^ *((_DWORD *)Te4_1_0 + BYTE1(v26)) ^ *((_DWORD *)Te4_3_0 + ((unsigned int)v24 >> 24)) ^ *((_DWORD *)Te4_2_0 + (unsigned __int8)((unsigned int)v25 >> 16)));
      *(_DWORD *)(v34 + 8) = _byteswap_ulong(*(_DWORD *)(v30 + 16 * v31 + 8) ^ *((_DWORD *)Te4_0_0 + (unsigned __int8)v24) ^ *((_DWORD *)Te4_1_0 + BYTE1(v23)) ^ *((_DWORD *)Te4_3_0 + ((unsigned int)v25 >> 24)) ^ *((_DWORD *)Te4_2_0 + (unsigned __int8)((unsigned int)v26 >> 16)));
      *(_DWORD *)(v34 + 12) = _byteswap_ulong(*(_DWORD *)(v30 + 16 * v31 + 12) ^ *((_DWORD *)Te4_0_0
                                                                                 + (unsigned __int8)v25) ^ *((_DWORD *)Te4_1_0 + BYTE1(v24)) ^ v28);
      if ( v32 == 1 )
        break;
      --v32;
      a3 = v33 + 16;
      v34 += 16LL;
    }
  }
}
// 5ADF0: using guessed type __int64 TE0[128];
// 5B1F0: using guessed type __int64 TE1[128];
// 5B5F0: using guessed type __int64 TE2[128];
// 5B9F0: using guessed type __int64 TE3[128];
// 5BDF0: using guessed type __int64 Te4_3_0[128];
// 5C1F0: using guessed type __int64 Te4_2_0[128];
// 5C5F0: using guessed type __int64 Te4_1_0[128];
// 5C9F0: using guessed type __int64 Te4_0_0[128];

//----- (000000000001B44B) ----------------------------------------------------
__int64 (*ccaes_xts_decrypt_mode())[3]
{
  __int64 v0; // rax@1
  bool v1; // zf@1
  __int64 (*result)[3]; // rax@1

  LODWORD(v0) = cpuid_features();
  v1 = (v0 & 0x200000000000000uLL) >> 57 == 0;
  result = (__int64 (*)[3])ccaes_intel_xts_decrypt_opt_mode;
  if ( !v1 )
    result = off_69040[0];
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 69040: using guessed type __int64 (*off_69040[2])[3];
// 69248: using guessed type __int64 ccaes_intel_xts_decrypt_opt_mode[3];

//----- (000000000001B476) ----------------------------------------------------
__int64 __fastcall ccder_decode_rsa_pub(__int64 a1, unsigned __int64 a2, unsigned __int64 a3)
{
  unsigned __int64 v3; // r14@1
  signed __int64 v4; // rax@1
  __int64 v5; // rax@1
  __int64 v6; // r14@1
  unsigned __int64 v8; // [sp+8h] [bp-18h]@1

  v8 = a3;
  v3 = *(_QWORD *)a1;
  v4 = ccder_decode_constructed_tl(a3, a2, a3, 2305843009213693968LL, (__int64)&v8);
  v5 = ccder_decode_uint(v4, v4, v8, v3, a1 + 16);
  v6 = ccder_decode_uint(v5, v5, v8, v3, 16LL * *(_QWORD *)a1 + a1 + 24);
  if ( v6 )
    cczp_init(a1);
  return v6;
}

//----- (000000000001B4F4) ----------------------------------------------------
__int64 (*ccaes_xts_encrypt_mode())[3]
{
  __int64 v0; // rax@1
  bool v1; // zf@1
  __int64 (*result)[3]; // rax@1

  LODWORD(v0) = cpuid_features();
  v1 = (v0 & 0x200000000000000uLL) >> 57 == 0;
  result = (__int64 (*)[3])ccaes_intel_xts_encrypt_opt_mode;
  if ( !v1 )
    result = off_69048;
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 69048: using guessed type __int64 (*off_69048)[3];
// 692C8: using guessed type __int64 ccaes_intel_xts_encrypt_opt_mode[3];

//----- (000000000001B51F) ----------------------------------------------------
__int64 ccblowfish_ecb_decrypt_mode()
{
  return (__int64)ccblowfish_ltc_ecb_decrypt_mode;
}
// 69388: using guessed type __int64 ccblowfish_ltc_ecb_decrypt_mode[2];

//----- (000000000001B52C) ----------------------------------------------------
__int64 ccblowfish_ecb_encrypt_mode()
{
  return (__int64)ccblowfish_ltc_ecb_encrypt_mode;
}
// 693A8: using guessed type __int64 ccblowfish_ltc_ecb_encrypt_mode[2];

//----- (000000000001B539) ----------------------------------------------------
__int64 *ccblowfish_cbc_encrypt_mode()
{
  __int64 v0; // rdx@1
  __int64 *result; // rax@1

  v0 = ccblowfish_ltc_ecb_encrypt_mode[1];
  cbc_blowfish_encrypt = ((v0 + 15) & 0xFFFFFFFFFFFFFFF8LL)
                       + ((ccblowfish_ltc_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &cbc_blowfish_encrypt;
  qword_6B698 = v0;
  qword_6B6A0 = (__int64)ccmode_cbc_init;
  qword_6B6A8 = (__int64)ccmode_cbc_encrypt;
  qword_6B6B0 = (__int64)ccblowfish_ltc_ecb_encrypt_mode;
  return result;
}
// 693A8: using guessed type __int64 ccblowfish_ltc_ecb_encrypt_mode[2];
// 6B690: using guessed type __int64 cbc_blowfish_encrypt;
// 6B698: using guessed type __int64 qword_6B698;
// 6B6A0: using guessed type __int64 qword_6B6A0;
// 6B6A8: using guessed type __int64 qword_6B6A8;
// 6B6B0: using guessed type __int64 qword_6B6B0;

//----- (000000000001B598) ----------------------------------------------------
__int64 *ccblowfish_cbc_decrypt_mode()
{
  __int64 v0; // rdx@1
  __int64 *result; // rax@1

  v0 = ccblowfish_ltc_ecb_decrypt_mode[1];
  cbc_blowfish_decrypt = ((v0 + 15) & 0xFFFFFFFFFFFFFFF8LL)
                       + ((ccblowfish_ltc_ecb_decrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &cbc_blowfish_decrypt;
  qword_6B6C0 = v0;
  qword_6B6C8 = (__int64)ccmode_cbc_init;
  qword_6B6D0 = (__int64)ccmode_cbc_decrypt;
  qword_6B6D8 = (__int64)ccblowfish_ltc_ecb_decrypt_mode;
  return result;
}
// 69388: using guessed type __int64 ccblowfish_ltc_ecb_decrypt_mode[2];
// 6B6B8: using guessed type __int64 cbc_blowfish_decrypt;
// 6B6C0: using guessed type __int64 qword_6B6C0;
// 6B6C8: using guessed type __int64 qword_6B6C8;
// 6B6D0: using guessed type __int64 qword_6B6D0;
// 6B6D8: using guessed type __int64 qword_6B6D8;

//----- (000000000001B5F7) ----------------------------------------------------
__int64 *ccblowfish_cfb_encrypt_mode()
{
  __int64 *result; // rax@1

  cfb_blowfish_encrypt = ((2 * ccblowfish_ltc_ecb_encrypt_mode[1] + 30) & 0xFFFFFFFFFFFFFFF0LL)
                       + ((ccblowfish_ltc_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &cfb_blowfish_encrypt;
  qword_6B6E8 = 1LL;
  qword_6B6F0 = (__int64)ccmode_cfb_init;
  qword_6B6F8 = (__int64)ccmode_cfb_encrypt;
  qword_6B700 = (__int64)ccblowfish_ltc_ecb_encrypt_mode;
  return result;
}
// 693A8: using guessed type __int64 ccblowfish_ltc_ecb_encrypt_mode[2];
// 6B6E0: using guessed type __int64 cfb_blowfish_encrypt;
// 6B6E8: using guessed type __int64 qword_6B6E8;
// 6B6F0: using guessed type __int64 qword_6B6F0;
// 6B6F8: using guessed type __int64 qword_6B6F8;
// 6B700: using guessed type __int64 qword_6B700;

//----- (000000000001B65B) ----------------------------------------------------
__int64 *ccblowfish_cfb_decrypt_mode()
{
  __int64 *result; // rax@1

  cfb_blowfish_decrypt = ((2 * ccblowfish_ltc_ecb_encrypt_mode[1] + 30) & 0xFFFFFFFFFFFFFFF0LL)
                       + ((ccblowfish_ltc_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &cfb_blowfish_decrypt;
  qword_6B710 = 1LL;
  qword_6B718 = (__int64)ccmode_cfb_init;
  qword_6B720 = (__int64)ccmode_cfb_decrypt;
  qword_6B728 = (__int64)ccblowfish_ltc_ecb_encrypt_mode;
  return result;
}
// 693A8: using guessed type __int64 ccblowfish_ltc_ecb_encrypt_mode[2];
// 6B708: using guessed type __int64 cfb_blowfish_decrypt;
// 6B710: using guessed type __int64 qword_6B710;
// 6B718: using guessed type __int64 qword_6B718;
// 6B720: using guessed type __int64 qword_6B720;
// 6B728: using guessed type __int64 qword_6B728;

//----- (000000000001B6BF) ----------------------------------------------------
__int64 *ccblowfish_cfb8_encrypt_mode()
{
  __int64 *result; // rax@1

  cfb8_blowfish_encrypt = ((2 * ccblowfish_ltc_ecb_encrypt_mode[1] + 14) & 0xFFFFFFFFFFFFFFF0LL)
                        + ((ccblowfish_ltc_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL)
                        + 8;
  result = &cfb8_blowfish_encrypt;
  qword_6B738 = 1LL;
  qword_6B740 = (__int64)ccmode_cfb8_init;
  qword_6B748 = (__int64)ccmode_cfb8_encrypt;
  qword_6B750 = (__int64)ccblowfish_ltc_ecb_encrypt_mode;
  return result;
}
// 693A8: using guessed type __int64 ccblowfish_ltc_ecb_encrypt_mode[2];
// 6B730: using guessed type __int64 cfb8_blowfish_encrypt;
// 6B738: using guessed type __int64 qword_6B738;
// 6B740: using guessed type __int64 qword_6B740;
// 6B748: using guessed type __int64 qword_6B748;
// 6B750: using guessed type __int64 qword_6B750;

//----- (000000000001B725) ----------------------------------------------------
__int64 *ccblowfish_cfb8_decrypt_mode()
{
  __int64 *result; // rax@1

  cfb8_blowfish_decrypt = ((2 * ccblowfish_ltc_ecb_encrypt_mode[1] + 14) & 0xFFFFFFFFFFFFFFF0LL)
                        + ((ccblowfish_ltc_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL)
                        + 8;
  result = &cfb8_blowfish_decrypt;
  qword_6B760 = 1LL;
  qword_6B768 = (__int64)ccmode_cfb8_init;
  qword_6B770 = (__int64)ccmode_cfb8_decrypt;
  qword_6B778 = (__int64)ccblowfish_ltc_ecb_encrypt_mode;
  return result;
}
// 693A8: using guessed type __int64 ccblowfish_ltc_ecb_encrypt_mode[2];
// 6B758: using guessed type __int64 cfb8_blowfish_decrypt;
// 6B760: using guessed type __int64 qword_6B760;
// 6B768: using guessed type __int64 qword_6B768;
// 6B770: using guessed type __int64 qword_6B770;
// 6B778: using guessed type __int64 qword_6B778;

//----- (000000000001B78B) ----------------------------------------------------
__int64 *ccblowfish_ctr_crypt_mode()
{
  __int64 *result; // rax@1

  ctr_blowfish = ((2 * ccblowfish_ltc_ecb_encrypt_mode[1] + 30) & 0xFFFFFFFFFFFFFFF0LL)
               + ((ccblowfish_ltc_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &ctr_blowfish;
  qword_6B788 = 1LL;
  qword_6B790 = (__int64)ccmode_ctr_init;
  qword_6B798 = (__int64)ccmode_ctr_crypt;
  qword_6B7A0 = (__int64)ccblowfish_ltc_ecb_encrypt_mode;
  return result;
}
// 693A8: using guessed type __int64 ccblowfish_ltc_ecb_encrypt_mode[2];
// 6B780: using guessed type __int64 ctr_blowfish;
// 6B788: using guessed type __int64 qword_6B788;
// 6B790: using guessed type __int64 qword_6B790;
// 6B798: using guessed type __int64 qword_6B798;
// 6B7A0: using guessed type __int64 qword_6B7A0;

//----- (000000000001B7EF) ----------------------------------------------------
__int64 *ccblowfish_ofb_crypt_mode()
{
  __int64 *result; // rax@1

  ofb_blowfish = ((ccblowfish_ltc_ecb_encrypt_mode[1] + 23) & 0xFFFFFFFFFFFFFFF8LL)
               + ((ccblowfish_ltc_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &ofb_blowfish;
  qword_6B7B0 = 1LL;
  qword_6B7B8 = (__int64)ccmode_ofb_init;
  qword_6B7C0 = (__int64)ccmode_ofb_crypt;
  qword_6B7C8 = (__int64)ccblowfish_ltc_ecb_encrypt_mode;
  return result;
}
// 693A8: using guessed type __int64 ccblowfish_ltc_ecb_encrypt_mode[2];
// 6B7A8: using guessed type __int64 ofb_blowfish;
// 6B7B0: using guessed type __int64 qword_6B7B0;
// 6B7B8: using guessed type __int64 qword_6B7B8;
// 6B7C0: using guessed type __int64 qword_6B7C0;
// 6B7C8: using guessed type __int64 qword_6B7C8;

//----- (000000000001B852) ----------------------------------------------------
__int64 __fastcall ccblowfish_ltc_setup(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // r10@1
  __int64 v6; // r9@1
  __int64 v7; // rbx@1
  signed int v8; // esi@2
  int v9; // edi@2
  int v10; // eax@3
  __int64 v11; // rdi@3
  __int64 v12; // rcx@7
  __int64 v13; // rsi@7
  signed __int64 v14; // rdi@8
  __int64 v15; // rax@8
  __int64 v16; // r15@12
  signed __int64 v17; // rax@14
  signed __int64 v18; // rcx@14
  unsigned int v19; // er13@15
  signed __int64 v20; // r15@15
  signed __int64 v22; // [sp+8h] [bp-48h]@15
  signed __int64 v23; // [sp+10h] [bp-40h]@15
  char v24[4]; // [sp+18h] [bp-38h]@11
  unsigned int v25; // [sp+1Ch] [bp-34h]@13
  __int64 v26; // [sp+20h] [bp-30h]@1

  v4 = a2;
  v26 = *(_QWORD *)off_69010[0];
  v5 = 0LL;
  v6 = 0LL;
  LODWORD(v7) = 0;
  do
  {
    v8 = 4;
    v9 = 0;
    do
    {
      v10 = v9 << 8;
      v11 = (unsigned int)v7;
      v7 = (unsigned int)(v7 + 1);
      v9 = v10 | *(_BYTE *)(a4 + v11);
      if ( v7 == a3 )
        LODWORD(v7) = 0;
      --v8;
    }
    while ( v8 );
    *(_DWORD *)(v4 + 4 * v6 + 4096) = *((_DWORD *)ORIG_P + v6) ^ v9;
    ++v6;
  }
  while ( v6 != 18 );
  v12 = 0LL;
  v13 = 0LL;
  do
  {
    v14 = 256LL;
    v15 = v5;
    do
    {
      *(_DWORD *)(v4 + v15) = *(_DWORD *)((char *)ORIG_S + v15);
      v15 += 4LL;
      --v14;
    }
    while ( v14 );
    ++v13;
    v5 += 1024LL;
  }
  while ( v13 != 4 );
  do
    v24[v12++] = 0;
  while ( v12 != 8 );
  v16 = 0LL;
  do
  {
    ccblowfish_ltc_ecb_encrypt(v4, 1LL, (__int64)v24, (__int64)v24);
    *(_DWORD *)(v4 + 4 * v16 + 4096) = _byteswap_ulong(*(unsigned int *)v24);
    *(_DWORD *)(v4 + 4 * v16 + 4100) = _byteswap_ulong(v25);
    v16 += 2LL;
  }
  while ( (unsigned int)v16 < 0x12 );
  v17 = v4 + 4;
  v18 = 0LL;
  do
  {
    v22 = v18;
    v23 = v17;
    v19 = 0;
    v20 = v17;
    do
    {
      ccblowfish_ltc_ecb_encrypt(v4, 1LL, (__int64)v24, (__int64)v24);
      *(_DWORD *)(v20 - 4) = _byteswap_ulong(*(unsigned int *)v24);
      *(_DWORD *)v20 = _byteswap_ulong(v25);
      v20 += 8LL;
      v19 += 2;
    }
    while ( v19 < 0x100 );
    v18 = v22 + 1;
    v17 = v23 + 1024;
  }
  while ( v22 != 3 );
  return *(_QWORD *)off_69010[0];
}
// 5DE20: using guessed type __int64 ORIG_P[10];
// 5DE70: using guessed type __int64 ORIG_S[512];
// 69010: using guessed type __int64 off_69010[2];
// 1B852: using guessed type char var_38[4];

//----- (000000000001B9C0) ----------------------------------------------------
void __fastcall ccblowfish_ltc_ecb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r9@1
  __int64 i; // r10@1
  int v6; // ebx@2
  int v7; // eax@2
  __int64 v8; // r8@2
  int v9; // ebx@3
  int v10; // ecx@3
  int v11; // edx@3
  __int64 v12; // rax@3
  int v13; // ebx@4

  v4 = a4;
  for ( i = a3; a2; --a2 )
  {
    v6 = _byteswap_ulong(*(_DWORD *)i);
    v7 = _byteswap_ulong(*(_DWORD *)(i + 4));
    v8 = 0LL;
    do
    {
      v9 = *(_DWORD *)(a1 + 4 * v8 + 4096) ^ v6;
      v10 = *(_DWORD *)(a1 + 4 * v8 + 4100) ^ v7 ^ (*(_DWORD *)(a1 + 4LL * (unsigned __int8)v9 + 3072)
                                                  + (*(_DWORD *)(a1 + 4LL * BYTE1(v9) + 2048) ^ (*(_DWORD *)(a1 + 4LL * ((unsigned int)v9 >> 24))
                                                                                               + *(_DWORD *)(a1 + 4LL * (unsigned __int8)((unsigned int)v9 >> 16) + 1024))));
      v11 = *(_DWORD *)(a1 + 4 * v8 + 4104) ^ v9 ^ (*(_DWORD *)(a1
                                                              + 4LL
                                                              * (unsigned __int8)(*(_BYTE *)(a1 + 4 * v8 + 4100) ^ v7 ^ (*(_BYTE *)(a1 + 4LL * (unsigned __int8)v9 + 3072) + (*(_BYTE *)(a1 + 4LL * BYTE1(v9) + 2048) ^ (*(_BYTE *)(a1 + 4LL * ((unsigned int)v9 >> 24)) + *(_BYTE *)(a1 + 4LL * (unsigned __int8)((unsigned int)v9 >> 16) + 1024)))))
                                                              + 3072)
                                                  + (*(_DWORD *)(a1 + 4LL * BYTE1(v10) + 2048) ^ (*(_DWORD *)(a1 + 4LL * ((unsigned int)v10 >> 24))
                                                                                                + *(_DWORD *)(a1 + 4LL * (unsigned __int8)((unsigned int)v10 >> 16) + 1024))));
      v12 = (unsigned __int8)((*(_DWORD *)(a1 + 4 * v8 + 4104) ^ v9 ^ (*(_DWORD *)(a1
                                                                                 + 4LL
                                                                                 * (unsigned __int8)(*(_BYTE *)(a1 + 4 * v8 + 4100) ^ v7 ^ (*(_BYTE *)(a1 + 4LL * (unsigned __int8)v9 + 3072) + (*(_BYTE *)(a1 + 4LL * BYTE1(v9) + 2048) ^ (*(_BYTE *)(a1 + 4LL * ((unsigned int)v9 >> 24)) + *(_BYTE *)(a1 + 4LL * (unsigned __int8)((unsigned int)v9 >> 16) + 1024)))))
                                                                                 + 3072)
                                                                     + (*(_DWORD *)(a1 + 4LL * BYTE1(v10) + 2048) ^ (unsigned int)(*(_DWORD *)(a1 + 4LL * ((unsigned int)v10 >> 24)) + *(_DWORD *)(a1 + 4LL * (unsigned __int8)((unsigned int)v10 >> 16) + 1024))))) >> 16);
      v7 = *(_DWORD *)(a1 + 4 * v8 + 4108) ^ v10 ^ (*(_DWORD *)(a1 + 4LL * (unsigned __int8)v11 + 3072)
                                                  + (*(_DWORD *)(a1 + 4LL * BYTE1(v11) + 2048) ^ (*(_DWORD *)(a1 + 4LL * ((unsigned int)v11 >> 24))
                                                                                                + *(_DWORD *)(a1 + 4 * v12 + 1024))));
      v6 = v11 ^ (*(_DWORD *)(a1 + 4LL * (unsigned __int8)v7 + 3072)
                + (*(_DWORD *)(a1 + 4LL * BYTE1(v7) + 2048) ^ (*(_DWORD *)(a1 + 4LL * ((unsigned int)v7 >> 24))
                                                             + *(_DWORD *)(a1
                                                                         + 4LL
                                                                         * (unsigned __int8)((unsigned int)v7 >> 16)
                                                                         + 1024))));
      v8 += 4LL;
    }
    while ( (signed int)v8 < 16 );
    v13 = *(_DWORD *)(a1 + 4160) ^ v6;
    *(_DWORD *)v4 = _byteswap_ulong(*(_DWORD *)(a1 + 4164) ^ v7);
    *(_DWORD *)(v4 + 4) = _byteswap_ulong(v13);
    i += 8LL;
    v4 += 8LL;
  }
}

//----- (000000000001BAF7) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_rsa_pub_size(__int64 *a1)
{
  __int64 v1; // r14@1
  signed __int64 v2; // r15@1
  signed __int64 v3; // rax@1

  v1 = *a1;
  v2 = ccder_sizeof_integer(*a1, (__int64)(a1 + 2));
  v3 = ccder_sizeof_integer(v1, (__int64)&a1[2 * v1 + 3]);
  return ccder_sizeof(2305843009213693968LL, v3 + v2);
}

//----- (000000000001BB47) ----------------------------------------------------
void __fastcall ccblowfish_ltc_ecb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r9@1
  __int64 i; // r10@1
  unsigned int v6; // ebx@2
  unsigned int v7; // eax@2
  signed __int64 v8; // r8@2
  int v9; // ecx@3
  int v10; // edx@3
  int v11; // eax@3

  v4 = a4;
  for ( i = a3; a2; --a2 )
  {
    v6 = *(_DWORD *)(a1 + 4164) ^ _byteswap_ulong(*(_DWORD *)i);
    v7 = *(_DWORD *)(a1 + 4160) ^ _byteswap_ulong(*(_DWORD *)(i + 4));
    v8 = 1039LL;
    do
    {
      v9 = v7 ^ (*(_DWORD *)(a1 + 4LL * (unsigned __int8)v6 + 3072)
               + (*(_DWORD *)(a1 + 4LL * BYTE1(v6) + 2048) ^ (*(_DWORD *)(a1 + 4LL * (v6 >> 24))
                                                            + *(_DWORD *)(a1 + 4LL * (unsigned __int8)(v6 >> 16) + 1024))));
      v10 = *(_DWORD *)(a1 + 4 * v8) ^ v6 ^ (*(_DWORD *)(a1 + 4LL * (unsigned __int8)v9 + 3072)
                                           + (*(_DWORD *)(a1 + 4LL * BYTE1(v9) + 2048) ^ (*(_DWORD *)(a1 + 4LL * ((unsigned int)v9 >> 24))
                                                                                        + *(_DWORD *)(a1 + 4LL * (unsigned __int8)((v7 ^ (*(_DWORD *)(a1 + 4LL * (unsigned __int8)v6 + 3072) + (*(_DWORD *)(a1 + 4LL * BYTE1(v6) + 2048) ^ (*(_DWORD *)(a1 + 4LL * (v6 >> 24)) + *(_DWORD *)(a1 + 4LL * (unsigned __int8)(v6 >> 16) + 1024))))) >> 16) + 1024))));
      v11 = *(_DWORD *)(a1 + 4 * v8 - 4) ^ v9 ^ (*(_DWORD *)(a1 + 4LL * (unsigned __int8)v10 + 3072)
                                               + (*(_DWORD *)(a1 + 4LL * BYTE1(v10) + 2048) ^ (*(_DWORD *)(a1 + 4LL * ((unsigned int)v10 >> 24))
                                                                                             + *(_DWORD *)(a1 + 4LL * (unsigned __int8)((unsigned int)v10 >> 16) + 1024))));
      v6 = *(_DWORD *)(a1 + 4 * v8 - 8) ^ v10 ^ (*(_DWORD *)(a1 + 4LL * (unsigned __int8)v11 + 3072)
                                               + (*(_DWORD *)(a1 + 4LL * BYTE1(v11) + 2048) ^ (*(_DWORD *)(a1 + 4LL * ((unsigned int)v11 >> 24))
                                                                                             + *(_DWORD *)(a1 + 4LL * (unsigned __int8)((unsigned int)v11 >> 16) + 1024))));
      v7 = *(_DWORD *)(a1 + 4 * v8 - 12) ^ v11;
      v8 -= 4LL;
    }
    while ( (signed int)v8 - 1024 > 0 );
    *(_DWORD *)v4 = _byteswap_ulong(v7);
    *(_DWORD *)(v4 + 4) = _byteswap_ulong(v6);
    v4 += 8LL;
    i += 8LL;
  }
}

//----- (000000000001BC79) ----------------------------------------------------
__int64 cccast_ecb_decrypt_mode()
{
  return (__int64)cccast_eay_ecb_decrypt_mode;
}
// 693C8: using guessed type __int64 cccast_eay_ecb_decrypt_mode[2];

//----- (000000000001BC86) ----------------------------------------------------
__int64 cccast_ecb_encrypt_mode()
{
  return (__int64)cccast_eay_ecb_encrypt_mode;
}
// 693E8: using guessed type __int64 cccast_eay_ecb_encrypt_mode[2];

//----- (000000000001BC93) ----------------------------------------------------
__int64 *cccast_cbc_encrypt_mode()
{
  __int64 v0; // rdx@1
  __int64 *result; // rax@1

  v0 = cccast_eay_ecb_encrypt_mode[1];
  cbc_cast_encrypt = ((v0 + 15) & 0xFFFFFFFFFFFFFFF8LL) + ((cccast_eay_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &cbc_cast_encrypt;
  qword_6B7D8 = v0;
  qword_6B7E0 = (__int64)ccmode_cbc_init;
  qword_6B7E8 = (__int64)ccmode_cbc_encrypt;
  qword_6B7F0 = (__int64)cccast_eay_ecb_encrypt_mode;
  return result;
}
// 693E8: using guessed type __int64 cccast_eay_ecb_encrypt_mode[2];
// 6B7D0: using guessed type __int64 cbc_cast_encrypt;
// 6B7D8: using guessed type __int64 qword_6B7D8;
// 6B7E0: using guessed type __int64 qword_6B7E0;
// 6B7E8: using guessed type __int64 qword_6B7E8;
// 6B7F0: using guessed type __int64 qword_6B7F0;

//----- (000000000001BCF2) ----------------------------------------------------
__int64 *cccast_cbc_decrypt_mode()
{
  __int64 v0; // rdx@1
  __int64 *result; // rax@1

  v0 = cccast_eay_ecb_decrypt_mode[1];
  cbc_cast_decrypt = ((v0 + 15) & 0xFFFFFFFFFFFFFFF8LL) + ((cccast_eay_ecb_decrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &cbc_cast_decrypt;
  qword_6B800 = v0;
  qword_6B808 = (__int64)ccmode_cbc_init;
  qword_6B810 = (__int64)ccmode_cbc_decrypt;
  qword_6B818 = (__int64)cccast_eay_ecb_decrypt_mode;
  return result;
}
// 693C8: using guessed type __int64 cccast_eay_ecb_decrypt_mode[2];
// 6B7F8: using guessed type __int64 cbc_cast_decrypt;
// 6B800: using guessed type __int64 qword_6B800;
// 6B808: using guessed type __int64 qword_6B808;
// 6B810: using guessed type __int64 qword_6B810;
// 6B818: using guessed type __int64 qword_6B818;

//----- (000000000001BD51) ----------------------------------------------------
__int64 *cccast_cfb_encrypt_mode()
{
  __int64 *result; // rax@1

  cfb_cast_encrypt = ((2 * cccast_eay_ecb_encrypt_mode[1] + 30) & 0xFFFFFFFFFFFFFFF0LL)
                   + ((cccast_eay_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &cfb_cast_encrypt;
  qword_6B828 = 1LL;
  qword_6B830 = (__int64)ccmode_cfb_init;
  qword_6B838 = (__int64)ccmode_cfb_encrypt;
  qword_6B840 = (__int64)cccast_eay_ecb_encrypt_mode;
  return result;
}
// 693E8: using guessed type __int64 cccast_eay_ecb_encrypt_mode[2];
// 6B820: using guessed type __int64 cfb_cast_encrypt;
// 6B828: using guessed type __int64 qword_6B828;
// 6B830: using guessed type __int64 qword_6B830;
// 6B838: using guessed type __int64 qword_6B838;
// 6B840: using guessed type __int64 qword_6B840;

//----- (000000000001BDB5) ----------------------------------------------------
__int64 *cccast_cfb_decrypt_mode()
{
  __int64 *result; // rax@1

  cfb_cast_decrypt = ((2 * cccast_eay_ecb_encrypt_mode[1] + 30) & 0xFFFFFFFFFFFFFFF0LL)
                   + ((cccast_eay_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &cfb_cast_decrypt;
  qword_6B850 = 1LL;
  qword_6B858 = (__int64)ccmode_cfb_init;
  qword_6B860 = (__int64)ccmode_cfb_decrypt;
  qword_6B868 = (__int64)cccast_eay_ecb_encrypt_mode;
  return result;
}
// 693E8: using guessed type __int64 cccast_eay_ecb_encrypt_mode[2];
// 6B848: using guessed type __int64 cfb_cast_decrypt;
// 6B850: using guessed type __int64 qword_6B850;
// 6B858: using guessed type __int64 qword_6B858;
// 6B860: using guessed type __int64 qword_6B860;
// 6B868: using guessed type __int64 qword_6B868;

//----- (000000000001BE19) ----------------------------------------------------
__int64 *cccast_cfb8_encrypt_mode()
{
  __int64 *result; // rax@1

  cfb8_cast_encrypt = ((2 * cccast_eay_ecb_encrypt_mode[1] + 14) & 0xFFFFFFFFFFFFFFF0LL)
                    + ((cccast_eay_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL)
                    + 8;
  result = &cfb8_cast_encrypt;
  qword_6B878 = 1LL;
  qword_6B880 = (__int64)ccmode_cfb8_init;
  qword_6B888 = (__int64)ccmode_cfb8_encrypt;
  qword_6B890 = (__int64)cccast_eay_ecb_encrypt_mode;
  return result;
}
// 693E8: using guessed type __int64 cccast_eay_ecb_encrypt_mode[2];
// 6B870: using guessed type __int64 cfb8_cast_encrypt;
// 6B878: using guessed type __int64 qword_6B878;
// 6B880: using guessed type __int64 qword_6B880;
// 6B888: using guessed type __int64 qword_6B888;
// 6B890: using guessed type __int64 qword_6B890;

//----- (000000000001BE7F) ----------------------------------------------------
__int64 *cccast_cfb8_decrypt_mode()
{
  __int64 *result; // rax@1

  cfb8_cast_decrypt = ((2 * cccast_eay_ecb_encrypt_mode[1] + 14) & 0xFFFFFFFFFFFFFFF0LL)
                    + ((cccast_eay_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL)
                    + 8;
  result = &cfb8_cast_decrypt;
  qword_6B8A0 = 1LL;
  qword_6B8A8 = (__int64)ccmode_cfb8_init;
  qword_6B8B0 = (__int64)ccmode_cfb8_decrypt;
  qword_6B8B8 = (__int64)cccast_eay_ecb_encrypt_mode;
  return result;
}
// 693E8: using guessed type __int64 cccast_eay_ecb_encrypt_mode[2];
// 6B898: using guessed type __int64 cfb8_cast_decrypt;
// 6B8A0: using guessed type __int64 qword_6B8A0;
// 6B8A8: using guessed type __int64 qword_6B8A8;
// 6B8B0: using guessed type __int64 qword_6B8B0;
// 6B8B8: using guessed type __int64 qword_6B8B8;

//----- (000000000001BEE5) ----------------------------------------------------
__int64 *cccast_ctr_crypt_mode()
{
  __int64 *result; // rax@1

  ctr_cast = ((2 * cccast_eay_ecb_encrypt_mode[1] + 30) & 0xFFFFFFFFFFFFFFF0LL)
           + ((cccast_eay_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &ctr_cast;
  qword_6B8C8 = 1LL;
  qword_6B8D0 = (__int64)ccmode_ctr_init;
  qword_6B8D8 = (__int64)ccmode_ctr_crypt;
  qword_6B8E0 = (__int64)cccast_eay_ecb_encrypt_mode;
  return result;
}
// 693E8: using guessed type __int64 cccast_eay_ecb_encrypt_mode[2];
// 6B8C0: using guessed type __int64 ctr_cast;
// 6B8C8: using guessed type __int64 qword_6B8C8;
// 6B8D0: using guessed type __int64 qword_6B8D0;
// 6B8D8: using guessed type __int64 qword_6B8D8;
// 6B8E0: using guessed type __int64 qword_6B8E0;

//----- (000000000001BF49) ----------------------------------------------------
__int64 *cccast_ofb_crypt_mode()
{
  __int64 *result; // rax@1

  ofb_cast = ((cccast_eay_ecb_encrypt_mode[1] + 23) & 0xFFFFFFFFFFFFFFF8LL)
           + ((cccast_eay_ecb_encrypt_mode[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  result = &ofb_cast;
  qword_6B8F0 = 1LL;
  qword_6B8F8 = (__int64)ccmode_ofb_init;
  qword_6B900 = (__int64)ccmode_ofb_crypt;
  qword_6B908 = (__int64)cccast_eay_ecb_encrypt_mode;
  return result;
}
// 693E8: using guessed type __int64 cccast_eay_ecb_encrypt_mode[2];
// 6B8E8: using guessed type __int64 ofb_cast;
// 6B8F0: using guessed type __int64 qword_6B8F0;
// 6B8F8: using guessed type __int64 qword_6B8F8;
// 6B900: using guessed type __int64 qword_6B900;
// 6B908: using guessed type __int64 qword_6B908;

//----- (000000000001BFAC) ----------------------------------------------------
__int64 __fastcall cced25519_make_key_pair(__int64 a1, __int64 a2, __int64 a3, const void *a4)
{
  const void *v4; // rbx@1
  __int64 v5; // r14@1
  __int64 v6; // r13@1
  __int64 v7; // rax@1
  char v9; // [sp+0h] [bp-110h]@1
  char v10; // [sp+A0h] [bp-70h]@1
  char v11; // [sp+BFh] [bp-51h]@1
  __int64 v12; // [sp+E0h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v6 = off_69010[0];
  v12 = *(_QWORD *)off_69010[0];
  LODWORD(v7) = (*(int (__fastcall **)(__int64, signed __int64, const void *))a2)(a2, 32LL, a4);
  ccdigest(v7, v4, (__int64)&v10, a1, 0x20uLL);
  v10 &= 0xF8u;
  v11 = v11 & 0x3F | 0x40;
  ge_scalarmult_base((__int64)&v9, (__int64)&v10);
  ge_p3_tobytes(v5, (__int64)&v9);
  return *(_QWORD *)v6;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001C04B) ----------------------------------------------------
__int64 __fastcall cccast_ecb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 v6; // r15@1
  unsigned int v8; // [sp+8h] [bp-38h]@2
  unsigned int v9; // [sp+Ch] [bp-34h]@2
  __int64 i; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v6 = a2;
  for ( i = *(_QWORD *)off_69010[0]; v6; --v6 )
  {
    v8 = _byteswap_ulong(*(_DWORD *)v5);
    v9 = _byteswap_ulong(*(_DWORD *)(v5 + 4));
    CC_CAST_decrypt((__int64)&v8, a1);
    *(_DWORD *)v4 = _byteswap_ulong(v8);
    *(_DWORD *)(v4 + 4) = _byteswap_ulong(v9);
    v5 += 8LL;
    v4 += 8LL;
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001C0E4) ----------------------------------------------------
__int64 __fastcall ccrsa_get_fullkey_components(__int64 *a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9)
{
  __int64 v9; // r13@1
  signed __int64 v10; // r14@1
  __int64 v11; // r15@1
  __int64 v12; // r12@1
  signed __int64 v13; // rax@1
  signed int v14; // edx@1
  signed __int64 v15; // r13@2
  signed __int64 v16; // rax@2
  __int64 v17; // r14@2
  __int64 v18; // r15@2
  signed __int64 v19; // rax@2
  signed __int64 v20; // rax@3
  __int64 v22; // rbx@6
  signed __int64 v23; // rax@6
  __int64 v24; // [sp+8h] [bp-58h]@2
  __int64 v25; // [sp+18h] [bp-48h]@1
  __int64 v26; // [sp+20h] [bp-40h]@1
  __int64 v27; // [sp+28h] [bp-38h]@1
  __int64 v28; // [sp+30h] [bp-30h]@1

  v28 = a6;
  v25 = a5;
  v9 = a4;
  v26 = a3;
  v27 = *a1;
  v10 = 4 * *a1;
  v11 = a1[v10 + 3];
  v12 = (__int64)&a1[v10 + 5];
  v13 = ccn_write_uint_size(a1[v10 + 3], v12);
  v14 = -1;
  if ( (unsigned __int64)v13 <= *(_QWORD *)a7 )
  {
    v24 = v9;
    v15 = (signed __int64)&a1[v10 + 3];
    *(_QWORD *)a7 = v13;
    ccn_write_uint(v11, v12, v13, v28);
    v16 = 16LL * *(_QWORD *)v15;
    v17 = *(_QWORD *)(v16 + v15 + 24);
    v18 = v16 + v15 + 40;
    v19 = ccn_write_uint_size(*(_QWORD *)(v16 + v15 + 24), v18);
    if ( (unsigned __int64)v19 <= *(_QWORD *)a9
      && (*(_QWORD *)a9 = v19,
          ccn_write_uint(v17, v18, v19, a8),
          v20 = ccn_write_uint_size(v27, (__int64)(a1 + 2)),
          (unsigned __int64)v20 <= *(_QWORD *)v26) )
    {
      *(_QWORD *)v26 = v20;
      ccn_write_uint(v27, (__int64)(a1 + 2), v20, a2);
      v22 = (__int64)&a1[3 * *a1 + 3];
      v23 = ccn_write_uint_size(v27, v22);
      v14 = -1;
      if ( (unsigned __int64)v23 <= *(_QWORD *)v25 )
      {
        *(_QWORD *)v25 = v23;
        ccn_write_uint(v27, v22, v23, v24);
        v14 = 0;
      }
    }
    else
    {
      v14 = -1;
    }
  }
  return (unsigned int)v14;
}

//----- (000000000001C21A) ----------------------------------------------------
__int64 __fastcall cccast_ecb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 v6; // r15@1
  unsigned int v8; // [sp+8h] [bp-38h]@2
  unsigned int v9; // [sp+Ch] [bp-34h]@2
  __int64 i; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v6 = a2;
  for ( i = *(_QWORD *)off_69010[0]; v6; --v6 )
  {
    v8 = _byteswap_ulong(*(_DWORD *)v5);
    v9 = _byteswap_ulong(*(_DWORD *)(v5 + 4));
    CC_CAST_encrypt((__int64)&v8, a1);
    *(_DWORD *)v4 = _byteswap_ulong(v8);
    *(_DWORD *)(v4 + 4) = _byteswap_ulong(v9);
    v5 += 8LL;
    v4 += 8LL;
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001C2B3) ----------------------------------------------------
__int64 __fastcall cccast_setup(__int64 a1, __int64 a2, unsigned __int64 a3, __int64 a4)
{
  return CC_CAST_set_key(a2, a3, a4);
}

//----- (000000000001C2C6) ----------------------------------------------------
__int64 __usercall ccder_decode_bitstring@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, unsigned __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>)
{
  signed __int64 v5; // rax@1
  __int64 v6; // rcx@1
  __int64 v7; // rcx@2
  __int64 v9; // [sp+8h] [bp-18h]@1

  v5 = ccder_decode_tl(a1, a2, a3, 3LL, (__int64)&v9);
  v6 = 0LL;
  if ( v5 )
  {
    v7 = v9;
    *(_QWORD *)a5 = 8 * v9 - 8 - *(_BYTE *)v5;
    *(_QWORD *)a4 = v5 + 1;
    v6 = v7 + v5;
  }
  return v6;
}

//----- (000000000001C31A) ----------------------------------------------------
signed __int64 __usercall ccder_decode_constructed_tl@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, unsigned __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>)
{
  signed __int64 result; // rax@1
  __int64 v6; // [sp+0h] [bp-10h]@1

  v6 = a1;
  result = ccder_decode_tl(a1, a2, a3, a4, (__int64)&v6);
  *(_QWORD *)a5 = result + v6;
  return result;
}

//----- (000000000001C33D) ----------------------------------------------------
signed __int64 __fastcall ccder_decode_len(__int64 a1, unsigned __int64 a2, unsigned __int64 a3)
{
  signed __int64 result; // rax@1
  signed __int64 v4; // r8@3
  unsigned __int64 v5; // rcx@3
  unsigned __int64 v6; // rax@7
  __int64 v7; // rcx@7
  signed __int64 v8; // rsi@7

  result = 0LL;
  if ( a2 && a2 < a3 )
  {
    v4 = a2 + 1;
    v5 = *(_BYTE *)a2;
    if ( (v5 & 0x80u) == 0LL )
      goto LABEL_15;
    result = 0LL;
    if ( (_DWORD)v5 == 131 )
    {
      result = 0LL;
      if ( (signed __int64)(a3 - v4) < 3 )
        return result;
      v6 = ((unsigned __int64)*(_BYTE *)(a2 + 1) << 16) | ((unsigned __int64)*(_BYTE *)(a2 + 2) << 8);
      v7 = *(_BYTE *)(a2 + 3);
      v8 = a2 + 4;
      goto LABEL_10;
    }
    LODWORD(v5) = (unsigned __int8)v5;
    if ( (unsigned __int8)v5 == 130 )
    {
      result = 0LL;
      if ( (signed __int64)(a3 - v4) < 2 )
        return result;
      v6 = (unsigned __int64)*(_BYTE *)(a2 + 1) << 8;
      v7 = *(_BYTE *)(a2 + 2);
      v8 = a2 + 3;
LABEL_10:
      v5 = v6 | v7;
LABEL_14:
      v4 = v8;
LABEL_15:
      result = 0LL;
      if ( a3 - v4 >= v5 )
      {
        *(_QWORD *)a1 = v5;
        result = v4;
      }
      return result;
    }
    if ( (_DWORD)v5 == 129 )
    {
      result = 0LL;
      if ( (signed __int64)(a3 - v4) > 0 )
      {
        v5 = *(_BYTE *)(a2 + 1);
        v8 = a2 + 2;
        goto LABEL_14;
      }
    }
  }
  return result;
}

//----- (000000000001C3F9) ----------------------------------------------------
__int64 __fastcall ccder_decode_oid(__int64 a1, unsigned __int64 a2, unsigned __int64 a3)
{
  signed __int64 v3; // rax@1
  __int64 v4; // rcx@1
  __int64 v6; // [sp+8h] [bp-18h]@1

  v3 = ccder_decode_tl(a3, a2, a3, 6LL, (__int64)&v6);
  v4 = 0LL;
  if ( v3 )
  {
    *(_QWORD *)a1 = a2;
    v4 = v6 + v3;
  }
  return v4;
}

//----- (000000000001C43E) ----------------------------------------------------
__int64 __fastcall ccder_decode_seqii(unsigned __int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4, unsigned __int64 a5)
{
  __int64 v5; // r14@1
  signed __int64 v6; // rax@1
  __int64 v7; // rax@1
  __int64 v8; // rax@1
  __int64 v9; // rcx@1
  unsigned __int64 v11; // [sp+0h] [bp-20h]@1

  v5 = a3;
  v11 = a5;
  v6 = ccder_decode_sequence_tl((__int64)&v11, a4, a5);
  v7 = ccder_decode_uint(v6, v6, v11, a1, a2);
  v8 = ccder_decode_uint(v7, v7, v11, a1, v5);
  v9 = 0LL;
  if ( v8 == v11 )
    v9 = v8;
  return v9;
}

//----- (000000000001C4A0) ----------------------------------------------------
signed __int64 __fastcall ccder_decode_sequence_tl(__int64 a1, unsigned __int64 a2, unsigned __int64 a3)
{
  return ccder_decode_constructed_tl(a3, a2, a3, 2305843009213693968LL, a1);
}

//----- (000000000001C4C6) ----------------------------------------------------
__int64 __fastcall ccrsa_get_pubkey_components(__int64 *a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  __int64 v6; // r15@1
  signed __int64 v7; // rax@1
  signed int v8; // er12@1
  __int64 v9; // r13@2
  __int64 v10; // rbx@2
  signed __int64 v11; // rax@2
  __int64 v13; // [sp+0h] [bp-40h]@1
  __int64 v14; // [sp+10h] [bp-30h]@1

  v14 = a5;
  v13 = a4;
  v5 = a3;
  v6 = *a1;
  v7 = ccn_write_uint_size(*a1, (__int64)(a1 + 2));
  v8 = -1;
  if ( (unsigned __int64)v7 <= *(_QWORD *)v5 )
  {
    *(_QWORD *)v5 = v7;
    ccn_write_uint(v6, (__int64)(a1 + 2), v7, a2);
    v9 = *a1;
    v10 = (__int64)&a1[2 * *a1 + 3];
    v11 = ccn_write_uint_size(*a1, v10);
    if ( (unsigned __int64)v11 <= *(_QWORD *)v14 )
    {
      *(_QWORD *)v14 = v11;
      ccn_write_uint(v9, v10, v11, v13);
      v8 = 0;
    }
  }
  return (unsigned int)v8;
}

//----- (000000000001C568) ----------------------------------------------------
void __fastcall ccecies_decrypt_gcm_setup(__int64 a1, __int64 a2, __int64 a3, int a4, int a5, int a6)
{
  *(_QWORD *)a1 = a2;
  *(_QWORD *)(a1 + 16) = a3;
  *(_DWORD *)(a1 + 32) = a6;
  *(_DWORD *)(a1 + 24) = a4;
  *(_DWORD *)(a1 + 28) = a5;
}

//----- (000000000001C580) ----------------------------------------------------
signed __int64 __fastcall ccder_decode_tag(__int64 a1, unsigned __int64 a2, unsigned __int64 a3)
{
  signed __int64 result; // rax@1
  __int64 v4; // r8@3
  signed __int64 v5; // rsi@3
  unsigned __int64 v6; // r10@3
  unsigned __int64 v7; // r9@7
  char v8; // cl@7

  result = 0LL;
  if ( a2 && a2 < a3 )
  {
    v4 = *(_BYTE *)a2;
    v5 = a2 + 1;
    v6 = 0LL;
    if ( (v4 & 0x1F) == 31 )
    {
      result = 0LL;
      while ( a3 != v5 && !(v6 >> 57) )
      {
        v7 = v6 << 7;
        v8 = *(_BYTE *)v5++;
        v6 = (v6 << 7) | v8 & 0x7F;
        if ( !(v8 & 0x80) )
        {
          result = 0LL;
          if ( v7 >> 61 )
            return result;
          goto LABEL_11;
        }
      }
    }
    else
    {
      v6 = v4 & 0x1F;
LABEL_11:
      *(_QWORD *)a1 = v6 | ((v4 & 0xFFFFFFFFFFFFFFE0LL) << 56);
      result = v5;
    }
  }
  return result;
}

//----- (000000000001C5F3) ----------------------------------------------------
signed __int64 __usercall ccder_decode_tl@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, unsigned __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>)
{
  unsigned __int64 v5; // rbx@1
  signed __int64 v6; // rcx@1
  signed __int64 result; // rax@1
  __int64 v8; // [sp+0h] [bp-20h]@1

  v8 = a1;
  v5 = a3;
  v6 = ccder_decode_tag((__int64)&v8, a2, a3);
  result = 0LL;
  if ( v6 )
  {
    if ( v8 == a4 )
      result = ccder_decode_len(a5, v6, v5);
  }
  return result;
}

//----- (000000000001C63E) ----------------------------------------------------
__int64 __usercall ccder_decode_uint@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, unsigned __int64 a3@<rcx>, unsigned __int64 a4@<rdi>, __int64 a5@<rsi>)
{
  signed __int64 v5; // rbx@1
  __int64 result; // rax@1
  int v7; // ecx@4
  unsigned __int64 v8; // [sp+0h] [bp-20h]@1

  v8 = a1;
  v5 = ccder_decode_tl(a1, a2, a3, 2LL, (__int64)&v8);
  result = 0LL;
  if ( v8 )
  {
    if ( v5 )
    {
      result = 0LL;
      if ( *(_BYTE *)v5 >= 0 )
      {
        v7 = ccn_read_uint(a4, a5, v8, v5);
        result = 0LL;
        if ( v7 >= 0 )
          result = v8 + v5;
      }
    }
  }
  return result;
}

//----- (000000000001C69E) ----------------------------------------------------
unsigned __int64 __fastcall ccder_decode_uint64(__int64 a1, unsigned __int64 a2, unsigned __int64 a3)
{
  signed __int64 v3; // rbx@1
  unsigned __int64 result; // rax@1
  unsigned __int64 v5; // rdx@2
  int v6; // ecx@5
  unsigned __int64 v7; // [sp+8h] [bp-18h]@1

  v3 = ccder_decode_tl(a3, a2, a3, 2LL, (__int64)&v7);
  result = 0LL;
  if ( v3 )
  {
    v5 = v7;
    if ( v7 )
    {
      result = 0LL;
      if ( *(_BYTE *)v3 >= 0 )
      {
        if ( a1 )
        {
          v6 = ccn_read_uint(1uLL, a1, v7, v3);
          result = 0LL;
          if ( v6 < 0 )
            return result;
          v5 = v7;
        }
        result = v5 + v3;
      }
    }
  }
  return result;
}

//----- (000000000001C710) ----------------------------------------------------
__int64 __fastcall ccder_encode_body(size_t a1, const void *a2, __int64 a3, size_t a4)
{
  __int64 result; // rax@1
  __int64 v5; // rbx@2

  result = 0LL;
  if ( a1 + a3 <= a4 )
  {
    v5 = a4 - a1;
    memmove((void *)(a4 - a1), a2, a1);
    result = v5;
  }
  return result;
}

//----- (000000000001C73E) ----------------------------------------------------
unsigned __int64 __fastcall ccder_encode_body_nocopy(__int64 a1, __int64 a2, unsigned __int64 a3)
{
  unsigned __int64 result; // rax@1

  result = 0LL;
  if ( a1 + a2 <= a3 )
    result = a3 - a1;
  return result;
}

//----- (000000000001C754) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_constructed_tl(unsigned __int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4)
{
  return ccder_encode_tl(a1, a2 - a4, a3, a4);
}

//----- (000000000001C761) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_implicit_integer(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5)
{
  unsigned __int64 v5; // r14@1
  __int64 v6; // r15@1
  __int64 v7; // r12@1
  __int64 v8; // rax@1
  unsigned __int64 v9; // rbx@1
  __int64 v10; // rax@1
  unsigned __int64 v11; // r14@1
  signed __int64 result; // rax@2

  v5 = a5;
  v6 = a4;
  v7 = a3;
  v8 = ccn_write_int_size(a2, a3);
  v9 = v8;
  v10 = ccder_encode_body_nocopy(v8, v6, v5);
  v11 = v10;
  if ( v10 )
  {
    ccn_write_int(a2, v7, v9, v10);
    result = ccder_encode_tl(a1, v9, v6, v11);
  }
  else
  {
    result = 0LL;
  }
  return result;
}

//----- (000000000001C7E5) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_implicit_octet_string(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5)
{
  unsigned __int64 v5; // r14@1
  __int64 v6; // r15@1
  __int64 v7; // r12@1
  __int64 v8; // rax@1
  unsigned __int64 v9; // rbx@1
  __int64 v10; // rax@1
  unsigned __int64 v11; // r14@1
  signed __int64 result; // rax@2

  v5 = a5;
  v6 = a4;
  v7 = a3;
  v8 = ccn_write_uint_size(a2, a3);
  v9 = v8;
  v10 = ccder_encode_body_nocopy(v8, v6, v5);
  v11 = v10;
  if ( v10 )
  {
    ccn_write_uint(a2, v7, v9, v10);
    result = ccder_encode_tl(a1, v9, v6, v11);
  }
  else
  {
    result = 0LL;
  }
  return result;
}

//----- (000000000001C869) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_implicit_raw_octet_string(unsigned __int64 a1, size_t a2, const void *a3, __int64 a4, size_t a5)
{
  __int64 v5; // r14@1
  __int64 v6; // rax@1

  v5 = a4;
  v6 = ccder_encode_body(a2, a3, a4, a5);
  return ccder_encode_tl(a1, a2, v5, v6);
}

//----- (000000000001C8A8) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_implicit_uint64(unsigned __int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4)
{
  __int64 v4; // rbx@1
  signed __int64 result; // rax@1
  __int64 v6; // rcx@1
  __int64 v7; // [sp+8h] [bp-18h]@1
  __int64 v8; // [sp+10h] [bp-10h]@1

  v4 = off_69010[0];
  v8 = *(_QWORD *)off_69010[0];
  v7 = a2;
  result = ccder_encode_implicit_integer(a1, 1LL, (__int64)&v7, a3, a4);
  v6 = *(_QWORD *)v4;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001C8EF) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_integer(__int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4)
{
  return ccder_encode_implicit_integer(2uLL, a1, a2, a3, a4);
}

//----- (000000000001C90D) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_len(unsigned __int64 a1, __int64 a2, unsigned __int64 a3)
{
  signed __int64 result; // rax@2
  signed __int64 v4; // rdx@3

  if ( a1 > 0x7F )
  {
    if ( a1 > 0xFF )
    {
      if ( a1 > 0xFFFF )
      {
        if ( a1 > 0xFFFFFF )
        {
          result = 0LL;
          if ( a2 + 5 > a3 )
            return result;
          *(_BYTE *)(a3 - 1) = a1;
          *(_BYTE *)(a3 - 2) = BYTE1(a1);
          *(_BYTE *)(a3 - 3) = a1 >> 16;
          *(_BYTE *)(a3 - 4) = a1 >> 24;
          *(_BYTE *)(a3 - 5) = -124;
          v4 = a3 - 5;
        }
        else
        {
          result = 0LL;
          if ( a2 + 4 > a3 )
            return result;
          *(_BYTE *)(a3 - 1) = a1;
          *(_BYTE *)(a3 - 2) = BYTE1(a1);
          *(_BYTE *)(a3 - 3) = a1 >> 16;
          *(_BYTE *)(a3 - 4) = -125;
          v4 = a3 - 4;
        }
      }
      else
      {
        result = 0LL;
        if ( a2 + 3 > a3 )
          return result;
        *(_BYTE *)(a3 - 1) = a1;
        *(_BYTE *)(a3 - 2) = BYTE1(a1);
        *(_BYTE *)(a3 - 3) = -126;
        v4 = a3 - 3;
      }
    }
    else
    {
      result = 0LL;
      if ( a2 + 2 > a3 )
        return result;
      *(_BYTE *)(a3 - 1) = a1;
      *(_BYTE *)(a3 - 2) = -127;
      v4 = a3 - 2;
    }
  }
  else
  {
    result = 0LL;
    if ( a2 + 1 > a3 )
      return result;
    *(_BYTE *)(a3 - 1) = a1;
    v4 = a3 - 1;
  }
  return v4;
}

//----- (000000000001C9D6) ----------------------------------------------------
__int64 __fastcall ccnistkdf_fb_hmac_fixed(__int64 a1, int a2, unsigned __int64 a3, void *a4, unsigned __int64 a5, const void *a6, unsigned __int8 a7, __int64 a8, unsigned __int64 a9, void *a10)
{
  unsigned __int64 v10; // r8@1
  __int64 v11; // rsi@1
  unsigned __int64 v12; // rtt@1
  unsigned __int64 v13; // rax@1
  size_t v14; // rdx@1
  bool v15; // zf@1
  __int64 result; // rax@1
  __int64 v17; // rax@6
  __int64 v18; // r15@6
  unsigned __int64 v19; // rax@6
  char *v20; // r13@6
  __int64 v21; // rbx@7
  unsigned __int8 v22; // al@7
  unsigned __int64 v23; // r14@7
  __int64 v24; // rcx@7
  unsigned __int64 v25; // rbx@8
  unsigned __int64 v26; // r13@8
  __int64 v27; // rax@10
  signed __int64 v28; // rax@10
  signed __int64 v29; // rax@10
  void *v30; // rbx@13
  __int64 v31; // rcx@14
  __int64 v32; // [sp+0h] [bp-A0h]@6
  size_t v33; // [sp+10h] [bp-90h]@6
  void *v34; // [sp+18h] [bp-88h]@6
  void *v35; // [sp+20h] [bp-80h]@6
  void *v36; // [sp+28h] [bp-78h]@6
  unsigned __int64 v37; // [sp+30h] [bp-70h]@1
  int v38; // [sp+3Ch] [bp-64h]@1
  unsigned __int64 v39; // [sp+40h] [bp-60h]@1
  const void *v40; // [sp+48h] [bp-58h]@1
  unsigned __int64 v41; // [sp+50h] [bp-50h]@1
  const void *v42; // [sp+58h] [bp-48h]@8
  __int64 v43; // [sp+60h] [bp-40h]@8
  unsigned __int32 v44; // [sp+6Ch] [bp-34h]@11
  __int64 v45; // [sp+70h] [bp-30h]@1
  __int64 v46; // [sp+78h] [bp-28h]@6

  v40 = a6;
  v39 = a5;
  v10 = a3;
  v38 = a2;
  v11 = off_69010[0];
  v45 = *(_QWORD *)off_69010[0];
  v37 = *(_QWORD *)a1;
  v12 = a9;
  v13 = (v37 * (v12 / v37) < a9) + v12 / v37;
  v41 = v13;
  v14 = v37 * v13;
  v15 = v13 >> 32 == 0;
  result = 0xFFFFFFFFLL;
  if ( v15 && v10 && a4 && a9 && a10 )
  {
    v34 = (char *)&v32 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0LL);
    v33 = v14;
    v17 = *(_QWORD *)(a1 + 8);
    v18 = (__int64)((char *)&v46
                  - ((((*(_QWORD *)(a1 + 16) + 2 * v17 + 19) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL));
    v19 = (((v17 + 7) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL;
    v20 = (char *)&v46 - v19;
    v36 = (char *)&v46 - v19;
    cchmac_init(a1, v18, v10, a4);
    v35 = (void *)(v18 + 8);
    memcpy(v20, (const void *)(v18 + 8), *(_QWORD *)(a1 + 8));
    if ( v41 )
    {
      v21 = a8;
      v22 = a7;
      v23 = 1LL;
      v24 = (__int64)v34;
      do
      {
        v42 = (const void *)v21;
        v43 = v24;
        v25 = v22;
        v26 = 0LL;
        if ( v38 )
          v26 = v23;
        memcpy(v35, v36, *(_QWORD *)(a1 + 8));
        v27 = *(_QWORD *)(a1 + 16);
        *(_QWORD *)v18 = 8 * v27;
        v28 = *(_QWORD *)(a1 + 8) + v27;
        *(_DWORD *)(v18 + v28 + 8) = 0;
        LODWORD(v29) = ccdigest_update(v28, v25, v42, a1, v18);
        if ( v26 )
        {
          v44 = _byteswap_ulong(v26);
          LODWORD(v29) = ccdigest_update(v29, 4uLL, &v44, a1, v18);
        }
        ccdigest_update(v29, v39, v40, a1, v18);
        v21 = v43;
        cchmac_final(a1, v18, v43);
        ++v23;
        v22 = v37;
        v24 = v21 + v37;
      }
      while ( v23 <= v41 );
    }
    v30 = v34;
    memcpy(a10, v34, a9);
    cc_clear(v33, v30);
    cc_clear(*(_QWORD *)(a1 + 16) + 2LL * *(_QWORD *)(a1 + 8) + 12, (void *)v18);
    cc_clear(*(_QWORD *)(a1 + 8), v36);
    result = 0LL;
    v11 = off_69010[0];
  }
  v31 = *(_QWORD *)v11;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001CC3A) ----------------------------------------------------
__int64 __fastcall ccnistkdf_fb_hmac(__int64 a1, int a2, __int64 a3, void *a4, size_t a5, const void *a6, size_t a7, const void *a8, __int64 a9, unsigned __int64 a10, unsigned __int64 a11, void *a12)
{
  size_t v12; // r14@1
  unsigned __int64 v13; // rax@1
  char *v14; // rbx@1
  signed __int64 v15; // r13@4
  unsigned __int64 v16; // r12@7
  unsigned int v17; // er14@7
  __int64 result; // rax@7
  __int64 v19; // [sp+20h] [bp-50h]@1
  char v20[12]; // [sp+2Ch] [bp-44h]@1
  void *v21; // [sp+38h] [bp-38h]@1
  __int64 v22; // [sp+40h] [bp-30h]@1

  v12 = a5;
  v21 = a4;
  *(_QWORD *)&v20[4] = a3;
  *(_DWORD *)v20 = a2;
  v19 = a1;
  v22 = *(_QWORD *)off_69010[0];
  v13 = (a5 + a7 + 20) & 0xFFFFFFFFFFFFFFF0LL;
  v14 = (char *)&v19 - v13;
  if ( a5 && a6 )
    memcpy((char *)&v19 - v13, a6, a5);
  v14[v12] = 0;
  v15 = v12 + 1;
  if ( a7 && a8 )
    memcpy(&v14[v15], a8, a7);
  v16 = v12 + a7 + 5;
  *(_DWORD *)(&v14[a7] + v15) = _byteswap_ulong(8 * a11);
  v17 = ccnistkdf_fb_hmac_fixed(v19, SHIDWORD(a9), a10, (void *)a11, v16, v14, a9, a10, a11, a12);
  cc_clear(v16, v14);
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v22 )
    result = v17;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001CD50) ----------------------------------------------------
__int64 __fastcall ccrsa_eme_pkcs1v15_decode(unsigned __int64 *a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4)
{
  signed __int64 v4; // r9@1
  unsigned __int64 v5; // r8@1
  signed __int64 v6; // r10@1
  unsigned __int64 v7; // r14@1
  unsigned __int64 v8; // rbx@2
  bool v13; // cf@3
  unsigned __int64 v14; // r8@4
  unsigned __int64 v17; // r9@6
  unsigned __int64 v18; // rbx@6
  __int64 v19; // r11@6
  __int64 v20; // r14@6
  signed __int64 v21; // r15@6
  unsigned __int64 v22; // rax@6
  signed __int64 v23; // r12@8
  signed __int64 v24; // r12@10
  signed __int64 v25; // r13@10
  int v26; // er15@10
  unsigned __int8 v27; // al@11
  signed __int64 v28; // rsi@14
  unsigned __int64 v33; // rdx@16
  unsigned __int64 v34; // rdx@16
  unsigned __int64 v35; // rdx@16
  unsigned __int64 v36; // rax@16

  v4 = a3 + 7;
  v5 = (a3 + 7) >> 3;
  v6 = v5 - 1;
  v7 = a4;
  if ( (signed __int64)(v5 - 1) > 0 )
  {
    v8 = (v4 & 0xFFFFFFFFFFFFFFF8LL) + a4 - 16;
    v7 = a4;
    do
    {
      _R11 = *(_QWORD *)v7;
      __asm { bswap   r11 }
      _RAX = *(_QWORD *)(v8 + 8);
      __asm { bswap   rax }
      *(_QWORD *)v7 = _RAX;
      *(_QWORD *)(v8 + 8) = _R11;
      v7 += 8LL;
      v13 = v7 < v8;
      v8 -= 8LL;
    }
    while ( v13 );
  }
  v14 = v5 & 1;
  if ( v14 )
  {
    _RAX = *(_QWORD *)v7;
    __asm { bswap   rax }
    *(_QWORD *)v7 = _RAX;
  }
  v17 = v4 & 0xFFFFFFFFFFFFFFF8LL;
  v18 = *a1;
  v19 = *(_BYTE *)(a4 + v17 - a3);
  v20 = *(_BYTE *)(a4 + v17 - a3 + 1);
  *a1 = 0LL;
  v21 = 1LL;
  v22 = 0LL;
  if ( a3 >= 3 && v18 >= 3 )
  {
    v23 = 2 - a3;
    if ( 2 - v18 > 2 - a3 )
      v23 = 2 - v18;
    v24 = -v23;
    v25 = a4 + v17 + 2 - a3;
    LOBYTE(v26) = 0;
    do
    {
      v27 = v26 | ((unsigned __int16)(*(_BYTE *)(v25 - 1) + 255) >> 8) ^ 1;
      *(_BYTE *)a2 = *(_BYTE *)v25;
      v26 = v27;
      a2 += v27;
      v22 = *a1 + v27;
      *a1 = v22;
      ++v25;
      --v24;
    }
    while ( v24 );
    v21 = v26 ^ 1u;
  }
  if ( v6 > 0 )
  {
    v28 = v17 + a4 - 16;
    do
    {
      _RDI = *(_QWORD *)a4;
      __asm { bswap   rdi }
      _RBX = *(_QWORD *)(v28 + 8);
      __asm { bswap   rbx }
      *(_QWORD *)a4 = _RBX;
      *(_QWORD *)(v28 + 8) = _RDI;
      a4 += 8LL;
      v13 = a4 < v28;
      v28 -= 8LL;
    }
    while ( v13 );
  }
  v33 = ((-9223372036854775798LL - (a3 - v22)) | (a3 - v22 - 3)) & 0x8000000000000000LL;
  v34 = v33 | v21 | v19 | v20 ^ 2 | (v33 >> 32) | ((v33 | v21 | v19 | v20 ^ 2 | (v33 >> 32)) >> 16) | ((v33 | v21 | v19 | v20 ^ 2 | (v33 >> 32) | ((v33 | v21 | v19 | v20 ^ 2 | (v33 >> 32)) >> 16)) >> 8);
  v35 = v34 | (v34 >> 4) | ((v34 | (v34 >> 4)) >> 2);
  v36 = v35 | (v35 >> 1);
  if ( v14 )
  {
    _RDX = *(_QWORD *)a4;
    __asm { bswap   rdx }
    *(_QWORD *)a4 = _RDX;
  }
  return (unsigned int)-(v36 & 1);
}

//----- (000000000001CEEF) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_octet_string(__int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4)
{
  return ccder_encode_implicit_octet_string(4uLL, a1, a2, a3, a4);
}

//----- (000000000001CF0D) ----------------------------------------------------
int __fastcall ccec_generate_key_internal_fips(__int64 *a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r15@1
  __int64 v4; // rbx@1
  __int64 v5; // r13@1
  __int64 v6; // r12@1
  unsigned __int64 v7; // r15@1
  __int64 v8; // rax@2
  int result; // eax@2
  int v10; // eax@3
  int v11; // ecx@6
  int v12; // ecx@7
  __int64 v13; // rcx@8
  signed __int64 v14; // rax@8
  __int64 v15; // rcx@9
  __int64 v16; // [sp+0h] [bp-70h]@1
  __int64 v17; // [sp+8h] [bp-68h]@1
  __int64 v18; // [sp+10h] [bp-60h]@1
  const void *v19; // [sp+18h] [bp-58h]@1
  signed __int64 *v20; // [sp+20h] [bp-50h]@1
  __int64 v21; // [sp+28h] [bp-48h]@1
  __int64 *v22; // [sp+30h] [bp-40h]@1
  __int64 v23; // [sp+38h] [bp-38h]@1
  __int64 v24; // [sp+40h] [bp-30h]@1

  v3 = a3;
  v18 = a3;
  v23 = a2;
  v20 = a1;
  v24 = *(_QWORD *)off_69010[0];
  v4 = *a1;
  v17 = 8 * *a1;
  v19 = (char *)&v16 - ((24 * v4 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v22 = a1 + 2;
  ccec_projectify(a1, (char *)&v16 - ((24 * v4 + 15) & 0xFFFFFFFFFFFFFFF0LL), &a1[2 * v4 + 2]);
  *(_QWORD *)v3 = a1;
  v21 = v3 + 16;
  v5 = v3 + 24 * *a1 + 16;
  v6 = (__int64)&a1[4 * v4 + 4];
  ccn_sub1(v4, v3 + 16, v6, 2LL);
  v7 = 0LL;
  while ( 1 )
  {
    v8 = ccn_bitlen(v22[4 * v4], v6);
    result = ccn_random_bits(v8, v5, v23);
    if ( result )
      break;
    v10 = ccn_cmp(v4, v5, v21);
    ++v7;
    if ( v7 > 9 || v10 <= 0 )
    {
      result = -10;
      if ( v7 <= 9 )
      {
        ccn_add1(v4, v5, v5, 1LL);
        v11 = ccec_mult(v20, v21, v5, v19);
        result = -11;
        if ( !v11 )
        {
          v12 = ccec_affinify(v20, v21, (unsigned __int64 *)v21);
          result = -12;
          if ( !v12 )
          {
            v13 = v18;
            v14 = 16LL * **(_QWORD **)v18;
            *(_QWORD *)(v14 + v18 + 16) = 1LL;
            bzero((void *)(v14 + v13 + 24), v17 - 8);
            result = 0;
          }
        }
      }
      break;
    }
  }
  v15 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001D0AE) ----------------------------------------------------
__int64 __fastcall ccder_encode_oid(__int64 a1, __int64 a2, size_t a3)
{
  return ccder_encode_body(*(_BYTE *)(a1 + 1) + 2LL, (const void *)a1, a2, a3);
}

//----- (000000000001D0D2) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_raw_octet_string(size_t a1, const void *a2, __int64 a3, size_t a4)
{
  return ccder_encode_implicit_raw_octet_string(4uLL, a1, a2, a3, a4);
}

//----- (000000000001D0F0) ----------------------------------------------------
signed __int64 __fastcall ccec_make_priv(signed __int64 a1, unsigned __int64 a2, unsigned __int64 a3, unsigned __int64 a4, unsigned __int64 a5, unsigned __int64 a6, unsigned __int64 a7, __int64 a8)
{
  unsigned __int64 v8; // r12@1
  unsigned __int64 v9; // rbx@1
  unsigned __int64 *v10; // rax@1
  unsigned __int64 v11; // r13@1
  signed __int64 result; // rax@1
  signed __int64 v13; // rax@4
  unsigned __int64 v14; // [sp+8h] [bp-38h]@1
  unsigned __int64 v15; // [sp+10h] [bp-30h]@1

  v14 = a6;
  v15 = a5;
  v8 = a4;
  v9 = a3;
  v10 = (unsigned __int64 *)ccec_get_cp(a1);
  *(_QWORD *)a8 = v10;
  v11 = *v10;
  result = ccn_read_uint(*v10, a8 + 16, a2, v9);
  if ( !(_DWORD)result )
  {
    result = ccn_read_uint(v11, a8 + 8LL * **(_QWORD **)a8 + 16, v8, v15);
    if ( !(_DWORD)result )
    {
      result = ccn_read_uint(v11, a8 + 24LL * **(_QWORD **)a8 + 16, v14, a7);
      if ( !(_DWORD)result )
      {
        v13 = 16LL * **(_QWORD **)a8;
        *(_QWORD *)(a8 + v13 + 16) = 1LL;
        bzero((void *)(a8 + v13 + 24), 8 * v11 - 8);
        result = 0LL;
      }
    }
  }
  return result;
}

//----- (000000000001D1AB) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_tag(unsigned __int64 a1, __int64 a2, unsigned __int64 a3)
{
  unsigned __int64 v3; // rcx@1
  signed __int64 v4; // rax@1
  signed __int64 result; // rax@2
  signed __int64 v6; // rdx@6

  v3 = a1 >> 56;
  v4 = a1 & 0x1FFFFFFFFFFFFFFFLL;
  if ( (a1 & 0x1FFFFFFFFFFFFFFFLL) > 0x1E )
  {
    if ( (unsigned __int64)v4 > 0x7F )
    {
      if ( (unsigned __int64)v4 > 0x3FFF )
      {
        if ( (unsigned __int64)v4 > 0x1FFFFF )
        {
          if ( (unsigned __int64)v4 > 0xFFFFFFF )
          {
            result = 0LL;
            if ( a2 + 6 > a3 )
              return result;
            *(_BYTE *)(a3 - 1) = a1 & 0x7F;
            *(_BYTE *)(a3 - 2) = (a1 >> 7) | 0x80;
            *(_BYTE *)(a3 - 3) = (a1 >> 14) | 0x80;
            *(_BYTE *)(a3 - 4) = (a1 >> 21) | 0x80;
            *(_BYTE *)(a3 - 5) = (a1 >> 28) | 0x80;
            v6 = a3 - 5;
          }
          else
          {
            result = 0LL;
            if ( a2 + 5 > a3 )
              return result;
            *(_BYTE *)(a3 - 1) = a1 & 0x7F;
            *(_BYTE *)(a3 - 2) = (a1 >> 7) | 0x80;
            *(_BYTE *)(a3 - 3) = (a1 >> 14) | 0x80;
            *(_BYTE *)(a3 - 4) = (a1 >> 21) | 0x80;
            v6 = a3 - 4;
          }
        }
        else
        {
          result = 0LL;
          if ( a2 + 4 > a3 )
            return result;
          *(_BYTE *)(a3 - 1) = a1 & 0x7F;
          *(_BYTE *)(a3 - 2) = (a1 >> 7) | 0x80;
          *(_BYTE *)(a3 - 3) = (a1 >> 14) | 0x80;
          v6 = a3 - 3;
        }
      }
      else
      {
        result = 0LL;
        if ( a2 + 3 > a3 )
          return result;
        *(_BYTE *)(a3 - 1) = a1 & 0x7F;
        *(_BYTE *)(a3 - 2) = (a1 >> 7) | 0x80;
        v6 = a3 - 2;
      }
    }
    else
    {
      result = 0LL;
      if ( a2 + 2 > a3 )
        return result;
      *(_BYTE *)(a3 - 1) = a1;
      v6 = a3 - 1;
    }
    *(_BYTE *)(v6 - 1) = v3 | 0x1F;
    result = v6 - 1;
  }
  else
  {
    result = 0LL;
    if ( a2 + 1 <= a3 )
    {
      *(_BYTE *)(a3 - 1) = v3 & 0xE0 | a1;
      result = a3 - 1;
    }
  }
  return result;
}

//----- (000000000001D321) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_tl(unsigned __int64 a1, unsigned __int64 a2, __int64 a3, unsigned __int64 a4)
{
  __int64 v4; // rbx@1
  signed __int64 v5; // rax@1

  v4 = a3;
  v5 = ccder_encode_len(a2, a3, a4);
  return ccder_encode_tag(a1, v4, v5);
}

//----- (000000000001D34E) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_uint64(__int64 a1, __int64 a2, unsigned __int64 a3)
{
  return ccder_encode_implicit_uint64(2uLL, a1, a2, a3);
}

//----- (000000000001D36F) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof(__int64 a1, unsigned __int64 a2)
{
  signed __int64 v2; // rbx@1

  v2 = ccder_sizeof_tag(a1);
  return ccder_sizeof_len(a2) + a2 + v2;
}

//----- (000000000001D397) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_implicit_integer(__int64 a1, __int64 a2, __int64 a3)
{
  unsigned __int64 v3; // rax@1

  v3 = ccn_write_int_size(a2, a3);
  return ccder_sizeof(a1, v3);
}

//----- (000000000001D3BC) ----------------------------------------------------
int __usercall cc_print@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>)
{
  __int64 v4; // rbx@1
  __int64 v5; // r14@1

  v4 = a2;
  v5 = a4;
  printf("%s { %lu, ", a3, a4, a1);
  if ( a4 )
  {
    do
    {
      printf("%.02x", *(_BYTE *)v4++);
      --v5;
    }
    while ( v5 );
  }
  return printf("\n");
}

//----- (000000000001D41C) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_implicit_octet_string(__int64 a1, __int64 a2, __int64 a3)
{
  unsigned __int64 v3; // rax@1

  v3 = ccn_write_uint_size(a2, a3);
  return ccder_sizeof(a1, v3);
}

//----- (000000000001D441) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_implicit_raw_octet_string(__int64 a1, unsigned __int64 a2)
{
  return ccder_sizeof(a1, a2);
}

//----- (000000000001D44B) ----------------------------------------------------
__int64 *ccaes_gcm_decrypt_mode()
{
  __int64 (*v0)[2]; // rax@1

  v0 = ccaes_ecb_encrypt_mode();
  gcm_aes_decrypt = (((*v0)[0] + 7) & 0xFFFFFFFFFFFFFFF8LL) + 5 * (((*v0)[1] + 7) & 0xFFFFFFFFFFFFFFF8LL) + 384;
  qword_6B918 = 1LL;
  qword_6B920 = (__int64)ccmode_gcm_init;
  qword_6B928 = (__int64)ccmode_gcm_set_iv;
  qword_6B930 = (__int64)ccmode_gcm_gmac;
  qword_6B938 = (__int64)ccmode_gcm_decrypt;
  qword_6B940 = (__int64)ccmode_gcm_finalize;
  qword_6B948 = (__int64)ccmode_gcm_reset;
  qword_6B950 = (__int64)v0;
  return &gcm_aes_decrypt;
}
// 6B910: using guessed type __int64 gcm_aes_decrypt;
// 6B918: using guessed type __int64 qword_6B918;
// 6B920: using guessed type __int64 qword_6B920;
// 6B928: using guessed type __int64 qword_6B928;
// 6B930: using guessed type __int64 qword_6B930;
// 6B938: using guessed type __int64 qword_6B938;
// 6B940: using guessed type __int64 qword_6B940;
// 6B948: using guessed type __int64 qword_6B948;
// 6B950: using guessed type __int64 qword_6B950;

//----- (000000000001D4F0) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_implicit_uint64(__int64 a1, __int64 a2)
{
  __int64 v2; // rbx@1
  signed __int64 result; // rax@1
  __int64 v4; // rcx@1
  __int64 v5; // [sp+8h] [bp-18h]@1
  __int64 v6; // [sp+10h] [bp-10h]@1

  v2 = off_69010[0];
  v6 = *(_QWORD *)off_69010[0];
  v5 = a2;
  result = ccder_sizeof_implicit_integer(a1, 1LL, (__int64)&v5);
  v4 = *(_QWORD *)v2;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001D52E) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_integer(__int64 a1, __int64 a2)
{
  return ccder_sizeof_implicit_integer(2LL, a1, a2);
}

//----- (000000000001D549) ----------------------------------------------------
__int64 __fastcall ccecies_decrypt_gcm_composite(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9, unsigned int *a10, __int64 a11)
{
  __int64 v11; // r14@1
  __int64 v12; // r12@1
  __int64 *v13; // rsi@1
  __int64 v14; // rdi@1
  signed __int64 v15; // rax@1
  char *v16; // rbx@1
  __int64 v17; // r13@1
  char *v18; // r15@1
  int v19; // ecx@1
  unsigned __int64 **v20; // r14@2
  __int64 v21; // r13@2
  __int64 v22; // r12@2
  int v23; // eax@2
  __int64 v24; // rdi@3
  unsigned __int64 v25; // rsi@3
  unsigned __int64 v26; // r9@3
  __int64 v27; // r14@3
  const void *v28; // rdx@4
  unsigned __int64 v29; // rcx@4
  const void *v30; // r8@4
  int v31; // eax@6
  __int64 v32; // rsi@7
  char *v33; // rdi@10
  __int64 v34; // r14@10
  char *v35; // r12@10
  void *v36; // r15@10
  __int64 v37; // rsi@10
  int v38; // er14@11
  __int64 v40; // [sp+10h] [bp-90h]@1
  __int64 v41; // [sp+18h] [bp-88h]@1
  __int64 v42; // [sp+20h] [bp-80h]@2
  __int64 v43; // [sp+28h] [bp-78h]@1
  __int64 v44; // [sp+30h] [bp-70h]@1
  __int64 v45; // [sp+38h] [bp-68h]@1
  __int64 **v46; // [sp+40h] [bp-60h]@1
  size_t v47; // [sp+48h] [bp-58h]@1
  void *v48; // [sp+50h] [bp-50h]@1
  size_t v49; // [sp+58h] [bp-48h]@1
  void *v50; // [sp+60h] [bp-40h]@1
  unsigned __int64 v51; // [sp+68h] [bp-38h]@1
  __int64 v52; // [sp+70h] [bp-30h]@1

  v43 = a6;
  v41 = a5;
  v40 = a4;
  v44 = a3;
  v11 = a2;
  v45 = a2;
  v46 = a1;
  v12 = off_69010[0];
  v52 = *(_QWORD *)off_69010[0];
  v13 = *a1;
  v14 = **a1;
  v50 = (char *)&v40 - ((24 * v14 + 31) & 0xFFFFFFFFFFFFFFF0LL);
  v15 = ccn_bitlen(v14, (__int64)(v13 + 2));
  v47 = (unsigned __int64)(v15 + 7) >> 3;
  v51 = (unsigned __int64)(v15 + 7) >> 3;
  v16 = (char *)&v40 - ((v47 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v17 = *(_QWORD *)(v11 + 16);
  v18 = (char *)&v40 - ((*(_QWORD *)v17 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  v49 = *(_DWORD *)(v11 + 28);
  v48 = (char *)&v40 - ((v49 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v19 = -2;
  if ( *(_BYTE *)(v11 + 32) & 2 )
  {
    v42 = v17;
    v20 = (unsigned __int64 **)v46;
    v21 = ((unsigned __int64)(ccn_bitlen(**v46, (__int64)(*v46 + 2)) + 7) >> 2) | 1;
    v22 = (__int64)v50;
    ccec_x963_import_pub(*v20, v21, a10, (__int64)v50);
    v23 = ccec_compute_key((__int64 **)v20, v22, (__int64)&v51, v16);
    v19 = -1;
    if ( v23 )
      goto LABEL_13;
    v24 = *(_QWORD *)v45;
    v25 = v51;
    v26 = *(_DWORD *)(v45 + 24);
    v27 = v45;
    if ( *(_BYTE *)(v45 + 32) & 1 )
    {
      v51 = (unsigned __int64)v16;
      v28 = v16;
      v29 = v21;
      v30 = a10;
    }
    else
    {
      v51 = (unsigned __int64)v16;
      v28 = v16;
      v29 = v40;
      v30 = (const void *)v41;
    }
    v31 = ccansikdf_x963(v24, v25, v28, v29, v30, v26, (void *)v51);
    v12 = off_69010[0];
    v17 = v42;
    v19 = -1;
    if ( !v31 )
    {
      (*(void (__fastcall **)(__int64, char *, _QWORD, char *))(v42 + 16))(v42, v18, *(_DWORD *)(v27 + 24), v16);
      (*(void (__fastcall **)(char *, signed __int64, _QWORD))(v17 + 24))(v18, 16LL, ecies_iv_data_0);
      v32 = v43;
      if ( !a7 || !v43 )
        v32 = 0LL;
      (*(void (__fastcall **)(char *, __int64))(v17 + 32))(v18, v32);
      (*(void (__fastcall **)(char *, __int64, __int64, __int64))(v17 + 40))(v18, a8, a9, v44);
      v33 = v18;
      v34 = v49;
      v35 = v18;
      v36 = v48;
      (*(void (__fastcall **)(char *, size_t, void *))(v17 + 48))(v33, v49, v48);
      v37 = (__int64)v36;
      v18 = v35;
      v19 = -((unsigned int)cc_cmp_safe(v34, v37, a11) != 0);
      v12 = off_69010[0];
    }
  }
  while ( 1 )
  {
    v38 = v19;
    cc_clear(*(_QWORD *)v17, v18);
    cc_clear(v47, v16);
    cc_clear(v49, v48);
    cc_clear(24LL * **(_QWORD **)v50 + 16, v50);
    if ( *(_QWORD *)v12 == v52 )
      break;
LABEL_13:
    v12 = off_69010[0];
    v17 = v42;
  }
  return (unsigned int)v38;
}
// 5EE70: using guessed type __int64 ecies_iv_data_0[2];
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001D7E3) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_len(unsigned __int64 a1)
{
  signed __int64 result; // rax@1

  result = 1LL;
  if ( a1 >= 0x80 )
  {
    result = 2LL;
    if ( a1 >= 0x100 )
    {
      result = 3LL;
      if ( a1 >= 0x10000 )
      {
        result = 4LL;
        if ( a1 >= 0x1000000 )
        {
          result = 5LL;
          if ( a1 >> 32 )
          {
            result = 6LL;
            if ( a1 >> 40 )
            {
              result = 7LL;
              if ( a1 >> 48 )
                result = (a1 >> 56 != 0) | 8LL;
            }
          }
        }
      }
    }
  }
  return result;
}

//----- (000000000001D859) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_octet_string(__int64 a1, __int64 a2)
{
  return ccder_sizeof_implicit_octet_string(4LL, a1, a2);
}

//----- (000000000001D874) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_oid(__int64 a1)
{
  return *(_BYTE *)(a1 + 1) + 2LL;
}

//----- (000000000001D882) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_raw_octet_string(unsigned __int64 a1)
{
  return ccder_sizeof_implicit_raw_octet_string(4LL, a1);
}

//----- (000000000001D8C0) ----------------------------------------------------
void __usercall ONE_1(__int64 a1@<rax>)
{
  *(_DWORD *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  JUMPOUT(Lbswap_mask_1);
}

//----- (000000000001D940) ----------------------------------------------------
int __usercall ccm128_decrypt@<eax>(int a1@<ecx>, const __m128i *a2@<rdi>, __m128i *a3@<rsi>, __int64 a4@<r8>, const __m128i *a5@<r9>, __m128i a6@<xmm0>, __m128i a7@<xmm1>, __m128i a8@<xmm2>, __m128i a9@<xmm3>, __m128i a10@<xmm4>, __m128i a11@<xmm5>, int a12)
{
  __m128i v12; // xmm5@1
  __m128i v13; // xmm2@1
  __int128 v36; // [sp+0h] [bp-70h]@1
  __int128 v37; // [sp+10h] [bp-60h]@1
  __int128 v38; // [sp+20h] [bp-50h]@1
  __int128 v39; // [sp+30h] [bp-40h]@1
  __int128 v40; // [sp+40h] [bp-30h]@1
  __int128 v41; // [sp+50h] [bp-20h]@1

  _mm_store_si128((__m128i *)&v36, a6);
  _mm_store_si128((__m128i *)&v37, a7);
  _mm_store_si128((__m128i *)&v38, a8);
  _mm_store_si128((__m128i *)&v39, a9);
  _mm_store_si128((__m128i *)&v40, a10);
  _mm_store_si128((__m128i *)&v41, a11);
  v12 = _mm_load_si128((const __m128i *)((char *)ONE_1 + (unsigned int)(16 * a12)));
  v13 = _mm_shuffle_epi8(_mm_loadu_si128(a5), Lbswap_mask_1);
  _XMM3 = _mm_xor_si128(
            _mm_shuffle_epi8(
              _mm_or_si128(_mm_andnot_si128(v12, v13), _mm_and_si128(_mm_add_epi64(v13, *(__m128i *)ONE_1), v12)),
              Lbswap_mask_1),
            _mm_loadu_si128((const __m128i *)a4));
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 16));
  __asm { aesenc  xmm3, xmm4 }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 32));
  __asm { aesenc  xmm3, xmm4 }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 48));
  __asm { aesenc  xmm3, xmm4 }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 64));
  __asm { aesenc  xmm3, xmm4 }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 80));
  __asm { aesenc  xmm3, xmm4 }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 96));
  __asm { aesenc  xmm3, xmm4 }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 112));
  __asm { aesenc  xmm3, xmm4 }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 128));
  __asm { aesenc  xmm3, xmm4 }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 144));
  __asm { aesenc  xmm3, xmm4 }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a4 + 160));
  __asm { aesenclast xmm3, xmm4 }
  _mm_storeu_si128(a3, _mm_xor_si128(_XMM3, _mm_loadu_si128(a2)));
  JUMPOUT(a1 - 1, 0, &loc_1DB5A);
  return Main_Loop_0(&a2[1], &a3[1]);
}
// 1DA5A: using guessed type int __fastcall Main_Loop_0(_QWORD, _QWORD);

//----- (000000000001DA5A) ----------------------------------------------------
#error "1DC17: positive sp value has been found (funcsize=0)"

//----- (000000000001DC18) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_tag(__int64 a1)
{
  signed __int64 v1; // rcx@1
  signed __int64 result; // rax@1

  v1 = a1 & 0x1FFFFFFFFFFFFFFFLL;
  result = 1LL;
  if ( (a1 & 0x1FFFFFFFFFFFFFFFuLL) >= 0x1F )
  {
    result = 2LL;
    if ( (unsigned __int64)v1 >= 0x80 )
    {
      result = 3LL;
      if ( (unsigned __int64)v1 >= 0x4000 )
      {
        result = 4LL;
        if ( (unsigned __int64)v1 >= 0x200000 )
          result = ((unsigned __int64)v1 > 0xFFFFFFF) + 5LL;
      }
    }
  }
  return result;
}

//----- (000000000001DC71) ----------------------------------------------------
signed __int64 __fastcall ccder_sizeof_uint64(__int64 a1)
{
  return ccder_sizeof_implicit_uint64(2LL, a1);
}

//----- (000000000001DC86) ----------------------------------------------------
__int64 ccdes3_ecb_decrypt_mode()
{
  return (__int64)ccdes3_ltc_ecb_decrypt_mode;
}
// 69408: using guessed type __int64 ccdes3_ltc_ecb_decrypt_mode[2];

//----- (000000000001DC93) ----------------------------------------------------
__int64 ccdes3_ecb_encrypt_mode()
{
  return (__int64)ccdes3_ltc_ecb_encrypt_mode;
}
// 69428: using guessed type __int64 ccdes3_ltc_ecb_encrypt_mode[2];

//----- (000000000001DCA0) ----------------------------------------------------
__int64 __fastcall ccdes3_ltc_setup(__int64 a1, void *a2, __int64 a3, __int64 a4)
{
  return ltc_des3_setup(a4, a3, a2);
}

//----- (000000000001DCB6) ----------------------------------------------------
__int64 __fastcall ltc_des3_setup(__int64 a1, __int64 a2, void *a3)
{
  void *v3; // rbx@1
  __int64 result; // rax@2

  v3 = a3;
  if ( a2 == 24 )
  {
    deskey(a1, 0, a3);
    deskey(a1 + 8, 1u, (char *)v3 + 128);
    deskey(a1 + 16, 0, (char *)v3 + 256);
    deskey(a1, 1u, (char *)v3 + 640);
    deskey(a1 + 8, 0, (char *)v3 + 512);
    result = deskey(a1 + 16, 1u, (char *)v3 + 384);
  }
  return result;
}

//----- (000000000001DD5B) ----------------------------------------------------
__int64 __fastcall ccdes168_ltc_setup(__int64 a1, void *a2, __int64 a3, __int64 a4)
{
  void *v4; // r8@1
  __int64 v5; // r14@1
  __int64 v6; // rdi@1
  signed int v7; // er11@1
  int v8; // edx@1
  __int64 v9; // rax@2
  int v10; // esi@2
  char v12[24]; // [sp+0h] [bp-30h]@8
  __int64 v13; // [sp+18h] [bp-18h]@1

  v4 = a2;
  v5 = off_69010[0];
  v13 = *(_QWORD *)off_69010[0];
  v6 = 0LL;
  v7 = 0;
  v8 = 0;
  do
  {
    v9 = v8;
    v10 = *(_BYTE *)(a4 + v8) << v7;
    if ( v7 >= 2 )
    {
      ++v8;
      LOBYTE(v10) = -(char)(1 << (8 - v7)) & v10 | ((1 << (8 - v7)) - 1) & ((unsigned int)*(_BYTE *)(a4 + v9 + 1) >> (8 - v7));
LABEL_6:
      --v7;
      goto LABEL_8;
    }
    if ( v7 > 0 )
    {
      ++v8;
      goto LABEL_6;
    }
    v7 += 7;
LABEL_8:
    v12[v6++] = v10;
  }
  while ( v6 != 24 );
  ltc_des3_setup((__int64)v12, 24LL, v4);
  return *(_QWORD *)v5;
}
// 69010: using guessed type __int64 off_69010[2];
// 1DD5B: using guessed type char var_30[24];

//----- (000000000001DE0B) ----------------------------------------------------
__int64 __fastcall ltc_des3_ecb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 v6; // r15@1
  unsigned int v8; // [sp+8h] [bp-38h]@2
  unsigned int v9; // [sp+Ch] [bp-34h]@2
  __int64 i; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v6 = a2;
  for ( i = *(_QWORD *)off_69010[0]; v6; --v6 )
  {
    v8 = _byteswap_ulong(*(_DWORD *)v5);
    v9 = _byteswap_ulong(*(_DWORD *)(v5 + 4));
    desfunc3((__int64)&v8, a1 + 384);
    *(_DWORD *)v4 = _byteswap_ulong(v8);
    *(_DWORD *)(v4 + 4) = _byteswap_ulong(v9);
    v5 += 8LL;
    v4 += 8LL;
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001DEAB) ----------------------------------------------------
__int64 __fastcall cced25519_verify(__int64 a1, unsigned __int64 a2, const void *a3, __int64 a4, __int64 a5)
{
  const void *v5; // r15@1
  __int64 v6; // r14@1
  char *v7; // rbx@1
  int v8; // ecx@1
  __int64 result; // rax@1
  __int64 v10; // rax@2
  signed __int64 v11; // rax@2
  signed __int64 v12; // rax@2
  __int64 v13; // rcx@3
  const void *v14; // [sp+0h] [bp-1B0h]@1
  char v15; // [sp+8h] [bp-1A8h]@2
  char v16; // [sp+80h] [bp-130h]@1
  char v17; // [sp+120h] [bp-90h]@2
  char v18; // [sp+140h] [bp-70h]@2
  __int64 v19; // [sp+180h] [bp-30h]@1

  v5 = (const void *)a5;
  v6 = a4;
  v14 = a3;
  v19 = *(_QWORD *)off_69010[0];
  v7 = (char *)&v14
     - ((((*(_QWORD *)(a1 + 8) + *(_QWORD *)(a1 + 16) + 19LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v8 = ge_frombytes_negate_vartime((__int64)&v16, a5);
  result = 0xFFFFFFFFLL;
  if ( !v8 )
  {
    v10 = ccdigest_init(a1, (__int64)v7);
    LODWORD(v11) = ccdigest_update(v10, 0x20uLL, (const void *)v6, a1, (__int64)v7);
    LODWORD(v12) = ccdigest_update(v11, 0x20uLL, v5, a1, (__int64)v7);
    ccdigest_update(v12, a2, v14, a1, (__int64)v7);
    (*(void (__fastcall **)(__int64, char *, char *))(a1 + 56))(a1, v7, &v18);
    cc_clear(*(_QWORD *)(a1 + 8) + *(_QWORD *)(a1 + 16) + 12LL, v7);
    sc_reduce((__int64)&v18);
    ge_double_scalarmult_vartime((__int64)&v15, (__int64)&v18, (__int64)&v16, v6 + 32);
    ge_tobytes((__int64)&v17, (__int64)&v15);
    result = crypto_verify_32((__int64)&v17, v6);
  }
  v13 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001DFF7) ----------------------------------------------------
__int64 __fastcall ltc_des3_ecb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 v6; // r15@1
  unsigned int v8; // [sp+8h] [bp-38h]@2
  unsigned int v9; // [sp+Ch] [bp-34h]@2
  __int64 i; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v6 = a2;
  for ( i = *(_QWORD *)off_69010[0]; v6; --v6 )
  {
    v8 = _byteswap_ulong(*(_DWORD *)v5);
    v9 = _byteswap_ulong(*(_DWORD *)(v5 + 4));
    desfunc3((__int64)&v8, a1);
    *(_DWORD *)v4 = _byteswap_ulong(v8);
    *(_DWORD *)(v4 + 4) = _byteswap_ulong(v9);
    v5 += 8LL;
    v4 += 8LL;
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001E090) ----------------------------------------------------
__int64 *ccdes_cbc_encrypt_mode()
{
  __int64 v0; // rax@1
  __int64 v1; // rdx@1

  v0 = ccdes_ecb_encrypt_mode();
  v1 = *(_QWORD *)(v0 + 8);
  cbc_des_encrypt = ((v1 + 15) & 0xFFFFFFFFFFFFFFF8LL) + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6B960 = v1;
  qword_6B968 = (__int64)ccmode_cbc_init;
  qword_6B970 = (__int64)ccmode_cbc_encrypt;
  qword_6B978 = v0;
  return &cbc_des_encrypt;
}
// 6B958: using guessed type __int64 cbc_des_encrypt;
// 6B960: using guessed type __int64 qword_6B960;
// 6B968: using guessed type __int64 qword_6B968;
// 6B970: using guessed type __int64 qword_6B970;
// 6B978: using guessed type __int64 qword_6B978;

//----- (000000000001E0F0) ----------------------------------------------------
__int64 *ccdes_cbc_decrypt_mode()
{
  __int64 v0; // rax@1
  __int64 v1; // rdx@1

  v0 = ccdes_ecb_decrypt_mode();
  v1 = *(_QWORD *)(v0 + 8);
  cbc_des_decrypt = ((v1 + 15) & 0xFFFFFFFFFFFFFFF8LL) + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6B988 = v1;
  qword_6B990 = (__int64)ccmode_cbc_init;
  qword_6B998 = (__int64)ccmode_cbc_decrypt;
  qword_6B9A0 = v0;
  return &cbc_des_decrypt;
}
// 6B980: using guessed type __int64 cbc_des_decrypt;
// 6B988: using guessed type __int64 qword_6B988;
// 6B990: using guessed type __int64 qword_6B990;
// 6B998: using guessed type __int64 qword_6B998;
// 6B9A0: using guessed type __int64 qword_6B9A0;

//----- (000000000001E150) ----------------------------------------------------
__int64 *ccdes_cfb_encrypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes_ecb_encrypt_mode();
  cfb_des_encrypt = ((2LL * *(_QWORD *)(v0 + 8) + 30) & 0xFFFFFFFFFFFFFFF0LL)
                  + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6B9B0 = 1LL;
  qword_6B9B8 = (__int64)ccmode_cfb_init;
  qword_6B9C0 = (__int64)ccmode_cfb_encrypt;
  qword_6B9C8 = v0;
  return &cfb_des_encrypt;
}
// 6B9A8: using guessed type __int64 cfb_des_encrypt;
// 6B9B0: using guessed type __int64 qword_6B9B0;
// 6B9B8: using guessed type __int64 qword_6B9B8;
// 6B9C0: using guessed type __int64 qword_6B9C0;
// 6B9C8: using guessed type __int64 qword_6B9C8;

//----- (000000000001E1B5) ----------------------------------------------------
__int64 *ccdes_cfb_decrypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes_ecb_encrypt_mode();
  cfb_des_decrypt = ((2LL * *(_QWORD *)(v0 + 8) + 30) & 0xFFFFFFFFFFFFFFF0LL)
                  + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6B9D8 = 1LL;
  qword_6B9E0 = (__int64)ccmode_cfb_init;
  qword_6B9E8 = (__int64)ccmode_cfb_decrypt;
  qword_6B9F0 = v0;
  return &cfb_des_decrypt;
}
// 6B9D0: using guessed type __int64 cfb_des_decrypt;
// 6B9D8: using guessed type __int64 qword_6B9D8;
// 6B9E0: using guessed type __int64 qword_6B9E0;
// 6B9E8: using guessed type __int64 qword_6B9E8;
// 6B9F0: using guessed type __int64 qword_6B9F0;

//----- (000000000001E21A) ----------------------------------------------------
__int64 *ccdes_cfb8_encrypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes_ecb_encrypt_mode();
  cfb8_des_encrypt = ((2LL * *(_QWORD *)(v0 + 8) + 14) & 0xFFFFFFFFFFFFFFF0LL)
                   + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL)
                   + 8;
  qword_6BA00 = 1LL;
  qword_6BA08 = (__int64)ccmode_cfb8_init;
  qword_6BA10 = (__int64)ccmode_cfb8_encrypt;
  qword_6BA18 = v0;
  return &cfb8_des_encrypt;
}
// 6B9F8: using guessed type __int64 cfb8_des_encrypt;
// 6BA00: using guessed type __int64 qword_6BA00;
// 6BA08: using guessed type __int64 qword_6BA08;
// 6BA10: using guessed type __int64 qword_6BA10;
// 6BA18: using guessed type __int64 qword_6BA18;

//----- (000000000001E281) ----------------------------------------------------
__int64 *ccdes_cfb8_decrypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes_ecb_encrypt_mode();
  cfb8_des_decrypt = ((2LL * *(_QWORD *)(v0 + 8) + 14) & 0xFFFFFFFFFFFFFFF0LL)
                   + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL)
                   + 8;
  qword_6BA28 = 1LL;
  qword_6BA30 = (__int64)ccmode_cfb8_init;
  qword_6BA38 = (__int64)ccmode_cfb8_decrypt;
  qword_6BA40 = v0;
  return &cfb8_des_decrypt;
}
// 6BA20: using guessed type __int64 cfb8_des_decrypt;
// 6BA28: using guessed type __int64 qword_6BA28;
// 6BA30: using guessed type __int64 qword_6BA30;
// 6BA38: using guessed type __int64 qword_6BA38;
// 6BA40: using guessed type __int64 qword_6BA40;

//----- (000000000001E2E8) ----------------------------------------------------
__int64 *ccdes_ctr_crypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes_ecb_encrypt_mode();
  ctr_des = ((2LL * *(_QWORD *)(v0 + 8) + 30) & 0xFFFFFFFFFFFFFFF0LL) + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BA50 = 1LL;
  qword_6BA58 = (__int64)ccmode_ctr_init;
  qword_6BA60 = (__int64)ccmode_ctr_crypt;
  qword_6BA68 = v0;
  return &ctr_des;
}
// 6BA48: using guessed type __int64 ctr_des;
// 6BA50: using guessed type __int64 qword_6BA50;
// 6BA58: using guessed type __int64 qword_6BA58;
// 6BA60: using guessed type __int64 qword_6BA60;
// 6BA68: using guessed type __int64 qword_6BA68;

//----- (000000000001E34D) ----------------------------------------------------
__int64 *ccdes_ofb_crypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes_ecb_encrypt_mode();
  ofb_des = ((*(_QWORD *)(v0 + 8) + 23LL) & 0xFFFFFFFFFFFFFFF8LL) + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BA78 = 1LL;
  qword_6BA80 = (__int64)ccmode_ofb_init;
  qword_6BA88 = (__int64)ccmode_ofb_crypt;
  qword_6BA90 = v0;
  return &ofb_des;
}
// 6BA70: using guessed type __int64 ofb_des;
// 6BA78: using guessed type __int64 qword_6BA78;
// 6BA80: using guessed type __int64 qword_6BA80;
// 6BA88: using guessed type __int64 qword_6BA88;
// 6BA90: using guessed type __int64 qword_6BA90;

//----- (000000000001E3B1) ----------------------------------------------------
__int64 *ccdes3_cbc_encrypt_mode()
{
  __int64 v0; // rax@1
  __int64 v1; // rdx@1

  v0 = ccdes3_ecb_encrypt_mode();
  v1 = *(_QWORD *)(v0 + 8);
  cbc_des3_encrypt = ((v1 + 15) & 0xFFFFFFFFFFFFFFF8LL) + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BAA0 = v1;
  qword_6BAA8 = (__int64)ccmode_cbc_init;
  qword_6BAB0 = (__int64)ccmode_cbc_encrypt;
  qword_6BAB8 = v0;
  return &cbc_des3_encrypt;
}
// 6BA98: using guessed type __int64 cbc_des3_encrypt;
// 6BAA0: using guessed type __int64 qword_6BAA0;
// 6BAA8: using guessed type __int64 qword_6BAA8;
// 6BAB0: using guessed type __int64 qword_6BAB0;
// 6BAB8: using guessed type __int64 qword_6BAB8;

//----- (000000000001E411) ----------------------------------------------------
__int64 *ccdes3_cbc_decrypt_mode()
{
  __int64 v0; // rax@1
  __int64 v1; // rdx@1

  v0 = ccdes3_ecb_decrypt_mode();
  v1 = *(_QWORD *)(v0 + 8);
  cbc_des3_decrypt = ((v1 + 15) & 0xFFFFFFFFFFFFFFF8LL) + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BAC8 = v1;
  qword_6BAD0 = (__int64)ccmode_cbc_init;
  qword_6BAD8 = (__int64)ccmode_cbc_decrypt;
  qword_6BAE0 = v0;
  return &cbc_des3_decrypt;
}
// 6BAC0: using guessed type __int64 cbc_des3_decrypt;
// 6BAC8: using guessed type __int64 qword_6BAC8;
// 6BAD0: using guessed type __int64 qword_6BAD0;
// 6BAD8: using guessed type __int64 qword_6BAD8;
// 6BAE0: using guessed type __int64 qword_6BAE0;

//----- (000000000001E471) ----------------------------------------------------
__int64 *ccdes3_cfb_encrypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes3_ecb_encrypt_mode();
  cfb_des3_encrypt = ((2LL * *(_QWORD *)(v0 + 8) + 30) & 0xFFFFFFFFFFFFFFF0LL)
                   + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BAF0 = 1LL;
  qword_6BAF8 = (__int64)ccmode_cfb_init;
  qword_6BB00 = (__int64)ccmode_cfb_encrypt;
  qword_6BB08 = v0;
  return &cfb_des3_encrypt;
}
// 6BAE8: using guessed type __int64 cfb_des3_encrypt;
// 6BAF0: using guessed type __int64 qword_6BAF0;
// 6BAF8: using guessed type __int64 qword_6BAF8;
// 6BB00: using guessed type __int64 qword_6BB00;
// 6BB08: using guessed type __int64 qword_6BB08;

//----- (000000000001E4D6) ----------------------------------------------------
__int64 *ccdes3_cfb_decrypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes3_ecb_encrypt_mode();
  cfb_des3_decrypt = ((2LL * *(_QWORD *)(v0 + 8) + 30) & 0xFFFFFFFFFFFFFFF0LL)
                   + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BB18 = 1LL;
  qword_6BB20 = (__int64)ccmode_cfb_init;
  qword_6BB28 = (__int64)ccmode_cfb_decrypt;
  qword_6BB30 = v0;
  return &cfb_des3_decrypt;
}
// 6BB10: using guessed type __int64 cfb_des3_decrypt;
// 6BB18: using guessed type __int64 qword_6BB18;
// 6BB20: using guessed type __int64 qword_6BB20;
// 6BB28: using guessed type __int64 qword_6BB28;
// 6BB30: using guessed type __int64 qword_6BB30;

//----- (000000000001E53B) ----------------------------------------------------
__int64 *ccdes3_cfb8_encrypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes3_ecb_encrypt_mode();
  cfb8_des3_encrypt = ((2LL * *(_QWORD *)(v0 + 8) + 14) & 0xFFFFFFFFFFFFFFF0LL)
                    + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL)
                    + 8;
  qword_6BB40 = 1LL;
  qword_6BB48 = (__int64)ccmode_cfb8_init;
  qword_6BB50 = (__int64)ccmode_cfb8_encrypt;
  qword_6BB58 = v0;
  return &cfb8_des3_encrypt;
}
// 6BB38: using guessed type __int64 cfb8_des3_encrypt;
// 6BB40: using guessed type __int64 qword_6BB40;
// 6BB48: using guessed type __int64 qword_6BB48;
// 6BB50: using guessed type __int64 qword_6BB50;
// 6BB58: using guessed type __int64 qword_6BB58;

//----- (000000000001E5A2) ----------------------------------------------------
__int64 *ccdes3_cfb8_decrypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes3_ecb_encrypt_mode();
  cfb8_des3_decrypt = ((2LL * *(_QWORD *)(v0 + 8) + 14) & 0xFFFFFFFFFFFFFFF0LL)
                    + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL)
                    + 8;
  qword_6BB68 = 1LL;
  qword_6BB70 = (__int64)ccmode_cfb8_init;
  qword_6BB78 = (__int64)ccmode_cfb8_decrypt;
  qword_6BB80 = v0;
  return &cfb8_des3_decrypt;
}
// 6BB60: using guessed type __int64 cfb8_des3_decrypt;
// 6BB68: using guessed type __int64 qword_6BB68;
// 6BB70: using guessed type __int64 qword_6BB70;
// 6BB78: using guessed type __int64 qword_6BB78;
// 6BB80: using guessed type __int64 qword_6BB80;

//----- (000000000001E609) ----------------------------------------------------
__int64 *ccdes3_ctr_crypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes3_ecb_encrypt_mode();
  ctr_des3 = ((2LL * *(_QWORD *)(v0 + 8) + 30) & 0xFFFFFFFFFFFFFFF0LL) + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BB90 = 1LL;
  qword_6BB98 = (__int64)ccmode_ctr_init;
  qword_6BBA0 = (__int64)ccmode_ctr_crypt;
  qword_6BBA8 = v0;
  return &ctr_des3;
}
// 6BB88: using guessed type __int64 ctr_des3;
// 6BB90: using guessed type __int64 qword_6BB90;
// 6BB98: using guessed type __int64 qword_6BB98;
// 6BBA0: using guessed type __int64 qword_6BBA0;
// 6BBA8: using guessed type __int64 qword_6BBA8;

//----- (000000000001E66E) ----------------------------------------------------
__int64 *ccdes3_ofb_crypt_mode()
{
  __int64 v0; // rax@1

  v0 = ccdes3_ecb_encrypt_mode();
  ofb_des3 = ((*(_QWORD *)(v0 + 8) + 23LL) & 0xFFFFFFFFFFFFFFF8LL) + ((*(_QWORD *)v0 + 7LL) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BBB8 = 1LL;
  qword_6BBC0 = (__int64)ccmode_ofb_init;
  qword_6BBC8 = (__int64)ccmode_ofb_crypt;
  qword_6BBD0 = v0;
  return &ofb_des3;
}
// 6BBB0: using guessed type __int64 ofb_des3;
// 6BBB8: using guessed type __int64 qword_6BBB8;
// 6BBC0: using guessed type __int64 qword_6BBC0;
// 6BBC8: using guessed type __int64 qword_6BBC8;
// 6BBD0: using guessed type __int64 qword_6BBD0;

//----- (000000000001E6D2) ----------------------------------------------------
char __fastcall ccrsa_test_verify_pkcs1v15_vector(__int64 a1)
{
  __int64 v1; // r12@1
  unsigned __int64 v2; // rbx@1
  unsigned __int64 v3; // rax@1
  char *v4; // r14@1
  char *v5; // r15@1
  __int64 *v6; // r13@1
  __int64 v7; // rax@1
  __int64 v8; // rbx@1
  const void *v9; // r14@1
  char result; // al@1
  unsigned __int64 v11; // [sp+10h] [bp-50h]@1
  unsigned __int64 *v12; // [sp+18h] [bp-48h]@1
  __int64 v13; // [sp+20h] [bp-40h]@1
  char v14; // [sp+2Fh] [bp-31h]@1
  __int64 v15; // [sp+30h] [bp-30h]@1

  v1 = a1;
  v15 = *(_QWORD *)off_69010[0];
  v13 = *(_QWORD *)a1;
  v2 = (unsigned __int64)(*(_QWORD *)(a1 + 8) + 63LL) >> 6;
  v11 = 8 * v2;
  v12 = (unsigned __int64 *)((char *)&v11 - ((*(_QWORD *)v13 + 15LL) & 0xFFFFFFFFFFFFFFF0LL));
  v3 = (8 * v2 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v4 = (char *)&v11 - v3;
  v5 = (char *)&v11 - v3;
  v6 = (__int64 *)((char *)&v11 - ((24 * v2 + 39) & 0xFFFFFFFFFFFFFFF0LL));
  *v6 = v2;
  *(unsigned __int64 *)((char *)&v11 - v3) = *(_QWORD *)(a1 + 24);
  bzero((char *)&v11 + -v3 + 8, 8 * v2 - 8);
  ccn_read_uint(v2, (__int64)v5, v11, *(_QWORD *)(a1 + 16));
  v7 = (__int64)ccrsa_init_pub((__int64)v6, v5, v4);
  v8 = v13;
  v9 = v12;
  ccdigest(v7, *(const void **)(v1 + 40), (__int64)v12, v13, *(_QWORD *)(v1 + 32));
  ccrsa_verify_pkcs1v15(
    v6,
    *(_QWORD *)(v8 + 32),
    *(_QWORD *)v8,
    v9,
    *(_QWORD *)(v1 + 48),
    *(_QWORD *)(v1 + 56),
    (__int64)&v14);
  result = *(_BYTE *)(a1 + 64) == 0;
  if ( v14 )
    result = *(_BYTE *)(a1 + 64) != 0;
  if ( *(_QWORD *)off_69010[0] == v15 )
    result ^= 1u;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001E830) ----------------------------------------------------
__int64 ccdes_ecb_decrypt_mode()
{
  return (__int64)ccdes_ltc_ecb_decrypt_mode;
}
// 69468: using guessed type __int64 ccdes_ltc_ecb_decrypt_mode[2];

//----- (000000000001E83D) ----------------------------------------------------
__int64 ccdes_ecb_encrypt_mode()
{
  return (__int64)ccdes_ltc_ecb_encrypt_mode;
}
// 69488: using guessed type __int64 ccdes_ltc_ecb_encrypt_mode[2];

//----- (000000000001E84A) ----------------------------------------------------
__int64 __fastcall ccec_verify_internal(const void *a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 v6; // r12@1
  __int64 v7; // rbx@1
  unsigned __int64 v8; // rax@1
  __int64 v9; // r13@1
  char *v10; // r14@1
  __int64 v11; // r15@1
  signed __int64 v12; // rax@1
  __int64 v13; // rdi@4
  int v14; // eax@5
  signed __int64 v15; // rax@6
  __int64 v16; // r12@8
  __int64 v17; // r14@8
  __int64 v18; // r13@8
  __int64 v19; // r14@9
  int v20; // eax@12
  unsigned __int64 v21; // rax@13
  int v22; // eax@15
  signed __int64 *v23; // r13@17
  __int64 v25; // [sp+0h] [bp-B0h]@1
  __int64 v26; // [sp+8h] [bp-A8h]@5
  __int64 v27; // [sp+10h] [bp-A0h]@4
  __int64 v28; // [sp+18h] [bp-98h]@3
  __int64 *v29; // [sp+20h] [bp-90h]@3
  __int64 v30; // [sp+28h] [bp-88h]@1
  size_t v31; // [sp+30h] [bp-80h]@1
  __int64 v32; // [sp+38h] [bp-78h]@1
  __int64 v33; // [sp+40h] [bp-70h]@1
  unsigned __int64 v34; // [sp+48h] [bp-68h]@1
  unsigned __int64 v35; // [sp+50h] [bp-60h]@1
  const void *v36; // [sp+58h] [bp-58h]@1
  __int64 v37; // [sp+60h] [bp-50h]@1
  __int64 v38; // [sp+68h] [bp-48h]@1
  size_t v39; // [sp+70h] [bp-40h]@1
  void *v40; // [sp+78h] [bp-38h]@1
  __int64 v41; // [sp+80h] [bp-30h]@1
  __int64 v42; // [sp+88h] [bp-28h]@17

  v30 = a6;
  v38 = a5;
  v35 = a3;
  v34 = a2;
  v36 = a1;
  v41 = *(_QWORD *)off_69010[0];
  v6 = *(_QWORD *)a1;
  v7 = **(_QWORD **)a1;
  *(_BYTE *)a6 = 0;
  v8 = (8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v9 = (__int64)((char *)&v25 - v8);
  v10 = (char *)&v25 - v8;
  v33 = (__int64)((char *)&v25 - v8);
  v32 = (__int64)((char *)&v25 + -v8 - v8);
  v39 = 24LL * *(_QWORD *)v6;
  v31 = (8 * (v39 >> 3) + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v40 = (void *)(v32 - v31);
  v11 = a4;
  v12 = ccn_n(v7, a4);
  LODWORD(v37) = -1;
  if ( !v12 )
    goto LABEL_21;
  if ( !ccn_n(v7, v38) )
    goto LABEL_21;
  v29 = (__int64 *)v6;
  v28 = v6 + 16;
  v6 += 16 + 8 * (4 * v7 | 2);
  if ( (unsigned int)ccn_cmp(v7, v11, v6) != -1 )
    goto LABEL_21;
  v27 = v11;
  v13 = v7;
  v11 = v7;
  v7 = v38;
  if ( (unsigned int)ccn_cmp(v13, v38, v6) != -1 )
    goto LABEL_21;
  v26 = v6;
  v6 = v7;
  v7 = v11;
  v14 = ccn_read_uint(v11, v9, v34, v35);
  if ( v14 < 0 )
  {
LABEL_20:
    LODWORD(v37) = v14;
    goto LABEL_21;
  }
  LODWORD(v35) = v14;
  v11 = (__int64)&v29[4 * v11 + 2];
  cczp_mod_inv((__int64)&v29[4 * v7 + 2], v10, (const void *)v6);
  cczp_mul((signed __int64 *)v11, v33, v9, (unsigned __int64 *)v10);
  cczp_mul((signed __int64 *)v11, v32, v27, (unsigned __int64 *)v10);
  v10 = (char *)v36 + 16 * v7 + 16;
  v15 = ccn_n(v7, (__int64)v10);
  LODWORD(v37) = -2;
  if ( v15 != 1 || *(_QWORD *)v10 != 1LL )
    goto LABEL_21;
  v16 = (__int64)((char *)v36 + 16);
  v36 = (const void *)v16;
  v34 = 2 * v7;
  v17 = v28;
  v18 = v28 + 16 * v7;
  v38 = v28 + 16 * v7;
  v11 = (__int64)v40;
  v25 = (__int64)((char *)v40 + 8 * v7);
  v37 = v16 + 8 * v7;
  ccn_sub(v7, (__int64)((char *)v40 + 8 * v7), v28, v16 + 8 * v7);
  if ( !(unsigned int)ccn_cmp(v7, v18, v16) )
  {
    v19 = v17 + 8 * (v34 + v7);
    if ( !(unsigned int)ccn_cmp(v7, v19, v37) || !(unsigned int)ccn_cmp(v7, v19, v25) )
    {
      v23 = v29;
      ccec_projectify(v29, (void *)v11, (const void *)v38);
      v38 = (__int64)&v42;
      v10 = (char *)&v42 - v31;
      v6 = (__int64)v23;
      if ( !(unsigned int)ccec_mult(v23, (__int64)((char *)&v42 - v31), v33, (const void *)v11)
        && !(unsigned int)ccec_mult(v23, v11, v32, v36) )
        goto LABEL_23;
      v14 = v35;
      goto LABEL_20;
    }
  }
  v6 = (__int64)v29;
  ccec_projectify(v29, (void *)v11, (const void *)v38);
  ccec_twin_mult(v6, v11, v33, v11, v32, v36);
  while ( 1 )
  {
    v20 = ccec_affinify((signed __int64 *)v6, v11, (unsigned __int64 *)v11);
    LODWORD(v37) = v35;
    v10 = (char *)v7;
    v11 = v26;
    if ( !v20 )
    {
      v7 = (__int64)v40;
      v21 = v34;
      *((_QWORD *)v40 + v34) = 1LL;
      bzero((void *)(v7 + 8 * v21 + 8), 8LL * (_QWORD)v10 - 8);
      if ( (signed int)ccn_cmp((__int64)v10, v7, v11) >= 0 )
        ccn_sub((__int64)v10, v7, v7, v11);
      v22 = ccn_cmp((__int64)v10, v7, v27);
      LODWORD(v37) = 0;
      if ( !v22 )
        *(_BYTE *)v30 = 1;
    }
LABEL_21:
    cc_clear(v39, v40);
    if ( *(_QWORD *)off_69010[0] == v41 )
      break;
LABEL_23:
    ccec_full_add((signed __int64 *)v6, (void *)v11, v10, (const void *)v11);
  }
  return (unsigned int)v37;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000001EC31) ----------------------------------------------------
__int64 __fastcall ccdes_key_is_weak(void *a1, __int64 a2)
{
  unsigned __int64 v2; // r15@1
  const void *v3; // rbx@2
  __int64 result; // rax@4

  v2 = 0LL;
  if ( a2 == 8 )
  {
    v3 = weak_keys;
    while ( memcmp(v3, a1, 8uLL) )
    {
      ++v2;
      v3 = (char *)v3 + 8;
      result = 0LL;
      if ( v2 > 0xF )
        return result;
    }
  }
  return 0xFFFFFFFFLL;
}
// 6B4F0: using guessed type __int64 weak_keys[16];

//----- (000000000001EC83) ----------------------------------------------------
__int64 __fastcall ccdes_key_set_odd_parity(__int64 a1, __int64 a2)
{
  __int64 result; // rax@2

  if ( a2 )
  {
    result = (__int64)odd_parity;
    do
    {
      *(_BYTE *)a1 = *((_BYTE *)odd_parity + *(_BYTE *)a1);
      ++a1;
      --a2;
    }
    while ( a2 );
  }
  return result;
}
// 5EE80: using guessed type __int64 odd_parity[32];

//----- (000000000001ECC0) ----------------------------------------------------
void __usercall ONE_2(__int64 a1@<rax>)
{
  *(_DWORD *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  TWO_0(a1);
}

//----- (000000000001ECD0) ----------------------------------------------------
void __usercall TWO_0(__int64 a1@<rax>)
{
  LOBYTE(a1) = *(_BYTE *)a1 + a1;
  *(_BYTE *)a1 = a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  JUMPOUT(Lbswap_mask_2);
}

//----- (000000000001ED00) ----------------------------------------------------
// local variable allocation has failed, the output may be wrong!
__int64 __usercall gcmEncrypt_avx1@<rax>(__int64 _RDX@<rdx>, unsigned __int64 a2@<rcx>, __int64 _RDI@<rdi>, __int64 _RSI@<rsi>, __int64 a5@<r8>, __int64 _R9@<r9>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM4@<xmm4>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>, __int128 _XMM8@<xmm8>, __int128 _XMM9@<xmm9>, __int128 _XMM10@<xmm10>, __int128 _XMM11@<xmm11>, __int128 _XMM12@<xmm12>, __int128 _XMM13@<xmm13>, __int128 _XMM14@<xmm14>, __int128 _XMM15@<xmm15>, __int128 a23, __int128 a24, __int128 a25, __int128 a26, __int128 a27, __int64 a28, __int128 a29, __int128 a30, int a31, int a32, int a33, int a34, int a35, int a36, int a37, int a38, int a39, __int128 a40, __int128 a41, __int128 a42, __int128 a43, __int128 a44, __int128 a45, __int128 a46, __int128 a47, __int128 a48, __int128 a49, __int128 a50, __int128 a51, __int128 a52, __int128 a53, __int128 a54, __int128 a55)
{
  unsigned int v73; // er10@1

  __asm
  {
    vmovdqa [rsp+180h+var_100], xmm0
    vmovdqa [rsp+180h+var_F0], xmm1
    vmovdqa [rsp+180h+var_E0], xmm2
    vmovdqa [rsp+180h+var_D0], xmm3
    vmovdqa [rsp+180h+var_C0], xmm4
    vmovdqa [rsp+180h+var_B0], xmm5
    vmovdqa [rsp+180h+var_A0], xmm6
    vmovdqa [rsp+180h+var_90], xmm7
    vmovdqa [rsp+180h+var_80], xmm8
    vmovdqa [rsp+180h+var_70], xmm9
    vmovdqa [rsp+180h+var_60], xmm10
    vmovdqa [rsp+180h+var_50], xmm11
    vmovdqa [rsp+180h+var_40], xmm12
    vmovdqa [rsp+180h+var_30], xmm13
    vmovdqa [rsp+180h+var_20], xmm14
    vmovdqa [rsp+180h+var_10], xmm15
    vmovdqu xmm15, xmmword ptr [rdx+20h]
    vmovdqu xmm0, xmmword ptr [rdx+10h]
  }
  v73 = *(_DWORD *)(_R9 + 240);
  __asm
  {
    vpshufb xmm15, xmm15, xmmword ptr cs:_Lbswap_mask_2
    vpshufb xmm0, xmm0, xmmword ptr cs:_Lbswap_mask_2
  }
  JUMPOUT(a2, 128LL, &loc_1F9AE);
  __asm
  {
    vmovdqa xmm5, xmmword ptr cs:TWO_0
    vmovdqa xmm6, xmmword ptr cs:_Lbswap_mask_2
    vmovdqa xmm4, xmmword ptr [r9]
    vpaddd  xmm8, xmm15, xmmword ptr cs:ONE_2
    vpaddd  xmm9, xmm15, xmm5
    vpshufb xmm7, xmm15, xmm6
    vpaddd  xmm10, xmm8, xmm5
    vpshufb xmm8, xmm8, xmm6
    vpxor   xmm7, xmm7, xmm4
    vpaddd  xmm11, xmm9, xmm5
    vpshufb xmm9, xmm9, xmm6
    vpxor   xmm8, xmm8, xmm4
    vpaddd  xmm12, xmm10, xmm5
    vpshufb xmm10, xmm10, xmm6
    vpxor   xmm9, xmm9, xmm4
    vpaddd  xmm13, xmm11, xmm5
    vpshufb xmm11, xmm11, xmm6
    vpxor   xmm10, xmm10, xmm4
    vpaddd  xmm14, xmm12, xmm5
    vpshufb xmm12, xmm12, xmm6
    vpxor   xmm11, xmm11, xmm4
    vpaddd  xmm15, xmm13, xmm5
    vpshufb xmm13, xmm13, xmm6
    vpshufb xmm14, xmm14, xmm6
    vpxor   xmm12, xmm12, xmm4
    vpxor   xmm13, xmm13, xmm4
    vpxor   xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr [r9+10h]
    vaesenc xmm7, xmm7, xmm4
    vaesenc xmm8, xmm8, xmm4
    vaesenc xmm9, xmm9, xmm4
    vaesenc xmm10, xmm10, xmm4
    vaesenc xmm11, xmm11, xmm4
    vaesenc xmm12, xmm12, xmm4
    vaesenc xmm13, xmm13, xmm4
    vaesenc xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr [r9+20h]
    vaesenc xmm7, xmm7, xmm4
    vaesenc xmm8, xmm8, xmm4
    vaesenc xmm9, xmm9, xmm4
    vaesenc xmm10, xmm10, xmm4
    vaesenc xmm11, xmm11, xmm4
    vaesenc xmm12, xmm12, xmm4
    vaesenc xmm13, xmm13, xmm4
    vaesenc xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr [r9+30h]
    vaesenc xmm7, xmm7, xmm4
    vaesenc xmm8, xmm8, xmm4
    vaesenc xmm9, xmm9, xmm4
    vaesenc xmm10, xmm10, xmm4
    vaesenc xmm11, xmm11, xmm4
    vaesenc xmm12, xmm12, xmm4
    vaesenc xmm13, xmm13, xmm4
    vaesenc xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr [r9+40h]
    vaesenc xmm7, xmm7, xmm4
    vaesenc xmm8, xmm8, xmm4
    vaesenc xmm9, xmm9, xmm4
    vaesenc xmm10, xmm10, xmm4
    vaesenc xmm11, xmm11, xmm4
    vaesenc xmm12, xmm12, xmm4
    vaesenc xmm13, xmm13, xmm4
    vaesenc xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr [r9+50h]
    vaesenc xmm7, xmm7, xmm4
    vaesenc xmm8, xmm8, xmm4
    vaesenc xmm9, xmm9, xmm4
    vaesenc xmm10, xmm10, xmm4
    vaesenc xmm11, xmm11, xmm4
    vaesenc xmm12, xmm12, xmm4
    vaesenc xmm13, xmm13, xmm4
    vaesenc xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr [r9+60h]
    vaesenc xmm7, xmm7, xmm4
    vaesenc xmm8, xmm8, xmm4
    vaesenc xmm9, xmm9, xmm4
    vaesenc xmm10, xmm10, xmm4
    vaesenc xmm11, xmm11, xmm4
    vaesenc xmm12, xmm12, xmm4
    vaesenc xmm13, xmm13, xmm4
    vaesenc xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr [r9+70h]
    vaesenc xmm7, xmm7, xmm4
    vaesenc xmm8, xmm8, xmm4
    vaesenc xmm9, xmm9, xmm4
    vaesenc xmm10, xmm10, xmm4
    vaesenc xmm11, xmm11, xmm4
    vaesenc xmm12, xmm12, xmm4
    vaesenc xmm13, xmm13, xmm4
    vaesenc xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr [r9+80h]
    vaesenc xmm7, xmm7, xmm4
    vaesenc xmm8, xmm8, xmm4
    vaesenc xmm9, xmm9, xmm4
    vaesenc xmm10, xmm10, xmm4
    vaesenc xmm11, xmm11, xmm4
    vaesenc xmm12, xmm12, xmm4
    vaesenc xmm13, xmm13, xmm4
    vaesenc xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr [r9+90h]
    vaesenc xmm7, xmm7, xmm4
    vaesenc xmm8, xmm8, xmm4
    vaesenc xmm9, xmm9, xmm4
    vaesenc xmm10, xmm10, xmm4
    vaesenc xmm11, xmm11, xmm4
    vaesenc xmm12, xmm12, xmm4
    vaesenc xmm13, xmm13, xmm4
    vaesenc xmm14, xmm14, xmm4
    vmovdqu xmm6, xmmword ptr [r9+0A0h]
  }
  if ( v73 > 0xA0 )
  {
    __asm
    {
      vmovdqu xmm4, xmmword ptr [r9+0A0h]
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vaesenc xmm11, xmm11, xmm4
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vaesenc xmm14, xmm14, xmm4
      vmovdqu xmm4, xmmword ptr [r9+0B0h]
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vaesenc xmm11, xmm11, xmm4
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vaesenc xmm14, xmm14, xmm4
      vmovdqu xmm6, xmmword ptr [r9+0C0h]
    }
    if ( v73 > 0xC0 )
    {
      __asm
      {
        vmovdqu xmm4, xmmword ptr [r9+0C0h]
        vaesenc xmm7, xmm7, xmm4
        vaesenc xmm8, xmm8, xmm4
        vaesenc xmm9, xmm9, xmm4
        vaesenc xmm10, xmm10, xmm4
        vaesenc xmm11, xmm11, xmm4
        vaesenc xmm12, xmm12, xmm4
        vaesenc xmm13, xmm13, xmm4
        vaesenc xmm14, xmm14, xmm4
        vmovdqu xmm4, xmmword ptr [r9+0D0h]
        vaesenc xmm7, xmm7, xmm4
        vaesenc xmm8, xmm8, xmm4
        vaesenc xmm9, xmm9, xmm4
        vaesenc xmm10, xmm10, xmm4
        vaesenc xmm11, xmm11, xmm4
        vaesenc xmm12, xmm12, xmm4
        vaesenc xmm13, xmm13, xmm4
        vaesenc xmm14, xmm14, xmm4
        vmovdqu xmm6, xmmword ptr [r9+0E0h]
      }
    }
  }
  __asm
  {
    vmovdqu xmm4, xmmword ptr [rdi]
    vpxor   xmm4, xmm6, xmm4
    vaesenclast xmm7, xmm7, xmm4
    vmovdqu xmm4, xmmword ptr [rdi+10h]
    vpxor   xmm4, xmm6, xmm4
    vaesenclast xmm8, xmm8, xmm4
    vmovdqu xmm4, xmmword ptr [rdi+20h]
    vpxor   xmm4, xmm6, xmm4
    vaesenclast xmm9, xmm9, xmm4
    vmovdqu xmm4, xmmword ptr [rdi+30h]
    vpxor   xmm4, xmm6, xmm4
    vaesenclast xmm10, xmm10, xmm4
    vmovdqu xmm4, xmmword ptr [rdi+40h]
    vpxor   xmm4, xmm6, xmm4
    vaesenclast xmm11, xmm11, xmm4
    vmovdqu xmm4, xmmword ptr [rdi+50h]
    vpxor   xmm4, xmm6, xmm4
    vaesenclast xmm12, xmm12, xmm4
    vmovdqu xmm4, xmmword ptr [rdi+60h]
    vpxor   xmm4, xmm6, xmm4
    vaesenclast xmm13, xmm13, xmm4
    vmovdqu xmm4, xmmword ptr [rdi+70h]
    vpxor   xmm4, xmm6, xmm4
    vaesenclast xmm14, xmm14, xmm4
    vmovdqu xmm4, xmmword ptr cs:_Lbswap_mask_2
    vmovdqu xmmword ptr [rsi], xmm7
    vpshufb xmm7, xmm7, xmm4
    vmovdqu xmmword ptr [rsi+10h], xmm8
    vpshufb xmm8, xmm8, xmm4
    vmovdqu xmmword ptr [rsi+20h], xmm9
    vpshufb xmm9, xmm9, xmm4
    vmovdqu xmmword ptr [rsi+30h], xmm10
    vpshufb xmm10, xmm10, xmm4
    vmovdqu xmmword ptr [rsi+40h], xmm11
    vpshufb xmm11, xmm11, xmm4
    vmovdqu xmmword ptr [rsi+50h], xmm12
    vpshufb xmm12, xmm12, xmm4
    vmovdqu xmmword ptr [rsi+60h], xmm13
    vpshufb xmm13, xmm13, xmm4
    vmovdqu xmmword ptr [rsi+70h], xmm14
    vpshufb xmm14, xmm14, xmm4
  }
  JUMPOUT(a2 < 0x100, End_Main_Encrypt_Loop);
  return Main_Encrypt_Loop(
           _RDX,
           a2 - 256,
           _RDI + 128,
           _RSI + 128,
           a5,
           _R9,
           v73,
           _XMM0,
           _XMM1,
           _XMM2,
           _XMM3,
           _XMM5,
           _XMM6,
           _XMM7,
           _XMM8,
           _XMM9,
           _XMM10,
           _XMM11,
           _XMM12,
           _XMM13,
           _XMM14,
           _XMM15,
           a23,
           a24,
           a25,
           a26,
           a27,
           a28,
           a29,
           a30,
           a31,
           a32,
           a33,
           a34,
           a35,
           a36,
           a37,
           a38,
           a39,
           a40,
           a41,
           a42,
           a43,
           a44,
           a45,
           a46,
           a47,
           a48,
           a49,
           a50,
           a51,
           a52,
           a53,
           a54,
           a55);
}
// 1ED00: array has been used for an input argument
// 1F7F7: using guessed type __int64 __fastcall End_Main_Encrypt_Loop(_DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128);

//----- (000000000001F200) ----------------------------------------------------
// local variable allocation has failed, the output may be wrong!
__int64 __usercall Main_Encrypt_Loop@<rax>(int a1@<edx>, __int64 a2@<rcx>, __int64 _RDI@<rdi>, __int64 _RSI@<rsi>, __int64 _R8@<r8>, __int64 _R9@<r9>, unsigned int a7@<r10d>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>, __int128 _XMM8@<xmm8>, __int128 _XMM9@<xmm9>, __int128 _XMM10@<xmm10>, __int128 _XMM11@<xmm11>, __int128 _XMM12@<xmm12>, __int128 _XMM13@<xmm13>, __int128 _XMM14@<xmm14>, __int128 _XMM15@<xmm15>, __int128 a23, __int128 a24, __int128 a25, __int128 a26, __int128 a27, __int64 a28, __int128 a29, __int128 a30, int a31, int a32, int a33, int a34, int a35, int a36, int a37, int a38, int a39, __int128 a40, __int128 a41, __int128 a42, __int128 a43, __int128 a44, __int128 a45, __int128 a46, __int128 a47, __int128 a48, __int128 a49, __int128 a50, __int128 a51, __int128 a52, __int128 a53, __int128 a54, __int128 a55)
{
  unsigned __int8 v284; // of@4
  int v286; // [sp+28h] [bp+28h]@5
  int v287; // [sp+30h] [bp+30h]@5
  int v288; // [sp+48h] [bp+48h]@5
  int v291; // [sp+50h] [bp+50h]@5
  int v292; // [sp+58h] [bp+58h]@5

  do
  {
    __asm
    {
      vpxor   xmm0, xmm0, xmm7
      vmovdqa xmm5, xmmword ptr cs:TWO_0
      vmovdqa [rsp+arg_68], xmm0
      vmovdqu xmm0, xmmword ptr [r9]
      vmovdqa [rsp+arg_58], xmm8
      vpaddd  xmm8, xmm15, xmmword ptr cs:ONE_2
      vmovdqa [rsp+arg_48], xmm9
      vpaddd  xmm9, xmm15, xmm5
      vpshufb xmm7, xmm15, xmm4
      vmovdqa [rsp+arg_38], xmm10
      vpaddd  xmm10, xmm8, xmm5
      vpshufb xmm8, xmm8, xmm4
      vmovdqa [rsp+arg_28], xmm11
      vpaddd  xmm11, xmm9, xmm5
      vpshufb xmm9, xmm9, xmm4
      vpxor   xmm7, xmm7, xmm0
      vmovdqa [rsp+arg_18], xmm12
      vpaddd  xmm12, xmm10, xmm5
      vpshufb xmm10, xmm10, xmm4
      vpxor   xmm8, xmm8, xmm0
      vmovdqa [rsp+arg_8], xmm13
      vpaddd  xmm13, xmm11, xmm5
      vpshufb xmm11, xmm11, xmm4
      vpxor   xmm9, xmm9, xmm0
      vmovdqa xmm6, xmm14
      vpaddd  xmm14, xmm12, xmm5
      vpshufb xmm12, xmm12, xmm4
      vpxor   xmm10, xmm10, xmm0
      vpaddd  xmm15, xmm13, xmm5
      vmovdqu xmm5, xmmword ptr [r8]
      vpshufb xmm13, xmm13, xmm4
      vpclmulqdq xmm2, xmm6, xmm5, 11h
      vpxor   xmm11, xmm11, xmm0
      vpclmulqdq xmm3, xmm6, xmm5, 0
      vmovdqu xmm5, xmmword ptr [r8+80h]
      vpshufb xmm14, xmm14, xmm4
      vpshufd xmm4, xmm6, 4Eh
      vpxor   xmm12, xmm12, xmm0
      vpxor   xmm6, xmm4, xmm6
      vpxor   xmm13, xmm13, xmm0
      vpxor   xmm14, xmm14, xmm0
      vpclmulqdq xmm1, xmm6, xmm5, 0
      vmovdqu xmm4, xmmword ptr [r9+10h]
      vmovdqu xmm6, [rsp+arg_8]
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+10h], 11h
      vpshufd xmm5, xmm6, 4Eh
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vpxor   xmm2, xmm2, xmm0
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+10h], 0
      vaesenc xmm11, xmm11, xmm4
      vpxor   xmm5, xmm5, xmm6
      vpclmulqdq xmm6, xmm5, xmmword ptr [r8+90h], 0
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vpxor   xmm3, xmm3, xmm0
      vaesenc xmm14, xmm14, xmm4
      vpxor   xmm1, xmm1, xmm6
      vmovdqu xmm4, xmmword ptr [r9+20h]
      vmovdqu xmm6, [rsp+arg_18]
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+20h], 11h
      vpshufd xmm5, xmm6, 4Eh
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vpxor   xmm2, xmm2, xmm0
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+20h], 0
      vaesenc xmm11, xmm11, xmm4
      vpxor   xmm5, xmm5, xmm6
      vpclmulqdq xmm6, xmm5, xmmword ptr [r8+0A0h], 0
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vpxor   xmm3, xmm3, xmm0
      vaesenc xmm14, xmm14, xmm4
      vpxor   xmm1, xmm1, xmm6
      vmovdqu xmm4, xmmword ptr [r9+30h]
      vmovdqu xmm6, [rsp+arg_28]
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+30h], 11h
      vpshufd xmm5, xmm6, 4Eh
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vpxor   xmm2, xmm2, xmm0
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+30h], 0
      vaesenc xmm11, xmm11, xmm4
      vpxor   xmm5, xmm5, xmm6
      vpclmulqdq xmm6, xmm5, xmmword ptr [r8+0B0h], 0
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vpxor   xmm3, xmm3, xmm0
      vaesenc xmm14, xmm14, xmm4
      vpxor   xmm1, xmm1, xmm6
      vmovdqu xmm4, xmmword ptr [r9+40h]
      vmovdqu xmm6, [rsp+arg_38]
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+40h], 11h
      vpshufd xmm5, xmm6, 4Eh
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vpxor   xmm2, xmm2, xmm0
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+40h], 0
      vaesenc xmm11, xmm11, xmm4
      vpxor   xmm5, xmm5, xmm6
      vpclmulqdq xmm6, xmm5, xmmword ptr [r8+0C0h], 0
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vpxor   xmm3, xmm3, xmm0
      vaesenc xmm14, xmm14, xmm4
      vpxor   xmm1, xmm1, xmm6
      vmovdqu xmm4, xmmword ptr [r9+50h]
      vmovdqu xmm6, [rsp+arg_48]
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+50h], 11h
      vpshufd xmm5, xmm6, 4Eh
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vpxor   xmm2, xmm2, xmm0
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+50h], 0
      vaesenc xmm11, xmm11, xmm4
      vpxor   xmm5, xmm5, xmm6
      vpclmulqdq xmm6, xmm5, xmmword ptr [r8+0D0h], 0
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vpxor   xmm3, xmm3, xmm0
      vaesenc xmm14, xmm14, xmm4
      vpxor   xmm1, xmm1, xmm6
      vmovdqu xmm4, xmmword ptr [r9+60h]
      vmovdqu xmm6, [rsp+arg_58]
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+60h], 11h
      vpshufd xmm5, xmm6, 4Eh
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vpxor   xmm2, xmm2, xmm0
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+60h], 0
      vaesenc xmm11, xmm11, xmm4
      vpxor   xmm5, xmm5, xmm6
      vpclmulqdq xmm6, xmm5, xmmword ptr [r8+0E0h], 0
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vpxor   xmm3, xmm3, xmm0
      vaesenc xmm14, xmm14, xmm4
      vpxor   xmm1, xmm1, xmm6
      vmovdqu xmm4, xmmword ptr [r9+70h]
      vmovdqu xmm6, [rsp+arg_68]
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+70h], 11h
      vpshufd xmm5, xmm6, 4Eh
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vpxor   xmm2, xmm2, xmm0
      vpclmulqdq xmm0, xmm6, xmmword ptr [r8+70h], 0
      vaesenc xmm11, xmm11, xmm4
      vpxor   xmm5, xmm5, xmm6
      vpclmulqdq xmm6, xmm5, xmmword ptr [r8+0F0h], 0
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vpxor   xmm3, xmm3, xmm0
      vaesenc xmm14, xmm14, xmm4
      vpxor   xmm1, xmm1, xmm6
      vmovdqu xmm4, xmmword ptr [r9+80h]
      vpxor   xmm1, xmm1, xmm2
      vaesenc xmm7, xmm7, xmm4
      vpxor   xmm1, xmm1, xmm3
      vaesenc xmm8, xmm8, xmm4
      vaesenc xmm9, xmm9, xmm4
      vpsrldq xmm0, xmm1, 8
      vpslldq xmm1, xmm1, 8
      vaesenc xmm10, xmm10, xmm4
      vaesenc xmm11, xmm11, xmm4
      vpxor   xmm5, xmm2, xmm0
      vpxor   xmm0, xmm3, xmm1
      vaesenc xmm12, xmm12, xmm4
      vpclmulqdq xmm2, xmm0, cs:xmmword_1ECF0, 0
      vpshufd xmm0, xmm0, 4Eh
      vaesenc xmm13, xmm13, xmm4
      vaesenc xmm14, xmm14, xmm4
      vmovdqu xmm4, xmmword ptr [r9+90h]
      vpxor   xmm0, xmm2, xmm0
      vaesenc xmm7, xmm7, xmm4
      vaesenc xmm8, xmm8, xmm4
      vpclmulqdq xmm2, xmm0, cs:xmmword_1ECF0, 0
      vpshufd xmm0, xmm0, 4Eh
      vaesenc xmm9, xmm9, xmm4
      vaesenc xmm10, xmm10, xmm4
      vaesenc xmm11, xmm11, xmm4
      vaesenc xmm12, xmm12, xmm4
      vaesenc xmm13, xmm13, xmm4
      vpxor   xmm0, xmm2, xmm0
      vaesenc xmm14, xmm14, xmm4
      vpxor   xmm0, xmm0, xmm5
      vmovdqu xmm6, xmmword ptr [r9+0A0h]
    }
    if ( a7 > 0xA0 )
    {
      __asm
      {
        vmovdqu xmm4, xmmword ptr [r9+0A0h]
        vaesenc xmm7, xmm7, xmm4
        vaesenc xmm8, xmm8, xmm4
        vaesenc xmm9, xmm9, xmm4
        vaesenc xmm10, xmm10, xmm4
        vaesenc xmm11, xmm11, xmm4
        vaesenc xmm12, xmm12, xmm4
        vaesenc xmm13, xmm13, xmm4
        vaesenc xmm14, xmm14, xmm4
        vmovdqu xmm4, xmmword ptr [r9+0B0h]
        vaesenc xmm7, xmm7, xmm4
        vaesenc xmm8, xmm8, xmm4
        vaesenc xmm9, xmm9, xmm4
        vaesenc xmm10, xmm10, xmm4
        vaesenc xmm11, xmm11, xmm4
        vaesenc xmm12, xmm12, xmm4
        vaesenc xmm13, xmm13, xmm4
        vaesenc xmm14, xmm14, xmm4
        vmovdqu xmm6, xmmword ptr [r9+0C0h]
      }
      if ( a7 > 0xC0 )
      {
        __asm
        {
          vmovdqu xmm4, xmmword ptr [r9+0C0h]
          vaesenc xmm7, xmm7, xmm4
          vaesenc xmm8, xmm8, xmm4
          vaesenc xmm9, xmm9, xmm4
          vaesenc xmm10, xmm10, xmm4
          vaesenc xmm11, xmm11, xmm4
          vaesenc xmm12, xmm12, xmm4
          vaesenc xmm13, xmm13, xmm4
          vaesenc xmm14, xmm14, xmm4
          vmovdqu xmm4, xmmword ptr [r9+0D0h]
          vaesenc xmm7, xmm7, xmm4
          vaesenc xmm8, xmm8, xmm4
          vaesenc xmm9, xmm9, xmm4
          vaesenc xmm10, xmm10, xmm4
          vaesenc xmm11, xmm11, xmm4
          vaesenc xmm12, xmm12, xmm4
          vaesenc xmm13, xmm13, xmm4
          vaesenc xmm14, xmm14, xmm4
          vmovdqu xmm6, xmmword ptr [r9+0E0h]
        }
      }
    }
    __asm
    {
      vmovdqu xmm4, xmmword ptr [rdi]
      vpxor   xmm4, xmm6, xmm4
      vaesenclast xmm7, xmm7, xmm4
      vmovdqu xmm4, xmmword ptr [rdi+10h]
      vpxor   xmm4, xmm6, xmm4
      vaesenclast xmm8, xmm8, xmm4
      vmovdqu xmm4, xmmword ptr [rdi+20h]
      vpxor   xmm4, xmm6, xmm4
      vaesenclast xmm9, xmm9, xmm4
      vmovdqu xmm4, xmmword ptr [rdi+30h]
      vpxor   xmm4, xmm6, xmm4
      vaesenclast xmm10, xmm10, xmm4
      vmovdqu xmm4, xmmword ptr [rdi+40h]
      vpxor   xmm4, xmm6, xmm4
      vaesenclast xmm11, xmm11, xmm4
      vmovdqu xmm4, xmmword ptr [rdi+50h]
      vpxor   xmm4, xmm6, xmm4
      vaesenclast xmm12, xmm12, xmm4
      vmovdqu xmm4, xmmword ptr [rdi+60h]
      vpxor   xmm4, xmm6, xmm4
      vaesenclast xmm13, xmm13, xmm4
      vmovdqu xmm4, xmmword ptr [rdi+70h]
      vpxor   xmm4, xmm6, xmm4
      vaesenclast xmm14, xmm14, xmm4
      vmovdqu xmm4, xmmword ptr cs:_Lbswap_mask_2
      vmovdqu xmmword ptr [rsi], xmm7
      vpshufb xmm7, xmm7, xmm4
      vmovdqu xmmword ptr [rsi+10h], xmm8
      vpshufb xmm8, xmm8, xmm4
      vmovdqu xmmword ptr [rsi+20h], xmm9
      vpshufb xmm9, xmm9, xmm4
      vmovdqu xmmword ptr [rsi+30h], xmm10
      vpshufb xmm10, xmm10, xmm4
      vmovdqu xmmword ptr [rsi+40h], xmm11
      vpshufb xmm11, xmm11, xmm4
      vmovdqu xmmword ptr [rsi+50h], xmm12
      vpshufb xmm12, xmm12, xmm4
      vmovdqu xmmword ptr [rsi+60h], xmm13
      vpshufb xmm13, xmm13, xmm4
      vmovdqu xmmword ptr [rsi+70h], xmm14
      vpshufb xmm14, xmm14, xmm4
    }
    _RDI += 128LL;
    _RSI += 128LL;
    v284 = __OFSUB__(a2, 128LL);
    a2 -= 128LL;
  }
  while ( !((a2 < 0) ^ v284) );
  return End_Main_Encrypt_Loop(
           _RDI,
           _RSI,
           a1,
           a2,
           _R8,
           _R9,
           a23,
           DWORD2(a23),
           a24,
           DWORD2(a24),
           v286,
           v287,
           a26,
           DWORD2(a26),
           v288,
           v291,
           v292,
           a29,
           DWORD2(a29),
           a30,
           DWORD2(a30),
           a31,
           a32,
           a33,
           a34,
           a35,
           a36,
           a37,
           a38,
           a39,
           a40,
           a41,
           a42,
           a43,
           a44,
           a45,
           a46,
           a47,
           a48,
           a49,
           a50,
           a51,
           a52,
           a53,
           a54,
           a55);
}
// 1F200: array has been used for an input argument
// 1F200: variables would overlap: ST08_16.16 and ST08_24.24
// 1F200: variables would overlap: ST20_8.8 and ST18_16.16
// 1F200: variables would overlap: ST20_16.16 and ST18_16.16
// 1F200: variables would overlap: ST18_16.16 and ST20_16.16
// 1F200: variables would overlap: ST30_8.8 and ST28_16.16
// 1F200: variables would overlap: ST20_16.16 and ST10_16.16
// 1F200: variables would overlap: ST30_16.16 and ST28_16.16
// 1F200: variables would overlap: ST28_16.16 and ST30_16.16
// 1F200: variables would overlap: ST40_8.8 and ST38_16.16
// 1F200: variables would overlap: ST20_32.32 and ST18_16.16
// 1F200: variables would overlap: ST18_16.16 and ST20_32.32
// 1F200: variables would overlap: ST20_32.32 and ST10_16.16
// 1F200: variables would overlap: ST40_16.16 and ST38_16.16
// 1F200: variables would overlap: ST38_16.16 and ST40_16.16
// 1F200: variables would overlap: ST50_8.8 and ST48_16.16
// 1F200: variables would overlap: ST40_16.16 and ST30_16.16
// 1F200: variables would overlap: ST50_16.16 and ST48_16.16
// 1F200: variables would overlap: ST48_16.16 and ST50_16.16
// 1F200: variables would overlap: ST40_32.32 and ST38_16.16
// 1F200: variables would overlap: ST38_16.16 and ST40_32.32
// 1F200: variables would overlap: ST40_32.32 and ST30_16.16
// 1F7F7: using guessed type __int64 __fastcall End_Main_Encrypt_Loop(_DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128);

//----- (000000000001F7F7) ----------------------------------------------------
#error "1FD24: positive sp value has been found (funcsize=0)"

//----- (000000000001FD30) ----------------------------------------------------
int __usercall gcmDecrypt_avx1@<eax>(__int64 _RDX@<rdx>, unsigned __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r9>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM4@<xmm4>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>, __int128 _XMM8@<xmm8>, __int128 _XMM9@<xmm9>, __int128 _XMM10@<xmm10>, __int128 _XMM11@<xmm11>, __int128 _XMM12@<xmm12>, __int128 _XMM13@<xmm13>, __int128 _XMM14@<xmm14>, __int128 _XMM15@<xmm15>)
{
  __int64 v39; // r10@1

  __asm
  {
    vmovdqa [rsp+110h+var_110], xmm0
    vmovdqa [rsp+110h+var_100], xmm1
    vmovdqa [rsp+110h+var_F0], xmm2
    vmovdqa [rsp+110h+var_E0], xmm3
    vmovdqa [rsp+110h+var_D0], xmm4
    vmovdqa [rsp+110h+var_C0], xmm5
    vmovdqa [rsp+110h+var_B0], xmm6
    vmovdqa [rsp+110h+var_A0], xmm7
    vmovdqa [rsp+110h+var_90], xmm8
    vmovdqa [rsp+110h+var_80], xmm9
    vmovdqa [rsp+110h+var_70], xmm10
    vmovdqa [rsp+110h+var_60], xmm11
    vmovdqa [rsp+110h+var_50], xmm12
    vmovdqa [rsp+110h+var_40], xmm13
    vmovdqa [rsp+110h+var_30], xmm14
    vmovdqa [rsp+110h+var_20], xmm15
    vmovdqu xmm15, xmmword ptr [rdx+20h]
    vmovdqu xmm0, xmmword ptr [rdx+10h]
  }
  v39 = *(_DWORD *)(a5 + 240);
  __asm
  {
    vpshufb xmm15, xmm15, xmmword ptr cs:_Lbswap_mask_2
    vpshufb xmm0, xmm0, xmmword ptr cs:_Lbswap_mask_2
  }
  JUMPOUT(a2 < 0x80, &loc_2041E);
  return Main_Decrypt_Loop_0(a3, a4, _RDX, a2 - 128);
}
// 1FE00: using guessed type int __fastcall Main_Decrypt_Loop_0(_QWORD, _QWORD, _QWORD, _QWORD);

//----- (000000000001FE00) ----------------------------------------------------
#error "20763: positive sp value has been found (funcsize=0)"

//----- (0000000000020764) ----------------------------------------------------
__int64 __fastcall ccdes_ltc_setup(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 result; // rax@2

  v4 = a4;
  if ( a3 == 8 )
  {
    deskey(a4, 0, (void *)a2);
    result = deskey(v4, 1u, (void *)(a2 + 128));
  }
  return result;
}

//----- (00000000000207A1) ----------------------------------------------------
__int64 __fastcall ltc_des_ecb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 v6; // r15@1
  unsigned int v8; // [sp+8h] [bp-38h]@2
  unsigned int v9; // [sp+Ch] [bp-34h]@2
  __int64 i; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v6 = a2;
  for ( i = *(_QWORD *)off_69010[0]; v6; --v6 )
  {
    v8 = _byteswap_ulong(*(_DWORD *)v5);
    v9 = _byteswap_ulong(*(_DWORD *)(v5 + 4));
    desfunc((__int64)&v8, a1 + 128);
    *(_DWORD *)v4 = _byteswap_ulong(v8);
    *(_DWORD *)(v4 + 4) = _byteswap_ulong(v9);
    v5 += 8LL;
    v4 += 8LL;
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002083E) ----------------------------------------------------
__int64 __fastcall ccec_sign_internal(__int64 a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 *v6; // r15@1
  __int64 v7; // r14@1
  __int64 v8; // rbx@1
  __int64 v9; // r13@3
  __int64 *v10; // r14@4
  __int64 v11; // rbx@4
  __int64 v12; // r12@4
  __int64 v13; // rdi@4
  __int64 v14; // r15@5
  signed __int64 v15; // rax@7
  signed __int64 *v16; // r14@8
  __int64 v17; // r12@8
  __int64 v18; // rbx@8
  __int64 v19; // rsi@8
  __int64 result; // rax@10
  size_t v21; // [sp+0h] [bp-90h]@1
  __int64 v22; // [sp+8h] [bp-88h]@3
  signed __int64 *v23; // [sp+10h] [bp-80h]@3
  __int64 v24; // [sp+18h] [bp-78h]@1
  __int64 v25; // [sp+20h] [bp-70h]@1
  __int64 v26; // [sp+28h] [bp-68h]@1
  __int64 v27; // [sp+30h] [bp-60h]@3
  size_t *v28; // [sp+38h] [bp-58h]@1
  __int64 v29; // [sp+40h] [bp-50h]@1
  unsigned int v30; // [sp+4Ch] [bp-44h]@1
  unsigned __int64 v31; // [sp+50h] [bp-40h]@1
  __int64 v32; // [sp+58h] [bp-38h]@1
  __int64 v33; // [sp+60h] [bp-30h]@1

  v29 = a6;
  v25 = a5;
  v32 = a4;
  v26 = a1;
  v33 = *(_QWORD *)off_69010[0];
  v6 = *(__int64 **)a1;
  v31 = **(_QWORD **)a1;
  v7 = 4 * v31;
  v24 = (__int64)((char *)&v21 - ((8 * v31 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v21 = 32 * v31 + 16;
  v8 = (__int64)((char *)&v21 - v21);
  v28 = (size_t *)((char *)&v21 - v21);
  v30 = ccn_read_uint(v31, (__int64)((char *)&v21 - ((8 * v31 + 15) & 0xFFFFFFFFFFFFFFF0LL)), a2, a3);
  if ( (v30 & 0x80000000) == 0 )
  {
    v30 = ccec_generate_key_internal_fips(v6, v29, v8);
    if ( (v30 & 0x80000000) == 0 )
    {
      v23 = &v6[v7 + 2];
      v9 = v8 + 16;
      v27 = (__int64)&v6[(4 * v31 | 2) + 2];
      v22 = v26 + 16;
      do
      {
        v10 = v6;
        v11 = v31;
        v12 = v27;
        v13 = v31;
        if ( (signed int)ccn_cmp(v31, v9, v27) < 0 )
        {
          v14 = v32;
          ccn_set(v13, (void *)v32, (const void *)v9);
        }
        else
        {
          v14 = v32;
          ccn_sub(v13, v32, v9, v12);
        }
        v32 = v14;
        v15 = ccn_n(v11, v14);
        v6 = v10;
        v8 = (__int64)v28;
        if ( v15 )
        {
          v16 = v23;
          cczp_mod_inv((__int64)v23, (void *)(v9 + 24LL * *(_QWORD *)*v28), (const void *)(v9 + 24LL * *(_QWORD *)*v28));
          v17 = v8;
          v18 = v25;
          cczp_mul(v16, v25, v22 + 24LL * **(_QWORD **)v26, (unsigned __int64 *)v32);
          cczp_add(v16, v18, v24, v18);
          cczp_mul(v16, v18, v9 + 24LL * **(_QWORD **)v17, (unsigned __int64 *)v18);
          v19 = v18;
          v8 = v17;
          if ( ccn_n(v31, v19) )
            break;
        }
        v30 = ccec_generate_key_internal_fips(v6, v29, v8);
      }
      while ( (v30 & 0x80000000) == 0 );
    }
  }
  cc_clear(v21, (void *)v8);
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v33 )
    result = v30;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000020A57) ----------------------------------------------------
__int64 __fastcall ltc_des_ecb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 v6; // r15@1
  unsigned int v8; // [sp+8h] [bp-38h]@2
  unsigned int v9; // [sp+Ch] [bp-34h]@2
  __int64 i; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v6 = a2;
  for ( i = *(_QWORD *)off_69010[0]; v6; --v6 )
  {
    v8 = _byteswap_ulong(*(_DWORD *)v5);
    v9 = _byteswap_ulong(*(_DWORD *)(v5 + 4));
    desfunc((__int64)&v8, a1);
    *(_DWORD *)v4 = _byteswap_ulong(v8);
    *(_DWORD *)(v4 + 4) = _byteswap_ulong(v9);
    v5 += 8LL;
    v4 += 8LL;
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000020AF0) ----------------------------------------------------
__int64 __usercall ccdigest@<rax>(__int64 a1@<rax>, const void *a2@<rdx>, __int64 a3@<rcx>, __int64 a4@<rdi>, unsigned __int64 a5@<rsi>)
{
  __int64 v5; // r14@1
  const void *v6; // r15@1
  __int64 v7; // r13@1
  char *v8; // rbx@1
  __int64 v9; // rax@1
  __int64 v11; // [sp+0h] [bp-30h]@1

  v11 = a1;
  v5 = a3;
  v6 = a2;
  v7 = a4;
  v11 = *(_QWORD *)off_69010[0];
  v8 = (char *)&v11
     - ((((*(_QWORD *)(a4 + 8) + *(_QWORD *)(a4 + 16) + 19LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v9 = ccdigest_init(a4, (__int64)v8);
  ccdigest_update(v9, a5, v6, a4, (__int64)v8);
  (*(void (__fastcall **)(__int64, char *, __int64))(v7 + 56))(v7, v8, v5);
  cc_clear(*(_QWORD *)(v7 + 8) + *(_QWORD *)(v7 + 16) + 12LL, v8);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000020B99) ----------------------------------------------------
unsigned __int64 __fastcall ccec_signature_r_s_size(__int64 **a1)
{
  return (unsigned __int64)(ccn_bitlen(**a1, (__int64)(*a1 + 2)) + 7) >> 3;
}

//----- (0000000000020BB6) ----------------------------------------------------
__int64 __fastcall ccwrap_auth_decrypt(__int64 a1, __int64 a2, unsigned __int64 a3, const void *a4, __int64 a5, signed __int64 a6)
{
  __int64 v6; // r14@1
  signed int v7; // er15@1
  unsigned __int64 v8; // r12@2
  const void *v9; // r14@5
  char *v10; // rcx@5
  signed __int64 v11; // rbx@7
  const void *v12; // rbx@9
  signed __int64 v13; // r15@9
  void *v14; // r14@9
  signed __int64 v15; // r12@9
  const void *v18; // r13@10
  bool v19; // zf@11
  bool v20; // sf@11
  __int64 result; // rax@14
  unsigned __int64 v22; // [sp+8h] [bp-A8h]@5
  __int64 v23; // [sp+10h] [bp-A0h]@5
  void *v24; // [sp+18h] [bp-98h]@5
  signed __int64 v25; // [sp+20h] [bp-90h]@5
  signed __int64 v26; // [sp+28h] [bp-88h]@5
  signed __int64 v27; // [sp+30h] [bp-80h]@1
  signed __int64 v28; // [sp+38h] [bp-78h]@2
  signed __int64 v29; // [sp+40h] [bp-70h]@5
  signed __int64 v30; // [sp+48h] [bp-68h]@5
  signed __int64 v31; // [sp+50h] [bp-60h]@5
  signed __int64 v32; // [sp+58h] [bp-58h]@5
  __int64 v33; // [sp+70h] [bp-40h]@5
  char v34; // [sp+78h] [bp-38h]@5
  __int64 v35; // [sp+80h] [bp-30h]@1

  v27 = a6;
  v6 = off_69010[0];
  v35 = *(_QWORD *)off_69010[0];
  v7 = -1;
  if ( *(_QWORD *)(a1 + 8) == 16LL )
  {
    v8 = a3 >> 3;
    v28 = (a3 >> 3) - 1;
    if ( (unsigned __int64)v28 >= 2 && (!(a3 & 7) || (unsigned __int64)v28 <= 0x20000000) )
    {
      v22 = a3;
      v23 = a5;
      v9 = a4;
      memcpy(&v33, a4, 8uLL);
      v10 = &v34;
      v26 = (signed __int64)((char *)v9 + 8);
      v25 = 8 * v8 - 16;
      v24 = (void *)(v27 + 8 * v8 - 16);
      v31 = 6 * v8 - 6;
      v30 = 1 - v8;
      v29 = v8 - 2;
      v32 = 5LL;
      do
      {
        if ( v29 >= 0 )
        {
          v11 = v27;
          if ( v32 == 5 )
            v11 = v26;
          v12 = (const void *)(v25 + v11);
          v13 = v31;
          v14 = v24;
          v15 = v28;
          do
          {
            _RAX = v13;
            __asm { bswap   rax }
            v33 ^= _RAX;
            v18 = v10;
            memcpy(v10, v12, 8uLL);
            (*(void (__fastcall **)(__int64, signed __int64, __int64 *, __int64 *))(a1 + 24))(a2, 1LL, &v33, &v33);
            memcpy(v14, v18, 8uLL);
            v10 = (char *)v18;
            --v15;
            v12 = (char *)v12 - 8;
            v14 = (char *)v14 - 8;
            --v13;
          }
          while ( v15 > 0 );
        }
        v31 += v30;
        v19 = v32 == 0;
        v20 = v32-- < 0;
      }
      while ( !v20 && !v19 );
      v6 = off_69010[0];
      v7 = -1;
      if ( v33 == -6438275382588823898LL )
      {
        *(_QWORD *)v23 = v22 - 8;
        v7 = 0;
      }
    }
  }
  cc_clear(0x10uLL, &v33);
  result = *(_QWORD *)v6;
  if ( *(_QWORD *)v6 == v35 )
    result = (unsigned int)v7;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000020DB2) ----------------------------------------------------
void __fastcall ccmode_ccm_crypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r9@1
  __int64 v6; // rbx@1
  unsigned __int64 v7; // rsi@2
  unsigned __int64 v8; // r14@2
  unsigned __int64 v9; // rdx@2
  unsigned __int64 v10; // rdi@2
  __int64 v11; // r12@2
  unsigned __int64 v12; // rax@3
  bool v13; // zf@4
  __int64 v14; // r13@6
  __int64 v15; // r15@6
  unsigned __int64 v16; // [sp+8h] [bp-58h]@2
  signed __int64 v17; // [sp+18h] [bp-48h]@2
  unsigned __int64 v18; // [sp+20h] [bp-40h]@2
  __int64 v19; // [sp+28h] [bp-38h]@2
  __int64 v20; // [sp+30h] [bp-30h]@1

  v20 = a4;
  v5 = a3;
  v6 = a2;
  if ( a3 )
  {
    v19 = a1;
    v7 = *(_QWORD *)(*(_QWORD *)a1 + 8LL);
    v18 = v7;
    v8 = v6 + *(_QWORD *)(v6 + 80);
    v17 = a1 + 8;
    LODWORD(v9) = *(_DWORD *)(v6 + 68);
    v10 = v6 + v7;
    v16 = v6 + v7;
    v11 = 0LL;
    do
    {
      v12 = v10;
      if ( !(_DWORD)v9 )
      {
        do
        {
          v13 = (*(_BYTE *)(v12 - 1))++ == -1;
          if ( !v13 )
            break;
          --v12;
        }
        while ( v12 > v8 );
        v14 = a5;
        v15 = v5;
        (*(void (__fastcall **)(signed __int64, signed __int64, __int64, signed __int64))(*(_QWORD *)v19 + 24LL))(
          v17,
          1LL,
          v6,
          v6 + 48);
        v10 = v16;
        v7 = v18;
        v5 = v15;
        a4 = v20;
        a5 = v14;
        LODWORD(v9) = *(_DWORD *)(v6 + 68);
      }
      *(_BYTE *)(a5 + v11) = *(_BYTE *)(a4 + v11) ^ *(_BYTE *)(v6 + (unsigned int)v9 + 48);
      v9 = (unsigned int)(*(_DWORD *)(v6 + 68) + 1) % v7;
      *(_DWORD *)(v6 + 68) = v9;
      ++v11;
    }
    while ( v11 != v5 );
  }
}

//----- (0000000000020E85) ----------------------------------------------------
void __fastcall ccmode_ccm_encrypt(__int64 a1, __int64 a2, unsigned __int64 a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  __int64 v6; // r15@1
  unsigned __int64 v7; // r12@1
  int v8; // eax@1
  int v9; // edx@1

  v5 = a5;
  v6 = a4;
  v7 = a3;
  v8 = *(_DWORD *)(a2 + 64);
  v9 = 0;
  if ( v8 != 2 )
  {
    if ( v8 != 1 )
      return;
    v9 = *(_DWORD *)(a2 + 72) != 0;
    *(_DWORD *)(a2 + 64) = 2;
  }
  ccmode_ccm_macdata(a1, a2, v9, v7, a4);
  ccmode_ccm_crypt(a1, a2, v7, v6, v5);
}

//----- (0000000000020F04) ----------------------------------------------------
int __fastcall ccdigest_final_64be(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r14@1
  __int64 v4; // rbx@1
  __int64 v5; // rax@1
  signed __int64 v6; // rcx@1
  __int64 v7; // rdx@1
  __int64 v8; // rax@1
  __int64 v9; // rdx@1
  signed __int64 v10; // rcx@1
  unsigned int v11; // esi@1
  signed __int64 v12; // rdx@4
  signed __int64 v13; // rsi@4
  __int64 v14; // rdx@8
  unsigned int v15; // ecx@8
  signed __int64 v16; // rdx@9
  __int64 v17; // rcx@10
  signed __int64 v21; // rdx@11
  signed __int64 v22; // rbx@11
  int result; // eax@11
  unsigned int v24; // ecx@12
  unsigned __int64 v25; // rdx@12

  v3 = a3;
  v4 = a2;
  v5 = *(_QWORD *)(a1 + 8);
  v6 = *(_QWORD *)(a1 + 16) + v5 + 8;
  *(_QWORD *)v4 += (unsigned int)(8 * *(_DWORD *)(a2 + v6));
  v7 = *(_DWORD *)(a2 + v6);
  *(_DWORD *)(a2 + v6) = v7 + 1;
  *(_BYTE *)(a2 + v5 + v7 + 8) = -128;
  v8 = *(_QWORD *)(a1 + 8);
  v9 = *(_QWORD *)(a1 + 16);
  v10 = v8 + v9 + 8;
  v11 = *(_DWORD *)(a2 + v10);
  if ( v11 >= 0x39 )
  {
    if ( v11 > 0x3F )
    {
      v12 = v8 + 8;
    }
    else
    {
      v10 += v4;
      do
      {
        *(_DWORD *)v10 = v11 + 1;
        *(_BYTE *)(v4 + v8 + v11 + 8) = 0;
        v8 = *(_QWORD *)(a1 + 8);
        v12 = v8 + 8;
        v13 = v8 + 8 + *(_QWORD *)(a1 + 16);
        v10 = v4 + v13;
        v11 = *(_DWORD *)(v4 + v13);
      }
      while ( v11 < 0x40 );
    }
    (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(a1 + 48))(
      v4 + 8,
      1LL,
      v4 + v12,
      v10);
    v8 = *(_QWORD *)(a1 + 8);
    v9 = *(_QWORD *)(a1 + 16);
    *(_DWORD *)(v4 + v8 + v9 + 8) = 0;
  }
  v14 = v8 + v9;
  v15 = *(_DWORD *)(v4 + v14 + 8);
  if ( v15 <= 0x37 )
  {
    v16 = v4 + v14 + 8;
    do
    {
      *(_DWORD *)v16 = v15 + 1;
      *(_BYTE *)(v4 + v8 + v15 + 8) = 0;
      v8 = *(_QWORD *)(a1 + 8);
      v17 = v8 + *(_QWORD *)(a1 + 16);
      v16 = v4 + v17 + 8;
      v15 = *(_DWORD *)v16;
    }
    while ( v15 < 0x38 );
  }
  _RCX = *(_QWORD *)v4;
  __asm { bswap   rcx }
  *(_QWORD *)(v8 + v4 + 64) = _RCX;
  __asm { bswap   rcx }
  v21 = *(_QWORD *)(a1 + 8) + v4 + 8;
  v22 = v4 + 8;
  result = (*(int (__fastcall **)(signed __int64, signed __int64, signed __int64, __int64))(a1 + 48))(
             v22,
             1LL,
             v21,
             _RCX);
  if ( *(_QWORD *)a1 >= 4uLL )
  {
    result = 1;
    v24 = 0;
    v25 = 0LL;
    do
    {
      *(_DWORD *)(v3 + v24) = _byteswap_ulong(*(_DWORD *)(v22 + 4 * v25));
      v25 = (unsigned int)result++;
      v24 += 4;
    }
    while ( v25 < *(_QWORD *)a1 >> 2 );
  }
  return result;
}

//----- (0000000000021050) ----------------------------------------------------
int __fastcall ccdigest_final_64le(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r14@1
  __int64 v4; // r12@1
  __int64 v5; // rax@1
  signed __int64 v6; // rcx@1
  __int64 v7; // rdx@1
  __int64 v8; // rax@1
  __int64 v9; // rdx@1
  signed __int64 v10; // rcx@1
  unsigned int v11; // esi@1
  signed __int64 v12; // rdx@4
  signed __int64 v13; // rsi@4
  __int64 v14; // rdx@8
  unsigned int v15; // ecx@8
  signed __int64 v16; // rdx@9
  __int64 v17; // rcx@10
  signed __int64 v18; // rdx@11
  signed __int64 v19; // r12@11
  int result; // eax@11
  unsigned int v21; // ecx@12
  unsigned __int64 v22; // rdx@12

  v3 = a3;
  v4 = a2;
  v5 = *(_QWORD *)(a1 + 8);
  v6 = *(_QWORD *)(a1 + 16) + v5 + 8;
  *(_QWORD *)v4 += (unsigned int)(8 * *(_DWORD *)(a2 + v6));
  v7 = *(_DWORD *)(a2 + v6);
  *(_DWORD *)(a2 + v6) = v7 + 1;
  *(_BYTE *)(a2 + v5 + v7 + 8) = -128;
  v8 = *(_QWORD *)(a1 + 8);
  v9 = *(_QWORD *)(a1 + 16);
  v10 = v8 + v9 + 8;
  v11 = *(_DWORD *)(a2 + v10);
  if ( v11 >= 0x39 )
  {
    if ( v11 > 0x3F )
    {
      v12 = v8 + 8;
    }
    else
    {
      v10 += v4;
      do
      {
        *(_DWORD *)v10 = v11 + 1;
        *(_BYTE *)(v4 + v8 + v11 + 8) = 0;
        v8 = *(_QWORD *)(a1 + 8);
        v12 = v8 + 8;
        v13 = v8 + 8 + *(_QWORD *)(a1 + 16);
        v10 = v4 + v13;
        v11 = *(_DWORD *)(v4 + v13);
      }
      while ( v11 < 0x40 );
    }
    (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(a1 + 48))(
      v4 + 8,
      1LL,
      v4 + v12,
      v10);
    v8 = *(_QWORD *)(a1 + 8);
    v9 = *(_QWORD *)(a1 + 16);
    *(_DWORD *)(v4 + v8 + v9 + 8) = 0;
  }
  v14 = v8 + v9;
  v15 = *(_DWORD *)(v4 + v14 + 8);
  if ( v15 <= 0x37 )
  {
    v16 = v4 + v14 + 8;
    do
    {
      *(_DWORD *)v16 = v15 + 1;
      *(_BYTE *)(v4 + v8 + v15 + 8) = 0;
      v8 = *(_QWORD *)(a1 + 8);
      v17 = v8 + *(_QWORD *)(a1 + 16);
      v16 = v4 + v17 + 8;
      v15 = *(_DWORD *)v16;
    }
    while ( v15 < 0x38 );
  }
  *(_BYTE *)(v8 + v4 + 71) = *(_BYTE *)(v4 + 7);
  *(_BYTE *)(*(_QWORD *)(a1 + 8) + v4 + 70) = *(_BYTE *)(v4 + 6);
  *(_BYTE *)(*(_QWORD *)(a1 + 8) + v4 + 69) = *(_BYTE *)(v4 + 5);
  *(_BYTE *)(*(_QWORD *)(a1 + 8) + v4 + 68) = *(_BYTE *)(v4 + 4);
  *(_BYTE *)(*(_QWORD *)(a1 + 8) + v4 + 67) = *(_BYTE *)(v4 + 3);
  *(_BYTE *)(*(_QWORD *)(a1 + 8) + v4 + 66) = *(_BYTE *)(v4 + 2);
  *(_BYTE *)(*(_QWORD *)(a1 + 8) + v4 + 65) = *(_BYTE *)(v4 + 1);
  *(_BYTE *)(*(_QWORD *)(a1 + 8) + v4 + 64) = *(_BYTE *)v4;
  v18 = *(_QWORD *)(a1 + 8) + v4 + 8;
  v19 = v4 + 8;
  result = (*(int (__fastcall **)(signed __int64, signed __int64, signed __int64))(a1 + 48))(v19, 1LL, v18);
  if ( *(_QWORD *)a1 >= 4uLL )
  {
    result = 1;
    v21 = 0;
    v22 = 0LL;
    do
    {
      *(_BYTE *)(v3 + (v21 | 3LL)) = *(_BYTE *)(v19 + 4 * v22 + 3);
      *(_BYTE *)(v3 + (v21 | 2LL)) = *(_BYTE *)(v19 + 4 * v22 + 2);
      *(_BYTE *)(v3 + (v21 | 1LL)) = *(_BYTE *)(v19 + 4 * v22 + 1);
      *(_BYTE *)(v3 + v21) = *(_BYTE *)(v19 + 4 * v22);
      v22 = (unsigned int)result++;
      v21 += 4;
    }
    while ( v22 < *(_QWORD *)a1 >> 2 );
  }
  return result;
}

//----- (000000000002122E) ----------------------------------------------------
int __fastcall ccdigest_final_common(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rbx@1
  __int64 v4; // r15@1
  __int64 v5; // rax@1
  signed __int64 v6; // rcx@1
  __int64 v7; // r14@1
  __int64 v8; // rdx@1
  __int64 v9; // rdx@1
  unsigned __int64 v10; // rax@1
  signed __int64 v11; // rcx@1
  unsigned __int64 v12; // rsi@1
  int v13; // eax@3
  unsigned __int64 v14; // rax@4
  signed __int64 v15; // rsi@4
  bool v16; // cf@4
  unsigned __int64 v17; // rcx@6
  signed __int64 v18; // rsi@7
  int v19; // edi@7
  signed __int64 v23; // r12@12
  int result; // eax@12
  unsigned __int64 v25; // rcx@12
  unsigned int v26; // ecx@14
  unsigned __int64 v27; // rdx@14
  unsigned int v28; // ecx@18
  unsigned __int64 v29; // rdx@18
  signed int v30; // ecx@21
  unsigned __int64 v31; // rdx@21

  v3 = a2;
  v4 = a1;
  v5 = *(_QWORD *)(a1 + 8);
  v6 = *(_QWORD *)(a1 + 16) + v5 + 8;
  *(_QWORD *)v3 += (unsigned int)(8 * *(_DWORD *)(a2 + v6));
  v7 = a3;
  v8 = *(_DWORD *)(a2 + v6);
  *(_DWORD *)(a2 + v6) = v8 + 1;
  *(_BYTE *)(a2 + v5 + v8 + 8) = -128;
  v9 = *(_QWORD *)(a1 + 8);
  v10 = *(_QWORD *)(a1 + 16);
  v11 = v9 + v10 + 8;
  v12 = *(_DWORD *)(a2 + v11);
  if ( v12 > v10 - 8 )
  {
    if ( v12 < v10 )
    {
      v11 += v3;
      v13 = v12;
      do
      {
        *(_DWORD *)v11 = v13 + 1;
        *(_BYTE *)(v3 + v12 + v9 + 8) = 0;
        v9 = *(_QWORD *)(a1 + 8);
        v14 = *(_QWORD *)(a1 + 16);
        v15 = v9 + v14 + 8;
        v11 = v3 + v15;
        v12 = *(_DWORD *)(v3 + v15);
        v16 = v12 < v14;
        v13 = v12;
      }
      while ( v16 );
    }
    (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(a1 + 48))(
      v3 + 8,
      1LL,
      v3 + v9 + 8,
      v11);
    v9 = *(_QWORD *)(a1 + 8);
    v10 = *(_QWORD *)(a1 + 16);
    *(_DWORD *)(v3 + v9 + v10 + 8) = 0;
  }
  v17 = *(_DWORD *)(v3 + v9 + v10 + 8);
  if ( v17 < v10 - 8 )
  {
    v18 = v3 + v9 + v10 + 8;
    v19 = v17;
    do
    {
      *(_DWORD *)v18 = v19 + 1;
      *(_BYTE *)(v3 + v17 + v9 + 8) = 0;
      v9 = *(_QWORD *)(v4 + 8);
      v10 = *(_QWORD *)(v4 + 16);
      v18 = v3 + v9 + v10 + 8;
      v17 = *(_DWORD *)v18;
      v19 = *(_DWORD *)v18;
    }
    while ( v17 < v10 - 8 );
  }
  _RCX = *(_QWORD *)v3;
  if ( *(_QWORD *)v4 > 0x10uLL )
  {
    __asm { bswap   rcx }
    *(_QWORD *)(v3 + v9 + v10) = _RCX;
    __asm { bswap   rcx }
  }
  else
  {
    *(_BYTE *)(v3 + v10 + v9 + 7) = BYTE7(_RCX);
    *(_BYTE *)(v3 + *(_QWORD *)(v4 + 16) + *(_QWORD *)(v4 + 8) + 6) = *(_BYTE *)(v3 + 6);
    *(_BYTE *)(v3 + *(_QWORD *)(v4 + 16) + *(_QWORD *)(v4 + 8) + 5) = *(_BYTE *)(v3 + 5);
    *(_BYTE *)(v3 + *(_QWORD *)(v4 + 16) + *(_QWORD *)(v4 + 8) + 4) = *(_BYTE *)(v3 + 4);
    *(_BYTE *)(v3 + *(_QWORD *)(v4 + 16) + *(_QWORD *)(v4 + 8) + 3) = *(_BYTE *)(v3 + 3);
    *(_BYTE *)(v3 + *(_QWORD *)(v4 + 16) + *(_QWORD *)(v4 + 8) + 2) = *(_BYTE *)(v3 + 2);
    *(_BYTE *)(v3 + *(_QWORD *)(v4 + 16) + *(_QWORD *)(v4 + 8) + 1) = *(_BYTE *)(v3 + 1);
    _RCX = *(_QWORD *)(v4 + 8) + *(_QWORD *)(v4 + 16);
    *(_BYTE *)(v3 + _RCX) = *(_BYTE *)v3;
  }
  v23 = v3 + 8;
  result = (*(int (__fastcall **)(signed __int64, signed __int64, signed __int64, __int64))(v4 + 48))(
             v3 + 8,
             1LL,
             *(_QWORD *)(v4 + 8) + v3 + 8,
             _RCX);
  v25 = *(_QWORD *)v4;
  if ( *(_QWORD *)v4 > 0x10uLL )
  {
    result = 0;
    if ( v25 > 0x20 )
    {
      v30 = 1;
      v31 = 0LL;
      do
      {
        _RDX = *(_QWORD *)(v3 + 8 * v31 + 8);
        __asm { bswap   rdx }
        *(_QWORD *)(v7 + (unsigned int)result) = _RDX;
        __asm { bswap   rdx }
        v31 = (unsigned int)v30++;
        result += 8;
      }
      while ( v31 < *(_QWORD *)v4 >> 3 );
    }
    else
    {
      result = 1;
      v28 = 0;
      v29 = 0LL;
      do
      {
        *(_DWORD *)(v7 + v28) = _byteswap_ulong(*(_DWORD *)(v23 + 4 * v29));
        v29 = (unsigned int)result++;
        v28 += 4;
      }
      while ( v29 < *(_QWORD *)v4 >> 2 );
    }
  }
  else if ( v25 >= 4 )
  {
    result = 1;
    v26 = 0;
    v27 = 0LL;
    do
    {
      *(_BYTE *)(v7 + (v26 | 3LL)) = *(_BYTE *)(v23 + 4 * v27 + 3);
      *(_BYTE *)(v7 + (v26 | 2LL)) = *(_BYTE *)(v23 + 4 * v27 + 2);
      *(_BYTE *)(v7 + (v26 | 1LL)) = *(_BYTE *)(v23 + 4 * v27 + 1);
      *(_BYTE *)(v7 + v26) = *(_BYTE *)(v23 + 4 * v27);
      v27 = (unsigned int)result++;
      v26 += 4;
    }
    while ( v27 < *(_QWORD *)v4 >> 2 );
  }
  return result;
}

//----- (000000000002149A) ----------------------------------------------------
__int64 __fastcall ccdigest_init(__int64 a1, __int64 a2)
{
  __int64 v2; // rbx@1
  __int64 result; // rax@1

  v2 = a1;
  memcpy((void *)(a2 + 8), *(const void **)(v2 + 40), *(_QWORD *)(v2 + 8));
  *(_QWORD *)a2 = 0LL;
  result = *(_QWORD *)(v2 + 16) + *(_QWORD *)(v2 + 8);
  *(_DWORD *)(a2 + result + 8) = 0;
  return result;
}

//----- (00000000000214D5) ----------------------------------------------------
int __fastcall ccdigest_test(size_t *a1, unsigned __int64 a2, const void *a3, const void *a4)
{
  const void *v4; // r14@1
  __int64 v5; // r12@1
  __int64 v6; // rax@1
  char *v7; // rbx@1
  int result; // eax@1
  __int64 v9; // rcx@1
  __int64 v10; // [sp+0h] [bp-30h]@1
  __int64 v11; // [sp+8h] [bp-28h]@1

  v4 = a4;
  v5 = off_69010[0];
  v11 = *(_QWORD *)off_69010[0];
  v6 = (*a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v7 = (char *)&v10 - v6;
  ccdigest(v6, a3, (__int64)((char *)&v10 - v6), (__int64)a1, a2);
  result = memcmp(v7, v4, *a1);
  v9 = *(_QWORD *)v5;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002153F) ----------------------------------------------------
int __fastcall ccdigest_test_chunk(__int64 a1, unsigned __int64 a2, const void *a3, void *a4, unsigned __int64 a5)
{
  unsigned __int64 v5; // r13@1
  const void *v6; // r12@1
  unsigned __int64 v7; // rbx@1
  __int64 v8; // r14@1
  __int64 v9; // rax@1
  const void *v10; // rbx@4
  int result; // eax@4
  __int64 v12; // rcx@4
  void *v13; // [sp+0h] [bp-40h]@1
  void *v14; // [sp+8h] [bp-38h]@1
  __int64 v15; // [sp+10h] [bp-30h]@1

  v5 = a5;
  v14 = a4;
  v6 = a3;
  v7 = a2;
  v15 = *(_QWORD *)off_69010[0];
  v8 = (__int64)((char *)&v13
               - ((((*(_QWORD *)(a1 + 8) + *(_QWORD *)(a1 + 16) + 19LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v13 = (char *)&v13 - ((*(_QWORD *)a1 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  v9 = ccdigest_init(a1, v8);
  while ( v7 > v5 )
  {
    LODWORD(v9) = ccdigest_update(v9, v5, v6, a1, v8);
    v6 = (char *)v6 + v5;
    v7 -= v5;
  }
  ccdigest_update(v9, v7, v6, a1, v8);
  v10 = v13;
  (*(void (__fastcall **)(__int64, __int64, void *))(a1 + 56))(a1, v8, v13);
  result = memcmp(v10, v14, *(_QWORD *)a1);
  v12 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002161D) ----------------------------------------------------
int __fastcall ccdigest_test_vector(size_t *a1, __int64 a2)
{
  return ccdigest_test(a1, *(_QWORD *)a2, *(const void **)(a2 + 8), *(const void **)(a2 + 16));
}

//----- (0000000000021635) ----------------------------------------------------
int __fastcall ccdigest_test_chunk_vector(__int64 a1, __int64 a2, unsigned __int64 a3)
{
  return ccdigest_test_chunk(a1, *(_QWORD *)a2, *(const void **)(a2 + 8), *(void **)(a2 + 16), a3);
}

//----- (0000000000021650) ----------------------------------------------------
signed __int64 __fastcall ccrsa_oaep_decode_parameter(__int64 a1, __int64 a2, void *a3, __int64 a4, __int64 a5, unsigned __int64 a6, const void *a7)
{
  __int64 v7; // rsi@1
  size_t v8; // r13@1
  signed __int64 v9; // rax@1
  unsigned __int64 v10; // rax@1
  unsigned __int64 v11; // rax@1
  char *v12; // r15@1
  unsigned __int64 v13; // r12@1
  unsigned __int64 v14; // rax@1
  char *v15; // rdx@1
  signed __int64 v16; // r14@1
  unsigned __int64 v17; // r9@1
  __int64 v18; // r10@1
  unsigned __int64 v19; // rax@2
  bool v24; // cf@3
  const void *v27; // r14@6
  signed __int64 result; // rax@6
  const void *v29; // rsi@8
  __int64 v30; // rax@8
  void *v31; // rbx@8
  void *v32; // rbx@10
  __int64 v33; // rax@11
  __int64 v34; // rbx@14
  void *v35; // rdi@18
  char v36; // al@20
  bool v37; // zf@23
  size_t v38; // rdx@24
  const void *v39; // rsi@25
  __int64 v40; // rbx@25
  __int64 v41; // [sp+0h] [bp-90h]@1
  void *v42; // [sp+8h] [bp-88h]@8
  __int64 v43; // [sp+10h] [bp-80h]@8
  __int64 v44; // [sp+18h] [bp-78h]@1
  void *v45; // [sp+20h] [bp-70h]@1
  unsigned __int64 v46; // [sp+28h] [bp-68h]@1
  unsigned __int64 v47; // [sp+30h] [bp-60h]@1
  void *v48; // [sp+38h] [bp-58h]@1
  void *v49; // [sp+40h] [bp-50h]@1
  void *v50; // [sp+48h] [bp-48h]@1
  unsigned __int64 v51; // [sp+50h] [bp-40h]@1
  size_t v52; // [sp+58h] [bp-38h]@1
  __int64 v53; // [sp+60h] [bp-30h]@1

  v46 = a6;
  v45 = a3;
  v44 = a2;
  v7 = off_69010[0];
  v53 = *(_QWORD *)off_69010[0];
  v52 = a4 - 1;
  v8 = *(_QWORD *)a1;
  v50 = (char *)&v41 - ((*(_QWORD *)a1 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  v9 = a4 - 1 - v8;
  v51 = v9;
  v10 = (unsigned __int64)(v9 + 7) >> 3;
  v47 = v10;
  v11 = (8 * v10 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v12 = (char *)&v41 - v11;
  v49 = (char *)&v41 - v11;
  v13 = (v8 + 7) >> 3;
  v14 = (8 * v13 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v15 = (char *)v49 - v14;
  v48 = (char *)&v41 - v14;
  v16 = a4 + 7;
  v17 = (unsigned __int64)(a4 + 7) >> 3;
  v18 = a5;
  if ( (signed __int64)(v17 - 1) > 0 )
  {
    v19 = (v16 & 0xFFFFFFFFFFFFFFF8LL) + a5 - 16;
    v18 = a5;
    do
    {
      _R11 = *(_QWORD *)v18;
      __asm { bswap   r11 }
      _RBX = *(_QWORD *)(v19 + 8);
      __asm { bswap   rbx }
      *(_QWORD *)v18 = _RBX;
      *(_QWORD *)(v19 + 8) = _R11;
      v18 += 8LL;
      v24 = v18 < v19;
      v19 -= 8LL;
    }
    while ( v24 );
  }
  if ( v17 & 1 )
  {
    _RAX = *(_QWORD *)v18;
    __asm { bswap   rax }
    *(_QWORD *)v18 = _RAX;
  }
  v27 = (const void *)((v16 & 0xFFFFFFFFFFFFFFF8LL) - a4);
  result = 0xFFFFFFFFLL;
  if ( !*((_BYTE *)v27 + a5) )
  {
    if ( v52 > 2 * v8 + 1 )
    {
      v42 = v15;
      v41 = a5;
      memcpy(v15, (char *)v27 + a5 + 1, v8);
      v43 = a1;
      v29 = (char *)v27 + *(_QWORD *)a1 + v41 + 1;
      v27 = v42;
      v30 = (__int64)memcpy(v12, v29, v52 - *(_QWORD *)a1);
      ccdigest(v30, a7, (__int64)v50, a1, v46);
      v31 = v48;
      ccmgf(a1, *(_QWORD *)a1, v48, v51, v12);
      for ( ; v13; --v13 )
        *((_QWORD *)v27 + v13 - 1) ^= *((_QWORD *)v31 + v13 - 1);
      v32 = v49;
      ccmgf(v43, v51, v49, *(_QWORD *)v43, v27);
      v13 = v47;
      if ( v47 )
      {
        v33 = v47;
        do
        {
          *(_QWORD *)&v12[8 * v33 - 8] ^= *((_QWORD *)v32 + v33 - 1);
          --v33;
        }
        while ( v33 );
      }
      if ( !memcmp(v50, v12, v8) )
        goto LABEL_18;
      v34 = off_69010[0];
      result = 0xFFFFFFFFLL;
      goto LABEL_17;
    }
    result = 4294967294LL;
  }
  v34 = v7;
LABEL_17:
  while ( *(_QWORD *)v34 != v53 )
  {
LABEL_18:
    v8 += (size_t)v12;
    v34 = off_69010[0];
    v35 = v45;
    while ( v8 < (unsigned __int64)&v12[v51 - 1] )
    {
      v36 = *(_BYTE *)v8++;
      if ( v36 )
        goto LABEL_23;
    }
    v36 = *(_BYTE *)v8++;
LABEL_23:
    v37 = v36 == 1;
    result = 0xFFFFFFFFLL;
    if ( v37 )
    {
      v38 = (size_t)(&v12[v51] - v8);
      result = 4294967294LL;
      if ( *(_QWORD *)v44 >= v38 )
      {
        *(_QWORD *)v44 = v38;
        v39 = (const void *)v8;
        v8 = v34;
        memcpy(v35, v39, v38);
        v13 *= 8LL;
        cc_clear(v13, v12);
        cc_clear(v13, v49);
        v40 = v43;
        cc_clear((*(_QWORD *)v43 + 7LL) & 0xFFFFFFFFFFFFFFF8LL, v48);
        cc_clear((*(_QWORD *)v40 + 7LL) & 0xFFFFFFFFFFFFFFF8LL, (void *)v27);
        v34 = v8;
        result = 0LL;
      }
    }
  }
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002193C) ----------------------------------------------------
__int64 __fastcall ccrsa_sign_pkcs1v15(__int64 *a1, __int64 a2, size_t a3, const void *a4, __int64 a5, void *a6)
{
  __int64 v6; // r15@1
  __int64 *v7; // r13@1
  __int64 v8; // rbx@1
  unsigned __int64 v9; // rax@1
  char *v10; // r12@1
  signed int v11; // er14@1
  unsigned __int64 v12; // r14@2
  signed int v13; // eax@2
  unsigned __int64 v14; // rdx@2
  unsigned __int64 v15; // rdi@3
  __int64 v16; // r14@3
  __int64 v17; // rsi@3
  unsigned __int64 v18; // rbx@3
  unsigned __int64 v19; // rax@3
  size_t v20; // r15@3
  void *v21; // r13@5
  __int64 result; // rax@6
  __int64 v23; // [sp+0h] [bp-60h]@1
  __int64 v24; // [sp+8h] [bp-58h]@3
  size_t v25; // [sp+10h] [bp-50h]@1
  __int64 v26; // [sp+18h] [bp-48h]@1
  const void *v27; // [sp+20h] [bp-40h]@1
  void *v28; // [sp+28h] [bp-38h]@1
  __int64 v29; // [sp+30h] [bp-30h]@1

  v28 = a6;
  v6 = a5;
  v27 = a4;
  v25 = a3;
  v26 = a2;
  v7 = a1;
  v29 = *(_QWORD *)off_69010[0];
  v8 = *a1;
  v9 = ccn_write_uint_size(*a1, (__int64)(a1 + 2));
  v10 = (char *)&v23 - ((8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v11 = -2;
  if ( *(_QWORD *)v6 >= v9 )
  {
    *(_QWORD *)v6 = v9;
    v12 = v9;
    v13 = ccrsa_emsa_pkcs1v15_encode(v9, (__int64)v28, v25, v27, v26);
    v14 = v12;
    v11 = v13;
    if ( !v13 )
    {
      v15 = v8;
      v24 = v8;
      v16 = v8;
      v17 = (__int64)((char *)&v23 - ((8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL));
      v18 = v14;
      ccn_read_uint(v15, v17, v14, (unsigned __int64)v28);
      ccrsa_priv_crypt(&v7[4 * *v7 + 3], (__int64)v10, v10);
      v19 = ccn_write_uint_size(v16, (__int64)v10);
      v11 = 0;
      v20 = v18 - v19;
      if ( v18 <= v19 )
        v20 = 0LL;
      v21 = v28;
      bzero(v28, v20);
      ccn_write_uint(v24, (__int64)v10, v18 - v20, (__int64)((char *)v21 + v20));
    }
  }
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v29 )
    result = (unsigned int)v11;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000021A65) ----------------------------------------------------
int __usercall ccdigest_update@<eax>(signed __int64 a1@<rax>, unsigned __int64 a2@<rdx>, const void *a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>)
{
  const void *v5; // r15@1
  unsigned __int64 v6; // r13@1
  __int64 v7; // r14@1
  __int64 v8; // r8@1
  unsigned __int64 v9; // rcx@2
  __int64 v10; // rdi@3
  __int64 v11; // rax@3
  unsigned __int64 v12; // rbx@5
  __int64 v13; // r12@5
  unsigned __int64 v14; // rbx@5
  char *v15; // r12@5
  signed __int64 v16; // rdi@6
  size_t v17; // r12@6
  __int64 v18; // rbx@8
  __int64 v19; // rdx@8
  __int64 v20; // rsi@8
  signed __int64 v22; // [sp+0h] [bp-30h]@2

  v5 = a3;
  v6 = a2;
  v7 = a5;
  v8 = a4;
  if ( a2 )
  {
    v22 = a5 + 8;
    v9 = *(_QWORD *)(a4 + 16);
    do
    {
      v10 = *(_QWORD *)(v8 + 8);
      v11 = *(_DWORD *)(v7 + v10 + v9 + 8);
      if ( *(_DWORD *)(v7 + v10 + v9 + 8) || v6 <= v9 )
      {
        v16 = v10 + 8;
        v17 = v9 - v11;
        if ( v6 < v9 - v11 )
          v17 = v6;
        v18 = v8;
        memcpy((void *)(v7 + v11 + v16), v5, v17);
        v8 = v18;
        v19 = *(_QWORD *)(v18 + 8);
        v9 = *(_QWORD *)(v18 + 16);
        a1 = v19 + v9 + 8;
        v20 = (unsigned int)(v17 + *(_DWORD *)(v7 + a1));
        *(_DWORD *)(v7 + a1) = v20;
        v6 -= v17;
        v15 = (char *)v5 + v17;
        if ( v20 == v9 )
        {
          (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64))(v18 + 48))(v22, 1LL, v7 + v19 + 8);
          v8 = v18;
          v9 = *(_QWORD *)(v18 + 16);
          a1 = v9 + *(_QWORD *)(v18 + 8);
          *(_QWORD *)v7 += (unsigned int)(8 * *(_DWORD *)(v7 + a1 + 8));
          *(_DWORD *)(v7 + a1 + 8) = 0;
        }
      }
      else
      {
        v12 = v6 / v9;
        v13 = v8;
        LODWORD(a1) = (*(int (__fastcall **)(signed __int64, unsigned __int64, const void *))(v8 + 48))(
                        v22,
                        v6 / v9,
                        v5);
        v8 = v13;
        v9 = *(_QWORD *)(v13 + 16);
        v14 = v9 * v12;
        v6 -= v14;
        *(_QWORD *)v7 += 8 * v14;
        v15 = (char *)v5 + v14;
      }
      v5 = v15;
    }
    while ( v6 );
  }
  return a1;
}

//----- (0000000000021B86) ----------------------------------------------------
unsigned __int64 __fastcall ccder_decode_rsa_pub_n(unsigned __int64 a1, unsigned __int64 a2)
{
  __int64 v2; // rbx@1
  signed __int64 v3; // rdx@1
  unsigned __int64 result; // rax@1
  __int64 v5; // rcx@2
  __int64 v6; // rcx@4
  unsigned __int64 v7; // [sp+8h] [bp-418h]@1
  char v8; // [sp+10h] [bp-410h]@2
  __int64 v9; // [sp+410h] [bp-10h]@1

  v2 = off_69010[0];
  v9 = *(_QWORD *)off_69010[0];
  v7 = a2;
  v3 = ccder_decode_constructed_tl(a2, a1, a2, 2305843009213693968LL, (__int64)&v7);
  result = 0LL;
  if ( v3 )
  {
    v5 = ccder_decode_uint(0LL, v3, v7, 0x80uLL, (__int64)&v8);
    result = 0LL;
    if ( v5 )
      result = (unsigned __int64)(ccn_bitlen(128LL, (__int64)&v8) + 63) >> 6;
  }
  v6 = *(_QWORD *)v2;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000021C26) ----------------------------------------------------
char (__fastcall *__fastcall ccdrbg_factory_nistctr(__int64 a1, __int64 a2))(void *a1)
{
  char (__fastcall *result)(void *); // rax@1

  *(_QWORD *)a1 = *(_QWORD *)(a2 + 8)
                + 96LL
                - (unsigned __int64)(*(_QWORD *)(a2 + 8) + 2LL * *(_QWORD *)(*(_QWORD *)a2 + 8LL) - 1)
                % *(_QWORD *)(*(_QWORD *)a2 + 8LL)
                + *(_QWORD *)(a2 + 8)
                + 2LL * *(_QWORD *)(*(_QWORD *)a2 + 8LL)
                - 1
                + 2LL * **(_QWORD **)a2
                + 4LL * *(_QWORD *)(*(_QWORD *)a2 + 8LL);
  *(_QWORD *)(a1 + 8) = init;
  *(_QWORD *)(a1 + 24) = CCADRBGGenerate;
  *(_QWORD *)(a1 + 16) = CCADRBGReseed;
  result = CCADRBGDestroy;
  *(_QWORD *)(a1 + 32) = CCADRBGDestroy;
  *(_QWORD *)(a1 + 40) = a2;
  return result;
}

//----- (0000000000021C8F) ----------------------------------------------------
__int64 __fastcall init(__int64 a1, __int64 a2, unsigned __int64 a3, __int64 a4, int a5, __int64 a6, unsigned __int64 a7, const void *a8)
{
  return nistctr_init(
           a1,
           a2,
           **(_QWORD **)(a1 + 40),
           *(_QWORD *)(*(_QWORD *)(a1 + 40) + 8LL),
           a2 + 96,
           a4,
           a3,
           a6,
           a5,
           a8,
           a7,
           *(_DWORD *)(*(_QWORD *)(a1 + 40) + 16LL),
           *(_DWORD *)(*(_QWORD *)(a1 + 40) + 20LL));
}

//----- (0000000000021CF2) ----------------------------------------------------
signed __int64 __fastcall CCADRBGGenerate(__int64 a1, unsigned __int64 a2, __int64 a3, unsigned __int64 a4, __int64 a5)
{
  __int64 v5; // r12@1
  __int64 v6; // r13@1
  __int64 v7; // r15@1
  signed __int64 v8; // rbx@1
  char *v9; // r9@1
  __int64 v10; // r14@1
  signed __int64 v11; // r10@1
  signed __int64 result; // rax@3
  const void *v13; // r13@5
  signed __int64 v14; // r14@8
  void *v15; // rbx@8
  size_t v16; // r15@8
  __int64 v17; // r14@10
  int v18; // eax@12
  void *v19; // rdi@13
  size_t v20; // rsi@13
  __int64 v21; // rbx@15
  __int64 v22; // r15@16
  unsigned __int64 v23; // r14@16
  __int64 v24; // rdi@18
  __int64 v25; // r13@18
  const void *v26; // r15@18
  void *v27; // rdi@18
  signed __int64 v28; // rax@21
  size_t v29; // r15@25
  __int64 v30; // rdi@27
  __int64 v31; // r14@27
  const void *v32; // rbx@27
  const void *v33; // r12@27
  int v34; // er13@27
  void *v35; // r14@27
  size_t v36; // r13@30
  __int64 v37; // rsi@33
  __int64 v38; // [sp+0h] [bp-90h]@1
  bool v39; // [sp+Fh] [bp-81h]@4
  __int64 v40; // [sp+10h] [bp-80h]@8
  __int64 v41; // [sp+18h] [bp-78h]@4
  void *v42; // [sp+20h] [bp-70h]@4
  int v43; // [sp+28h] [bp-68h]@18
  void *v44; // [sp+30h] [bp-60h]@1
  void *v45; // [sp+38h] [bp-58h]@1
  void *v46; // [sp+40h] [bp-50h]@8
  size_t v47; // [sp+48h] [bp-48h]@8
  int v48; // [sp+54h] [bp-3Ch]@10
  __int64 v49; // [sp+58h] [bp-38h]@10
  __int64 v50; // [sp+60h] [bp-30h]@1

  v5 = a3;
  v6 = off_69010[0];
  v50 = *(_QWORD *)off_69010[0];
  v7 = *(_QWORD *)(a1 + 8);
  v8 = *(_QWORD *)(v7 + 8) & 0x1FFFFFFFFFFFFFFFLL;
  v9 = (char *)&v38 - ((4 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v10 = *(_QWORD *)(a1 + 16);
  v11 = ((unsigned __int64)(v10 + *(_QWORD *)(v7 + 8)) >> 2) & 0x7FFFFFFFFFFFFFFLL;
  v45 = (void *)(a2 / v8);
  v44 = (char *)&v38 - ((v8 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  if ( !a2 )
    goto LABEL_38;
  if ( *(_DWORD *)(a1 + 80) < 0x186A0u || (result = 4294967294LL, !*(_DWORD *)(a1 + 84)) )
  {
    v41 = a1;
    v42 = (char *)&v38 - ((4 * v11 + 15) & 0xFFFFFFFFFFFFFFF0LL);
    v39 = a5 != 0 && a4 != 0;
    if ( v39 )
    {
      v13 = (const void *)a5;
      if ( *(_DWORD *)(v41 + 88) )
      {
        if ( a4 >= 0x65 )
        {
          bzero(v42, 4 * v11);
LABEL_9:
          result = 4294967293LL;
          v6 = off_69010[0];
          goto LABEL_39;
        }
        v47 = a2;
        v46 = (char *)&v38 - ((4 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL);
        v49 = a5;
        v48 = a4;
        v40 = v11;
        v17 = v41;
        v15 = v42;
        df(v41, (__int64)&v49, (__int64)&v48, 1, v42, 4 * v11);
      }
      else
      {
        v47 = a2;
        v46 = (char *)&v38 - ((4 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL);
        v14 = (*(_QWORD *)(v7 + 8) + v10) & 0x1FFFFFFFFFFFFFFFLL;
        v40 = v11;
        v15 = v42;
        v16 = a4;
        bzero(v42, 4 * v11);
        if ( v14 < v16 )
          goto LABEL_9;
        memcpy(v15, v13, v16);
        v17 = v41;
      }
      v41 = v17;
      v42 = v15;
      v18 = drbg_update(v17, (__int64)v15);
      v11 = v40;
      v9 = (char *)v46;
      a2 = v47;
      v6 = off_69010[0];
      if ( v18 )
      {
        v19 = v42;
        v20 = 4 * v40;
LABEL_37:
        bzero(v19, v20);
LABEL_38:
        result = 4294967293LL;
        goto LABEL_39;
      }
    }
    v47 = a2;
    if ( v45 )
    {
      v46 = v9;
      v21 = v41;
      if ( !(v5 & 3) )
      {
        v40 = v11;
        v22 = v41;
        v23 = 0LL;
        do
        {
          if ( *(_DWORD *)(v22 + 84) )
          {
            v24 = v22;
            v25 = v22;
            v26 = v44;
            generate_block(v24, (__int64)v44);
            generate_block(v25, v5);
            v43 = memcmp((const void *)v5, v26, *(_QWORD *)(*(_QWORD *)(v25 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFLL);
            v27 = (void *)v26;
            v22 = v25;
            bzero(v27, *(_QWORD *)(*(_QWORD *)(v25 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFLL);
            if ( !v43 )
              goto LABEL_40;
          }
          else
          {
            generate_block(v22, v5);
          }
          v28 = *(_QWORD *)(*(_QWORD *)(v22 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFLL;
          v47 -= v28;
          v5 += v28 & 0xFFFFFFFFFFFFFFFCLL;
          ++v23;
        }
        while ( v23 < (unsigned __int64)v45 );
        v11 = v40;
        v21 = v22;
      }
    }
    else
    {
      v46 = v9;
      v21 = v41;
    }
    v40 = v11;
    if ( v47 )
    {
      v29 = *(_QWORD *)(*(_QWORD *)(v21 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFLL;
      do
      {
        v45 = (void *)v5;
        if ( *(_DWORD *)(v21 + 84) )
        {
          v30 = v21;
          v31 = v21;
          v32 = v44;
          generate_block(v30, (__int64)v44);
          v33 = v46;
          generate_block(v31, (__int64)v46);
          v34 = memcmp(v33, v32, *(_QWORD *)(*(_QWORD *)(v31 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFLL);
          bzero((void *)v32, *(_QWORD *)(*(_QWORD *)(v31 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFLL);
          v21 = v31;
          v35 = v45;
          if ( !v34 )
            goto LABEL_40;
        }
        else
        {
          v33 = v46;
          generate_block(v21, (__int64)v46);
          v35 = v45;
        }
        v46 = (void *)v33;
        v36 = v47;
        if ( v47 < (*(_QWORD *)(*(_QWORD *)(v21 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFuLL) )
          v29 = v47;
        memcpy(v35, v33, v29);
        v5 = (__int64)((char *)v35 + v29);
        v47 = v36 - v29;
      }
      while ( v36 != v29 );
    }
    v37 = (__int64)v42;
    v6 = off_69010[0];
    if ( !v39 )
      v37 = *(_QWORD *)(v21 + 40);
    if ( (unsigned int)drbg_update(v21, v37) )
    {
      v20 = 4 * v40;
      v19 = v42;
      goto LABEL_37;
    }
    ++*(_DWORD *)(v21 + 80);
    cc_clear(4 * v40, v42);
    result = 0LL;
  }
LABEL_39:
  while ( *(_QWORD *)v6 != v50 )
  {
LABEL_40:
    bzero(v42, 4 * v40);
    result = 4294967293LL;
    v6 = off_69010[0];
  }
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000022143) ----------------------------------------------------
signed __int64 __fastcall CCADRBGReseed(__int64 a1, __int64 a2, __int64 a3, unsigned __int64 a4, __int64 a5)
{
  const void *v5; // r13@1
  size_t v6; // r12@1
  __int64 v7; // r15@1
  __int64 v8; // rdx@1
  unsigned __int64 v9; // r9@1
  char *v10; // rbx@1
  signed __int64 result; // rax@2
  int v12; // ecx@4
  unsigned __int64 v13; // rcx@8
  signed __int64 i; // rax@10
  int v15; // ecx@12
  __int64 v16; // rcx@14
  __int64 v17; // [sp+0h] [bp-50h]@1
  int v18; // [sp+8h] [bp-48h]@4
  int v19; // [sp+Ch] [bp-44h]@6
  __int64 v20; // [sp+10h] [bp-40h]@4
  __int64 v21; // [sp+18h] [bp-38h]@6
  __int64 v22; // [sp+20h] [bp-30h]@1

  v5 = (const void *)a5;
  v6 = a4;
  v7 = a3;
  v8 = off_69010[0];
  v22 = *(_QWORD *)off_69010[0];
  v9 = (unsigned __int64)(8LL * *(_QWORD *)(a1 + 16) + 8LL * *(_QWORD *)(*(_QWORD *)(a1 + 8) + 8LL)) >> 5;
  v10 = (char *)&v17 - ((4 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  if ( *(_DWORD *)(a1 + 88) )
  {
    result = 4294967293LL;
    if ( a4 > 0x64 || (unsigned __int64)(a2 - 16) > 0x6D )
      goto LABEL_14;
    v20 = v7;
    v18 = a2;
    v12 = 1;
    if ( a5 && v6 )
    {
      v21 = a5;
      v19 = v6;
      v12 = 2;
    }
    df(a1, (__int64)&v20, (__int64)&v18, v12, v10, 4 * v9);
  }
  else
  {
    v13 = (unsigned __int64)(8LL * *(_QWORD *)(a1 + 16) + 8LL * *(_QWORD *)(*(_QWORD *)(a1 + 8) + 8LL)) >> 3;
    result = 4294967293LL;
    if ( v13 != a2 || v13 < v6 )
      goto LABEL_14;
    bzero(v10, 4 * v9);
    memcpy(v10, v5, v6);
    for ( i = (*(_QWORD *)(a1 + 16) + *(_QWORD *)(*(_QWORD *)(a1 + 8) + 8LL)) & 0x1FFFFFFFFFFFFFFFLL; i; --i )
      v10[i - 1] ^= *(_BYTE *)(v7 + i - 1);
  }
  v15 = drbg_update(a1, (__int64)v10);
  result = 4294967293LL;
  v8 = off_69010[0];
  if ( !v15 )
  {
    *(_DWORD *)(a1 + 80) = 1;
    result = 0LL;
  }
LABEL_14:
  v16 = *(_QWORD *)v8;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002229B) ----------------------------------------------------
char __fastcall CCADRBGDestroy(void *a1)
{
  void *v1; // rbx@1
  char result; // al@1

  v1 = a1;
  cc_clear(
    *((_QWORD *)a1 + 2)
  + 2LL * *(_QWORD *)(*((_QWORD *)a1 + 1) + 8LL)
  - 1
  - (unsigned __int64)(*((_QWORD *)a1 + 2) + 2LL * *(_QWORD *)(*((_QWORD *)a1 + 1) + 8LL) - 1)
  % *(_QWORD *)(*((_QWORD *)a1 + 1) + 8LL),
    *((void **)a1 + 3));
  cc_clear(*(_QWORD *)(*((_QWORD *)v1 + 1) + 8LL), *((void **)v1 + 4));
  cc_clear(*((_QWORD *)v1 + 2) + *(_QWORD *)(*((_QWORD *)v1 + 1) + 8LL), *((void **)v1 + 5));
  cc_clear(*(_QWORD *)(*((_QWORD *)v1 + 1) + 8LL), *((void **)v1 + 8));
  cc_clear(**((_QWORD **)v1 + 1), *((void **)v1 + 6));
  cc_clear(**((_QWORD **)v1 + 1), *((void **)v1 + 7));
  result = cc_clear(0x60uLL, a1);
  *((_DWORD *)a1 + 20) = -1;
  return result;
}

//----- (000000000002233B) ----------------------------------------------------
__int64 __fastcall df(__int64 a1, __int64 a2, __int64 a3, int a4, void *a5, size_t a6)
{
  int v6; // er12@1
  __int64 v7; // r13@1
  __int64 v8; // rbx@1
  __int64 v9; // rdi@1
  __int64 v10; // r11@1
  unsigned int v11; // er14@1
  unsigned __int64 v12; // r10@1
  __int64 v13; // rax@4
  __int64 v14; // rcx@4
  __int64 v15; // rcx@6
  __int64 v16; // r14@7
  __int64 v17; // rbx@7
  void *v18; // r15@7
  bool v19; // zf@8
  __int64 v20; // rbx@8
  unsigned __int64 *v21; // r14@8
  const void **v22; // r12@8
  __int64 v23; // rbx@10
  __int64 v24; // rax@10
  __int64 v25; // r8@13
  size_t v26; // r14@13
  char *v27; // r15@14
  char *v28; // rdx@14
  size_t v29; // rbx@14
  void *v31; // [sp+0h] [bp-E0h]@1
  unsigned __int64 v32; // [sp+8h] [bp-D8h]@3
  size_t v33; // [sp+10h] [bp-D0h]@3
  void *v34; // [sp+18h] [bp-C8h]@1
  __int64 v35; // [sp+20h] [bp-C0h]@7
  unsigned __int32 v36; // [sp+2Ch] [bp-B4h]@7
  __int64 v37; // [sp+30h] [bp-B0h]@7
  __int64 v38; // [sp+38h] [bp-A8h]@1
  int v39; // [sp+44h] [bp-9Ch]@1
  __int64 v40; // [sp+48h] [bp-98h]@1
  void *v41; // [sp+50h] [bp-90h]@3
  __int64 v42; // [sp+58h] [bp-88h]@8
  char v43; // [sp+60h] [bp-80h]@14
  int v44; // [sp+A8h] [bp-38h]@8
  unsigned __int32 v45; // [sp+ACh] [bp-34h]@8
  __int64 v46; // [sp+B0h] [bp-30h]@1

  v6 = a4;
  v39 = a4;
  v40 = a3;
  v38 = a2;
  v7 = a1;
  v8 = off_69010[0];
  v46 = *(_QWORD *)off_69010[0];
  v9 = *(_QWORD *)(a1 + 8);
  v10 = *(_QWORD *)(v7 + 16);
  v11 = 0;
  v12 = (16LL * *(_QWORD *)(v9 + 8)
       + 8 * v10
       - 1
       - (16LL * *(_QWORD *)(v9 + 8) + 8 * v10 - 1) % (unsigned __int64)(8LL * *(_QWORD *)(v9 + 8))) >> 5;
  v34 = (char *)&v31 - ((4 * v12 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  if ( a6 - 1 <= 0x3F )
  {
    v41 = (char *)&v31 - ((*(_QWORD *)v9 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
    v32 = v12;
    v33 = a6;
    if ( a4 )
    {
      v13 = (unsigned int)a4;
      v11 = 0;
      v14 = v40;
      do
      {
        v11 += *(_DWORD *)v14;
        v14 += 4LL;
        --v13;
      }
      while ( v13 );
    }
    v31 = a5;
    v15 = *(_QWORD *)(v9 + 8);
    if ( (8 * v10 - 1 + 16LL * *(_QWORD *)(v9 + 8)) / (unsigned __int64)(8 * v15) )
    {
      v16 = _byteswap_ulong(v11);
      v37 = v16;
      v36 = _byteswap_ulong(v33);
      v35 = (unsigned int)v6;
      v17 = 0LL;
      v18 = v34;
      do
      {
        v42 = v17;
        memcpy(
          v18,
          (const void *)(*(_QWORD *)(v7 + 24) + v17 * (v15 & 0x1FFFFFFFFFFFFFFFLL)),
          v15 & 0x1FFFFFFFFFFFFFFFLL);
        *(_QWORD *)(v7 + 72) = 0LL;
        v44 = v16;
        v45 = v36;
        df_bcc_update(v7, &v44, 8uLL, (__int64)v18);
        v19 = v6 == 0;
        v20 = v35;
        v21 = (unsigned __int64 *)v40;
        v22 = (const void **)v38;
        if ( !v19 )
        {
          do
          {
            df_bcc_update(v7, *v22, *(_DWORD *)v21, (__int64)v18);
            ++v22;
            v21 = (unsigned __int64 *)((char *)v21 + 4);
            --v20;
          }
          while ( v20 );
        }
        v23 = *(_QWORD *)(v7 + 64);
        df_bcc_update(v7, df_bcc_final_endmark, 1uLL, (__int64)v18);
        v24 = *(_QWORD *)(v7 + 72);
        if ( v24 )
        {
          bzero((void *)(v23 + v24), (*(_QWORD *)(*(_QWORD *)(v7 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFLL) - v24);
          bcc_update(v7, v23, 1LL, (__int64)v18);
        }
        v9 = *(_QWORD *)(v7 + 8);
        v10 = *(_QWORD *)(v7 + 16);
        v15 = *(_QWORD *)(v9 + 8);
        v18 = (char *)v18 + 4 * ((*(_QWORD *)(v9 + 8) >> 2) & 0x7FFFFFFFFFFFFFFLL);
        v17 = v42 + 1;
        v6 = v39;
        LODWORD(v16) = v37;
      }
      while ( v42 + 1 < (16LL * *(_QWORD *)(v9 + 8) + 8 * v10 - 1) / (unsigned __int64)(8 * v15) );
    }
    (*(void (__fastcall **)(__int64, void *, signed __int64, void *))(v9 + 16))(
      v9,
      v41,
      v10 & 0x1FFFFFFFFFFFFFFFLL,
      v34);
    v25 = *(_QWORD *)(v7 + 8);
    v26 = v33 / (*(_QWORD *)(v25 + 8) & 0x1FFFFFFFFFFFFFFFuLL)
        + ((((*(_QWORD *)(v25 + 8) & 0x1FFFFFFFFFFFFFFFLL) - 1) & v33) != 0);
    if ( v26 )
    {
      v27 = &v43;
      v28 = (char *)v34 + 4 * ((*(_QWORD *)(v7 + 16) >> 2) & 0x7FFFFFFFFFFFFFFLL);
      v29 = 0LL;
      do
      {
        (*(void (__fastcall **)(void *, signed __int64, char *, char *))(v25 + 24))(v41, 1LL, v28, v27);
        v25 = *(_QWORD *)(v7 + 8);
        v28 = v27;
        v27 += 4 * ((*(_QWORD *)(v25 + 8) >> 2) & 0x7FFFFFFFFFFFFFFLL);
        ++v29;
      }
      while ( v29 < v26 );
    }
    memcpy(v31, &v43, v33);
    cc_clear(**(_QWORD **)(v7 + 8), v41);
    cc_clear(4 * v32, v34);
    cc_clear(0x40uLL, &v43);
    v8 = off_69010[0];
  }
  else
  {
    cc_clear(*(_QWORD *)v9, (char *)&v31 - ((*(_QWORD *)v9 + 15LL) & 0xFFFFFFFFFFFFFFF0LL));
  }
  return *(_QWORD *)v8;
}
// 5EF80: using guessed type __int64 df_bcc_final_endmark[2];
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000226E1) ----------------------------------------------------
signed __int64 __fastcall drbg_update(__int64 a1, __int64 a2)
{
  __int64 v2; // r13@1
  __int64 v3; // r12@1
  __int64 v4; // r8@1
  __int64 v5; // rcx@1
  char *v6; // r15@1
  unsigned __int64 v7; // rax@1
  unsigned __int64 v8; // rbx@2
  __int64 v9; // rdx@3
  signed __int64 v10; // rax@3
  unsigned int v11; // ecx@5
  __int64 v12; // rax@8
  signed int v13; // ecx@9
  unsigned __int64 v14; // rdi@9
  unsigned __int64 v15; // rax@11
  __int64 v16; // r8@12
  signed __int64 v17; // rcx@12
  signed __int64 v18; // rax@12
  __int64 v19; // rsi@12
  __int64 v20; // rdi@12
  signed __int64 result; // rax@14
  __int64 v22; // rcx@16
  __int64 v23; // [sp+0h] [bp-40h]@1
  unsigned __int64 v24; // [sp+8h] [bp-38h]@1
  __int64 v25; // [sp+10h] [bp-30h]@1

  v2 = a2;
  v3 = a1;
  v25 = *(_QWORD *)off_69010[0];
  v4 = *(_QWORD *)(a1 + 8);
  v5 = *(_QWORD *)(a1 + 16);
  v24 = (16LL * *(_QWORD *)(v4 + 8)
       + 8 * v5
       - 1
       - (16LL * *(_QWORD *)(v4 + 8) + 8 * v5 - 1) % (unsigned __int64)(8LL * *(_QWORD *)(v4 + 8))) >> 5;
  v6 = (char *)&v23 - ((4 * v24 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v7 = *(_QWORD *)(v4 + 8);
  if ( (v7 + v5) & 0x1FFFFFFFFFFFFFFCLL )
  {
    v8 = (unsigned __int64)((char *)&v23 - ((4 * v24 + 15) & 0xFFFFFFFFFFFFFFF0LL));
    do
    {
      v9 = *(_QWORD *)(a1 + 32);
      v10 = ((v7 >> 2) & 0x7FFFFFFFFFFFFFFLL) - 1;
      do
      {
        if ( v10 < 0 )
          break;
        v11 = _byteswap_ulong(*(_DWORD *)(v9 + 4 * v10)) + 1;
        *(_DWORD *)(v9 + 4 * v10--) = _byteswap_ulong(v11);
      }
      while ( !v11 );
      (*(void (__fastcall **)(_QWORD, signed __int64, __int64, unsigned __int64))(v4 + 24))(
        *(_QWORD *)(a1 + 48),
        1LL,
        v9,
        v8);
      v4 = *(_QWORD *)(a1 + 8);
      v5 = *(_QWORD *)(a1 + 16);
      v7 = *(_QWORD *)(v4 + 8);
      v8 += 4 * ((*(_QWORD *)(v4 + 8) >> 2) & 0x7FFFFFFFFFFFFFFLL);
    }
    while ( v8 < (unsigned __int64)&v6[4 * (((v7 + v5) >> 2) & 0x7FFFFFFFFFFFFFFLL)] );
  }
  if ( !memcmp(v6, &v6[v5 & 0x1FFFFFFFFFFFFFFFLL], v5 & 0x1FFFFFFFFFFFFFFFLL) )
  {
    bzero(v6, 4 * v24);
    result = 0xFFFFFFFFLL;
  }
  else
  {
    v12 = *(_QWORD *)(a1 + 16);
    if ( v12 & 0x1FFFFFFFFFFFFFFCLL )
    {
      v13 = 1;
      v14 = 0LL;
      do
      {
        *(_DWORD *)&v6[4 * v14] ^= *(_DWORD *)v2;
        v2 += 4LL;
        v14 = (unsigned int)v13++;
      }
      while ( v14 < (v12 & 0x1FFFFFFFFFFFFFFFuLL) >> 2 );
    }
    (*(void (__fastcall **)(_QWORD, _QWORD, signed __int64, char *))(*(_QWORD *)(v3 + 8) + 16LL))(
      *(_QWORD *)(v3 + 8),
      *(_QWORD *)(v3 + 48),
      v12 & 0x1FFFFFFFFFFFFFFFLL,
      v6);
    v15 = *(_QWORD *)(*(_QWORD *)(v3 + 8) + 8LL);
    if ( v15 & 0x1FFFFFFFFFFFFFFCLL )
    {
      v16 = *(_QWORD *)(v3 + 32);
      v17 = (*(_QWORD *)(v3 + 16) >> 2) & 0x7FFFFFFFFFFFFFFLL;
      v18 = (v15 >> 2) & 0x7FFFFFFFFFFFFFFLL;
      v19 = 0LL;
      v20 = 0LL;
      do
      {
        *(_DWORD *)(v16 + 4 * v20) = *(_DWORD *)&v6[4 * (v17 + v20)] ^ *(_DWORD *)(v2 + 4 * v19++);
        v20 = (unsigned int)v19;
      }
      while ( (unsigned int)v19 < (unsigned __int64)v18 );
    }
    cc_clear(4 * v24, v6);
    result = 0LL;
  }
  v22 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000228EC) ----------------------------------------------------
void *__fastcall df_bcc_update(__int64 a1, const void *a2, unsigned __int64 a3, __int64 a4)
{
  unsigned __int64 v4; // r15@1
  __int64 v5; // r14@1
  signed __int64 v6; // rdx@1
  __int64 v7; // rcx@1
  size_t v8; // r12@2
  __int64 v9; // r14@4
  void *result; // rax@4
  unsigned __int64 v11; // rcx@4
  __int64 v12; // rcx@7
  unsigned __int64 v13; // rsi@7
  unsigned __int64 v14; // r12@8
  signed __int64 v15; // rdx@9
  signed __int64 v16; // r13@10
  signed __int64 v17; // rax@10
  signed __int64 v18; // rcx@12
  unsigned __int64 v19; // [sp+0h] [bp-40h]@7
  __int64 v20; // [sp+8h] [bp-38h]@1
  void *v21; // [sp+10h] [bp-30h]@4

  v20 = a4;
  v4 = a3;
  v5 = (__int64)a2;
  v6 = 0x1FFFFFFFFFFFFFFFLL;
  v7 = *(_QWORD *)(a1 + 72);
  if ( !v7 )
  {
    v21 = *(void **)(a1 + 64);
    goto LABEL_7;
  }
  v8 = (*(_QWORD *)(*(_QWORD *)(a1 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFLL) - v7;
  if ( v8 > v4 )
    v8 = v4;
  v21 = *(void **)(a1 + 64);
  v9 = *(_QWORD *)(a1 + 72);
  memcpy((void *)(*(_QWORD *)(a1 + 64) + v7), a2, v8);
  result = (void *)(unsigned int)v8;
  v11 = (unsigned int)v8 + v9;
  if ( v11 >= (*(_QWORD *)(*(_QWORD *)(a1 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFuLL) )
  {
    v4 -= (unsigned int)v8;
    bcc_update(a1, *(_QWORD *)(a1 + 64), 1LL, v20);
    v6 = 0x1FFFFFFFFFFFFFFFLL;
    v5 = (__int64)((char *)a2 + v8);
LABEL_7:
    v12 = *(_QWORD *)(*(_QWORD *)(a1 + 8) + 8LL);
    v13 = v6 & *(_QWORD *)(*(_QWORD *)(a1 + 8) + 8LL);
    result = (void *)(v4 / v13);
    v19 = v4 / v13;
    if ( v4 / v13 )
    {
      v14 = 0LL;
      if ( v5 & 3 )
      {
        v15 = 0x1FFFFFFFFFFFFFFFLL;
        do
        {
          v16 = v15;
          memcpy(v21, (const void *)v5, v15 & v12);
          bcc_update(a1, *(_QWORD *)(a1 + 64), 1LL, v20);
          v15 = v16;
          v12 = *(_QWORD *)(*(_QWORD *)(a1 + 8) + 8LL);
          v17 = v16 & *(_QWORD *)(*(_QWORD *)(a1 + 8) + 8LL);
          v5 += v17;
          v4 -= v17;
          result = (void *)v19;
          ++v14;
        }
        while ( v14 < v19 );
      }
      else
      {
        bcc_update(a1, v5, (__int64)result, v20);
        result = *(void **)(a1 + 8);
        v18 = v4 / v13 * (*((_QWORD *)result + 1) & 0x1FFFFFFFFFFFFFFFLL);
        v5 += v18;
        v4 -= v18;
      }
    }
    v11 = 0LL;
    if ( v4 )
    {
      result = memcpy(v21, (const void *)v5, v4);
      v11 = v4;
    }
  }
  *(_QWORD *)(a1 + 72) = v11;
  return result;
}

//----- (0000000000022A8A) ----------------------------------------------------
__int64 __fastcall bcc_update(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  __int64 v5; // r13@1
  __int64 v6; // rax@1
  char *v7; // rbx@1
  signed __int64 i; // r14@2
  unsigned __int64 v9; // rcx@4
  signed __int64 v10; // rcx@5
  signed __int64 v11; // rdx@5
  unsigned __int64 v12; // rsi@7
  __int64 v14; // [sp+0h] [bp-40h]@1
  __int64 v15; // [sp+8h] [bp-38h]@1
  __int64 v16; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v15 = a3;
  v5 = a2;
  v16 = *(_QWORD *)off_69010[0];
  v6 = *(_QWORD *)(a1 + 8);
  v7 = (char *)&v14 - ((4 * ((*(_QWORD *)(v6 + 8) >> 2) & 0x7FFFFFFFFFFFFFFLL) + 15) & 0xFFFFFFFFFFFFFFF0LL);
  if ( a3 )
  {
    for ( i = 1LL; ; ++i )
    {
      v9 = *(_QWORD *)(v6 + 8);
      if ( v9 & 0x1FFFFFFFFFFFFFFCLL )
      {
        v10 = (v9 >> 2) & 0x7FFFFFFFFFFFFFFLL;
        v11 = 1LL;
        if ( (unsigned __int64)v10 > 1 )
          v11 = v10;
        v12 = 0LL;
        do
        {
          *(_DWORD *)&v7[4 * v12] = *(_DWORD *)(v4 + 4 * v12) ^ *(_DWORD *)(v5 + 4 * v12);
          ++v12;
        }
        while ( v12 < v10 );
        v5 += 4 * v11;
      }
      (*(void (__fastcall **)(_QWORD, signed __int64, char *, __int64))(v6 + 24))(*(_QWORD *)(a1 + 56), 1LL, v7, v4);
      if ( i == v15 )
        break;
      v6 = *(_QWORD *)(a1 + 8);
    }
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000022B83) ----------------------------------------------------
int __fastcall generate_block(__int64 a1, __int64 a2)
{
  __int64 v2; // r8@1
  __int64 v3; // r9@1
  __int64 v4; // rdx@1
  signed __int64 v5; // rsi@1
  unsigned int v6; // ecx@3

  v2 = a2;
  v3 = *(_QWORD *)(a1 + 8);
  v4 = *(_QWORD *)(a1 + 32);
  v5 = ((*(_QWORD *)(v3 + 8) >> 2) & 0x7FFFFFFFFFFFFFFLL) - 1;
  do
  {
    if ( v5 < 0 )
      break;
    v6 = _byteswap_ulong(*(_DWORD *)(v4 + 4 * v5)) + 1;
    *(_DWORD *)(v4 + 4 * v5--) = _byteswap_ulong(v6);
  }
  while ( !v6 );
  return (*(int (__fastcall **)(_QWORD, signed __int64, __int64, __int64))(v3 + 24))(*(_QWORD *)(a1 + 48), 1LL, v4, v2);
}

//----- (0000000000022BD7) ----------------------------------------------------
__int64 __fastcall nistctr_init(__int64 a1, __int64 a2, __int64 a3, size_t a4, __int64 a5, __int64 a6, unsigned __int64 a7, __int64 a8, int a9, const void *a10, unsigned __int64 a11, int a12, int a13)
{
  size_t v13; // r12@1
  __int64 v14; // r14@1
  unsigned __int64 v15; // rcx@1
  __int64 v16; // r13@1
  char *v17; // r15@1
  unsigned __int64 v18; // rsi@1
  unsigned __int64 v19; // rsi@1
  unsigned __int64 v20; // rax@1
  unsigned __int64 v21; // rsi@1
  unsigned __int64 v22; // rsi@1
  unsigned __int64 v23; // rcx@1
  signed int v24; // esi@2
  unsigned __int64 v25; // rdi@2
  signed int v26; // er13@5
  signed __int64 v27; // rax@10
  __int64 i; // rdx@10
  int v29; // ecx@13
  void *v30; // rbx@18
  size_t v31; // r13@18
  __int64 result; // rax@22
  __int64 v33; // [sp+0h] [bp-80h]@1
  __int64 v34; // [sp+8h] [bp-78h]@5
  __int64 v35; // [sp+10h] [bp-70h]@1
  void *v36; // [sp+18h] [bp-68h]@1
  __int64 v37; // [sp+20h] [bp-60h]@13
  __int64 v38; // [sp+28h] [bp-58h]@13
  const void *v39; // [sp+30h] [bp-50h]@15
  int v40; // [sp+44h] [bp-3Ch]@13
  int v41; // [sp+48h] [bp-38h]@13
  int v42; // [sp+4Ch] [bp-34h]@15
  __int64 v43; // [sp+50h] [bp-30h]@1

  v35 = a6;
  v13 = a4;
  v14 = a2;
  v43 = *(_QWORD *)off_69010[0];
  v36 = (char *)&v33 - ((a4 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v15 = *(_QWORD *)(a3 + 8);
  v16 = (v15 + v13) >> 2;
  v17 = (char *)&v33 - ((4 * v16 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  *(_QWORD *)a2 = a1;
  *(_QWORD *)(a2 + 8) = a3;
  *(_QWORD *)(a2 + 16) = v13;
  *(_QWORD *)(a2 + 24) = a5;
  v18 = v13 + 2 * v15 - 1 - (v13 + 2 * v15 - 1) % v15;
  *(_QWORD *)(v14 + 32) = a5 + v18;
  v19 = v15 + v18;
  v20 = a5 + v19;
  *(_QWORD *)(v14 + 40) = a5 + v19;
  v21 = v15 + v13 + v19;
  *(_QWORD *)(v14 + 64) = a5 + v21;
  v22 = v15 + v21;
  *(_QWORD *)(v14 + 48) = a5 + v22;
  *(_QWORD *)(v14 + 56) = a5 + *(_QWORD *)a3 + v22;
  *(_DWORD *)(v14 + 84) = a12;
  *(_DWORD *)(v14 + 88) = a13;
  v23 = 8 * v13 + 8 * v15;
  if ( v23 >= 0x20 )
  {
    v24 = 1;
    v25 = 0LL;
    do
    {
      *(_DWORD *)(v20 + 4 * v25) = 0;
      v25 = (unsigned int)v24++;
    }
    while ( v25 < v23 >> 5 );
  }
  if ( a13 )
  {
    v34 = v16;
    v26 = -3;
    if ( a7 - 16 > 0x6D )
      goto LABEL_22;
    if ( a11 >= 0x65 )
    {
      CCADRBGDestroy((void *)v14);
      goto LABEL_22;
    }
    df_initialize(a7 - 16, v14);
    v37 = v35;
    v40 = a7;
    v38 = a8;
    v41 = a9;
    v29 = 2;
    if ( a10 )
    {
      v16 = v34;
      if ( a11 )
      {
        v39 = a10;
        v42 = a11;
        v29 = 3;
      }
    }
    else
    {
      v16 = v34;
    }
    df(v14, (__int64)&v37, (__int64)&v40, v29, v17, 4 * v16);
  }
  else
  {
    if ( v23 >> 3 != a7 || a7 < a11 )
      goto LABEL_20;
    bzero((char *)&v33 - ((4 * v16 + 15) & 0xFFFFFFFFFFFFFFF0LL), 4 * v16);
    memcpy((char *)&v33 - ((4 * v16 + 15) & 0xFFFFFFFFFFFFFFF0LL), a10, a11);
    v27 = (*(_QWORD *)(v14 + 16) + *(_QWORD *)(*(_QWORD *)(v14 + 8) + 8LL)) & 0x1FFFFFFFFFFFFFFFLL;
    for ( i = v35; v27; --v27 )
      v17[v27 - 1] ^= *(_BYTE *)(i + v27 - 1);
  }
  v30 = v36;
  bzero(v36, v13);
  (*(void (__fastcall **)(_QWORD, _QWORD, size_t, void *))(*(_QWORD *)(v14 + 8) + 16LL))(
    *(_QWORD *)(v14 + 8),
    *(_QWORD *)(v14 + 48),
    v13,
    v30);
  bzero(*(void **)(v14 + 32), *(_QWORD *)(*(_QWORD *)(v14 + 8) + 8LL) & 0x1FFFFFFFFFFFFFFFLL);
  v31 = 4 * v16;
  if ( (unsigned int)drbg_update(v14, (__int64)v17) )
  {
    bzero(v17, v31);
LABEL_20:
    CCADRBGDestroy((void *)v14);
    v26 = -3;
    goto LABEL_22;
  }
  cc_clear(v31, v17);
  *(_DWORD *)(v14 + 80) = 1;
  v26 = 0;
LABEL_22:
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v43 )
    result = (unsigned int)v26;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000022E80) ----------------------------------------------------
__int64 __usercall df_initialize@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>)
{
  __int64 v2; // r15@1
  __int64 v3; // rdx@1
  char *v4; // rcx@1
  __int64 v5; // rdi@1
  signed __int64 v6; // rbx@1
  char *v7; // r14@1
  unsigned __int64 v8; // rax@1
  signed __int64 v9; // rdx@1
  __int64 v10; // rsi@3
  signed int v11; // er12@4
  unsigned __int64 v12; // rbx@4
  signed __int64 v13; // rsi@5
  __int64 v14; // rbx@5
  __int64 v16; // [sp+0h] [bp-30h]@1
  __int64 v17; // [sp+8h] [bp-28h]@1

  v16 = a1;
  v2 = a2;
  v16 = *(_QWORD *)off_69010[0];
  v3 = *(_QWORD *)(a2 + 16);
  v4 = (char *)&v16 - (((*(_QWORD *)(a2 + 16) & 0x1FFFFFFFFFFFFFFFLL) + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v5 = *(_QWORD *)(a2 + 8);
  v6 = (*(_QWORD *)(v5 + 8) >> 2) & 0x7FFFFFFFFFFFFFFLL;
  v7 = (char *)&v17 - ((4 * v6 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v8 = 0LL;
  v9 = v3 & 0x1FFFFFFFFFFFFFFFLL;
  if ( v9 )
  {
    do
    {
      v4[v8] = v8;
      v8 = (unsigned int)(v8 + 1);
    }
    while ( v8 < v9 );
  }
  (*(void (__fastcall **)(__int64, _QWORD))(v5 + 16))(v5, *(_QWORD *)(v2 + 56));
  bzero(v7, 4 * v6);
  v10 = *(_QWORD *)(*(_QWORD *)(v2 + 8) + 8LL);
  if ( (16LL * *(_QWORD *)(*(_QWORD *)(v2 + 8) + 8LL) + 8LL * *(_QWORD *)(v2 + 16) - 1) / (unsigned __int64)(8 * v10) )
  {
    v11 = 1;
    v12 = 0LL;
    do
    {
      *(_DWORD *)v7 = _byteswap_ulong(v11 - 1);
      v13 = v10 & 0x1FFFFFFFFFFFFFFFLL;
      v14 = *(_QWORD *)(v2 + 24) + v13 * v12;
      bzero((void *)v14, v13);
      bcc_update(v2, (__int64)v7, 1LL, v14);
      v12 = (unsigned int)v11;
      v10 = *(_QWORD *)(*(_QWORD *)(v2 + 8) + 8LL);
      ++v11;
    }
    while ( v12 < (16LL * *(_QWORD *)(*(_QWORD *)(v2 + 8) + 8LL) + 8LL * *(_QWORD *)(v2 + 16) - 1)
                / (unsigned __int64)(8 * v10) );
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000022FCF) ----------------------------------------------------
__int64 __usercall cccmac@<rax>(__int64 a1@<rax>, unsigned __int64 a2@<rdx>, signed __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>, __int64 a6@<r8>)
{
  __int64 v6; // r14@1
  signed __int64 v7; // r15@1
  unsigned __int64 v8; // r12@1
  char *v9; // rbx@1
  __int64 v11; // [sp+0h] [bp-30h]@1

  v11 = a1;
  v6 = a6;
  v7 = a3;
  v8 = a2;
  v11 = *(_QWORD *)off_69010[0];
  v9 = (char *)&v11
     - ((((*(_QWORD *)(a4 + 8) + *(_QWORD *)a4 + 47LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL);
  cccmac_init(a4, (__int64)v9, a5);
  cccmac_final(a4, (signed __int64)v9, v8, v7, v6);
  cc_clear(*(_QWORD *)(a4 + 8) + *(_QWORD *)a4 + 40LL, v9);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000023074) ----------------------------------------------------
__int64 __fastcall ccec_affinify(signed __int64 *a1, __int64 a2, unsigned __int64 *a3)
{
  unsigned __int64 *v3; // r15@1
  __int64 v4; // rbx@1
  const void *v5; // r13@1
  signed __int64 v6; // rax@1
  signed int v7; // ecx@1
  unsigned __int64 v8; // rax@2
  __int64 v9; // r14@2
  __int64 v10; // r15@2
  unsigned __int64 *v11; // r13@2
  __int64 result; // rax@3
  __int64 v13; // [sp+0h] [bp-60h]@2
  __int64 v14; // [sp+8h] [bp-58h]@2
  unsigned __int64 *v15; // [sp+10h] [bp-50h]@2
  int v16; // [sp+1Ch] [bp-44h]@2
  __int64 *v17; // [sp+20h] [bp-40h]@2
  __int64 v18; // [sp+28h] [bp-38h]@1
  __int64 v19; // [sp+30h] [bp-30h]@1

  v3 = a3;
  v4 = off_69010[0];
  v19 = *(_QWORD *)off_69010[0];
  v18 = *a1;
  v5 = &a3[2 * v18];
  v6 = ccn_n(v18, (__int64)&a3[2 * v18]);
  v7 = -1;
  if ( v6 )
  {
    v17 = &v13;
    v14 = a2;
    v8 = (8 * v18 + 15) & 0xFFFFFFFFFFFFFFF0LL;
    v9 = (__int64)((char *)&v13 - v8);
    v15 = v3;
    v10 = (__int64)((char *)&v13 - v8);
    v16 = cczp_mod_inv((__int64)a1, (char *)&v13 - v8, v5);
    cczp_sqr(a1, v10, v9);
    v11 = v15;
    cczp_mul(a1, a2, v10, v15);
    v4 = off_69010[0];
    cczp_mul(a1, v10, v10, (unsigned __int64 *)v9);
    cczp_mul(a1, v14 + 8 * v18, v10, &v11[v18]);
    v7 = v16;
  }
  result = *(_QWORD *)v4;
  if ( *(_QWORD *)v4 == v19 )
    result = (unsigned int)v7;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000023185) ----------------------------------------------------
__int64 __fastcall ccec_affinify_x_only(signed __int64 *a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r15@1
  signed __int64 v4; // r14@1
  __int64 v5; // rbx@1
  signed int v6; // er13@1
  unsigned __int64 v7; // rax@2
  char *v8; // r15@2
  __int64 v9; // r14@2
  __int64 v10; // rdx@2
  __int64 result; // rax@3
  __int64 v12; // [sp+0h] [bp-50h]@2
  __int64 *v13; // [sp+8h] [bp-48h]@2
  __int64 v14; // [sp+10h] [bp-40h]@1
  __int64 v15; // [sp+18h] [bp-38h]@1
  __int64 v16; // [sp+20h] [bp-30h]@1

  v15 = a3;
  v14 = a2;
  v3 = off_69010[0];
  v16 = *(_QWORD *)off_69010[0];
  v4 = *a1;
  v5 = a3 + 16 * *a1;
  v6 = -1;
  if ( ccn_n(*a1, v5) )
  {
    v13 = &v12;
    v7 = (8 * v4 + 15) & 0xFFFFFFFFFFFFFFF0LL;
    v8 = (char *)&v12 - v7;
    v9 = (__int64)((char *)&v12 - v7);
    v6 = cczp_mod_inv((__int64)a1, (char *)&v12 - v7, (const void *)v5);
    v10 = (__int64)v8;
    v3 = off_69010[0];
    cczp_sqr(a1, v9, v10);
    cczp_mul(a1, v14, v15, (unsigned __int64 *)v9);
  }
  result = *(_QWORD *)v3;
  if ( *(_QWORD *)v3 == v16 )
    result = (unsigned int)v6;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000023252) ----------------------------------------------------
#error "233A8: call analysis failed (funcsize=222)"

//----- (0000000000023573) ----------------------------------------------------
__int64 __fastcall ccec_compute_key(__int64 **a1, __int64 a2, __int64 a3, void *a4)
{
  __int64 v4; // r12@1
  __int64 *v5; // rbx@1
  __int64 v6; // r13@1
  __int64 v7; // r14@6
  unsigned __int64 v8; // r15@6
  bool v9; // cf@6
  unsigned __int64 v10; // rax@7
  size_t v11; // rbx@7
  void *v12; // r12@9
  __int64 result; // rax@10
  void *v14; // [sp+0h] [bp-40h]@1
  __int64 v15; // [sp+8h] [bp-38h]@1
  __int64 v16; // [sp+10h] [bp-30h]@1

  v4 = a3;
  v16 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v6 = (__int64)((char *)&v14 - ((24 * **a1 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v15 = 0xFFFFFFFFLL;
  if ( v5 == *(__int64 **)a2 )
  {
    v14 = a4;
    if ( !(unsigned int)ccec_check_pub((__int64 **)a2)
      && !(unsigned int)ccec_mult(v5, v6, (__int64)&a1[3 * **a1 + 2], (const void *)(a2 + 16))
      && ccec_is_point_projective(v5, v6)
      && !(unsigned int)ccec_affinify_x_only(v5, v6, v6) )
    {
      v7 = *v5;
      v8 = ccn_write_uint_size(*v5, (__int64)(v5 + 2));
      v9 = *(_QWORD *)v4 < v8;
      *(_QWORD *)v4 = v8;
      if ( !v9 )
      {
        v10 = ccn_write_uint_size(v7, v6);
        v11 = v8 - v10;
        if ( v8 <= v10 )
          v11 = 0LL;
        v15 = 0LL;
        v12 = v14;
        bzero(v14, v11);
        ccn_write_uint(v7, v6, v8 - v11, (__int64)((char *)v12 + v11));
      }
    }
  }
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v16 )
    result = v15;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000236B1) ----------------------------------------------------
__int64 *ccec_cp_192()
{
  return &ccec_cp192;
}
// 694A8: using guessed type __int64 ccec_cp192;

//----- (00000000000236BE) ----------------------------------------------------
__int64 __fastcall ccn_mod_192(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r13@1
  __int64 v4; // rbx@1
  __int64 v5; // r12@1
  __int64 v6; // rbx@1
  __int64 i; // r14@1
  __int64 v9; // [sp+0h] [bp-B0h]@1
  __int64 v10; // [sp+8h] [bp-A8h]@1
  __int64 v11; // [sp+10h] [bp-A0h]@1
  __int64 v12; // [sp+20h] [bp-90h]@1
  __int64 v13; // [sp+28h] [bp-88h]@1
  __int64 v14; // [sp+30h] [bp-80h]@1
  __int64 v15; // [sp+40h] [bp-70h]@1
  __int64 v16; // [sp+48h] [bp-68h]@1
  __int64 v17; // [sp+50h] [bp-60h]@1
  __int64 v18; // [sp+60h] [bp-50h]@1
  __int64 v19; // [sp+68h] [bp-48h]@1
  __int64 v20; // [sp+70h] [bp-40h]@1
  __int64 v21; // [sp+80h] [bp-30h]@1

  v3 = off_69010[0];
  v21 = *(_QWORD *)off_69010[0];
  v18 = *(_QWORD *)a3;
  v19 = *(_QWORD *)(a3 + 8);
  v20 = *(_QWORD *)(a3 + 16);
  v15 = *(_QWORD *)(a3 + 24);
  v16 = v15;
  v17 = 0LL;
  v12 = 0LL;
  v13 = *(_QWORD *)(a3 + 32);
  v14 = v13;
  v9 = *(_QWORD *)(a3 + 40);
  v10 = v9;
  v11 = v9;
  v4 = ccn_add(3LL, a2, (__int64)&v18, (__int64)&v15);
  v5 = v4 + ccn_add(3LL, a2, a2, (__int64)&v12);
  v6 = v5 + ccn_add(3LL, a2, a2, (__int64)&v9);
  for ( i = a1 + 16; v6 || (signed int)ccn_cmp(3LL, a2, i) >= 0; v6 -= ccn_sub(3LL, a2, a2, i) )
    ;
  return *(_QWORD *)v3;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000237EA) ----------------------------------------------------
__int64 *ccec_cp_224()
{
  return &ccec_cp224;
}
// 69560: using guessed type __int64 ccec_cp224;

//----- (00000000000237F7) ----------------------------------------------------
__int64 __fastcall ccn_mod_224(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r15@1
  __int64 v4; // r13@1
  __int64 v5; // r8@1
  __int64 v6; // rsi@1
  __int64 v7; // r9@1
  __int64 v8; // rax@1
  __int64 v9; // rbx@1
  __int64 v10; // rcx@1
  __int64 v11; // rbx@1
  __int64 v12; // r12@1
  __int64 v13; // r12@1
  __int64 v14; // rbx@1
  __int64 v15; // rcx@1
  __int64 v16; // rsi@1
  __int64 v17; // rdx@1
  __int64 v19; // [sp+0h] [bp-D0h]@1
  __int64 v20; // [sp+8h] [bp-C8h]@1
  __int64 v21; // [sp+10h] [bp-C0h]@1
  __int64 v22; // [sp+18h] [bp-B8h]@1
  __int64 v23; // [sp+20h] [bp-B0h]@1
  __int64 v24; // [sp+28h] [bp-A8h]@1
  __int64 v25; // [sp+30h] [bp-A0h]@1
  __int64 v26; // [sp+38h] [bp-98h]@1
  __int64 v27; // [sp+40h] [bp-90h]@1
  __int64 v28; // [sp+48h] [bp-88h]@1
  __int64 v29; // [sp+50h] [bp-80h]@1
  __int64 v30; // [sp+58h] [bp-78h]@1
  __int64 v31; // [sp+60h] [bp-70h]@1
  __int64 v32; // [sp+68h] [bp-68h]@1
  __int64 v33; // [sp+70h] [bp-60h]@1
  __int64 v34; // [sp+78h] [bp-58h]@1
  unsigned __int64 v35; // [sp+80h] [bp-50h]@1
  unsigned __int64 v36; // [sp+88h] [bp-48h]@1
  unsigned __int64 v37; // [sp+90h] [bp-40h]@1
  __int64 v38; // [sp+98h] [bp-38h]@1
  __int64 v39; // [sp+A0h] [bp-30h]@1

  v3 = a2;
  v4 = off_69010[0];
  v39 = *(_QWORD *)off_69010[0];
  v35 = ((unsigned __int64)*(_DWORD *)(a3 + 4) << 32) | *(_DWORD *)a3;
  v36 = ((unsigned __int64)*(_DWORD *)(a3 + 12) << 32) | *(_DWORD *)(a3 + 8);
  v37 = ((unsigned __int64)*(_DWORD *)(a3 + 20) << 32) | *(_DWORD *)(a3 + 16);
  v38 = *(_DWORD *)(a3 + 24);
  v31 = 0LL;
  v5 = *(_DWORD *)(a3 + 28);
  v32 = v5 << 32;
  v6 = *(_DWORD *)(a3 + 32);
  v7 = *(_DWORD *)(a3 + 36);
  v33 = v6 | (v7 << 32);
  v34 = *(_DWORD *)(a3 + 40);
  v27 = 0LL;
  v8 = *(_DWORD *)(a3 + 44);
  v28 = v8 << 32;
  v9 = *(_DWORD *)(a3 + 48);
  v10 = *(_DWORD *)(a3 + 52);
  v29 = v9 | (v10 << 32);
  v30 = 0LL;
  v23 = v5 | (v6 << 32);
  v24 = v7 | (v34 << 32);
  v25 = v8 | (v9 << 32);
  v26 = v10;
  v19 = v8 | (v9 << 32);
  v20 = v10;
  v21 = 0LL;
  v22 = 0LL;
  v11 = ccn_add(4LL, v3, (__int64)&v35, (__int64)&v31);
  v12 = v11 + ccn_add(4LL, v3, v3, (__int64)&v27);
  v13 = v12 - ccn_sub(4LL, (__int64)&v23, a1 + 16, (__int64)&v23);
  v14 = v13 + ccn_add(4LL, v3, v3, (__int64)&v23);
  v15 = (__int64)&v19;
  v16 = v3;
  v17 = v3;
  while ( 1 )
  {
    v14 -= ccn_sub(4LL, v16, v17, v15);
    if ( !v14 && (signed int)ccn_cmp(4LL, v3, a1 + 16) < 0 )
      break;
    v16 = v3;
    v17 = v3;
    v15 = a1 + 16;
  }
  return *(_QWORD *)v4;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000239EC) ----------------------------------------------------
__int64 *ccec_cp_256()
{
  return &ccec_cp256;
}
// 69648: using guessed type __int64 ccec_cp256;

//----- (00000000000239F9) ----------------------------------------------------
__int64 __fastcall ccn_mod_256(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r13@1
  __int64 v4; // r14@1
  __int64 v5; // r10@1
  __int64 v6; // r12@1
  __int64 v7; // ST10_8@1
  __int64 v8; // rcx@1
  __int64 v9; // rbx@1
  __int64 v10; // ST20_8@1
  __int64 v11; // rax@1
  __int64 v12; // r11@1
  __int64 v13; // rdi@1
  __int64 v14; // ST18_8@1
  __int64 v15; // ST08_8@1
  __int64 v16; // rdi@1
  __int64 v17; // r14@1
  __int64 v18; // r12@1
  __int64 v19; // r12@1
  __int64 v20; // r12@1
  __int64 v21; // r12@1
  __int64 v22; // r12@1
  __int64 v23; // r15@1
  __int64 v24; // r12@1
  __int64 v25; // rbx@1
  __int64 v26; // r15@1
  __int64 v27; // r12@1
  __int64 v28; // rbx@1
  __int64 v29; // r15@1
  __int64 v30; // rbx@1
  __int64 v31; // r15@1
  __int64 i; // rbx@1
  char v34; // [sp+30h] [bp-170h]@1
  __int64 v35; // [sp+50h] [bp-150h]@1
  __int64 v36; // [sp+58h] [bp-148h]@1
  unsigned __int64 v37; // [sp+60h] [bp-140h]@1
  unsigned __int64 v38; // [sp+68h] [bp-138h]@1
  __int64 v39; // [sp+70h] [bp-130h]@1
  __int64 v40; // [sp+78h] [bp-128h]@1
  __int64 v41; // [sp+80h] [bp-120h]@1
  __int64 v42; // [sp+88h] [bp-118h]@1
  __int64 v43; // [sp+90h] [bp-110h]@1
  __int64 v44; // [sp+98h] [bp-108h]@1
  __int64 v45; // [sp+A0h] [bp-100h]@1
  __int64 v46; // [sp+A8h] [bp-F8h]@1
  __int64 v47; // [sp+B0h] [bp-F0h]@1
  __int64 v48; // [sp+B8h] [bp-E8h]@1
  __int64 v49; // [sp+C0h] [bp-E0h]@1
  __int64 v50; // [sp+C8h] [bp-D8h]@1
  __int64 v51; // [sp+D0h] [bp-D0h]@1
  __int64 v52; // [sp+D8h] [bp-C8h]@1
  __int64 v53; // [sp+E0h] [bp-C0h]@1
  __int64 v54; // [sp+E8h] [bp-B8h]@1
  __int64 v55; // [sp+F0h] [bp-B0h]@1
  __int64 v56; // [sp+F8h] [bp-A8h]@1
  __int64 v57; // [sp+100h] [bp-A0h]@1
  __int64 v58; // [sp+108h] [bp-98h]@1
  __int64 v59; // [sp+110h] [bp-90h]@1
  __int64 v60; // [sp+118h] [bp-88h]@1
  __int64 v61; // [sp+120h] [bp-80h]@1
  __int64 v62; // [sp+128h] [bp-78h]@1
  __int64 v63; // [sp+130h] [bp-70h]@1
  __int64 v64; // [sp+138h] [bp-68h]@1
  __int64 v65; // [sp+140h] [bp-60h]@1
  __int64 v66; // [sp+148h] [bp-58h]@1
  unsigned __int64 v67; // [sp+150h] [bp-50h]@1
  unsigned __int64 v68; // [sp+158h] [bp-48h]@1
  unsigned __int64 v69; // [sp+160h] [bp-40h]@1
  unsigned __int64 v70; // [sp+168h] [bp-38h]@1
  __int64 v71; // [sp+170h] [bp-30h]@1

  v3 = a2;
  v4 = a1;
  v71 = *(_QWORD *)off_69010[0];
  v67 = ((unsigned __int64)*(_DWORD *)(a3 + 4) << 32) | *(_DWORD *)a3;
  v68 = ((unsigned __int64)*(_DWORD *)(a3 + 12) << 32) | *(_DWORD *)(a3 + 8);
  v69 = ((unsigned __int64)*(_DWORD *)(a3 + 20) << 32) | *(_DWORD *)(a3 + 16);
  v70 = ((unsigned __int64)*(_DWORD *)(a3 + 28) << 32) | *(_DWORD *)(a3 + 24);
  v63 = 0LL;
  v5 = *(_DWORD *)(a3 + 44);
  v64 = v5 << 32;
  v6 = *(_DWORD *)(a3 + 48);
  v7 = *(_DWORD *)(a3 + 52);
  v8 = *(_DWORD *)(a3 + 52);
  v65 = (v7 << 32) + v6;
  v9 = *(_DWORD *)(a3 + 56);
  v10 = *(_DWORD *)(a3 + 60);
  v66 = v9 | (v10 << 32);
  v59 = 0LL;
  v60 = v6 << 32;
  v61 = v8 | (v9 << 32);
  v62 = v10;
  v11 = *(_DWORD *)(a3 + 32);
  v12 = *(_DWORD *)(a3 + 36);
  v55 = (v12 << 32) + v11;
  v13 = *(_DWORD *)(a3 + 40);
  v14 = v13;
  v13 <<= 32;
  v15 = v13 + v12;
  v51 = v13 + v12;
  v52 = v5 | (v7 << 32);
  v16 = v11 | v13;
  v11 <<= 32;
  v53 = v9 | (v10 << 32);
  v54 = v11 + v7;
  v47 = (v6 << 32) | v5;
  v48 = v7;
  v49 = 0LL;
  v50 = v16;
  v43 = (v7 << 32) + v6;
  v44 = v9 | (v10 << 32);
  v45 = 0LL;
  v46 = v12 | (v5 << 32);
  v39 = v8 | (v9 << 32);
  v40 = v10 + v11;
  v41 = v15;
  v42 = v6 << 32;
  v56 = v14;
  v57 = 0LL;
  v58 = v9 | (v10 << 32);
  v35 = v9 | (v10 << 32);
  v36 = v12 << 32;
  v37 = v14 | ((unsigned __int64)*(_DWORD *)(a3 + 44) << 32);
  v38 = (unsigned __int64)*(_DWORD *)(a3 + 52) << 32;
  v17 = v4 + 16;
  v18 = 2 * ccn_add(4LL, (__int64)&v34, v17, v17);
  v19 = v18 - ccn_sub(4LL, (__int64)&v47, (__int64)&v34, (__int64)&v47);
  v20 = v19 - ccn_sub(4LL, (__int64)&v43, (__int64)&v34, (__int64)&v43);
  v21 = v20 - ccn_sub(4LL, (__int64)&v39, v17, (__int64)&v39);
  v22 = v21 - ccn_sub(4LL, (__int64)&v35, v17, (__int64)&v35);
  v23 = v22 + ccn_add(4LL, a2, (__int64)&v67, (__int64)&v63);
  v24 = v23 + ccn_add(4LL, v3, v3, (__int64)&v63);
  v25 = v24 + ccn_add(4LL, v3, v3, (__int64)&v59);
  v26 = v25 + ccn_add(4LL, v3, v3, (__int64)&v59);
  v27 = v26 + ccn_add(4LL, v3, v3, (__int64)&v55);
  v28 = v27 + ccn_add(4LL, v3, v3, (__int64)&v51);
  v29 = v28 + ccn_add(4LL, v3, v3, (__int64)&v47);
  v30 = v29 + ccn_add(4LL, v3, v3, (__int64)&v43);
  v31 = v30 + ccn_add(4LL, v3, v3, (__int64)&v39);
  for ( i = v31 + ccn_add(4LL, v3, v3, (__int64)&v35);
        i || (signed int)ccn_cmp(4LL, a2, v17) >= 0;
        i -= ccn_sub(4LL, a2, a2, v17) )
    ;
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000023E0B) ----------------------------------------------------
__int64 *ccec_cp_384()
{
  return &ccec_cp384;
}
// 69730: using guessed type __int64 ccec_cp384;

//----- (0000000000023E18) ----------------------------------------------------
__int64 __fastcall ccn_mod_384(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r15@1
  __int64 v4; // r14@1
  __int64 v5; // rbx@1
  __int64 v6; // ST30_8@1
  __int64 v7; // ST08_8@1
  __int64 v8; // r9@1
  __int64 v9; // ST28_8@1
  __int64 v10; // rdi@1
  __int64 v11; // r13@1
  __int64 v12; // rsi@1
  __int64 v13; // r8@1
  __int64 v14; // rcx@1
  __int64 v15; // r10@1
  __int64 v16; // rax@1
  __int64 v17; // rsi@1
  __int64 v18; // r8@1
  __int64 v19; // rdi@1
  __int64 v20; // r9@1
  __int64 v21; // rcx@1
  unsigned __int64 v22; // rdx@1
  __int64 v23; // r13@1
  __int64 v24; // rbx@1
  __int64 v25; // r12@1
  __int64 v26; // rbx@1
  __int64 v27; // r12@1
  __int64 v28; // rbx@1
  __int64 v29; // r12@1
  __int64 v30; // r14@1
  __int64 v31; // r12@1
  __int64 v32; // rbx@1
  __int64 v33; // rbx@1
  __int64 v34; // rcx@1
  __int64 v35; // rsi@1
  __int64 v36; // rdx@1
  __int64 v38; // [sp+40h] [bp-210h]@1
  __int64 v39; // [sp+48h] [bp-208h]@1
  __int64 v40; // [sp+50h] [bp-200h]@1
  __int64 v41; // [sp+58h] [bp-1F8h]@1
  __int64 v42; // [sp+60h] [bp-1F0h]@1
  __int64 v43; // [sp+68h] [bp-1E8h]@1
  unsigned __int64 v44; // [sp+70h] [bp-1E0h]@1
  __int64 v45; // [sp+78h] [bp-1D8h]@1
  __int64 v46; // [sp+80h] [bp-1D0h]@1
  __int64 v47; // [sp+88h] [bp-1C8h]@1
  __int64 v48; // [sp+90h] [bp-1C0h]@1
  __int64 v49; // [sp+98h] [bp-1B8h]@1
  unsigned __int64 v50; // [sp+A0h] [bp-1B0h]@1
  unsigned __int64 v51; // [sp+A8h] [bp-1A8h]@1
  unsigned __int64 v52; // [sp+B0h] [bp-1A0h]@1
  unsigned __int64 v53; // [sp+B8h] [bp-198h]@1
  unsigned __int64 v54; // [sp+C0h] [bp-190h]@1
  __int64 v55; // [sp+C8h] [bp-188h]@1
  __int64 v56; // [sp+D0h] [bp-180h]@1
  __int64 v57; // [sp+D8h] [bp-178h]@1
  __int64 v58; // [sp+E0h] [bp-170h]@1
  __int64 v59; // [sp+E8h] [bp-168h]@1
  __int64 v60; // [sp+F0h] [bp-160h]@1
  __int64 v61; // [sp+F8h] [bp-158h]@1
  __int64 v62; // [sp+100h] [bp-150h]@1
  __int64 v63; // [sp+108h] [bp-148h]@1
  __int64 v64; // [sp+110h] [bp-140h]@1
  __int64 v65; // [sp+118h] [bp-138h]@1
  __int64 v66; // [sp+120h] [bp-130h]@1
  __int64 v67; // [sp+128h] [bp-128h]@1
  __int64 v68; // [sp+130h] [bp-120h]@1
  __int64 v69; // [sp+138h] [bp-118h]@1
  __int64 v70; // [sp+140h] [bp-110h]@1
  __int64 v71; // [sp+148h] [bp-108h]@1
  __int64 v72; // [sp+150h] [bp-100h]@1
  __int64 v73; // [sp+158h] [bp-F8h]@1
  __int64 v74; // [sp+160h] [bp-F0h]@1
  __int64 v75; // [sp+168h] [bp-E8h]@1
  __int64 v76; // [sp+170h] [bp-E0h]@1
  __int64 v77; // [sp+178h] [bp-D8h]@1
  __int64 v78; // [sp+180h] [bp-D0h]@1
  __int64 v79; // [sp+188h] [bp-C8h]@1
  __int64 v80; // [sp+190h] [bp-C0h]@1
  __int64 v81; // [sp+198h] [bp-B8h]@1
  __int64 v82; // [sp+1A0h] [bp-B0h]@1
  __int64 v83; // [sp+1A8h] [bp-A8h]@1
  __int64 v84; // [sp+1B0h] [bp-A0h]@1
  __int64 v85; // [sp+1B8h] [bp-98h]@1
  __int64 v86; // [sp+1C0h] [bp-90h]@1
  __int64 v87; // [sp+1C8h] [bp-88h]@1
  __int64 v88; // [sp+1D0h] [bp-80h]@1
  __int64 v89; // [sp+1D8h] [bp-78h]@1
  __int64 v90; // [sp+1E0h] [bp-70h]@1
  __int64 v91; // [sp+1E8h] [bp-68h]@1
  unsigned __int64 v92; // [sp+1F0h] [bp-60h]@1
  unsigned __int64 v93; // [sp+1F8h] [bp-58h]@1
  unsigned __int64 v94; // [sp+200h] [bp-50h]@1
  unsigned __int64 v95; // [sp+208h] [bp-48h]@1
  unsigned __int64 v96; // [sp+210h] [bp-40h]@1
  unsigned __int64 v97; // [sp+218h] [bp-38h]@1
  __int64 v98; // [sp+220h] [bp-30h]@1

  v3 = a2;
  v4 = a1;
  v98 = *(_QWORD *)off_69010[0];
  v92 = ((unsigned __int64)*(_DWORD *)(a3 + 4) << 32) | *(_DWORD *)a3;
  v93 = ((unsigned __int64)*(_DWORD *)(a3 + 12) << 32) | *(_DWORD *)(a3 + 8);
  v94 = ((unsigned __int64)*(_DWORD *)(a3 + 20) << 32) | *(_DWORD *)(a3 + 16);
  v95 = ((unsigned __int64)*(_DWORD *)(a3 + 28) << 32) | *(_DWORD *)(a3 + 24);
  v96 = ((unsigned __int64)*(_DWORD *)(a3 + 36) << 32) | *(_DWORD *)(a3 + 32);
  v97 = ((unsigned __int64)*(_DWORD *)(a3 + 44) << 32) | *(_DWORD *)(a3 + 40);
  v86 = 0LL;
  v87 = 0LL;
  v5 = *(_DWORD *)(a3 + 84);
  v6 = *(_DWORD *)(a3 + 88);
  v88 = v5 | (v6 << 32);
  v7 = *(_DWORD *)(a3 + 92);
  v89 = *(_DWORD *)(a3 + 92);
  v90 = 0LL;
  v91 = 0LL;
  v8 = *(_DWORD *)(a3 + 48);
  v9 = *(_DWORD *)(a3 + 52);
  v80 = v8 | (v9 << 32);
  v10 = *(_DWORD *)(a3 + 56);
  v11 = *(_DWORD *)(a3 + 60);
  v81 = v10 | (v11 << 32);
  v12 = *(_DWORD *)(a3 + 64);
  v13 = *(_DWORD *)(a3 + 68);
  v82 = v12 | (v13 << 32);
  v14 = *(_DWORD *)(a3 + 72);
  v15 = *(_DWORD *)(a3 + 76);
  v83 = v14 | (v15 << 32);
  v16 = *(_DWORD *)(a3 + 80);
  v84 = v16 | (v5 << 32);
  v85 = (v7 << 32) | v6;
  v74 = v5 | (v6 << 32);
  v75 = v7 + (v8 << 32);
  v76 = v9 + (v10 << 32);
  v77 = v11 + (v12 << 32);
  v78 = v13 + (v14 << 32);
  v17 = v16;
  v16 <<= 32;
  v79 = v16 | v15;
  v68 = v7 << 32;
  v69 = v16;
  v70 = v8 | (v9 << 32);
  v71 = v10 | (v11 << 32);
  v72 = v82;
  v73 = v14 | (v15 << 32);
  v62 = 0LL;
  v63 = 0LL;
  v18 = *(_DWORD *)(a3 + 84);
  v64 = (v18 << 32) + v17;
  v19 = *(_DWORD *)(a3 + 88);
  v20 = *(_DWORD *)(a3 + 92);
  v56 = v17;
  v65 = (v20 << 32) + v19;
  v66 = 0LL;
  v67 = 0LL;
  v57 = v18 << 32;
  v58 = (v20 << 32) + v19;
  v59 = 0LL;
  v60 = 0LL;
  v61 = 0LL;
  v50 = v20 | ((unsigned __int64)*(_DWORD *)(a3 + 48) << 32);
  v51 = ((unsigned __int64)*(_DWORD *)(a3 + 56) << 32) | *(_DWORD *)(a3 + 52);
  v52 = ((unsigned __int64)*(_DWORD *)(a3 + 64) << 32) | *(_DWORD *)(a3 + 60);
  v53 = ((unsigned __int64)*(_DWORD *)(a3 + 72) << 32) | *(_DWORD *)(a3 + 68);
  v21 = *(_DWORD *)(a3 + 76);
  v22 = (unsigned __int64)*(_DWORD *)(a3 + 80) << 32;
  v54 = v22 | v21;
  v44 = v22;
  v55 = v18 | (v19 << 32);
  v45 = v18 | (v19 << 32);
  v38 = 0LL;
  v39 = v20 << 32;
  v46 = v20;
  v40 = v20;
  v47 = 0LL;
  v48 = 0LL;
  v49 = 0LL;
  v41 = 0LL;
  v42 = 0LL;
  v43 = 0LL;
  v23 = ccn_add(6LL, v3, (__int64)&v92, (__int64)&v86);
  v24 = v23 + ccn_add(6LL, v3, v3, (__int64)&v86);
  v25 = v24 + ccn_add(6LL, v3, v3, (__int64)&v80);
  v26 = v25 + ccn_add(6LL, v3, v3, (__int64)&v74);
  v27 = v26 + ccn_add(6LL, v3, v3, (__int64)&v68);
  v28 = v27 + ccn_add(6LL, v3, v3, (__int64)&v62);
  v29 = v28 + ccn_add(6LL, v3, v3, (__int64)&v56);
  v30 = v4 + 16;
  v31 = v29 - ccn_sub(6LL, (__int64)&v50, v30, (__int64)&v50);
  v32 = v31 + ccn_add(6LL, v3, v3, (__int64)&v50);
  v33 = v32 - ccn_sub(6LL, v3, v3, (__int64)&v44);
  v34 = (__int64)&v38;
  v35 = v3;
  v36 = v3;
  while ( 1 )
  {
    v33 -= ccn_sub(6LL, v35, v36, v34);
    if ( !v33 && (signed int)ccn_cmp(6LL, v3, v30) < 0 )
      break;
    v35 = v3;
    v36 = v3;
    v34 = v30;
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000024326) ----------------------------------------------------
__int64 *ccec_cp_521()
{
  return &ccec_cp521;
}
// 69878: using guessed type __int64 ccec_cp521;

//----- (0000000000024333) ----------------------------------------------------
__int64 __fastcall ccn_mod_521(__int64 *a1, __int64 a2, const void *a3)
{
  const void *v3; // rbx@1
  __int64 v4; // r13@1
  char v6; // [sp+0h] [bp-80h]@1
  __int64 v7; // [sp+40h] [bp-40h]@1
  __int64 v8; // [sp+50h] [bp-30h]@1

  v3 = a3;
  v4 = off_69010[0];
  v8 = *(_QWORD *)off_69010[0];
  memcpy(&v6, a3, 0x40uLL);
  v7 = *((_QWORD *)v3 + 8) & 0x1FFLL;
  ccn_shift_right(9LL, (void *)a2, (char *)v3 + 64, 9LL);
  cczp_add(a1, a2, (__int64)&v6, a2);
  return *(_QWORD *)v4;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000243C5) ----------------------------------------------------
unsigned int __usercall ccmode_gcm_set_iv@<eax>(unsigned int result@<eax>, __int64 a2@<rdx>, __int64 a3@<rdi>, unsigned __int64 a4@<rsi>, __m128i a5@<xmm0>, __m128i a6@<xmm1>, __m128i a7@<xmm2>, __m128i a8@<xmm3>)
{
  __int64 v8; // r15@1
  unsigned __int64 v9; // r13@4
  unsigned __int64 v10; // r13@6
  unsigned __int64 v11; // rcx@7
  char v12; // cl@14
  signed __int64 v13; // rcx@14

  v8 = a2;
  if ( !*(_DWORD *)(a3 + 92) )
  {
    result = *(_DWORD *)(a3 + 96);
    if ( a4 + *(_DWORD *)(a3 + 96) >= 0xD )
      *(_BYTE *)(a3 + 88) |= 1u;
    v9 = 0LL;
    if ( !result )
    {
      v9 = 0LL;
      if ( (unsigned __int64)((unsigned int)a4 & 0xFFFFFFF0) )
      {
        v10 = 0LL;
        do
        {
          v11 = 0LL;
          do
          {
            *(_QWORD *)(a3 + v11 + 16) ^= *(_QWORD *)(v8 + v10 + v11);
            v11 += 8LL;
          }
          while ( v11 < 0x10 );
          result = ccmode_gcm_mult_h(a3, (__m128i *)(a3 + 16), a5, a6, a7, a8);
          *(_QWORD *)(a3 + 104) += 128LL;
          v10 += 16LL;
        }
        while ( v10 < (a4 & 0xFFFFFFF0) );
        v9 = (unsigned int)a4 & 0xFFFFFFF0;
      }
      v8 += v9;
    }
    if ( v9 < a4 )
    {
      result = *(_DWORD *)(a3 + 96);
      do
      {
        v12 = *(_BYTE *)v8;
        *(_DWORD *)(a3 + 96) = result + 1;
        *(_BYTE *)(a3 + result + 64) = v12;
        result = *(_DWORD *)(a3 + 96);
        v13 = 64LL;
        if ( result == 16 )
        {
          do
          {
            *(_BYTE *)(a3 + v13 - 48) ^= *(_BYTE *)(a3 + v13);
            ++v13;
          }
          while ( v13 != 80 );
          ccmode_gcm_mult_h(a3, (__m128i *)(a3 + 16), a5, a6, a7, a8);
          *(_DWORD *)(a3 + 96) = 0;
          *(_QWORD *)(a3 + 104) += 128LL;
          result = 0;
        }
        ++v8;
        ++v9;
      }
      while ( v9 != a4 );
    }
  }
  return result;
}

//----- (00000000000244BE) ----------------------------------------------------
int __usercall ccec_alprint@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 *a3@<rdi>)
{
  __int64 v3; // r15@1
  __int64 v4; // rax@1
  __int64 v5; // rbx@1
  __int64 v6; // rax@1

  v3 = a2;
  LODWORD(v4) = printf("%s { x -> ", a1);
  v5 = *a3;
  ccn_print(v4, *a3, v3);
  LODWORD(v6) = printf(", y -> ", v3);
  ccn_print(v6, v5, v3 + 8 * v5);
  return printf("}\n");
}

//----- (000000000002451C) ----------------------------------------------------
int __usercall ccec_plprint@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 *a3@<rdi>)
{
  __int64 v3; // r15@1
  __int64 v4; // rax@1
  __int64 v5; // rbx@1
  __int64 v6; // rax@1
  __int64 v7; // rax@1

  v3 = a2;
  LODWORD(v4) = printf("%s { x -> ", a1);
  v5 = *a3;
  ccn_print(v4, *a3, v3);
  LODWORD(v6) = printf(", y -> ", v3);
  ccn_print(v6, v5, v3 + 8 * v5);
  LODWORD(v7) = printf(", z -> ");
  ccn_print(v7, v5, v3 + 16 * v5);
  return printf("}\n");
}

//----- (000000000002459A) ----------------------------------------------------
int __usercall ccec_print_full_key@<eax>(__int64 a1@<rax>, __int64 a2@<rdi>, __int64 **a3@<rsi>)
{
  __int64 **v3; // rbx@1
  __int64 v4; // rax@1

  v3 = a3;
  LODWORD(v4) = printf("full key %s { \n", a2, a1);
  ccec_plprint(v4, (__int64)(v3 + 2), *v3);
  printf("\npriv: ", "pubkey:");
  ccn_print(3 * **v3, **v3, (__int64)&v3[3 * **v3 + 2]);
  return printf("}\n");
}

//----- (0000000000024600) ----------------------------------------------------
int __fastcall ccec_print_public_key(__int64 a1, __int64 **a2)
{
  __int64 v2; // rax@1

  LODWORD(v2) = printf("public key ");
  return ccec_plprint(v2, (__int64)(a2 + 2), *a2);
}

//----- (000000000002462E) ----------------------------------------------------
int __usercall ccec_print_sig@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>)
{
  __int64 v4; // r14@1
  __int64 v5; // rbx@1

  v4 = a2;
  v5 = a4;
  printf("%s { %lu, ", a3, a4, a1);
  if ( a4 )
  {
    do
      printf("%.02x", *(_BYTE *)(v4 + v5-- - 1));
    while ( v5 );
  }
  return printf("\n");
}

//----- (000000000002468E) ----------------------------------------------------
__int64 __fastcall ccec_double(signed __int64 *a1, void *a2, const void *a3)
{
  __int64 v3; // rbx@1
  unsigned __int64 v4; // rax@1
  char *v5; // r12@1
  __int64 v6; // r14@1
  __int64 v7; // r15@1
  const void *v8; // r13@1
  __int64 *v9; // rbx@2
  __int64 v10; // r13@2
  __int64 v11; // r14@2
  __int64 v12; // r13@2
  const void *v13; // r13@2
  __int64 v14; // rbx@2
  void *v15; // r14@2
  void *v16; // rbx@3
  __int64 v17; // r15@3
  size_t v18; // r14@3
  __int64 v19; // rax@3
  __int64 v21; // [sp+0h] [bp-70h]@1
  signed __int64 *v22; // [sp+8h] [bp-68h]@1
  __int64 v23; // [sp+10h] [bp-60h]@1
  __int64 v24; // [sp+18h] [bp-58h]@2
  __int64 v25; // [sp+20h] [bp-50h]@1
  __int64 v26; // [sp+28h] [bp-48h]@1
  void *v27; // [sp+30h] [bp-40h]@1
  __int64 v28; // [sp+38h] [bp-38h]@1
  __int64 v29; // [sp+40h] [bp-30h]@1

  v27 = a2;
  v22 = a1;
  v29 = *(_QWORD *)off_69010[0];
  v3 = *a1;
  v26 = v3;
  v4 = (8 * v3 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v23 = (__int64)((char *)&v21 - v4);
  v5 = (char *)&v21 - v4;
  v6 = (__int64)((char *)&v21 - v4);
  v28 = (__int64)((char *)&v21 - v4);
  v7 = (__int64)((char *)&v21 - v4);
  v8 = a3;
  ccn_set(v3, (char *)&v21 - v4, a3);
  ccn_set(v3, v5, (char *)v8 + 8 * v3);
  v25 = 2 * v3;
  ccn_set(v3, (void *)v6, (char *)v8 + 16 * v3);
  if ( ccn_n(v3, v6) )
  {
    v9 = v22;
    v10 = v28;
    v24 = v6;
    cczp_sqr(v22, v28, v6);
    v11 = v23;
    cczp_sub(v9, v7, v23, v10);
    cczp_add(v9, v10, v11, v10);
    cczp_mul(v9, v7, v10, (unsigned __int64 *)v7);
    cczp_add(v9, v10, v7, v7);
    cczp_add(v9, v10, v10, v7);
    v12 = v24;
    cczp_mul(v9, v24, v24, (unsigned __int64 *)v5);
    cczp_add(v9, v12, v12, v12);
    cczp_sqr(v9, (__int64)v5, (__int64)v5);
    cczp_mul(v9, v7, v11, (unsigned __int64 *)v5);
    cczp_add(v9, v7, v7, v7);
    cczp_add(v9, v7, v7, v7);
    cczp_sqr(v9, v11, v28);
    cczp_sub(v9, v11, v11, v7);
    cczp_sub(v9, v11, v11, v7);
    cczp_sqr(v9, (__int64)v5, (__int64)v5);
    cczp_add(v9, (__int64)v5, (__int64)v5, (__int64)v5);
    cczp_add(v9, (__int64)v5, (__int64)v5, (__int64)v5);
    cczp_add(v9, (__int64)v5, (__int64)v5, (__int64)v5);
    v13 = (const void *)v11;
    cczp_sub(v9, v7, v7, v11);
    cczp_mul(v9, v7, v28, (unsigned __int64 *)v7);
    cczp_sub(v9, (__int64)v5, v7, (__int64)v5);
    v14 = v26;
    v15 = v27;
    ccn_set(v26, v27, v13);
    ccn_set(v14, (char *)v15 + 8 * v14, v5);
    ccn_set(v14, (char *)v15 + 8 * v25, (const void *)v24);
  }
  else
  {
    v16 = v27;
    *(_QWORD *)v27 = 1LL;
    v17 = v26;
    v18 = 8 * v26 - 8;
    bzero((char *)v16 + 8, v18);
    *((_QWORD *)v16 + v17) = 1LL;
    bzero((char *)v16 + 8 * v17 + 8, v18);
    v19 = v25;
    *((_QWORD *)v16 + v25) = 0LL;
    bzero((char *)v16 + 8 * v19 + 8, v18);
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002497C) ----------------------------------------------------
__int64 __fastcall ccder_decode_rsa_priv(__int64 a1, unsigned __int64 a2, unsigned __int64 a3)
{
  __int64 v3; // rax@1
  __int64 v4; // rbx@1
  unsigned __int64 v5; // r15@1
  __int64 v6; // r12@1
  signed __int64 v7; // rdx@1
  signed __int64 v8; // r12@1
  char *v9; // r13@1
  signed __int64 v10; // rax@1
  __int64 v11; // rax@1
  __int64 v12; // rax@1
  __int64 v13; // rax@1
  __int64 v14; // rax@1
  __int64 v15; // rcx@1
  __int64 result; // rax@1
  signed __int64 v17; // r15@2
  __int64 v18; // r14@2
  __int64 v19; // rax@2
  __int64 v20; // rax@2
  __int64 v21; // r15@2
  const void *v22; // rbx@3
  __int64 v23; // rax@3
  signed __int64 v24; // rcx@3
  unsigned __int64 v25; // rbx@3
  __int64 v26; // rax@3
  __int64 v27; // rax@3
  __int64 v28; // rbx@3
  __int64 v29; // rcx@5
  __int64 v30; // [sp+0h] [bp-60h]@1
  unsigned __int64 v31; // [sp+8h] [bp-58h]@2
  unsigned __int64 v32; // [sp+10h] [bp-50h]@2
  __int64 *v33; // [sp+18h] [bp-48h]@1
  unsigned __int64 v34; // [sp+20h] [bp-40h]@1
  __int64 v35; // [sp+28h] [bp-38h]@1
  __int64 v36; // [sp+30h] [bp-30h]@1

  v3 = a3;
  v4 = a1;
  v36 = *(_QWORD *)off_69010[0];
  v34 = a3;
  v5 = *(_QWORD *)a1;
  v6 = *(_QWORD *)a1 >> 1;
  v7 = 8 * v6 + 23;
  v8 = v6 + 1;
  v7 &= 0xFFFFFFFFFFFFFFF0LL;
  v9 = (char *)&v30 - v7;
  v33 = (__int64 *)((char *)&v30 - v7);
  v35 = 0LL;
  v10 = ccder_decode_constructed_tl(v3, a2, v34, 2305843009213693968LL, (__int64)&v34);
  v11 = ccder_decode_uint(v10, v10, v34, 1uLL, (__int64)&v35);
  v12 = ccder_decode_uint(v11, v11, v34, v5, a1 + 16);
  v13 = ccder_decode_uint(v12, v12, v34, v5, 16LL * *(_QWORD *)v4 + v4 + 24);
  v14 = ccder_decode_uint(v13, v13, v34, v5, v4 + 24LL * *(_QWORD *)v4 + 24);
  v15 = ccder_decode_uint(v14, v14, v34, v8, (__int64)v9);
  result = 0LL;
  if ( v15 )
  {
    v17 = 32 * v5;
    v30 = a1;
    v18 = a1 + v17 + 24;
    v17 |= 0x18uLL;
    v32 = v15;
    v19 = (unsigned __int64)(ccn_bitlen(v8, (__int64)v9) + 63) >> 6;
    v31 = v19;
    *(_QWORD *)(a1 + v17) = v19;
    ccn_set(v19, (void *)(a1 + v17 + 16), v9);
    v20 = cczp_init(v18);
    v21 = ccder_decode_uint(v20, v32, v34, v8, (__int64)v33);
    result = 0LL;
    if ( v21 )
    {
      v22 = v33;
      v23 = (unsigned __int64)(ccn_bitlen(v8, (__int64)v33) + 63) >> 6;
      v24 = 16LL * *(_QWORD *)v18;
      *(_QWORD *)(v24 + v18 + 24) = v23;
      ccn_set(v23, (void *)(v24 + v18 + 40), v22);
      cczp_init(16LL * *(_QWORD *)v18 + v18 + 24);
      v25 = v31;
      v26 = ccder_decode_uint(32LL * *(_QWORD *)v18, v21, v34, v31, 32LL * *(_QWORD *)v18 + v18 + 64);
      v27 = ccder_decode_uint(
              v26,
              v26,
              v34,
              *(_QWORD *)(16LL * *(_QWORD *)v18 + v18 + 24),
              v18 + 40LL * *(_QWORD *)v18 + 64);
      v28 = ccder_decode_uint(v27, v27, v34, v25, v18 + 48LL * *(_QWORD *)v18 + 64);
      result = 0LL;
      if ( v28 )
      {
        cczp_init(v30);
        result = v28;
      }
    }
  }
  v29 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000024BBB) ----------------------------------------------------
unsigned __int64 __fastcall ccec_export_pub(__int64 **a1, __int64 a2)
{
  __int64 *v2; // r15@1
  unsigned __int64 v3; // rbx@1
  __int64 v4; // r15@1
  unsigned __int64 v5; // rax@1
  size_t v6; // r13@1
  __int64 v7; // r12@3
  unsigned __int64 v8; // rax@3
  size_t v9; // r13@3

  v2 = *a1;
  v3 = (unsigned __int64)(ccn_bitlen(**a1, (__int64)(*a1 + 2)) + 7) >> 3;
  v4 = *v2;
  *(_BYTE *)a2 = 4;
  v5 = ccn_write_uint_size(v4, (__int64)(a1 + 2));
  v6 = v3 - v5;
  if ( v3 <= v5 )
    v6 = 0LL;
  bzero((void *)(a2 + 1), v6);
  ccn_write_uint(v4, (__int64)(a1 + 2), v3 - v6, v6 + a2 + 1);
  v7 = (__int64)&a1[**a1 + 2];
  v8 = ccn_write_uint_size(v4, v7);
  v9 = v3 - v8;
  if ( v3 <= v8 )
    v9 = 0LL;
  bzero((void *)(a2 + v3 + 1), v9);
  return ccn_write_uint(v4, v7, v3 - v9, a2 + v9 + v3 + 1);
}

//----- (0000000000024C9E) ----------------------------------------------------
__int64 __fastcall ccec_full_add(signed __int64 *a1, void *a2, const void *a3, const void *a4)
{
  const void *v4; // r13@1
  __int64 v5; // r12@1
  signed __int64 v6; // rbx@1
  void *v8; // rsi@4
  const void *v9; // rdx@4
  __int64 v10; // rdi@4
  signed __int64 v11; // [sp+8h] [bp-38h]@1
  const void *v12; // [sp+10h] [bp-30h]@1

  v12 = a4;
  v4 = a3;
  v5 = *a1;
  v11 = 2 * *a1;
  v6 = (signed __int64)((char *)a3 + 16 * *a1);
  if ( ccn_n(v5, (__int64)((char *)a3 + 16 * v5)) )
  {
    if ( ccn_n(v5, (__int64)((char *)v12 + 8 * v11)) )
      return ccec_add(a1, a2, v4, v12);
    ccn_set(v5, a2, v4);
    ccn_set(v5, (char *)a2 + 8 * v5, (char *)v4 + 8 * v5);
    v8 = (char *)a2 + 8 * v11;
    v10 = v5;
    v9 = (const void *)v6;
  }
  else
  {
    ccn_set(v5, a2, v12);
    ccn_set(v5, (char *)a2 + 8 * v5, (char *)v12 + 8 * v5);
    v8 = (char *)a2 + 8 * v11;
    v9 = (char *)v12 + 8 * v11;
    v10 = v5;
  }
  return (__int64)ccn_set(v10, v8, v9);
}

//----- (0000000000024D90) ----------------------------------------------------
__int64 __fastcall ccec_add(signed __int64 *a1, void *a2, const void *a3, const void *a4)
{
  const void *v4; // r15@1
  __int64 v5; // r14@1
  unsigned __int64 v6; // r12@1
  __int64 v7; // r13@1
  const void *v8; // rbx@1
  __int64 v9; // rbx@1
  __int64 v10; // r12@1
  __int64 v11; // r15@1
  signed __int64 *v12; // rbx@2
  signed __int64 *v13; // rbx@3
  __int64 v14; // r14@3
  __int64 v15; // r15@3
  __int64 v16; // r12@3
  __int64 v17; // r14@3
  __int64 v18; // r13@3
  __int64 v19; // r12@3
  __int64 v20; // r14@4
  __int64 v21; // r14@4
  __int64 v22; // r12@6
  __int64 v23; // r13@6
  __int64 v24; // r14@6
  __int64 v25; // r15@6
  __int64 v26; // r13@6
  __int64 v27; // r12@6
  void *v28; // r15@6
  signed __int64 v29; // rax@7
  void *v30; // rbx@7
  size_t v31; // r14@8
  __int64 v32; // rax@8
  const void *v33; // rax@10
  __int64 v35; // [sp+0h] [bp-A0h]@1
  size_t v36; // [sp+8h] [bp-98h]@1
  __int64 v37; // [sp+10h] [bp-90h]@3
  const void *v38; // [sp+18h] [bp-88h]@1
  void *v39; // [sp+20h] [bp-80h]@1
  unsigned __int64 *v40; // [sp+28h] [bp-78h]@1
  signed __int64 *v41; // [sp+30h] [bp-70h]@1
  const void *v42; // [sp+38h] [bp-68h]@1
  __int64 *v43; // [sp+40h] [bp-60h]@1
  __int64 v44; // [sp+48h] [bp-58h]@1
  __int64 v45; // [sp+50h] [bp-50h]@3
  __int64 *v46; // [sp+58h] [bp-48h]@1
  __int64 v47; // [sp+60h] [bp-40h]@1
  __int64 *v48; // [sp+68h] [bp-38h]@1
  __int64 v49; // [sp+70h] [bp-30h]@1

  v42 = a4;
  v4 = a3;
  v38 = a3;
  v39 = a2;
  v41 = a1;
  v49 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v6 = (8 * *a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v47 = (__int64)((char *)&v35 - v6);
  v44 = (__int64)((char *)&v35 - v6);
  v48 = (__int64 *)((char *)&v35 - v6);
  v43 = (__int64 *)((char *)&v35 + -v6 - v6);
  v46 = (__int64 *)((char *)v43 - v6);
  v40 = (unsigned __int64 *)((char *)v43 + -v6 - v6);
  v7 = (__int64)((char *)&v35 - v6);
  ccn_set(v5, (char *)&v35 - v6, a3);
  ccn_set(v5, (char *)&v35 - v6, (char *)v4 + 8 * v5);
  ccn_set(v5, v48, (char *)v38 + 16 * v5);
  v8 = v42;
  ccn_set(v5, v43, v42);
  ccn_set(v5, v46, (char *)v8 + 8 * v5);
  v9 = (__int64)((char *)&v35 - v6);
  v10 = (__int64)v40;
  *(_QWORD *)v9 = 1LL;
  v36 = 8 * v5 - 8;
  bzero((void *)(v9 + 8), v36);
  v11 = (__int64)((char *)v42 + 16 * v5);
  LODWORD(v38) = ccn_cmp(v5, v11, v9);
  if ( (_DWORD)v38 )
  {
    ccn_set(v5, (void *)v10, (const void *)v11);
    v12 = v41;
    cczp_sqr(v41, v7, v10);
    cczp_mul(v12, v47, v47, (unsigned __int64 *)v7);
    cczp_mul(v12, v7, v10, (unsigned __int64 *)v7);
    cczp_mul(v12, v44, v44, (unsigned __int64 *)v7);
  }
  v45 = v5;
  v42 = (const void *)(2 * v5);
  v13 = v41;
  v14 = (__int64)v48;
  cczp_sqr(v41, v7, (__int64)v48);
  v15 = (__int64)v43;
  cczp_mul(v13, (__int64)v43, (__int64)v43, (unsigned __int64 *)v7);
  cczp_mul(v13, v7, v14, (unsigned __int64 *)v7);
  v16 = (__int64)v46;
  v37 = v7;
  cczp_mul(v13, (__int64)v46, (__int64)v46, (unsigned __int64 *)v7);
  cczp_sub(v13, v15, v47, v15);
  v17 = v44;
  v18 = v16;
  cczp_sub(v13, v16, v44, v16);
  v19 = v17;
  if ( ccn_n(v45, v15) )
  {
    v20 = v47;
    cczp_add(v13, v47, v47, v47);
    cczp_sub(v13, v20, v20, v15);
    cczp_add(v13, v19, v19, v19);
    cczp_sub(v13, v19, v19, v18);
    v21 = (__int64)v48;
    if ( (_DWORD)v38 )
      cczp_mul(v13, (__int64)v48, (__int64)v48, v40);
    v22 = v15;
    cczp_mul(v13, v21, v21, (unsigned __int64 *)v15);
    v23 = v37;
    cczp_sqr(v13, v37, v15);
    cczp_mul(v13, v15, v15, (unsigned __int64 *)v23);
    v24 = v47;
    cczp_mul(v13, v23, v47, (unsigned __int64 *)v23);
    v25 = (__int64)v46;
    cczp_sqr(v13, v24, (__int64)v46);
    cczp_sub(v13, v24, v24, v23);
    cczp_sub(v13, v23, v23, v24);
    cczp_sub(v13, v23, v23, v24);
    cczp_mul(v13, v25, v25, (unsigned __int64 *)v23);
    v26 = v44;
    cczp_mul(v13, v22, v44, (unsigned __int64 *)v22);
    cczp_sub(v13, v26, v25, v22);
    v27 = v45;
    v28 = v39;
    ccn_set(v45, v39, (const void *)v24);
    cczp_div2(v13, (__int64)((char *)v28 + 8 * v27), v26);
    ccn_set(v27, (char *)v28 + 8 * (_QWORD)v42, v48);
  }
  else
  {
    v29 = ccn_n(v45, v18);
    v30 = v39;
    if ( v29 )
    {
      *(_QWORD *)v39 = 1LL;
      v31 = v36;
      bzero((char *)v30 + 8, v36);
      v32 = v45;
      *((_QWORD *)v30 + v45) = 1LL;
    }
    else
    {
      *(_QWORD *)v39 = 0LL;
      v31 = v36;
      bzero((char *)v30 + 8, v36);
      v32 = v45;
      *((_QWORD *)v30 + v45) = 0LL;
    }
    bzero((char *)v30 + 8 * v32 + 8, v31);
    v33 = v42;
    *((_QWORD *)v30 + (_QWORD)v42) = 0LL;
    bzero((char *)v30 + 8 * (_QWORD)v33 + 8, v31);
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000251D9) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_rsa_priv_size(__int64 *a1)
{
  __int64 v1; // r15@1
  signed __int64 v2; // r12@1
  __int64 *v3; // r13@1
  signed __int64 v4; // rbx@1
  signed __int64 v5; // rbx@1
  signed __int64 v6; // rbx@1
  signed __int64 v7; // rbx@1
  __int64 v8; // r15@1
  signed __int64 v9; // rbx@1
  __int64 v10; // r14@1
  signed __int64 v11; // rbx@1
  signed __int64 v12; // rbx@1
  signed __int64 v13; // rbx@1
  signed __int64 v14; // rax@1
  signed __int64 result; // rax@1
  __int64 v16; // rcx@1
  __int64 v17; // [sp+8h] [bp-38h]@1
  __int64 v18; // [sp+10h] [bp-30h]@1

  v18 = *(_QWORD *)off_69010[0];
  v1 = *a1;
  v2 = 4 * *a1;
  v3 = &a1[v2 + 3];
  v17 = 0LL;
  v4 = ccder_sizeof_integer(1LL, (__int64)&v17);
  v5 = ccder_sizeof_integer(v1, (__int64)(a1 + 2)) + v4;
  v6 = ccder_sizeof_integer(v1, (__int64)&a1[2 * v1 + 3]) + v5;
  v7 = ccder_sizeof_integer(v1, (__int64)&a1[3 * v1 + 3]) + v6;
  v8 = *v3;
  v9 = ccder_sizeof_integer(*v3, (__int64)&a1[v2 + 5]) + v7;
  v10 = v3[2 * v8 + 3];
  v11 = ccder_sizeof_integer(v3[2 * v8 + 3], (__int64)&v3[2 * v8 + 5]) + v9;
  v12 = ccder_sizeof_integer(v8, (__int64)&v3[4 * v8 + 6]) + v11;
  v13 = ccder_sizeof_integer(v10, (__int64)&v3[5 * v8 + 6]) + v12;
  v14 = ccder_sizeof_integer(v8, (__int64)&v3[6 * v8 + 6]);
  result = ccder_sizeof(2305843009213693968LL, v14 + v13);
  v16 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000025308) ----------------------------------------------------
__int64 __fastcall ccec_full_sub(__int64 *a1, void *a2, const void *a3, const void *a4)
{
  const void *v4; // r13@1
  __int64 v5; // r14@1
  char *v6; // rbx@1
  __int64 v8; // [sp+0h] [bp-40h]@1
  const void *v9; // [sp+8h] [bp-38h]@1
  __int64 v10; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v9 = a3;
  v10 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v6 = (char *)&v8 - ((24 * *a1 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  ccn_set(*a1, v6, a4);
  ccn_sub(v5, (__int64)&v6[8 * v5], (__int64)(a1 + 2), (__int64)((char *)v4 + 8 * v5));
  ccn_set(v5, &v6[16 * v5], (char *)v4 + 16 * v5);
  ccec_full_add(a1, a2, v9, v6);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000253C1) ----------------------------------------------------
__int64 __fastcall ccec_generate_key(__int64 *a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r14@1
  __int64 result; // rax@1

  v3 = a3;
  result = ccec_generate_key_internal(a1, a2, a3);
  if ( !(_DWORD)result )
    result = (unsigned int)((unsigned __int8)~ccec_pairwise_consistency_check(v3, a2) << 31 >> 31);
  return result;
}

//----- (00000000000253F2) ----------------------------------------------------
__int64 *__fastcall ccec_get_cp(signed __int64 a1)
{
  if ( a1 > 520 )
  {
    if ( a1 == 521 )
      return ccec_cp_521();
  }
  else if ( a1 > 383 )
  {
    if ( a1 == 384 )
      return ccec_cp_384();
  }
  else
  {
    if ( a1 == 192 )
      return ccec_cp_192();
    if ( a1 == 224 )
      return ccec_cp_224();
    if ( a1 == 256 )
      return ccec_cp_256();
  }
  return 0LL;
}

//----- (0000000000025457) ----------------------------------------------------
signed __int64 __fastcall ccec_keysize_is_supported(signed __int64 a1)
{
  signed __int64 result; // rax@1

  result = 1LL;
  if ( a1 > 520 )
  {
    if ( a1 == 521 )
      return result;
    goto LABEL_10;
  }
  if ( a1 > 383 )
  {
    if ( a1 == 384 )
      return result;
    goto LABEL_10;
  }
  if ( a1 != 192 && a1 != 224 && a1 != 256 )
LABEL_10:
    result = 0LL;
  return result;
}

//----- (00000000000254A7) ----------------------------------------------------
__int64 __fastcall ccec_get_fullkey_components(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8)
{
  __int64 v8; // r14@1
  signed int v9; // er15@1
  __int64 *v10; // r12@2
  signed __int64 v11; // rax@2

  v8 = **a1;
  v9 = -1;
  if ( !(unsigned int)ccec_get_pubkey_components(a1, a2, a3, a4, a5, a6) )
  {
    v10 = *a1;
    v11 = ccn_write_uint_size(v8, (__int64)&a1[3 * **a1 + 2]);
    if ( (unsigned __int64)v11 <= *(_QWORD *)a8 )
    {
      *(_QWORD *)a8 = v11;
      ccn_write_uint(v8, (__int64)&a1[3 * *v10 + 2], v11, a7);
      v9 = 0;
    }
  }
  return (unsigned int)v9;
}

//----- (0000000000025524) ----------------------------------------------------
char __fastcall cc_clear(size_t a1, void *a2)
{
  bzero(a2, a1);
  return *(_BYTE *)a2;
}

//----- (0000000000025551) ----------------------------------------------------
__int64 __fastcall ccec_get_pubkey_components(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 v6; // r12@1
  __int64 v7; // r13@1
  __int64 v8; // r14@1
  signed __int64 v9; // rax@1
  signed int v10; // er15@1
  __int64 *v11; // r12@2
  signed __int64 v12; // rax@2
  __int64 v14; // [sp+8h] [bp-48h]@1
  __int64 v15; // [sp+18h] [bp-38h]@1
  __int64 v16; // [sp+20h] [bp-30h]@1

  v16 = a6;
  v14 = a5;
  v6 = a4;
  v15 = a3;
  v7 = **a1;
  v8 = (__int64)(a1 + 2);
  v9 = ccn_write_uint_size(**a1, (__int64)(a1 + 2));
  v10 = -1;
  if ( (unsigned __int64)v9 <= *(_QWORD *)v6 )
  {
    *(_QWORD *)v6 = v9;
    ccn_write_uint(v7, v8, v9, v15);
    v11 = *a1;
    v12 = ccn_write_uint_size(v7, v8 + 8 * **a1);
    if ( (unsigned __int64)v12 <= *(_QWORD *)v16 )
    {
      *(_QWORD *)v16 = v12;
      ccn_write_uint(v7, v8 + 8 * *v11, v12, v14);
      *(_QWORD *)a2 = ccn_bitlen(**a1, (__int64)(*a1 + 2));
      v10 = 0;
    }
  }
  return (unsigned int)v10;
}

//----- (0000000000025610) ----------------------------------------------------
__int64 __fastcall ccrsa_encrypt_oaep(__int64 *a1, __int64 a2, __int64 a3, __int64 a4, void *a5, __int64 a6, void *a7, size_t a8, const void *a9)
{
  __int64 v9; // r15@1
  __int64 v10; // rbx@1
  __int64 v11; // rax@1
  __int64 v12; // r12@1
  signed int v13; // er14@1
  __int64 v14; // r14@2
  signed int v15; // eax@2
  __int64 v16; // rcx@2
  __int64 v17; // r13@3
  unsigned __int64 v18; // rax@3
  size_t v19; // r15@3
  void *v20; // rbx@5
  __int64 result; // rax@6
  __int64 v22; // [sp+10h] [bp-60h]@1
  __int64 v23; // [sp+18h] [bp-58h]@3
  void *v24; // [sp+20h] [bp-50h]@1
  __int64 v25; // [sp+28h] [bp-48h]@1
  __int64 v26; // [sp+30h] [bp-40h]@1
  __int64 v27; // [sp+38h] [bp-38h]@1
  __int64 v28; // [sp+40h] [bp-30h]@1

  v27 = a6;
  v24 = a5;
  v9 = a4;
  v26 = a3;
  v25 = a2;
  v28 = *(_QWORD *)off_69010[0];
  v10 = *a1;
  v11 = ccn_write_uint_size(*a1, (__int64)(a1 + 2));
  v12 = (__int64)((char *)&v22 - ((8 * v10 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v13 = -2;
  if ( *(_QWORD *)v9 >= (unsigned __int64)v11 )
  {
    *(_QWORD *)v9 = v11;
    v14 = v11;
    v15 = ccrsa_oaep_encode_parameter(
            v25,
            v26,
            v11,
            (char *)&v22 - ((8 * v10 + 15) & 0xFFFFFFFFFFFFFFF0LL),
            a8,
            a7,
            a8,
            a9);
    v16 = v14;
    v13 = v15;
    if ( !v15 )
    {
      v17 = v16;
      ccrsa_pub_crypt(
        a1,
        (__int64)((char *)&v22 - ((8 * v10 + 15) & 0xFFFFFFFFFFFFFFF0LL)),
        (__int64)((char *)&v22 - ((8 * v10 + 15) & 0xFFFFFFFFFFFFFFF0LL)));
      v23 = v10;
      v18 = ccn_write_uint_size(v10, (__int64)((char *)&v22 - ((8 * v10 + 15) & 0xFFFFFFFFFFFFFFF0LL)));
      v13 = 0;
      v19 = v17 - v18;
      if ( v17 <= v18 )
        v19 = 0LL;
      v20 = v24;
      bzero(v24, v19);
      ccn_write_uint(v23, v12, v17 - v19, (__int64)((char *)v20 + v19));
    }
  }
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v28 )
    result = (unsigned int)v13;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000025739) ----------------------------------------------------
__int64 __fastcall ccec_import_pub(unsigned __int64 *a1, __int64 a2, unsigned int *a3, __int64 a4)
{
  __int64 result; // rax@2

  if ( a2 & 1 )
    result = ccec_x963_import_pub(a1, a2, a3, a4);
  else
    result = 0xFFFFFFFFLL;
  return result;
}

//----- (0000000000025750) ----------------------------------------------------
bool __usercall ccec_is_point_affine@<al>(__int64 a1@<rax>, signed __int64 *a2@<rdi>, __int64 a3@<rsi>)
{
  __int64 v3; // r14@1
  unsigned __int64 v4; // rax@1
  __int64 v5; // rbx@1
  __int64 v6; // r13@1
  bool result; // al@1
  __int64 v8; // rcx@1
  __int64 v9; // [sp+0h] [bp-30h]@1
  __int64 v10; // [sp+8h] [bp-28h]@1

  v9 = a1;
  v9 = *(_QWORD *)off_69010[0];
  v3 = *a2;
  v4 = (8 * *a2 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v5 = (__int64)((char *)&v9 - v4);
  v6 = (__int64)((char *)&v10 - v4);
  cczp_sqr(a2, (__int64)((char *)&v9 - v4), a3);
  cczp_mul(a2, v5, v5, (unsigned __int64 *)a3);
  cczp_add(a2, v6, a3, a3);
  cczp_add(a2, v6, v6, a3);
  cczp_sub(a2, v5, v5, v6);
  cczp_add(a2, v5, v5, (__int64)&a2[v3 + 2]);
  cczp_sqr(a2, v6, a3 + 8 * v3);
  result = (unsigned int)ccn_cmp(v3, v6, v5) == 0;
  v8 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002583C) ----------------------------------------------------
signed __int64 __fastcall ccder_encode_rsa_pub(__int64 *a1, __int64 a2, unsigned __int64 a3)
{
  __int64 v3; // r14@1
  __int64 v4; // rbx@1
  signed __int64 v5; // rax@1
  signed __int64 v6; // rax@1

  v3 = a3;
  v4 = *a1;
  v5 = ccder_encode_integer(*a1, (__int64)&a1[2 * *a1 + 3], a2, a3);
  v6 = ccder_encode_integer(v4, (__int64)(a1 + 2), a2, v5);
  return ccder_encode_constructed_tl(0x2000000000000010uLL, v3, a2, v6);
}

//----- (000000000002589F) ----------------------------------------------------
__int64 __fastcall ccecies_decrypt_gcm(__int64 **a1, __int64 a2, unsigned __int64 a3, unsigned int *a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9, __int64 a10)
{
  unsigned int *v10; // r13@1
  unsigned __int64 v11; // r14@1
  unsigned __int64 v12; // rax@1
  __int64 v13; // rcx@1
  unsigned __int64 v14; // r14@2
  __int64 v15; // r15@2
  __int64 result; // rax@3
  __int64 v17; // [sp+28h] [bp-38h]@1
  __int64 v18; // [sp+30h] [bp-30h]@1

  v18 = a6;
  v17 = a5;
  v10 = a4;
  v11 = a3;
  v12 = ccecies_pub_key_size(a1, a2);
  v13 = *(_DWORD *)(a2 + 28);
  if ( v13 + v12 > v11 || (v14 = v11 - v13, v15 = v14 - v12, v14 - v12 > *(_QWORD *)a9) )
  {
    result = 0xFFFFFFFFLL;
  }
  else
  {
    result = ccecies_decrypt_gcm_composite(
               a1,
               a2,
               a10,
               v17,
               v18,
               a7,
               a8,
               v15,
               (__int64)((char *)v10 + v12),
               v10,
               (__int64)((char *)v10 + v14));
    *(_QWORD *)a9 = v15;
  }
  return result;
}

//----- (0000000000025941) ----------------------------------------------------
signed __int64 __fastcall ccec_make_pub(signed __int64 a1, unsigned __int64 a2, unsigned __int64 a3, unsigned __int64 a4, unsigned __int64 a5, __int64 a6)
{
  __int64 v6; // r13@1
  unsigned __int64 v7; // r15@1
  unsigned __int64 *v8; // rax@1
  unsigned __int64 v9; // r14@1
  signed __int64 result; // rax@1
  signed __int64 v11; // rax@3
  unsigned __int64 v12; // [sp+8h] [bp-38h]@1
  unsigned __int64 v13; // [sp+10h] [bp-30h]@1

  v6 = a6;
  v13 = a5;
  v12 = a4;
  v7 = a3;
  v8 = (unsigned __int64 *)ccec_get_cp(a1);
  *(_QWORD *)v6 = v8;
  v9 = *v8;
  result = ccn_read_uint(*v8, v6 + 16, a2, v7);
  if ( !(_DWORD)result )
  {
    result = ccn_read_uint(v9, v6 + 16 + 8LL * **(_QWORD **)v6, v12, v13);
    if ( !(_DWORD)result )
    {
      v11 = 16LL * **(_QWORD **)v6;
      *(_QWORD *)(v11 + v6 + 16) = 1LL;
      bzero((void *)(v11 + v6 + 24), 8 * v9 - 8);
      result = 0LL;
    }
  }
  return result;
}

//----- (00000000000259DE) ----------------------------------------------------
signed __int64 __fastcall ccrsa_emsa_pkcs1v15_verify(unsigned __int64 a1, __int64 a2, size_t a3, const void *a4, __int64 a5)
{
  const void *v5; // r13@1
  size_t v6; // r15@1
  __int64 v7; // r14@1
  __int64 v8; // r12@1
  signed __int64 result; // rax@1
  signed __int64 v10; // r10@2
  char v11; // al@2
  char v12; // dl@2
  signed __int64 v13; // rsi@2
  signed __int64 v14; // r14@3
  signed __int64 v15; // rcx@4
  signed __int64 v16; // r13@6
  int v17; // ebx@7
  unsigned int v18; // ebx@7
  const void *v19; // [sp+0h] [bp-30h]@4

  v5 = a4;
  v6 = a3;
  v7 = a2;
  v8 = *(_BYTE *)(a5 + 1);
  result = 0xFFFFFFFFLL;
  if ( v8 + a3 + 21 <= a1 )
  {
    v10 = v8 + a3 + 8;
    v11 = *(_BYTE *)(a2 + 2);
    v12 = *(_BYTE *)a2 | *(_BYTE *)(a2 + 1) ^ 1;
    v13 = a2 + 3;
    if ( a1 - 5 == v10 )
    {
      v14 = v7 + 2;
    }
    else
    {
      v19 = a4;
      v15 = v6 + v8 + 13 - a1;
      do
      {
        v12 |= ~v11;
        v11 = *(_BYTE *)v13++;
        ++v15;
      }
      while ( v15 );
      v16 = v7 + a1 - 10 - v6 - v8;
      v14 = a1 - 11 - v6 - v8 + v7;
      v13 = v16;
      v5 = v19;
    }
    v17 = v10 ^ *(_BYTE *)(v14 + 2) | (unsigned __int8)(v12 | v11 | *(_BYTE *)v13 ^ 0x30) | *(_BYTE *)(v14 + 3) ^ 0x30 | (v8 + 4) ^ *(_BYTE *)(v14 + 4);
    v18 = *(_BYTE *)(v14 + v8 + 9) ^ 4 | *(_BYTE *)(v14 + v8 + 8) | v17 | memcmp(
                                                                            (const void *)(v14 + 5),
                                                                            (const void *)a5,
                                                                            v8 + 2) | *(_BYTE *)(v14 + v8 + 7) ^ 5 | *(_BYTE *)(v14 + v8 + 10) ^ (unsigned int)v6;
    result = (unsigned __int8)(v18 | memcmp((const void *)(v14 + v8 + 11), v5, v6));
  }
  return result;
}

//----- (0000000000025B03) ----------------------------------------------------
__int64 __fastcall ccec_mult(signed __int64 *a1, __int64 a2, __int64 a3, const void *a4)
{
  __int64 v4; // r15@1
  __int64 v5; // r12@1
  __int64 v6; // rbx@1
  int v7; // eax@1
  const void *v8; // r15@3
  int v9; // eax@3
  __int64 *v10; // r14@5
  int v11; // eax@5
  signed int v12; // ecx@5
  __int64 v13; // rbx@5
  const void *v14; // rbx@7
  signed __int64 v15; // r15@8
  const void *v16; // rbx@11
  unsigned __int64 v17; // r15@12
  __int64 v18; // r14@12
  __int64 v19; // rbx@12
  __int64 v20; // r15@12
  signed __int64 v21; // rax@12
  unsigned __int64 v22; // rbx@12
  const void *v23; // r12@13
  signed __int64 *v24; // r15@13
  signed __int64 v25; // rdx@14
  signed __int64 v26; // rax@14
  __int64 v28; // [sp+0h] [bp-60h]@1
  __int64 v29; // [sp+8h] [bp-58h]@12
  signed __int64 *v30; // [sp+10h] [bp-50h]@1
  __int64 v31; // [sp+18h] [bp-48h]@1
  const void *v32; // [sp+20h] [bp-40h]@1
  __int64 v33; // [sp+28h] [bp-38h]@3
  __int64 v34; // [sp+30h] [bp-30h]@1
  __int64 v35; // [sp+38h] [bp-28h]@12

  v32 = a4;
  v4 = a3;
  v31 = a3;
  v30 = a1;
  v34 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v6 = (__int64)((char *)&v28 - ((8 * *a1 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  *(_QWORD *)v6 = 1LL;
  bzero((void *)(v6 + 8), 8 * v5 - 8);
  v7 = ccn_cmp(v5, v4, v6);
  if ( !v7 )
  {
    v14 = v32;
    ccn_set(v5, (void *)a2, v32);
    ccn_set(v5, (void *)(a2 + 8 * v5), (char *)v14 + 8 * v5);
    ccn_set(v5, (void *)(16 * v5 + a2), (char *)v14 + 16 * v5);
    v12 = 0;
    v13 = off_69010[0];
    goto LABEL_23;
  }
  if ( v7 < 0 )
  {
    v15 = 2 * v5;
    v13 = off_69010[0];
  }
  else
  {
    v33 = 2 * v5;
    v8 = (char *)v32 + 16 * v5;
    v9 = ccn_cmp(v5, (__int64)((char *)v32 + 16 * v5), v6);
    if ( v9 >= 0 )
    {
      if ( v9 <= 0 )
      {
        v16 = v32;
        ccn_set(v5, (void *)a2, v32);
        ccn_set(v5, (void *)(a2 + 8 * v5), (char *)v16 + 8 * v5);
        ccn_set(v5, (void *)(a2 + 8 * v33), v8);
      }
      else
      {
        v10 = v30;
        v11 = ccec_affinify(v30, a2, (unsigned __int64 *)v32);
        v12 = -1;
        v13 = off_69010[0];
        if ( v11 )
          goto LABEL_23;
        ccec_projectify(v10, (void *)a2, (const void *)a2);
      }
      v29 = v5 + 1;
      v17 = (8 * v5 + 23) & 0xFFFFFFFFFFFFFFF0LL;
      v33 = (__int64)((char *)&v35 - v17);
      ccn_set(v5, (char *)&v35 - v17, (const void *)v31);
      *(__int64 *)((char *)&v35 + 8 * v5 - v17) = 0LL;
      v18 = (__int64)((char *)&v35 - v17);
      v19 = v31;
      v20 = ccn_add(v5, (__int64)((char *)&v35 - v17), v31, v31);
      *(_QWORD *)(v18 + 8 * v5) = v20 + ccn_add(v5, v18, v18, v19);
      v21 = ccn_bitlen(v29, v18);
      v12 = 0;
      v22 = v21 - 2;
      if ( v21 == 2 )
        goto LABEL_25;
      v23 = v32;
      v24 = v30;
      do
      {
        ccec_double(v24, (void *)a2, (const void *)a2);
        v25 = (*(_QWORD *)(v33 + 8 * (v22 >> 6)) >> (v22 & 0x3F)) & 1LL;
        v26 = (*(_QWORD *)(v18 + 8 * (v22 >> 6)) >> (v22 & 0x3F)) & 1LL;
        if ( !v26 || v25 )
        {
          if ( !v26 && v25 )
            ccec_full_sub(v24, (void *)a2, (const void *)a2, v23);
        }
        else
        {
          ccec_full_add(v24, (void *)a2, (const void *)a2, v23);
        }
        --v22;
      }
      while ( v22 );
      v13 = off_69010[0];
      goto LABEL_22;
    }
    v13 = off_69010[0];
    v15 = v33;
  }
  *(_QWORD *)a2 = 1LL;
  bzero((void *)(a2 + 8), 8 * v5 - 8);
  *(_QWORD *)(a2 + 8 * v5) = 1LL;
  bzero((void *)(a2 + 8 * v5 + 8), 8 * v5 - 8);
  *(_QWORD *)(a2 + 8 * v15) = 0LL;
  bzero((void *)(a2 + 8 * v15 + 8), 8 * v5 - 8);
LABEL_22:
  v12 = 0;
LABEL_23:
  while ( *(_QWORD *)v13 != v34 )
LABEL_25:
    v13 = off_69010[0];
  return (unsigned int)v12;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000025E0D) ----------------------------------------------------
__int64 __fastcall cced25519_sign(__int64 a1, __int64 a2, unsigned __int64 a3, const void *a4, const void *a5, const void *a6)
{
  const void *v6; // r12@1
  unsigned __int64 v7; // r13@1
  __int64 v8; // rbx@1
  __int64 v9; // rax@1
  char *v10; // r15@1
  __int64 v11; // rax@1
  signed __int64 v12; // rax@1
  __int64 v13; // rax@1
  signed __int64 v14; // rax@1
  signed __int64 v15; // rax@1
  __int64 v17; // [sp+0h] [bp-1B0h]@1
  unsigned __int64 v18; // [sp+8h] [bp-1A8h]@1
  const void *v19; // [sp+10h] [bp-1A0h]@1
  const void *v20; // [sp+18h] [bp-198h]@1
  char v21; // [sp+20h] [bp-190h]@1
  char v22; // [sp+C0h] [bp-F0h]@1
  char v23; // [sp+100h] [bp-B0h]@1
  char v24; // [sp+140h] [bp-70h]@1
  char v25; // [sp+15Fh] [bp-51h]@1
  char v26; // [sp+160h] [bp-50h]@1
  __int64 v27; // [sp+180h] [bp-30h]@1

  v20 = a5;
  v6 = a4;
  v19 = a4;
  v7 = a3;
  v18 = a3;
  v8 = a1;
  v27 = *(_QWORD *)off_69010[0];
  v9 = (((*(_QWORD *)(a1 + 8) + *(_QWORD *)(a1 + 16) + 19LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v10 = (char *)&v17 - v9;
  ccdigest(v9, a6, (__int64)&v24, a1, 0x20uLL);
  v24 &= 0xF8u;
  v25 = v25 & 0x3F | 0x40;
  v11 = ccdigest_init(a1, (__int64)v10);
  LODWORD(v12) = ccdigest_update(v11, 0x20uLL, &v26, a1, (__int64)v10);
  ccdigest_update(v12, v7, v6, a1, (__int64)v10);
  (*(void (__fastcall **)(__int64, char *))(v8 + 56))(v8, v10);
  cc_clear(*(_QWORD *)(v8 + 8) + *(_QWORD *)(v8 + 16) + 12LL, v10);
  sc_reduce((__int64)&v23);
  ge_scalarmult_base((__int64)&v21, (__int64)&v23);
  ge_p3_tobytes(a2, (__int64)&v21);
  v13 = ccdigest_init(a1, (__int64)v10);
  LODWORD(v14) = ccdigest_update(v13, 0x20uLL, (const void *)a2, a1, (__int64)v10);
  LODWORD(v15) = ccdigest_update(v14, 0x20uLL, v20, a1, (__int64)v10);
  ccdigest_update(v15, v18, v19, a1, (__int64)v10);
  (*(void (__fastcall **)(__int64, char *, char *))(v8 + 56))(v8, v10, &v22);
  cc_clear(*(_QWORD *)(v8 + 8) + *(_QWORD *)(v8 + 16) + 12LL, v10);
  sc_reduce((__int64)&v22);
  sc_muladd(a2 + 32, (__int64)&v22, (__int64)&v24, (__int64)&v23);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000025FC3) ----------------------------------------------------
bool __fastcall ccec_pairwise_consistency_check(__int64 a1, __int64 a2)
{
  __int64 v2; // r13@1
  unsigned __int64 v3; // rax@1
  unsigned __int64 v4; // rbx@1
  bool result; // al@2
  __int64 v6; // rcx@4
  __int64 v7; // [sp+0h] [bp-60h]@1
  __int64 v8; // [sp+8h] [bp-58h]@1
  char v9; // [sp+10h] [bp-50h]@1
  __int64 v10; // [sp+30h] [bp-30h]@1

  v2 = off_69010[0];
  v10 = *(_QWORD *)off_69010[0];
  memset(&v9, 10, 0x14uLL);
  v3 = (unsigned __int64)(ccn_bitlen(**(_QWORD **)a1, *(_QWORD *)a1 + 16LL) + 7) >> 3;
  v8 = 2 * v3 + 9;
  v4 = (unsigned __int64)((char *)&v7 - ((2 * v3 + 24) & 0xFFFFFFFFFFFFFFF0LL));
  if ( (unsigned int)ccec_sign(a1, 0x14uLL, (unsigned __int64)&v9, (__int64)&v8, v4, a2) )
  {
    result = 0;
  }
  else
  {
    BYTE7(v7) = 0;
    ccec_verify((const void *)a1, 0x14uLL, (unsigned __int64)&v9, v8, v4, (__int64)((char *)&v7 + 7));
    result = BYTE7(v7) != 0;
  }
  v6 = *(_QWORD *)v2;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000026099) ----------------------------------------------------
void __fastcall ccec_projectify(__int64 *a1, void *a2, const void *a3)
{
  const void *v3; // r14@1
  void *v4; // r15@1
  __int64 v5; // rbx@1
  size_t v6; // rsi@1

  v3 = a3;
  v4 = a2;
  v5 = *a1;
  ccn_set(*a1, a2, a3);
  ccn_set(v5, (char *)a2 + 8 * v5, (char *)v3 + 8 * v5);
  v6 = 8 * v5 - 8;
  v5 *= 16LL;
  *(_QWORD *)((char *)v4 + v5) = 1LL;
  bzero((char *)v4 + v5 + 8, v6);
}

//----- (00000000000260EC) ----------------------------------------------------
__int64 __fastcall ccec_verify_composite(__int64 **a1, unsigned __int64 a2, unsigned __int64 a3, unsigned __int64 a4, unsigned __int64 a5, __int64 a6)
{
  unsigned __int64 v6; // r12@1
  unsigned __int64 v7; // r13@1
  unsigned __int64 v8; // rax@1
  __int64 v9; // r14@1
  __int64 v10; // r15@1
  unsigned __int64 v11; // rax@1
  int v12; // eax@1
  signed int v13; // er12@1
  unsigned __int64 v14; // r13@2
  unsigned __int64 v15; // rax@2
  __int64 result; // rax@4
  unsigned __int64 v17; // [sp+0h] [bp-50h]@1
  unsigned __int64 v18; // [sp+8h] [bp-48h]@1
  __int64 v19; // [sp+10h] [bp-40h]@1
  unsigned __int64 v20; // [sp+18h] [bp-38h]@1
  __int64 v21; // [sp+20h] [bp-30h]@1

  v19 = a6;
  v20 = a5;
  v6 = a4;
  v18 = a3;
  v17 = a2;
  v21 = *(_QWORD *)off_69010[0];
  v7 = **a1;
  v8 = (8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v9 = (__int64)((char *)&v17 - v8);
  v10 = (__int64)((char *)&v17 - v8);
  v11 = ccec_signature_r_s_size(a1);
  v12 = ccn_read_uint(v7, v9, v11, v6);
  v13 = -1;
  if ( !v12 )
  {
    v14 = **a1;
    v15 = ccec_signature_r_s_size(a1);
    if ( !(unsigned int)ccn_read_uint(v14, v10, v15, v20) )
      v13 = ccec_verify_internal(a1, v17, v18, v9, v10, v19);
  }
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v21 )
    result = (unsigned int)v13;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000261CD) ----------------------------------------------------
__int64 __fastcall ccec_sign(__int64 a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 v6; // rbx@1
  unsigned __int64 v7; // rax@1
  __int64 v8; // r12@1
  __int64 v9; // r14@1
  __int64 result; // rax@1
  __int64 *v11; // rax@2
  __int64 v12; // r15@2
  __int64 v13; // rbx@2
  signed __int64 v14; // r13@2
  signed __int64 v15; // rax@2
  signed __int64 v16; // rax@2
  signed __int64 v17; // rbx@2
  bool v18; // cf@2
  __int64 v19; // r13@3
  __int64 v20; // rbx@3
  __int64 v21; // r15@3
  signed __int64 v22; // rax@3
  signed __int64 v23; // rax@3
  __int64 v24; // rcx@4
  __int64 *v25; // [sp+0h] [bp-40h]@1
  __int64 v26; // [sp+8h] [bp-38h]@1
  __int64 v27; // [sp+10h] [bp-30h]@1

  v26 = a5;
  v6 = a4;
  v27 = *(_QWORD *)off_69010[0];
  v7 = (8LL * **(_QWORD **)a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v8 = (__int64)((char *)&v25 - v7);
  v9 = (__int64)((char *)&v25 - v7);
  result = ccec_sign_internal(a1, a2, a3, (__int64)((char *)&v25 - v7), (__int64)((char *)&v25 - v7), a6);
  if ( !(_DWORD)result )
  {
    v11 = *(__int64 **)a1;
    v25 = v11;
    v12 = v6;
    v13 = *v11;
    v14 = ccder_sizeof_integer(*v11, v8);
    v15 = ccder_sizeof_integer(v13, v9);
    v16 = ccder_sizeof(2305843009213693968LL, v15 + v14);
    v17 = v16;
    v18 = *(_QWORD *)v12 < (unsigned __int64)v16;
    *(_QWORD *)v12 = v16;
    result = 0xFFFFFFFFLL;
    if ( !v18 )
    {
      v19 = v26;
      v20 = v26 + v17;
      v21 = *v25;
      v22 = ccder_encode_integer(*v25, v9, v26, v20);
      v23 = ccder_encode_integer(v21, v8, v19, v22);
      ccder_encode_constructed_tl(0x2000000000000010uLL, v20, v19, v23);
      result = 0LL;
    }
  }
  v24 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000262E6) ----------------------------------------------------
__int64 __fastcall ccec_twin_mult(__int64 a1, __int64 a2, __int64 a3, size_t a4, __int64 a5, const void *a6)
{
  __int64 v6; // r13@1
  unsigned __int64 v7; // rbx@1
  const void *v8; // r15@1
  unsigned __int64 v9; // rax@1
  __int64 v10; // r14@1
  char *v11; // r12@1
  __int64 v12; // rbx@1
  __int64 v13; // r14@1
  __int64 v14; // r15@1
  __int64 v15; // rax@1
  size_t v16; // r12@1
  __int64 v17; // rax@1
  __int64 v18; // rax@1
  __int64 v19; // r14@1
  __int64 v20; // rbx@1
  signed __int64 v21; // r15@1
  signed __int64 v22; // r14@1
  __int64 i; // rdx@3
  __int64 v24; // r15@6
  size_t v25; // rbx@6
  __int64 v26; // rax@6
  __int64 v27; // rax@6
  signed __int64 v28; // r12@6
  signed __int64 v29; // rcx@7
  __int64 v30; // rax@7
  __int64 v31; // rdi@8
  unsigned __int64 v32; // rcx@11
  unsigned __int64 v33; // r8@11
  char v34; // cl@11
  signed __int64 v35; // rdx@11
  __int64 v36; // rsi@11
  signed __int64 *v37; // r14@11
  signed __int64 v38; // rdi@12
  __int64 v39; // rbx@15
  int v40; // ebx@18
  __int64 *v41; // rdi@23
  void *v42; // rsi@23
  const void *v43; // rdx@23
  __int64 v44; // rcx@23
  int v45; // eax@29
  int v46; // eax@34
  signed __int64 *v47; // rdi@37
  void *v48; // rsi@37
  const void *v49; // rdx@37
  __int64 v50; // rcx@37
  __int64 v52; // [sp+0h] [bp-F0h]@1
  __int64 v53; // [sp+8h] [bp-E8h]@1
  unsigned __int64 *v54; // [sp+10h] [bp-E0h]@1
  __int64 v55; // [sp+18h] [bp-D8h]@1
  unsigned __int64 *v56; // [sp+20h] [bp-D0h]@1
  __int64 v57; // [sp+28h] [bp-C8h]@1
  __int64 v58; // [sp+30h] [bp-C0h]@1
  __int64 v59; // [sp+38h] [bp-B8h]@1
  __int64 v60; // [sp+40h] [bp-B0h]@1
  size_t v61; // [sp+48h] [bp-A8h]@1
  __int64 v62; // [sp+50h] [bp-A0h]@1
  __int64 v63; // [sp+58h] [bp-98h]@1
  __int64 v64; // [sp+60h] [bp-90h]@1
  __int64 *v65; // [sp+68h] [bp-88h]@1
  __int64 v66; // [sp+70h] [bp-80h]@1
  __int64 v67; // [sp+78h] [bp-78h]@1
  unsigned __int64 v68; // [sp+80h] [bp-70h]@1
  int v69; // [sp+88h] [bp-68h]@15
  int v70; // [sp+8Ch] [bp-64h]@21
  __int64 v71[2]; // [sp+90h] [bp-60h]@10
  __int64 v72[2]; // [sp+A0h] [bp-50h]@5
  __int64 v73; // [sp+B0h] [bp-40h]@1
  __int64 v74; // [sp+B8h] [bp-38h]@1
  __int64 v75; // [sp+C0h] [bp-30h]@1

  v68 = (unsigned __int64)a6;
  v57 = a5;
  v61 = a4;
  v6 = a3;
  v59 = a2;
  v67 = a1;
  v75 = *(_QWORD *)off_69010[0];
  v7 = (24LL * *(_QWORD *)a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v66 = (__int64)((char *)&v52 - v7);
  v64 = (__int64)((char *)&v52 - v7);
  v8 = (const void *)a4;
  ccec_full_add((signed __int64 *)a1, (char *)&v52 - v7, (const void *)a4, a6);
  ccec_full_sub((__int64 *)a1, (char *)&v52 - v7, v8, (const void *)v68);
  v63 = (__int64)((char *)&v52 - v7);
  v65 = (__int64 *)((char *)&v52 + -v7 - v7);
  v62 = *(_QWORD *)a1;
  v9 = (8 * v62 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v60 = (__int64)((char *)&v52 - v9);
  v10 = (__int64)((char *)&v52 - v9);
  v11 = (char *)&v52 - v9;
  v58 = 2 * v62;
  v12 = 16 * v62;
  v55 = v61 + 16 * v62;
  v53 = v68 + 16 * v62;
  cczp_mul((signed __int64 *)a1, (__int64)((char *)&v52 - v9), v61 + 16 * v62, (unsigned __int64 *)(v68 + 16 * v62));
  v56 = (unsigned __int64 *)(v66 + v12);
  v54 = (unsigned __int64 *)(v64 + v12);
  cczp_mul((signed __int64 *)a1, v10, v66 + v12, (unsigned __int64 *)(v64 + v12));
  cczp_mul((signed __int64 *)a1, (__int64)v11, v60, (unsigned __int64 *)v10);
  cczp_mod_inv(a1, v11, v11);
  twin_mult_normalize((signed __int64 *)a1, v63, (unsigned __int64 *)v61, (__int64)v11, v53, (unsigned __int64 *)v10);
  twin_mult_normalize(
    (signed __int64 *)a1,
    (__int64)v65,
    (unsigned __int64 *)v68,
    (__int64)v11,
    v55,
    (unsigned __int64 *)v10);
  v13 = v60;
  twin_mult_normalize((signed __int64 *)a1, v66, (unsigned __int64 *)v66, (__int64)v11, v60, v54);
  v14 = v64;
  twin_mult_normalize((signed __int64 *)a1, v64, (unsigned __int64 *)v64, (__int64)v11, v13, v56);
  v15 = v63;
  *(_QWORD *)(v63 + v12) = 1LL;
  v16 = 8 * v62 - 8;
  v61 = 8 * v62 - 8;
  bzero((void *)(v15 + v12 + 8), v16);
  v17 = (__int64)v65;
  v65[(unsigned __int64)v12 / 8] = 1LL;
  bzero((void *)(v17 + v12 + 8), v16);
  v18 = v66;
  *(_QWORD *)(v66 + v12) = 1LL;
  bzero((void *)(v18 + v12 + 8), v16);
  *(_QWORD *)(v14 + v12) = 1LL;
  bzero((void *)(v14 + v12 + 8), v16);
  v73 = v6;
  v19 = v57;
  v74 = v57;
  v20 = v62;
  v21 = ccn_bitlen(v62, v6);
  v22 = ccn_bitlen(v20, v19);
  if ( v21 > (unsigned __int64)v22 )
    v22 = v21;
  v68 = (unsigned __int64)(v22 - 1) >> 6;
  for ( i = 0LL; ; ++i )
  {
    v72[i] = 8 * (unsigned __int8)(*(_QWORD *)(v6 + 8 * v68) >> ((v22 - 1) & 0x3F)) & 8 | (unsigned __int64)(4 * (unsigned __int8)(*(_QWORD *)(v6 + 8 * ((unsigned __int64)(v22 - 2) >> 6)) >> ((v22 - 2) & 0x3F)) & 4) | 2 * (unsigned __int8)(*(_QWORD *)(v6 + 8 * ((unsigned __int64)(v22 - 3) >> 6)) >> ((v22 - 3) & 0x3F)) & 2 | (*(_QWORD *)(v6 + 8 * ((unsigned __int64)(v22 - 4) >> 6)) >> ((v22 - 4) & 0x3F)) & 1LL;
    if ( i == 1 )
      break;
    v6 = *(&v74 + i);
  }
  v24 = v59;
  *(_QWORD *)v59 = 1LL;
  v25 = v61;
  bzero((void *)(v24 + 8), v61);
  v26 = v62;
  *(_QWORD *)(v24 + 8 * v62) = 1LL;
  bzero((void *)(v24 + 8 * v26 + 8), v25);
  v27 = v58;
  *(_QWORD *)(v24 + 8 * v58) = 0LL;
  bzero((void *)(v24 + 8 * v27 + 8), v25);
  v28 = v22 + 1;
  if ( v22 != -1 )
  {
    do
    {
      v29 = v28;
      v28 = v22;
      v30 = 0LL;
      do
      {
        v31 = v72[v30] & 0x1F ^ 0x1F;
        if ( !(v72[v30] & 0x20) )
          v31 = v72[v30] & 0x1F;
        v71[v30++] = v31;
      }
      while ( v30 != 2 );
      v32 = v29 - 6;
      v33 = v32 >> 6;
      v34 = v32 & 0x3F;
      v35 = 1LL;
      v36 = 0LL;
      v37 = (signed __int64 *)v67;
      do
      {
        v38 = 0LL;
        if ( (unsigned __int64)v28 >= 5 )
          v38 = (*(_QWORD *)(*(__int64 *)((char *)&v73 + 2 * v36) + 8 * v33) >> v34) & 1LL;
        if ( v71[(unsigned __int64)(2 * v36) / 8] >= (unsigned __int64)*((_BYTE *)f_2_2_11_t + v71[v35]) )
        {
          v38 |= 0x20uLL;
          v39 = v72[(unsigned __int64)(2 * v36) / 8];
          *(int *)((char *)&v69 + v36) = (~(unsigned __int8)((unsigned int)v72[(unsigned __int64)(2 * v36) / 8] >> 4) & 2)
                                       - 1;
        }
        else
        {
          *(int *)((char *)&v69 + v36) = 0;
          v39 = v72[(unsigned __int64)(2 * v36) / 8];
        }
        v72[(unsigned __int64)(2 * v36) / 8] = v38 ^ 2 * v39;
        v36 += 4LL;
        --v35;
      }
      while ( v36 != 8 );
      ccec_double(v37, (void *)v24, (const void *)v24);
      v40 = v69;
      if ( v69 != 1 )
      {
        if ( v69 )
        {
          if ( v69 == -1 )
          {
            if ( v70 == 1 )
            {
              v41 = v37;
              v42 = (void *)v24;
              v43 = (const void *)v24;
              v44 = v64;
              goto LABEL_27;
            }
            if ( !v70 )
            {
              v41 = v37;
              v42 = (void *)v24;
              v43 = (const void *)v24;
              v44 = v63;
LABEL_27:
              ccec_full_sub(v41, v42, v43, (const void *)v44);
              goto LABEL_41;
            }
            if ( v70 == -1 )
            {
              v41 = v37;
              v42 = (void *)v24;
              v43 = (const void *)v24;
              v44 = v66;
              goto LABEL_27;
            }
          }
          goto LABEL_41;
        }
        v45 = v70;
        if ( v70 == -1 )
        {
          ccec_full_sub(v37, (void *)v24, (const void *)v24, v65);
          v45 = v70;
        }
        if ( v45 == 1 )
          ccec_full_add(v37, (void *)v24, (const void *)v24, v65);
        if ( v40 != 1 )
          goto LABEL_41;
      }
      v46 = v70;
      if ( v70 == -1 )
      {
        ccec_full_add(v37, (void *)v24, (const void *)v24, (const void *)v64);
        v46 = v70;
      }
      if ( v46 == 1 )
      {
        v47 = v37;
        v48 = (void *)v24;
        v49 = (const void *)v24;
        v50 = v66;
LABEL_40:
        ccec_full_add(v47, v48, v49, (const void *)v50);
        goto LABEL_41;
      }
      if ( !v46 )
      {
        v47 = v37;
        v48 = (void *)v24;
        v49 = (const void *)v24;
        v50 = v63;
        goto LABEL_40;
      }
LABEL_41:
      v22 = v28 - 1;
    }
    while ( v28 );
  }
  return *(_QWORD *)off_69010[0];
}
// 5EF90: using guessed type __int64 f_2_2_11_t[4];
// 69010: using guessed type __int64 off_69010[2];
// 262E6: using guessed type __int64 var_50[2];
// 262E6: using guessed type __int64 var_60[2];

//----- (00000000000268AD) ----------------------------------------------------
__int64 __fastcall twin_mult_normalize(signed __int64 *a1, __int64 a2, unsigned __int64 *a3, __int64 a4, __int64 a5, unsigned __int64 *a6)
{
  __int64 v6; // r14@1
  __int64 v7; // rax@1
  unsigned __int64 v8; // rax@1
  __int64 v9; // r13@1
  __int64 v10; // rbx@1
  unsigned __int64 *v11; // r15@1
  __int64 v13; // [sp+0h] [bp-40h]@1
  unsigned __int64 *v14; // [sp+8h] [bp-38h]@1
  __int64 v15; // [sp+10h] [bp-30h]@1

  v6 = a4;
  v14 = a3;
  v15 = *(_QWORD *)off_69010[0];
  v7 = *a1;
  v13 = v7;
  v8 = (8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v9 = (__int64)((char *)&v13 - v8);
  v10 = (__int64)((char *)&v13 - v8);
  cczp_mul(a1, (__int64)((char *)&v13 - v8), a5, a6);
  cczp_mul(a1, v9, v6, (unsigned __int64 *)v10);
  cczp_sqr(a1, v10, v9);
  v11 = v14;
  cczp_mul(a1, a2, v10, v14);
  cczp_mul(a1, v10, v10, (unsigned __int64 *)v9);
  cczp_mul(a1, a2 + 8 * v13, v10, &v11[v13]);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000026990) ----------------------------------------------------
__int64 __fastcall ccec_verify(const void *a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4, unsigned __int64 a5, __int64 a6)
{
  const void *v6; // r13@1
  unsigned __int64 v7; // rdi@1
  unsigned __int64 v8; // rax@1
  __int64 v9; // r14@1
  __int64 v10; // r15@1
  unsigned __int64 v11; // rbx@1
  __int64 v12; // rcx@1
  __int64 result; // rax@1
  __int64 v14; // rcx@3
  unsigned __int64 v15; // [sp+0h] [bp-40h]@1
  __int64 v16; // [sp+8h] [bp-38h]@1
  __int64 v17; // [sp+10h] [bp-30h]@1

  v16 = a6;
  v15 = a3;
  v6 = a1;
  v17 = *(_QWORD *)off_69010[0];
  v7 = **(_QWORD **)a1;
  v8 = (8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v9 = (__int64)((char *)&v15 - v8);
  v10 = (__int64)((char *)&v15 - v8);
  v11 = a5 + a4;
  v12 = ccder_decode_seqii(v7, (__int64)((char *)&v15 - v8), (__int64)((char *)&v15 - v8), a5, a5 + a4);
  result = 0xFFFFFFFFLL;
  if ( v12 == v11 )
    result = ccec_verify_internal(v6, a2, v15, v9, v10, v16);
  v14 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000026A43) ----------------------------------------------------
unsigned __int64 __fastcall ccec_x963_export(int a1, __int64 a2, __int64 **a3)
{
  __int64 **v3; // r14@1
  __int64 *v4; // r15@1
  unsigned __int64 v5; // rbx@1
  __int64 v6; // rdi@1
  unsigned __int64 v7; // rax@1
  size_t v8; // r13@1
  __int64 v9; // r14@3
  unsigned __int64 v10; // rax@3
  size_t v11; // r15@3
  unsigned __int64 result; // rax@5
  __int64 v13; // r14@6
  unsigned __int64 v14; // rax@6
  size_t v15; // r13@6
  __int64 **v16; // [sp+0h] [bp-50h]@1
  signed __int64 v17; // [sp+8h] [bp-48h]@1
  __int64 v18; // [sp+18h] [bp-38h]@1
  int v19; // [sp+20h] [bp-30h]@1

  v3 = a3;
  v16 = a3;
  v19 = a1;
  v4 = *a3;
  v5 = (unsigned __int64)(ccn_bitlen(**a3, (__int64)(*a3 + 2)) + 7) >> 3;
  v6 = *v4;
  v18 = *v4;
  *(_BYTE *)a2 = 4;
  v17 = (signed __int64)(v3 + 2);
  v7 = ccn_write_uint_size(v6, (__int64)(v3 + 2));
  v8 = v5 - v7;
  if ( v5 <= v7 )
    v8 = 0LL;
  bzero((void *)(a2 + 1), v8);
  ccn_write_uint(v18, (__int64)(v3 + 2), v5 - v8, v8 + a2 + 1);
  v9 = (__int64)&v3[**v3 + 2];
  v10 = ccn_write_uint_size(v18, v9);
  v11 = v5 - v10;
  if ( v5 <= v10 )
    v11 = 0LL;
  bzero((void *)(a2 + v5 + 1), v11);
  result = ccn_write_uint(v18, v9, v5 - v11, a2 + v11 + v5 + 1);
  if ( v19 )
  {
    v13 = v17 + 24 * **v16;
    v14 = ccn_write_uint_size(v18, v17 + 24 * **v16);
    v15 = 0LL;
    if ( v5 > v14 )
      v15 = v5 - v14;
    bzero((void *)(a2 + 2 * v5 + 1), v15);
    result = ccn_write_uint(v18, v13, v5 - v15, a2 + v15 + 2 * v5 + 1);
  }
  return result;
}

//----- (0000000000026BA7) ----------------------------------------------------
signed __int64 __fastcall ccec_x963_import_priv_size(signed __int64 a1)
{
  signed __int64 result; // rax@1

  result = 192LL;
  if ( a1 > 198 )
  {
    if ( a1 == 199 )
      return 521LL;
  }
  else if ( a1 > 144 )
  {
    if ( a1 == 145 )
      return 384LL;
  }
  else
  {
    if ( a1 == 73 )
      return result;
    if ( a1 == 85 )
      return 224LL;
    if ( a1 == 97 )
      return 256LL;
  }
  return 0LL;
}

//----- (0000000000026C06) ----------------------------------------------------
__int64 __fastcall ccec_x963_import_priv(unsigned __int64 *a1, __int64 a2, unsigned int *a3, __int64 a4)
{
  __int64 v4; // r14@1
  unsigned int *v5; // r15@1
  unsigned int v6; // eax@1
  int v7; // er12@1
  signed int v8; // ecx@2
  unsigned __int64 v9; // rbx@3
  unsigned __int64 v10; // r13@3
  signed __int64 v11; // rax@5
  signed __int64 v13; // [sp+0h] [bp-30h]@3

  v4 = a4;
  v5 = a3;
  v6 = *(_BYTE *)a3;
  v7 = -1;
  if ( v6 <= 7 )
  {
    v8 = 208;
    if ( _bittest((const unsigned int *)&v8, (unsigned __int8)v6) )
    {
      v9 = (unsigned __int64)((unsigned __int128)(0x0AAAAAAAAAAAAAAABLL * (a2 - 1)) >> 64) >> 1;
      *(_QWORD *)v4 = a1;
      v10 = *a1;
      v13 = v4 + 16;
      if ( !(unsigned int)ccn_read_uint(*a1, v4 + 16, v9, (unsigned __int64)((char *)a3 + 1))
        && !(unsigned int)ccn_read_uint(v10, v13 + 8LL * **(_QWORD **)v4, v9, (unsigned __int64)((char *)v5 + v9 + 1)) )
      {
        v11 = 16LL * **(_QWORD **)v4;
        *(_QWORD *)(v11 + v4 + 16) = 1LL;
        bzero((void *)(v11 + v4 + 24), 8 * v10 - 8);
        v7 = -((unsigned int)ccn_read_uint(
                               v10,
                               v13 + 24LL * **(_QWORD **)v4,
                               v9,
                               (unsigned __int64)((char *)v5 + v9 + v9 + 1)) != 0);
      }
    }
  }
  return (unsigned int)v7;
}

//----- (0000000000026CFB) ----------------------------------------------------
__int64 __fastcall ccrsa_verify_oaep(__int64 *a1, __int64 a2, size_t a3, void *a4, unsigned __int64 a5, unsigned __int64 a6, __int64 a7)
{
  __int64 v7; // r15@1
  unsigned __int64 v8; // r12@1
  char *v9; // rbx@1
  __int64 v10; // r13@1
  int v11; // eax@1
  signed int v12; // er12@1
  size_t v13; // rdx@2
  __int64 result; // rax@4
  __int64 v15; // [sp+10h] [bp-70h]@1
  __int64 v16; // [sp+18h] [bp-68h]@1
  __int64 v17; // [sp+20h] [bp-60h]@1
  void *v18; // [sp+28h] [bp-58h]@1
  size_t v19; // [sp+30h] [bp-50h]@1
  unsigned __int64 v20; // [sp+38h] [bp-48h]@1
  unsigned __int64 v21; // [sp+40h] [bp-40h]@1
  __int64 v22; // [sp+48h] [bp-38h]@1
  __int64 v23; // [sp+50h] [bp-30h]@1

  v21 = a6;
  v20 = a5;
  v18 = a4;
  v19 = a3;
  v17 = a2;
  v7 = off_69010[0];
  v23 = *(_QWORD *)off_69010[0];
  v8 = *a1;
  v16 = ccn_write_uint_size(*a1, (__int64)(a1 + 2));
  v22 = *(_QWORD *)a2;
  v9 = (char *)&v15 - ((v22 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v10 = (__int64)((char *)&v15 - ((8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v11 = ccn_read_uint(v8, v10, v20, v21);
  v12 = -1;
  if ( !v11 )
  {
    ccrsa_pub_crypt(a1, v10, v10);
    v12 = ccrsa_oaep_decode_parameter(v17, (__int64)&v22, v9, v16, v10, 0LL, 0LL);
    v13 = v19;
    *(_BYTE *)a7 = v22 == v19;
    if ( memcmp(v9, v18, v13) )
      *(_BYTE *)a7 = 0;
  }
  result = *(_QWORD *)v7;
  if ( *(_QWORD *)v7 == v23 )
    result = (unsigned int)v12;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000026E13) ----------------------------------------------------
__int64 __fastcall ccmode_ccm_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  unsigned __int64 v6; // r15@1
  __int64 result; // rax@1
  int v8; // er13@1

  v5 = a5;
  v6 = a3;
  result = *(_DWORD *)(a2 + 64);
  v8 = 0;
  if ( (_DWORD)result != 2 )
  {
    if ( (_DWORD)result != 1 )
      return result;
    v8 = *(_DWORD *)(a2 + 72) != 0;
    *(_DWORD *)(a2 + 64) = 2;
  }
  ccmode_ccm_crypt(a1, a2, a3, a4, a5);
  return ccmode_ccm_macdata(a1, a2, v8, v6, v5);
}

//----- (0000000000026E91) ----------------------------------------------------
signed __int64 __fastcall ccec_x963_import_pub_size(signed __int64 a1)
{
  signed __int64 result; // rax@1

  result = 192LL;
  if ( a1 > 132 )
  {
    if ( a1 == 133 )
      return 521LL;
  }
  else if ( a1 > 96 )
  {
    if ( a1 == 97 )
      return 384LL;
  }
  else
  {
    if ( a1 == 49 )
      return result;
    if ( a1 == 57 )
      return 224LL;
    if ( a1 == 65 )
      return 256LL;
  }
  return 0LL;
}

//----- (0000000000026EEA) ----------------------------------------------------
__int64 __fastcall ccec_x963_import_pub(unsigned __int64 *a1, __int64 a2, unsigned int *a3, __int64 a4)
{
  __int64 v4; // r15@1
  unsigned int *v5; // r13@1
  unsigned int v6; // eax@1
  signed int v7; // er14@1
  signed int v8; // ecx@2
  unsigned __int64 v9; // rbx@3
  signed __int64 v10; // rax@5
  unsigned __int64 v12; // [sp+0h] [bp-30h]@3

  v4 = a4;
  v5 = a3;
  v6 = *(_BYTE *)a3;
  v7 = -1;
  if ( v6 <= 7 )
  {
    v8 = 208;
    if ( _bittest((const unsigned int *)&v8, (unsigned __int8)v6) )
    {
      v9 = (unsigned __int64)(a2 - 1) >> 1;
      *(_QWORD *)v4 = a1;
      v12 = *a1;
      if ( !(unsigned int)ccn_read_uint(*a1, v4 + 16, v9, (unsigned __int64)((char *)a3 + 1))
        && !(unsigned int)ccn_read_uint(
                            v12,
                            v4 + 16 + 8LL * **(_QWORD **)v4,
                            v9,
                            (unsigned __int64)((char *)v5 + v9 + 1)) )
      {
        v10 = 16LL * **(_QWORD **)v4;
        *(_QWORD *)(v10 + v4 + 16) = 1LL;
        bzero((void *)(v10 + v4 + 24), 8 * v12 - 8);
        v7 = 0;
      }
    }
  }
  return (unsigned int)v7;
}

//----- (0000000000026FA1) ----------------------------------------------------
__int64 __usercall cchmac@<rax>(__int64 a1@<rax>, const void *a2@<rdx>, unsigned __int64 a3@<rcx>, __int64 a4@<rdi>, unsigned __int64 a5@<rsi>, const void *a6@<r8>, __int64 a7@<r9>)
{
  __int64 v7; // r14@1
  const void *v8; // r15@1
  unsigned __int64 v9; // r12@1
  char *v10; // rbx@1
  signed __int64 v11; // rax@1
  __int64 v13; // [sp+0h] [bp-30h]@1

  v13 = a1;
  v7 = a7;
  v8 = a6;
  v9 = a3;
  v13 = *(_QWORD *)off_69010[0];
  v10 = (char *)&v13
      - ((((*(_QWORD *)(a4 + 16) + 2LL * *(_QWORD *)(a4 + 8) + 19) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v11 = cchmac_init(a4, (__int64)v10, a5, a2);
  cchmac_update(v11, v9, v8, a4, (__int64)v10);
  cchmac_final(a4, (__int64)v10, v7);
  cc_clear(*(_QWORD *)(a4 + 16) + 2LL * *(_QWORD *)(a4 + 8) + 12, v10);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000027057) ----------------------------------------------------
int __fastcall cchmac_final(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r14@1
  size_t v4; // rdx@1
  __int64 v5; // rcx@1

  v3 = a3;
  (*(void (__fastcall **)(__int64, __int64, signed __int64))(a1 + 56))(a1, a2, *(_QWORD *)(a1 + 8) + a2 + 8);
  v4 = *(_QWORD *)(a1 + 8);
  v5 = *(_QWORD *)(a1 + 16);
  *(_DWORD *)(a2 + v4 + v5 + 8) = *(_DWORD *)a1;
  *(_QWORD *)a2 = 8 * v5;
  memcpy((void *)(a2 + 8), (const void *)(a2 + v4 + v5 + 12), v4);
  return (*(int (__fastcall **)(__int64, __int64, __int64))(a1 + 56))(a1, a2, v3);
}

//----- (00000000000270C0) ----------------------------------------------------
void __usercall gcm_init(__int64 a1@<rdi>, const __m128i *a2@<rsi>, __m128i a3@<xmm0>, __m128i a4@<xmm1>, __m128i a5@<xmm2>, __m128i a6@<xmm3>, __m128i a7@<xmm4>)
{
  __m128i v7; // xmm2@1
  __m128i v15; // xmm3@1
  __m128i v17; // xmm4@1
  __m128i v18; // xmm1@1
  __m128i v21; // xmm1@1
  __m128i v30; // xmm3@1
  __m128i v32; // xmm4@1
  __m128i v33; // xmm1@1
  __m128i v36; // xmm1@1
  __m128i v45; // xmm3@1
  __m128i v47; // xmm4@1
  __m128i v48; // xmm1@1
  __m128i v51; // xmm1@1
  __m128i v60; // xmm3@1
  __m128i v62; // xmm4@1
  __m128i v63; // xmm1@1
  __m128i v66; // xmm1@1
  __m128i v75; // xmm3@1
  __m128i v77; // xmm4@1
  __m128i v78; // xmm1@1
  __m128i v81; // xmm1@1
  __m128i v90; // xmm3@1
  __m128i v92; // xmm4@1
  __m128i v93; // xmm1@1
  __m128i v96; // xmm1@1
  __m128i v105; // xmm3@1
  __m128i v107; // xmm4@1
  __m128i v108; // xmm1@1
  __m128i v111; // xmm1@1
  __m128i v113; // xmm0@1
  __int128 v114; // [sp+0h] [bp-50h]@1
  __int128 v115; // [sp+10h] [bp-40h]@1
  __int128 v116; // [sp+20h] [bp-30h]@1
  __int128 v117; // [sp+30h] [bp-20h]@1
  __int128 v118; // [sp+40h] [bp-10h]@1

  _mm_store_si128((__m128i *)&v114, a3);
  _mm_store_si128((__m128i *)&v115, a4);
  _mm_store_si128((__m128i *)&v116, a5);
  _mm_store_si128((__m128i *)&v117, a6);
  _mm_store_si128((__m128i *)&v118, a7);
  v7 = _mm_shuffle_epi8(_mm_loadu_si128(a2), (__m128i)xmmword_27A70);
  _XMM2 = _mm_xor_si128(
            _mm_or_si128(_mm_sll_epi64(v7, 1u), _mm_slli_si128(_mm_srl_epi64(v7, 0x3Fu), 8)),
            _mm_and_si128(_mm_sra_epi32(_mm_shuffle_epi32(v7, -1), 0x1Fu), (__m128i)xmmword_27A80));
  _mm_storeu_si128((__m128i *)a1, _XMM2);
  _mm_storeu_si128((__m128i *)(a1 + 128), _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2));
  _XMM1 = _XMM2;
  _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
  _XMM4 = _XMM3;
  __asm
  {
    pclmulqdq xmm0, xmm2, 0
    pclmulqdq xmm1, xmm2, 11h
    pclmulqdq xmm3, xmm4, 0
  }
  v15 = _mm_xor_si128(_mm_xor_si128(_XMM3, _XMM1), _XMM0);
  _XMM0 = _mm_xor_si128(_XMM0, _mm_slli_si128(v15, 8));
  v17 = _mm_xor_si128(_mm_srli_si128(v15, 8), _XMM1);
  v18 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, v18);
  v21 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, _mm_xor_si128(v17, v21));
  _mm_storeu_si128((__m128i *)(a1 + 16), _XMM0);
  _mm_storeu_si128((__m128i *)(a1 + 144), _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0));
  _XMM1 = _XMM0;
  _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0);
  _XMM4 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
  __asm
  {
    pclmulqdq xmm0, xmm2, 0
    pclmulqdq xmm1, xmm2, 11h
    pclmulqdq xmm3, xmm4, 0
  }
  v30 = _mm_xor_si128(_mm_xor_si128(_XMM3, _XMM1), _XMM0);
  _XMM0 = _mm_xor_si128(_XMM0, _mm_slli_si128(v30, 8));
  v32 = _mm_xor_si128(_mm_srli_si128(v30, 8), _XMM1);
  v33 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, v33);
  v36 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, _mm_xor_si128(v32, v36));
  _mm_storeu_si128((__m128i *)(a1 + 32), _XMM0);
  _mm_storeu_si128((__m128i *)(a1 + 160), _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0));
  _XMM1 = _XMM0;
  _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0);
  _XMM4 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
  __asm
  {
    pclmulqdq xmm0, xmm2, 0
    pclmulqdq xmm1, xmm2, 11h
    pclmulqdq xmm3, xmm4, 0
  }
  v45 = _mm_xor_si128(_mm_xor_si128(_XMM3, _XMM1), _XMM0);
  _XMM0 = _mm_xor_si128(_XMM0, _mm_slli_si128(v45, 8));
  v47 = _mm_xor_si128(_mm_srli_si128(v45, 8), _XMM1);
  v48 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, v48);
  v51 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, _mm_xor_si128(v47, v51));
  _mm_storeu_si128((__m128i *)(a1 + 48), _XMM0);
  _mm_storeu_si128((__m128i *)(a1 + 176), _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0));
  _XMM1 = _XMM0;
  _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0);
  _XMM4 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
  __asm
  {
    pclmulqdq xmm0, xmm2, 0
    pclmulqdq xmm1, xmm2, 11h
    pclmulqdq xmm3, xmm4, 0
  }
  v60 = _mm_xor_si128(_mm_xor_si128(_XMM3, _XMM1), _XMM0);
  _XMM0 = _mm_xor_si128(_XMM0, _mm_slli_si128(v60, 8));
  v62 = _mm_xor_si128(_mm_srli_si128(v60, 8), _XMM1);
  v63 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, v63);
  v66 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, _mm_xor_si128(v62, v66));
  _mm_storeu_si128((__m128i *)(a1 + 64), _XMM0);
  _mm_storeu_si128((__m128i *)(a1 + 192), _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0));
  _XMM1 = _XMM0;
  _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0);
  _XMM4 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
  __asm
  {
    pclmulqdq xmm0, xmm2, 0
    pclmulqdq xmm1, xmm2, 11h
    pclmulqdq xmm3, xmm4, 0
  }
  v75 = _mm_xor_si128(_mm_xor_si128(_XMM3, _XMM1), _XMM0);
  _XMM0 = _mm_xor_si128(_XMM0, _mm_slli_si128(v75, 8));
  v77 = _mm_xor_si128(_mm_srli_si128(v75, 8), _XMM1);
  v78 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, v78);
  v81 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, _mm_xor_si128(v77, v81));
  _mm_storeu_si128((__m128i *)(a1 + 80), _XMM0);
  _mm_storeu_si128((__m128i *)(a1 + 208), _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0));
  _XMM1 = _XMM0;
  _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0);
  _XMM4 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
  __asm
  {
    pclmulqdq xmm0, xmm2, 0
    pclmulqdq xmm1, xmm2, 11h
    pclmulqdq xmm3, xmm4, 0
  }
  v90 = _mm_xor_si128(_mm_xor_si128(_XMM3, _XMM1), _XMM0);
  _XMM0 = _mm_xor_si128(_XMM0, _mm_slli_si128(v90, 8));
  v92 = _mm_xor_si128(_mm_srli_si128(v90, 8), _XMM1);
  v93 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, v93);
  v96 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, _mm_xor_si128(v92, v96));
  _mm_storeu_si128((__m128i *)(a1 + 96), _XMM0);
  _mm_storeu_si128((__m128i *)(a1 + 224), _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0));
  _XMM1 = _XMM0;
  _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM0);
  _XMM4 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
  __asm
  {
    pclmulqdq xmm0, xmm2, 0
    pclmulqdq xmm1, xmm2, 11h
    pclmulqdq xmm3, xmm4, 0
  }
  v105 = _mm_xor_si128(_mm_xor_si128(_XMM3, _XMM1), _XMM0);
  _XMM0 = _mm_xor_si128(_XMM0, _mm_slli_si128(v105, 8));
  v107 = _mm_xor_si128(_mm_srli_si128(v105, 8), _XMM1);
  v108 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, v108);
  v111 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  v113 = _mm_xor_si128(_XMM0, _mm_xor_si128(v107, v111));
  _mm_storeu_si128((__m128i *)(a1 + 112), v113);
  _mm_storeu_si128((__m128i *)(a1 + 240), _mm_xor_si128(_mm_shuffle_epi32(v113, 78), v113));
}
// 27A70: using guessed type __int128 xmmword_27A70;
// 27A80: using guessed type __int128 xmmword_27A80;

//----- (0000000000027510) ----------------------------------------------------
void __usercall gcm_gmult(__m128i *a1@<rdx>, const __m128i *a2@<rdi>, const __m128i *a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm4>)
{
  __m128i v15; // xmm3@1
  __m128i v17; // xmm4@1
  __m128i v18; // xmm1@1
  __m128i v21; // xmm1@1
  __int128 v23; // [sp+0h] [bp-50h]@1
  __int128 v24; // [sp+10h] [bp-40h]@1
  __int128 v25; // [sp+20h] [bp-30h]@1
  __int128 v26; // [sp+30h] [bp-20h]@1
  __int128 v27; // [sp+40h] [bp-10h]@1

  _mm_store_si128((__m128i *)&v23, a4);
  _mm_store_si128((__m128i *)&v24, a5);
  _mm_store_si128((__m128i *)&v25, a6);
  _mm_store_si128((__m128i *)&v26, a7);
  _mm_store_si128((__m128i *)&v27, a8);
  _XMM2 = _mm_loadu_si128(a3);
  _XMM1 = _mm_shuffle_epi8(_mm_loadu_si128(a2), (__m128i)xmmword_27A70);
  _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM1, 78), _XMM1);
  _XMM4 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
  __asm
  {
    pclmulqdq xmm0, xmm2, 0
    pclmulqdq xmm1, xmm2, 11h
    pclmulqdq xmm3, xmm4, 0
  }
  v15 = _mm_xor_si128(_mm_xor_si128(_XMM3, _XMM1), _XMM0);
  _XMM0 = _mm_xor_si128(_XMM0, _mm_slli_si128(v15, 8));
  v17 = _mm_xor_si128(_mm_srli_si128(v15, 8), _XMM1);
  v18 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _XMM0 = _mm_xor_si128(_XMM0, v18);
  v21 = _mm_shuffle_epi32(_XMM0, 78);
  __asm { pclmulqdq xmm0, cs:xmmword_27A80, 10h }
  _mm_storeu_si128(a1, _mm_shuffle_epi8(_mm_xor_si128(_XMM0, _mm_xor_si128(v17, v21)), (__m128i)xmmword_27A70));
}
// 27A70: using guessed type __int128 xmmword_27A70;
// 27A80: using guessed type __int128 xmmword_27A80;

//----- (00000000000275F0) ----------------------------------------------------
void __usercall gcm_ghash(__int64 a1@<rdx>, __int64 a2@<rcx>, const __m128i *a3@<rdi>, __int64 _RSI@<rsi>, __m128i a5@<xmm0>, __m128i a6@<xmm1>, __m128i a7@<xmm2>, __m128i a8@<xmm3>, __m128i a9@<xmm4>, __m128i a10@<xmm5>, __m128i a11@<xmm6>, __m128i a12@<xmm7>)
{
  __m128i v12; // xmm0@1
  unsigned __int8 v13; // of@1
  signed __int64 i; // rcx@1
  __m128i v25; // xmm5@2
  __m128i v26; // xmm6@2
  __m128i v27; // xmm7@2
  __m128i v33; // xmm5@2
  __m128i v34; // xmm6@2
  __m128i v35; // xmm7@2
  __m128i v41; // xmm5@2
  __m128i v42; // xmm6@2
  __m128i v43; // xmm7@2
  __m128i v49; // xmm5@2
  __m128i v50; // xmm6@2
  __m128i v51; // xmm7@2
  __m128i v57; // xmm5@2
  __m128i v58; // xmm6@2
  __m128i v59; // xmm7@2
  __m128i v65; // xmm5@2
  __m128i v66; // xmm6@2
  __m128i v67; // xmm7@2
  __m128i v73; // xmm5@2
  __m128i v74; // xmm6@2
  __m128i v75; // xmm7@2
  bool v82; // sf@3
  const __m128i *v90; // rdx@4
  __m128i v92; // xmm4@5
  __m128i v99; // xmm7@6
  __int128 v106; // [sp+0h] [bp-80h]@1
  __int128 v107; // [sp+10h] [bp-70h]@1
  __int128 v108; // [sp+20h] [bp-60h]@1
  __int128 v109; // [sp+30h] [bp-50h]@1
  __int128 v110; // [sp+40h] [bp-40h]@1
  __int128 v111; // [sp+50h] [bp-30h]@1
  __int128 v112; // [sp+60h] [bp-20h]@1
  __int128 v113; // [sp+70h] [bp-10h]@1

  _mm_store_si128((__m128i *)&v106, a5);
  _mm_store_si128((__m128i *)&v107, a6);
  _mm_store_si128((__m128i *)&v108, a7);
  _mm_store_si128((__m128i *)&v109, a8);
  _mm_store_si128((__m128i *)&v110, a9);
  _mm_store_si128((__m128i *)&v111, a10);
  _mm_store_si128((__m128i *)&v112, a11);
  _mm_store_si128((__m128i *)&v113, a12);
  v12 = _mm_shuffle_epi8(_mm_loadu_si128(a3), (__m128i)xmmword_27A70);
  v13 = __OFSUB__(a2, 128LL);
  for ( i = a2 - 128; !((i < 0) ^ v13); i -= 128LL )
  {
    _XMM6 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(a1 + 112)), (__m128i)xmmword_27A70);
    _XMM7 = _mm_xor_si128(_mm_shuffle_epi32(_XMM6, 78), _XMM6);
    __asm
    {
      pclmulqdq xmm5, xmmword ptr [rsi], 0
      pclmulqdq xmm6, xmmword ptr [rsi], 11h
      pclmulqdq xmm7, xmmword ptr [rsi+80h], 0
    }
    _XMM2 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(a1 + 96)), (__m128i)xmmword_27A70);
    _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
    __asm
    {
      pclmulqdq xmm1, xmmword ptr [rsi+10h], 0
      pclmulqdq xmm2, xmmword ptr [rsi+10h], 11h
      pclmulqdq xmm3, xmmword ptr [rsi+90h], 0
    }
    v25 = _mm_xor_si128(_XMM5, _XMM1);
    v26 = _mm_xor_si128(_XMM6, _XMM2);
    v27 = _mm_xor_si128(_XMM7, _XMM3);
    _XMM2 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(a1 + 80)), (__m128i)xmmword_27A70);
    _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
    __asm
    {
      pclmulqdq xmm1, xmmword ptr [rsi+20h], 0
      pclmulqdq xmm2, xmmword ptr [rsi+20h], 11h
      pclmulqdq xmm3, xmmword ptr [rsi+0A0h], 0
    }
    v33 = _mm_xor_si128(v25, _XMM1);
    v34 = _mm_xor_si128(v26, _XMM2);
    v35 = _mm_xor_si128(v27, _XMM3);
    _XMM2 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(a1 + 64)), (__m128i)xmmword_27A70);
    _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
    __asm
    {
      pclmulqdq xmm1, xmmword ptr [rsi+30h], 0
      pclmulqdq xmm2, xmmword ptr [rsi+30h], 11h
      pclmulqdq xmm3, xmmword ptr [rsi+0B0h], 0
    }
    v41 = _mm_xor_si128(v33, _XMM1);
    v42 = _mm_xor_si128(v34, _XMM2);
    v43 = _mm_xor_si128(v35, _XMM3);
    _XMM2 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(a1 + 48)), (__m128i)xmmword_27A70);
    _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
    __asm
    {
      pclmulqdq xmm1, xmmword ptr [rsi+40h], 0
      pclmulqdq xmm2, xmmword ptr [rsi+40h], 11h
      pclmulqdq xmm3, xmmword ptr [rsi+0C0h], 0
    }
    v49 = _mm_xor_si128(v41, _XMM1);
    v50 = _mm_xor_si128(v42, _XMM2);
    v51 = _mm_xor_si128(v43, _XMM3);
    _XMM2 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(a1 + 32)), (__m128i)xmmword_27A70);
    _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
    __asm
    {
      pclmulqdq xmm1, xmmword ptr [rsi+50h], 0
      pclmulqdq xmm2, xmmword ptr [rsi+50h], 11h
      pclmulqdq xmm3, xmmword ptr [rsi+0D0h], 0
    }
    v57 = _mm_xor_si128(v49, _XMM1);
    v58 = _mm_xor_si128(v50, _XMM2);
    v59 = _mm_xor_si128(v51, _XMM3);
    _XMM2 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(a1 + 16)), (__m128i)xmmword_27A70);
    _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
    __asm
    {
      pclmulqdq xmm1, xmmword ptr [rsi+60h], 0
      pclmulqdq xmm2, xmmword ptr [rsi+60h], 11h
      pclmulqdq xmm3, xmmword ptr [rsi+0E0h], 0
    }
    v65 = _mm_xor_si128(v57, _XMM1);
    v66 = _mm_xor_si128(v58, _XMM2);
    v67 = _mm_xor_si128(v59, _XMM3);
    _XMM2 = _mm_xor_si128(_mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)a1), (__m128i)xmmword_27A70), v12);
    _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(_XMM2, 78), _XMM2);
    __asm
    {
      pclmulqdq xmm1, xmmword ptr [rsi+70h], 0
      pclmulqdq xmm2, xmmword ptr [rsi+70h], 11h
      pclmulqdq xmm3, xmmword ptr [rsi+0F0h], 0
    }
    v73 = _mm_xor_si128(v65, _XMM1);
    v74 = _mm_xor_si128(v66, _XMM2);
    v75 = _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v67, _XMM3), v73), v74);
    _XMM0 = _mm_xor_si128(_mm_slli_si128(v75, 8), v73);
    _XMM2 = _mm_shuffle_epi32((__m128i)xmmword_27A80, 78);
    __asm { pclmulqdq xmm2, xmm0, 0 }
    _XMM0 = _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM2);
    _XMM2 = _mm_shuffle_epi32((__m128i)xmmword_27A80, 78);
    __asm { pclmulqdq xmm2, xmm0, 0 }
    v12 = _mm_xor_si128(_mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM2), _mm_xor_si128(_mm_srli_si128(v75, 8), v74));
    a1 += 128LL;
    v13 = __OFSUB__(i, 128LL);
  }
  v13 = __OFADD__(112LL, i);
  v82 = i + 112 < 0;
  _RCX = i + 112;
  if ( !(v82 ^ v13) )
  {
    _XMM5 = _mm_xor_si128(_mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)a1), (__m128i)xmmword_27A70), v12);
    _XMM6 = _XMM5;
    _XMM7 = _mm_xor_si128(_mm_shuffle_epi32(_XMM5, 78), _XMM5);
    __asm
    {
      pclmulqdq xmm5, xmmword ptr [rsi+rcx], 0
      pclmulqdq xmm6, xmmword ptr [rsi+rcx], 11h
      pclmulqdq xmm7, xmmword ptr [rsi+rcx+80h], 0
    }
    v90 = (const __m128i *)(a1 + 16);
    v13 = __OFSUB__(_RCX, 16LL);
    for ( _RCX = _RCX - 16; !((_RCX < 0) ^ v13); _RCX -= 16LL )
    {
      v92 = _mm_shuffle_epi8(_mm_loadu_si128(v90), (__m128i)xmmword_27A70);
      _XMM2 = v92;
      __asm { pclmulqdq xmm2, xmmword ptr [rsi+rcx], 0 }
      _XMM5 = _mm_xor_si128(_XMM5, _XMM2);
      _XMM2 = v92;
      __asm { pclmulqdq xmm2, xmmword ptr [rsi+rcx], 11h }
      _XMM6 = _mm_xor_si128(_XMM6, _XMM2);
      _XMM3 = _mm_xor_si128(_mm_shuffle_epi32(v92, 78), v92);
      __asm { pclmulqdq xmm3, xmmword ptr [rsi+rcx+80h], 0 }
      _XMM7 = _mm_xor_si128(_XMM7, _XMM3);
      ++v90;
      v13 = __OFSUB__(_RCX, 16LL);
    }
    v99 = _mm_xor_si128(_mm_xor_si128(_XMM7, _XMM5), _XMM6);
    _XMM0 = _mm_xor_si128(_mm_slli_si128(v99, 8), _XMM5);
    _XMM2 = _mm_shuffle_epi32((__m128i)xmmword_27A80, 78);
    __asm { pclmulqdq xmm2, xmm0, 0 }
    _XMM0 = _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM2);
    _XMM2 = _mm_shuffle_epi32((__m128i)xmmword_27A80, 78);
    __asm { pclmulqdq xmm2, xmm0, 0 }
    v12 = _mm_xor_si128(
            _mm_xor_si128(_mm_shuffle_epi32(_XMM0, 78), _XMM2),
            _mm_xor_si128(_mm_srli_si128(v99, 8), _XMM6));
  }
  _mm_storeu_si128((__m128i *)a3, _mm_shuffle_epi8(v12, (__m128i)xmmword_27A70));
}
// 27A70: using guessed type __int128 xmmword_27A70;
// 27A80: using guessed type __int128 xmmword_27A80;

//----- (0000000000027A90) ----------------------------------------------------
signed __int64 __fastcall cchmac_init(__int64 a1, __int64 a2, unsigned __int64 a3, const void *a4)
{
  const void *v4; // r15@1
  unsigned __int64 v5; // r12@1
  __int64 v6; // rax@2
  unsigned __int64 v7; // rcx@2
  unsigned __int64 v8; // rax@2
  __int64 v9; // rax@3
  __int64 v10; // rax@7
  __int64 v11; // rcx@10
  unsigned __int64 v12; // rdx@10
  bool v13; // cf@10
  size_t v14; // rdx@10
  unsigned __int64 v15; // rax@13
  __int64 v16; // rax@15
  signed __int64 result; // rax@15

  v4 = a4;
  v5 = a3;
  if ( *(_QWORD *)(a1 + 16) >= a3 )
  {
    v8 = 0LL;
    if ( a3 )
    {
      v10 = 0LL;
      do
      {
        *(_BYTE *)(a2 + v10 + *(_QWORD *)(a1 + 8) + 8) = *((_BYTE *)a4 + v10) ^ 0x5C;
        ++v10;
      }
      while ( a3 != v10 );
      v8 = a3;
    }
  }
  else
  {
    v6 = ccdigest_init(a1, a2);
    ccdigest_update(v6, v5, v4, a1, a2);
    (*(void (__fastcall **)(__int64, __int64, signed __int64))(a1 + 56))(a1, a2, *(_QWORD *)(a1 + 8) + a2 + 8);
    v7 = *(_QWORD *)a1;
    v8 = 0LL;
    if ( *(_QWORD *)a1 )
    {
      v9 = 0LL;
      do
        *(_BYTE *)(a2 + v9++ + *(_QWORD *)(a1 + 8) + 8) ^= 0x5Cu;
      while ( v7 != v9 );
      v8 = v7;
    }
  }
  v11 = *(_QWORD *)(a1 + 16);
  v12 = *(_QWORD *)(a1 + 16);
  v13 = v12 < v8;
  v14 = v12 - v8;
  if ( !v13 && v14 != 0 )
  {
    memset((void *)(a2 + *(_QWORD *)(a1 + 8) + v8 + 8), 92, v14);
    v11 = *(_QWORD *)(a1 + 16);
  }
  memcpy((void *)(a2 + *(_QWORD *)(a1 + 8) + v11 + 12), *(const void **)(a1 + 40), *(_QWORD *)(a1 + 8));
  (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64))(a1 + 48))(
    a2 + *(_QWORD *)(a1 + 16) + *(_QWORD *)(a1 + 8) + 8LL + 4,
    1LL,
    a2 + *(_QWORD *)(a1 + 8) + 8);
  if ( *(_QWORD *)(a1 + 16) )
  {
    v15 = 0LL;
    do
      *(_BYTE *)(a2 + v15++ + *(_QWORD *)(a1 + 8) + 8) ^= 0x6Au;
    while ( v15 < *(_QWORD *)(a1 + 16) );
  }
  memcpy((void *)(a2 + 8), *(const void **)(a1 + 40), *(_QWORD *)(a1 + 8));
  (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64))(a1 + 48))(
    a2 + 8,
    1LL,
    *(_QWORD *)(a1 + 8) + a2 + 8);
  v16 = *(_QWORD *)(a1 + 16);
  *(_DWORD *)(a2 + v16 + *(_QWORD *)(a1 + 8) + 8) = 0;
  result = 8 * v16;
  *(_QWORD *)a2 = result;
  return result;
}

//----- (0000000000027BEA) ----------------------------------------------------
__int64 __fastcall ccmgf(__int64 a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5)
{
  __int64 v5; // r12@1
  unsigned __int64 v6; // rbx@1
  __int64 v7; // rax@1
  void *v8; // r14@1
  const void *v9; // r15@2
  __int64 v10; // rax@2
  signed __int64 v11; // rax@2
  __int64 v12; // rax@4
  size_t v14; // [sp+0h] [bp-50h]@1
  unsigned __int64 v15; // [sp+8h] [bp-48h]@1
  const void *i; // [sp+10h] [bp-40h]@2
  unsigned int v17; // [sp+1Ch] [bp-34h]@2
  __int64 v18; // [sp+20h] [bp-30h]@1

  v15 = a4;
  v18 = *(_QWORD *)off_69010[0];
  v5 = (__int64)((char *)&v14
               - ((((*(_QWORD *)(a1 + 8) + *(_QWORD *)(a1 + 16) + 19LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v6 = a2 / *(_QWORD *)a1;
  v7 = *(_QWORD *)a1 * (unsigned int)(a2 / *(_QWORD *)a1);
  v8 = (char *)a3 + v7;
  if ( v7 != a2 )
  {
    v14 = a2 - v7;
    i = a5;
    v9 = a3;
    v10 = ccdigest_init(a1, v5);
    LODWORD(v11) = ccdigest_update(v10, v15, i, a1, v5);
    v17 = _byteswap_ulong(v6);
    LODWORD(v6) = _byteswap_ulong(v17);
    ccdigest_update(v11, 4uLL, &v17, a1, v5);
    (*(void (__fastcall **)(__int64, __int64, const void *))(a1 + 56))(a1, v5, v9);
    memmove(v8, v9, v14);
    a5 = i;
  }
  for ( i = a5; (_DWORD)v6; LODWORD(v6) = v6 - 1 )
  {
    v8 = (char *)v8 - *(_QWORD *)a1;
    v12 = ccdigest_init(a1, v5);
    ccdigest_update(v12, v15, i, a1, v5);
    v17 = _byteswap_ulong(v6 - 1);
    ccdigest_update(_byteswap_ulong(v17), 4uLL, &v17, a1, v5);
    (*(void (__fastcall **)(__int64, __int64, void *))(a1 + 56))(a1, v5, v8);
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000027D39) ----------------------------------------------------
int __usercall cchmac_update@<eax>(signed __int64 a1@<rax>, unsigned __int64 a2@<rdx>, const void *a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>)
{
  return ccdigest_update(a1, a2, a3, a4, a5);
}

//----- (0000000000027D43) ----------------------------------------------------
unsigned __int64 __fastcall ccecies_encrypt_gcm_ciphertext_size(__int64 **a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r14@1
  unsigned __int64 v4; // rax@1
  unsigned __int64 v5; // rcx@1

  v3 = a3;
  v4 = ccecies_pub_key_size(a1, a2);
  v5 = 0LL;
  if ( v4 )
    v5 = *(_DWORD *)(a2 + 28) + v3 + v4;
  return v5;
}

//----- (0000000000027D70) ----------------------------------------------------
__int64 ccnistkdf_dpi_hmac()
{
  return 0LL;
}

//----- (0000000000027D78) ----------------------------------------------------
__int64 ccmd5_di()
{
  return (__int64)ccmd5_ltc_di;
}
// 69A50: using guessed type __int64 ccmd5_ltc_di[4];

//----- (0000000000027D85) ----------------------------------------------------
__int64 __fastcall md5_compress(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rax@2
  int v4; // er9@4
  int v5; // er10@4
  int v6; // ebx@4
  int v7; // er11@4
  int v8; // er11@4
  int v9; // ecx@4
  int v10; // ecx@4
  int v11; // ebx@4
  int v12; // ebx@4
  int v13; // er9@4
  int v14; // er9@4
  int v15; // er10@4
  int v16; // er10@4
  int v17; // ecx@4
  int v18; // ecx@4
  int v19; // er11@4
  int v20; // er11@4
  int v21; // er14@4
  int v22; // er14@4
  int v23; // eax@4
  int v24; // eax@4
  int v25; // er10@4
  int v26; // er10@4
  int v27; // er15@4
  int v28; // er15@4
  int v29; // ebx@4
  int v30; // ebx@4
  int v31; // eax@4
  int v32; // eax@4
  int v33; // er11@4
  int v34; // er11@4
  int v35; // er14@4
  int v36; // er14@4
  int v37; // ebx@4
  int v38; // ebx@4
  int v39; // eax@4
  int v40; // eax@4
  int v41; // er11@4
  int v42; // er11@4
  int v43; // er9@4
  int v44; // er9@4
  int v45; // ebx@4
  int v46; // ebx@4
  int v47; // eax@4
  int v48; // eax@4
  int v49; // er14@4
  int v50; // er14@4
  int v51; // er15@4
  int v52; // er15@4
  int v53; // er10@4
  int v54; // er10@4
  int v55; // er12@4
  int v56; // er12@4
  int v57; // er13@4
  int v58; // er13@4
  int v59; // er15@4
  int v60; // er15@4
  int v61; // ecx@4
  int v62; // ecx@4
  int v63; // ebx@4
  int v64; // ebx@4
  int v65; // er13@4
  int v66; // er13@4
  int v67; // er8@4
  int v68; // er8@4
  int v69; // eax@4
  int v70; // eax@4
  int v71; // er15@4
  int v72; // er15@4
  int v73; // er14@4
  int v74; // er14@4
  int v75; // ebx@4
  int v76; // ebx@4
  int v77; // er9@4
  int v78; // er9@4
  int v79; // ecx@4
  int v80; // ecx@4
  int v81; // er13@4
  int v82; // er13@4
  int v83; // er8@4
  int v84; // er8@4
  int v85; // er11@4
  int v86; // er11@4
  int v87; // er15@4
  int v88; // er15@4
  int v89; // ebx@4
  int v90; // ebx@4
  int v91; // er10@4
  int v92; // er10@4
  int v93; // er8@4
  int v94; // er8@4
  int v95; // ecx@4
  int v96; // ecx@4
  int v97; // er15@4
  int v98; // er15@4
  int v99; // ebx@4
  int v100; // ebx@4
  int v101; // er13@4
  int v102; // er13@4
  int v103; // er12@4
  int v104; // er12@4
  int v105; // er15@4
  int v106; // er15@4
  int v107; // er14@4
  int v108; // er14@4
  int v109; // ebx@4
  int v110; // ebx@4
  int v111; // er11@4
  int v112; // er11@4
  int v113; // er15@4
  int v114; // er15@4
  int v115; // er12@4
  int v116; // er12@4
  int v117; // er13@4
  int v118; // er13@4
  int v119; // er8@4
  int v120; // er8@4
  int v121; // eax@4
  int v122; // eax@4
  int v123; // er11@4
  int v124; // er11@4
  int v125; // er9@4
  int v126; // er9@4
  int v127; // er8@4
  int v128; // er8@4
  int v129; // eax@4
  int v130; // eax@4
  int v131; // ebx@4
  int v132; // ebx@4
  int v133; // ecx@4
  int v135; // [sp+0h] [bp-70h]@3
  int v136; // [sp+4h] [bp-6Ch]@4
  int v137; // [sp+8h] [bp-68h]@4
  int v138; // [sp+Ch] [bp-64h]@4
  int v139; // [sp+10h] [bp-60h]@4
  int v140; // [sp+14h] [bp-5Ch]@4
  int v141; // [sp+18h] [bp-58h]@4
  int v142; // [sp+1Ch] [bp-54h]@4
  int v143; // [sp+20h] [bp-50h]@4
  int v144; // [sp+24h] [bp-4Ch]@4
  int v145; // [sp+28h] [bp-48h]@4
  int v146; // [sp+2Ch] [bp-44h]@4
  int v147; // [sp+30h] [bp-40h]@4
  int v148; // [sp+34h] [bp-3Ch]@4
  int v149; // [sp+38h] [bp-38h]@4
  int v150; // [sp+3Ch] [bp-34h]@4
  __int64 i; // [sp+40h] [bp-30h]@1

  for ( i = *(_QWORD *)off_69010[0]; a2; --a2 )
  {
    v3 = 0LL;
    do
    {
      *(&v135 + v3) = (*(_BYTE *)(a3 + 4 * v3 + 3) << 24) | (*(_BYTE *)(a3 + 4 * v3 + 2) << 16) | (*(_BYTE *)(a3 + 4 * v3 + 1) << 8) | *(_BYTE *)(a3 + 4 * v3);
      ++v3;
    }
    while ( v3 != 16 );
    v4 = *(_DWORD *)(a1 + 4);
    v5 = *(_DWORD *)(a1 + 8);
    v6 = *(_DWORD *)(a1 + 12);
    v7 = __ROL4__((v6 ^ v4 & (v5 ^ *(_DWORD *)(a1 + 12))) + v135 + *(_DWORD *)a1 - 680876936, 7);
    v8 = v4 + v7;
    v9 = __ROL4__((v5 ^ v8 & (v4 ^ *(_DWORD *)(a1 + 8))) + v136 + v6 - 389564586, 12);
    v10 = v8 + v9;
    v11 = __ROL4__((v4 ^ v10 & (v4 ^ v8)) + v137 + v5 + 606105819, 17);
    v12 = v10 + v11;
    v13 = __ROL4__((v8 ^ v12 & (v8 ^ v10)) + v138 + v4 - 1044525330, 22);
    v14 = v12 + v13;
    v15 = __ROL4__((v10 ^ v14 & (v10 ^ v12)) + v139 + v8 - 176418897, 7);
    v16 = v14 + v15;
    v17 = __ROL4__((v12 ^ v16 & (v12 ^ v14)) + v140 + v10 + 1200080426, 12);
    v18 = v16 + v17;
    v19 = __ROL4__((v14 ^ v18 & (v14 ^ v16)) + v141 + v12 - 1473231341, 17);
    v20 = v18 + v19;
    v21 = __ROL4__((v16 ^ v20 & (v16 ^ v18)) + v142 + v14 - 45705983, 22);
    v22 = v20 + v21;
    v23 = __ROL4__((v18 ^ v22 & (v18 ^ v20)) + v143 + v16 + 1770035416, 7);
    v24 = v22 + v23;
    v25 = __ROL4__((v20 ^ v24 & (v20 ^ v22)) + v144 + v18 - 1958414417, 12);
    v26 = v24 + v25;
    v27 = __ROL4__((v22 ^ v26 & (v22 ^ v24)) + v145 + v20 - 42063, 17);
    v28 = v26 + v27;
    v29 = __ROL4__((v24 ^ v28 & (v24 ^ v26)) + v146 + v22 - 1990404162, 22);
    v30 = v28 + v29;
    v31 = __ROL4__((v26 ^ v30 & (v26 ^ v28)) + v147 + v24 + 1804603682, 7);
    v32 = v30 + v31;
    v33 = __ROL4__((v28 ^ v32 & (v28 ^ v30)) + v148 + v26 - 40341101, 12);
    v34 = v32 + v33;
    v35 = __ROL4__((v30 ^ v34 & (v30 ^ v32)) + v149 + v28 - 1502002290, 17);
    v36 = v34 + v35;
    v37 = __ROL4__((v32 ^ v36 & (v32 ^ v34)) + v150 + v30 + 1236535329, 22);
    v38 = v36 + v37;
    v39 = __ROL4__((v36 ^ v34 & (v36 ^ v38)) + v136 + v32 - 165796510, 5);
    v40 = v38 + v39;
    v41 = __ROL4__((v38 ^ v36 & (v38 ^ v40)) + v141 + v34 - 1069501632, 9);
    v42 = v40 + v41;
    v43 = __ROL4__((v40 ^ v38 & (v40 ^ v42)) + v146 + v36 + 643717713, 14);
    v44 = v42 + v43;
    v45 = __ROL4__((v42 ^ v40 & (v42 ^ v44)) + v135 + v38 - 373897302, 20);
    v46 = v44 + v45;
    v47 = __ROL4__((v44 ^ v42 & (v44 ^ v46)) + v140 + v40 - 701558691, 5);
    v48 = v46 + v47;
    v49 = __ROL4__((v46 ^ v44 & (v46 ^ v48)) + v145 + v42 + 38016083, 9);
    v50 = v48 + v49;
    v51 = __ROL4__((v48 ^ v46 & (v48 ^ v50)) + v150 + v44 - 660478335, 14);
    v52 = v50 + v51;
    v53 = __ROL4__((v50 ^ v48 & (v50 ^ v52)) + v139 + v46 - 405537848, 20);
    v54 = v52 + v53;
    v55 = __ROL4__((v52 ^ v50 & (v52 ^ v54)) + v144 + v48 + 568446438, 5);
    v56 = v54 + v55;
    v57 = __ROL4__((v54 ^ v52 & (v54 ^ v56)) + v149 + v50 - 1019803690, 9);
    v58 = v56 + v57;
    v59 = __ROL4__((v56 ^ v54 & (v56 ^ v58)) + v138 + v52 - 187363961, 14);
    v60 = v58 + v59;
    v61 = __ROL4__((v58 ^ v56 & (v58 ^ v60)) + v143 + v54 + 1163531501, 20);
    v62 = v60 + v61;
    v63 = __ROL4__((v60 ^ v58 & (v60 ^ v62)) + v148 + v56 - 1444681467, 5);
    v64 = v62 + v63;
    v65 = __ROL4__((v62 ^ v60 & (v62 ^ v64)) + v137 + v58 - 51403784, 9);
    v66 = v64 + v65;
    v67 = __ROL4__((v64 ^ v62 & (v64 ^ v66)) + v142 + v60 + 1735328473, 14);
    v68 = v66 + v67;
    v69 = __ROL4__((v66 ^ v64 & (v66 ^ v68)) + v147 + v62 - 1926607734, 20);
    v70 = v68 + v69;
    v71 = __ROL4__((v70 ^ v66 ^ v68) + v140 + v64 - 378558, 4);
    v72 = v70 + v71;
    v73 = __ROL4__((v72 ^ v68 ^ v70) + v143 + v66 - 2022574463, 11);
    v74 = v72 + v73;
    v75 = __ROL4__((v74 ^ v70 ^ v72) + v146 + v68 + 1839030562, 16);
    v76 = v74 + v75;
    v77 = __ROL4__((v76 ^ v72 ^ v74) + v149 + v70 - 35309556, 23);
    v78 = v76 + v77;
    v79 = __ROL4__((v78 ^ v74 ^ v76) + v136 + v72 - 1530992060, 4);
    v80 = v78 + v79;
    v81 = __ROL4__((v80 ^ v76 ^ v78) + v139 + v74 + 1272893353, 11);
    v82 = v80 + v81;
    v83 = __ROL4__((v82 ^ v78 ^ v80) + v142 + v76 - 155497632, 16);
    v84 = v82 + v83;
    v85 = __ROL4__((v84 ^ v80 ^ v82) + v145 + v78 - 1094730640, 23);
    v86 = v84 + v85;
    v87 = __ROL4__((v86 ^ v82 ^ v84) + v148 + v80 + 681279174, 4);
    v88 = v86 + v87;
    v89 = __ROL4__((v88 ^ v84 ^ v86) + v135 + v82 - 358537222, 11);
    v90 = v88 + v89;
    v91 = __ROL4__((v90 ^ v86 ^ v88) + v138 + v84 - 722521979, 16);
    v92 = v90 + v91;
    v93 = __ROL4__((v92 ^ v88 ^ v90) + v141 + v86 + 76029189, 23);
    v94 = v92 + v93;
    v95 = __ROL4__((v94 ^ v90 ^ v92) + v144 + v88 - 640364487, 4);
    v96 = v94 + v95;
    v97 = __ROL4__((v96 ^ v92 ^ v94) + v147 + v90 - 421815835, 11);
    v98 = v96 + v97;
    v99 = __ROL4__((v98 ^ v94 ^ v96) + v150 + v92 + 530742520, 16);
    v100 = v98 + v99;
    v101 = __ROL4__((v100 ^ v96 ^ v98) + v137 + v94 - 995338651, 23);
    v102 = v100 + v101;
    v103 = __ROL4__((v100 ^ (v102 | ~v98)) + v135 + v96 - 198630844, 6);
    v104 = v102 + v103;
    v105 = __ROL4__((v102 ^ (v104 | ~v100)) + v142 + v98 + 1126891415, 10);
    v106 = v104 + v105;
    v107 = __ROL4__((v104 ^ (v106 | ~v102)) + v149 + v100 - 1416354905, 15);
    v108 = v106 + v107;
    v109 = __ROL4__((v106 ^ (v108 | ~v104)) + v140 + v102 - 57434055, 21);
    v110 = v108 + v109;
    v111 = __ROL4__((v108 ^ (v110 | ~v106)) + v147 + v104 + 1700485571, 6);
    v112 = v110 + v111;
    v113 = __ROL4__((v110 ^ (v112 | ~v108)) + v138 + v106 - 1894986606, 10);
    v114 = v112 + v113;
    v115 = __ROL4__((v112 ^ (v114 | ~v110)) + v145 + v108 - 1051523, 15);
    v116 = v114 + v115;
    v117 = __ROL4__((v114 ^ (v116 | ~v112)) + v136 + v110 - 2054922799, 21);
    v118 = v116 + v117;
    v119 = __ROL4__((v116 ^ (v118 | ~v114)) + v143 + v112 + 1873313359, 6);
    v120 = v118 + v119;
    v121 = __ROL4__((v118 ^ (v120 | ~v116)) + v150 + v114 - 30611744, 10);
    v122 = v120 + v121;
    v123 = __ROL4__((v120 ^ (v122 | ~v118)) + v141 + v116 - 1560198380, 15);
    v124 = v122 + v123;
    v125 = __ROL4__((v122 ^ (v124 | ~v120)) + v148 + v118 + 1309151649, 21);
    v126 = v124 + v125;
    v127 = __ROL4__((v124 ^ (v126 | ~v122)) + v139 + v120 - 145523070, 6);
    v128 = v126 + v127;
    v129 = __ROL4__((v126 ^ (v128 | ~v124)) + v146 + v122 - 1120210379, 10);
    v130 = v128 + v129;
    v131 = __ROL4__((v128 ^ (v130 | ~v126)) + v137 + v124 + 718787259, 15);
    v132 = v130 + v131;
    v133 = __ROL4__((v130 ^ (v132 | ~v128)) + v144 + v126 - 343485551, 21);
    *(_DWORD *)a1 += v128;
    *(_DWORD *)(a1 + 4) += v132 + v133;
    *(_DWORD *)(a1 + 8) += v132;
    *(_DWORD *)(a1 + 12) += v130;
    a3 += 64LL;
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000028547) ----------------------------------------------------
__int64 __fastcall ccmode_cbc_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  __int64 v6; // r12@1
  __int64 v7; // rcx@1
  __int64 v8; // r15@2
  signed __int64 v9; // rdi@2
  char *v10; // r13@2
  unsigned __int64 v11; // rax@4
  unsigned __int64 v12; // rcx@4
  char v13; // cl@5
  __int64 v15; // [sp+0h] [bp-50h]@2
  __int64 *v16; // [sp+8h] [bp-48h]@2
  __int64 v17; // [sp+10h] [bp-40h]@2
  __int64 v18; // [sp+18h] [bp-38h]@2
  __int64 v19; // [sp+20h] [bp-30h]@1

  v5 = a5;
  v6 = a4;
  v7 = off_69010[0];
  v19 = *(_QWORD *)off_69010[0];
  if ( a3 )
  {
    v18 = a3;
    v16 = &v15;
    v8 = *(_QWORD *)a1;
    v9 = a1 + 8;
    v17 = v9;
    v10 = (char *)&v15 - ((((*(_QWORD *)(v8 + 8) + 7LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL);
    while ( 1 )
    {
      (*(void (__fastcall **)(signed __int64, signed __int64, __int64, char *))(v8 + 24))(v9, 1LL, v6, v10);
      v11 = 0LL;
      v12 = 0LL;
      if ( *(_QWORD *)(v8 + 8) )
      {
        do
        {
          v13 = *(_BYTE *)(a2 + v11) ^ v10[v11];
          *(_BYTE *)(a2 + v11) = *(_BYTE *)(v6 + v11);
          *(_BYTE *)(v5 + v11++) = v13;
          v12 = *(_QWORD *)(v8 + 8);
        }
        while ( v11 < v12 );
      }
      if ( v18 == 1 )
        break;
      --v18;
      v5 += v12;
      v6 += v12;
      v9 = v17;
    }
    v7 = off_69010[0];
  }
  return *(_QWORD *)v7;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000028628) ----------------------------------------------------
unsigned __int64 __fastcall ccder_decode_rsa_priv_n(unsigned __int64 a1, unsigned __int64 a2)
{
  __int64 v2; // rbx@1
  signed __int64 v3; // rdx@1
  unsigned __int64 result; // rax@1
  __int64 v5; // rdx@2
  __int64 v6; // rcx@4
  __int64 v7; // rcx@6
  unsigned __int64 v8; // [sp+8h] [bp-428h]@1
  char v9; // [sp+10h] [bp-420h]@4
  __int64 v10; // [sp+418h] [bp-18h]@1
  __int64 v11; // [sp+420h] [bp-10h]@1

  v2 = off_69010[0];
  v11 = *(_QWORD *)off_69010[0];
  v8 = a2;
  v10 = 0LL;
  v3 = ccder_decode_constructed_tl(a2, a1, a2, 2305843009213693968LL, (__int64)&v8);
  result = 0LL;
  if ( v3 )
  {
    v5 = ccder_decode_uint(0LL, v3, v8, 1uLL, (__int64)&v10);
    result = 0LL;
    if ( v5 )
    {
      if ( !v10 )
      {
        v6 = ccder_decode_uint(0LL, v5, v8, 0x80uLL, (__int64)&v9);
        result = 0LL;
        if ( v6 )
          result = (unsigned __int64)(ccn_bitlen(128LL, (__int64)&v9) + 63) >> 6;
      }
    }
  }
  v7 = *(_QWORD *)v2;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000286F8) ----------------------------------------------------
__int64 __fastcall ccrsa_priv_crypt(__int64 *a1, __int64 a2, const void *a3)
{
  const void *v3; // r14@1
  signed __int64 *v4; // rbx@1
  signed __int64 v5; // r12@1
  signed __int64 v6; // rax@1

  v3 = a3;
  v4 = a1;
  v5 = ccn_bitlen(*a1, (__int64)(a1 + 2));
  v6 = ccn_bitlen(v4[2 * *v4 + 3], (__int64)&v4[2 * *v4 + 5]);
  cczp_crt_power(
    (unsigned __int64)(v5 + v6 + 63) >> 6,
    a2,
    v3,
    v4,
    (__int64)&v4[4 * *v4 + 6],
    (unsigned __int64 *)&v4[6 * *v4 + 6],
    (unsigned __int64)&v4[2 * *v4 + 3],
    (__int64)&v4[5 * *v4 + 6]);
  return 0LL;
}

//----- (0000000000028795) ----------------------------------------------------
__int64 __fastcall cczp_crt_power(__int64 a1, __int64 a2, const void *a3, signed __int64 *a4, __int64 a5, unsigned __int64 *a6, unsigned __int64 a7, __int64 a8)
{
  unsigned __int64 v8; // r12@1
  char *v9; // r15@1
  char *v10; // rbx@1
  __int64 *v11; // r14@1
  __int64 v12; // r13@1
  size_t v13; // r12@1
  __int64 v14; // rdi@1
  void *v15; // r13@1
  __int64 v16; // rbx@2
  signed __int64 *v17; // r15@3
  __int64 v18; // r15@5
  __int64 v19; // r12@5
  __int64 v20; // rax@5
  const void *v22; // [sp+0h] [bp-80h]@1
  __int64 v23; // [sp+8h] [bp-78h]@1
  unsigned __int64 *v24; // [sp+10h] [bp-70h]@1
  signed __int64 *v25; // [sp+18h] [bp-68h]@1
  size_t v26; // [sp+20h] [bp-60h]@1
  __int64 v27; // [sp+28h] [bp-58h]@1
  __int64 v28; // [sp+30h] [bp-50h]@1
  void *v29; // [sp+38h] [bp-48h]@1
  __int64 v30; // [sp+40h] [bp-40h]@1
  __int64 v31; // [sp+48h] [bp-38h]@1
  __int64 v32; // [sp+50h] [bp-30h]@1

  v24 = a6;
  v23 = a5;
  v8 = (unsigned __int64)a4;
  v25 = a4;
  v22 = a3;
  v27 = a2;
  v28 = a1;
  v32 = *(_QWORD *)off_69010[0];
  v31 = *a4;
  v9 = (char *)&v22 - ((8 * v31 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v30 = *(_QWORD *)a7;
  v10 = (char *)&v22 - ((8 * v30 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v29 = (char *)&v22 - ((8 * v30 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v26 = 16 * v31;
  v11 = (__int64 *)(&v22 - 2 * v31);
  bzero(&v11[a1], 8 * (2 * v31 - a1));
  ccn_set(a1, v11, v22);
  cczp_modn(v8, v9, a1, (__int64)v11);
  cczp_power((__int64 *)v8, (__int64)v9, (__int64)v9, v23);
  v12 = v31;
  cczp_modn(a7, v10, a1, (__int64)v11);
  cczp_power((__int64 *)a7, (__int64)v10, (__int64)v10, a8);
  v23 = (__int64)&v11[v30];
  v13 = 8 * (v12 - v30);
  bzero(&v11[v30], 8 * (v12 - v30));
  ccn_set(v30, v11, v10);
  v14 = v12;
  v15 = v9;
  if ( (signed int)ccn_cmp(v14, (__int64)v9, (__int64)v11) < 0 )
  {
    v16 = v27;
    ccn_sub(v31, v27, (__int64)v11, (__int64)v9);
    v17 = v25;
    cczp_mul(v25, v16, v16, v24);
    if ( ccn_n(v31, v16) )
      ccn_sub(v31, v16, (__int64)(v17 + 2), v16);
  }
  else
  {
    v16 = v27;
    ccn_sub(v31, v27, (__int64)v9, (__int64)v11);
    cczp_mul(v25, v16, v16, v24);
  }
  v18 = v30;
  bzero((char *)v15 + 8 * v30, v13);
  ccn_set(v18, v15, (const void *)(a7 + 16));
  v19 = v31;
  ccn_mul(v31, (__int64)v11, v16, (unsigned __int64 *)v15);
  v20 = ccn_add(v18, v16, (__int64)v11, (__int64)v29);
  ccn_add1(v28 - v18, v16 + 8 * v18, v23, v20);
  cc_clear(8 * v19, v15);
  cc_clear(8 * v18, v29);
  cc_clear(v26, v11);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000028A09) ----------------------------------------------------
int __usercall ccmode_cbc_encrypt@<eax>(int result@<eax>, __int64 a2@<rdx>, __int64 a3@<rcx>, __int64 a4@<rdi>, void *a5@<rsi>, char *a6@<r8>)
{
  __int64 v6; // r13@1
  __int64 v7; // r15@1
  __int64 v8; // r14@2
  size_t v9; // rdx@2
  void *i; // rax@2
  const void *j; // rbx@4

  v6 = a3;
  v7 = a2;
  if ( a2 )
  {
    v8 = *(_QWORD *)a4;
    v9 = *(_QWORD *)(*(_QWORD *)a4 + 8LL);
    for ( i = a5; ; i = (void *)j )
    {
      for ( j = a6; v9; --v9 )
        a6[v9 - 1] = *(_BYTE *)(v6 + v9 - 1) ^ *((char *)i + v9 - 1);
      (*(void (__fastcall **)(signed __int64, signed __int64, char *, char *))(v8 + 24))(a4 + 8, 1LL, a6, a6);
      --v7;
      v9 = *(_QWORD *)(v8 + 8);
      if ( !v7 )
        break;
      v6 += v9;
      a6 = (char *)j + v9;
    }
    result = (unsigned __int64)memcpy(a5, j, v9);
  }
  return result;
}

//----- (0000000000028AA2) ----------------------------------------------------
int __fastcall ccmode_cbc_init(__int64 a1, __int64 a2)
{
  __int64 v2; // rdi@1

  v2 = *(_QWORD *)(a1 + 32);
  *(_QWORD *)a2 = v2;
  return (*(int (__fastcall **)(__int64, __int64))(v2 + 16))(v2, a2 + 8);
}

//----- (0000000000028AB8) ----------------------------------------------------
int __fastcall ccmode_cfb8_decrypt(__int64 a1, __int64 a2, char *a3, __int64 a4)
{
  char *v4; // r15@1
  __int64 v5; // r12@2
  __int64 v6; // rdx@2
  unsigned __int64 v7; // rax@2
  void *v8; // rbx@2
  signed __int64 v9; // r13@2
  char v10; // al@4
  int result; // eax@4
  const void *v12; // [sp+0h] [bp-40h]@2
  signed __int64 v13; // [sp+8h] [bp-38h]@2
  __int64 v14; // [sp+10h] [bp-30h]@1

  v14 = a4;
  v4 = a3;
  if ( a2 )
  {
    v5 = *(_QWORD *)a1;
    v6 = *(_QWORD *)(*(_QWORD *)a1 + 8LL);
    v7 = (unsigned __int64)(v6 + 7) >> 3;
    v8 = (void *)(a1 + 8 * v7 + 8);
    v13 = a1 + 16 * v7 + 8;
    v12 = (const void *)(a1 + 8 * v7 + 9);
    v9 = 1 - a2;
    memmove(v8, v12, v6 - 1);
    while ( 1 )
    {
      v10 = *v4;
      *((char *)v8 + *(_QWORD *)(v5 + 8) - 1) = *v4;
      *(_BYTE *)v14 = *(_BYTE *)(a1 + 8) ^ v10;
      result = (*(int (__fastcall **)(signed __int64, signed __int64, void *))(v5 + 24))(v13, 1LL, v8);
      if ( !v9 )
        break;
      ++v4;
      ++v14;
      ++v9;
      memmove(v8, v12, *(_QWORD *)(v5 + 8) - 1LL);
    }
  }
  return result;
}

//----- (0000000000028B6C) ----------------------------------------------------
int __fastcall ccmode_cfb8_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  __int64 v5; // r12@2
  __int64 v6; // rdx@2
  unsigned __int64 v7; // rax@2
  void *v8; // rbx@2
  signed __int64 v9; // r13@2
  char v10; // al@4
  int result; // eax@4
  const void *v12; // [sp+0h] [bp-40h]@2
  signed __int64 v13; // [sp+8h] [bp-38h]@2
  __int64 v14; // [sp+10h] [bp-30h]@1

  v14 = a4;
  v4 = a3;
  if ( a2 )
  {
    v5 = *(_QWORD *)a1;
    v6 = *(_QWORD *)(*(_QWORD *)a1 + 8LL);
    v7 = (unsigned __int64)(v6 + 7) >> 3;
    v8 = (void *)(a1 + 8 * v7 + 8);
    v13 = a1 + 16 * v7 + 8;
    v12 = (const void *)(a1 + 8 * v7 + 9);
    v9 = 1 - a2;
    memmove(v8, v12, v6 - 1);
    while ( 1 )
    {
      v10 = *(_BYTE *)(a1 + 8) ^ *(_BYTE *)v4;
      *(_BYTE *)v14 = v10;
      *((char *)v8 + *(_QWORD *)(v5 + 8) - 1) = v10;
      result = (*(int (__fastcall **)(signed __int64, signed __int64, void *))(v5 + 24))(v13, 1LL, v8);
      if ( !v9 )
        break;
      ++v14;
      ++v4;
      ++v9;
      memmove(v8, v12, *(_QWORD *)(v5 + 8) - 1LL);
    }
  }
  return result;
}

//----- (0000000000028C20) ----------------------------------------------------
__int64 __usercall ccmode_gcm_mult_h@<rax>(__int64 a1@<rdi>, __m128i *a2@<rsi>, __m128i a3@<xmm0>, __m128i a4@<xmm1>, __m128i a5@<xmm2>, __m128i a6@<xmm3>)
{
  __int64 v6; // r12@1
  __int64 v7; // rax@1
  __int64 v8; // rax@2
  char v10; // [sp+0h] [bp-40h]@4
  __int64 v11; // [sp+18h] [bp-28h]@1

  v6 = off_69010[0];
  v11 = *(_QWORD *)off_69010[0];
  LODWORD(v7) = cpuid_features();
  if ( _bittest((const unsigned __int64 *)&v7, 0x39u)
    && (LODWORD(v8) = cpuid_features(), _bittest((const unsigned __int64 *)&v8, 0x29u)) )
  {
    ccmode_gcm_gf_mult(a2, (const __m128i *)(a1 + 128), a2, a3, a4, a5, a6);
  }
  else
  {
    ccmode_gcm_gf_mult((__m128i *)&v10, (const __m128i *)a1, a2, a3, a4, a5, a6);
    memcpy(a2, &v10, 0x10uLL);
  }
  return *(_QWORD *)v6;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000028CAE) ----------------------------------------------------
int __fastcall ccmode_cfb8_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, const void *a5)
{
  __int64 v5; // rbx@1
  signed __int64 v6; // r15@1
  size_t v7; // rdx@1
  unsigned __int64 v8; // rax@1
  signed __int64 v9; // r14@1
  void *v10; // rdi@1
  __int64 v12; // [sp+8h] [bp-38h]@1
  __int64 v13; // [sp+10h] [bp-30h]@1

  v13 = a4;
  v12 = a3;
  v5 = *(_QWORD *)(a1 + 32);
  *(_QWORD *)a2 = v5;
  v6 = a2 + 8;
  v7 = *(_QWORD *)(v5 + 8);
  v8 = (v7 + 7) & 0xFFFFFFFFFFFFFFF8LL;
  v9 = a2 + v8 + 8;
  v10 = (void *)(a2 + v8 + 8);
  if ( a5 )
    memcpy(v10, a5, v7);
  else
    bzero(v10, *(_QWORD *)(v5 + 8));
  (*(void (__fastcall **)(__int64, signed __int64, __int64, __int64))(v5 + 16))(
    v5,
    v6 + 2 * ((*(_QWORD *)(*(_QWORD *)a2 + 8LL) + 7LL) & 0x7FFFFFFFFFFFFFF8LL),
    v12,
    v13);
  return (*(int (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(v5 + 24))(
           v6 + 2 * ((*(_QWORD *)(*(_QWORD *)a2 + 8LL) + 7LL) & 0x7FFFFFFFFFFFFFF8LL),
           1LL,
           v9,
           v6);
}

//----- (0000000000028D5E) ----------------------------------------------------
__int64 __fastcall ccmode_cfb_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r11@1
  __int64 v5; // rbx@1
  __int64 result; // rax@1
  __int64 v7; // r8@2
  __int64 v8; // r10@2
  unsigned __int64 v9; // r9@2
  signed __int64 v10; // rcx@2
  signed __int64 i; // r14@2
  __int64 v12; // r13@5
  signed __int64 v13; // r15@5
  __int64 v14; // r12@5
  signed __int64 v15; // [sp+0h] [bp-40h]@2
  signed __int64 v16; // [sp+8h] [bp-38h]@2
  __int64 v17; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v17 = a1;
  result = *(_QWORD *)(a1 + 8);
  if ( a2 )
  {
    v7 = *(_QWORD *)a1;
    v8 = *(_QWORD *)(*(_QWORD *)a1 + 8LL);
    v9 = (unsigned __int64)(v8 + 7) >> 3;
    v16 = a1 + 16;
    v10 = a1 + 8 * v9 + 16;
    v15 = a1 + 16 * v9 + 16;
    for ( i = 1 - a2; ; ++i )
    {
      if ( result == v8 )
      {
        v12 = v7;
        v13 = v10;
        v14 = v4;
        (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64))(v7 + 24))(v15, 1LL, v16);
        v4 = v14;
        v10 = v13;
        v7 = v12;
        a1 = v17;
        result = 0LL;
      }
      *(_BYTE *)(a1 + result + 16) = *(_BYTE *)v5;
      *(_BYTE *)v4 = *(_BYTE *)v5 ^ *(_BYTE *)(v10 + result++);
      if ( !i )
        break;
      ++v5;
      ++v4;
      v8 = *(_QWORD *)(v7 + 8);
    }
  }
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

//----- (0000000000028E1E) ----------------------------------------------------
__int64 __fastcall ccmode_cfb_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // rbx@1
  __int64 v5; // r11@1
  __int64 result; // rax@1
  __int64 v7; // r8@2
  __int64 v8; // r10@2
  unsigned __int64 v9; // r9@2
  signed __int64 v10; // rcx@2
  signed __int64 i; // r14@2
  __int64 v12; // r13@5
  signed __int64 v13; // r15@5
  __int64 v14; // r12@5
  char v15; // dl@6
  signed __int64 v16; // [sp+0h] [bp-40h]@2
  signed __int64 v17; // [sp+8h] [bp-38h]@2
  __int64 v18; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v18 = a1;
  result = *(_QWORD *)(a1 + 8);
  if ( a2 )
  {
    v7 = *(_QWORD *)a1;
    v8 = *(_QWORD *)(*(_QWORD *)a1 + 8LL);
    v9 = (unsigned __int64)(v8 + 7) >> 3;
    v17 = a1 + 16;
    v10 = a1 + 8 * v9 + 16;
    v16 = a1 + 16 * v9 + 16;
    for ( i = 1 - a2; ; ++i )
    {
      if ( result == v8 )
      {
        v12 = v7;
        v13 = v10;
        v14 = v5;
        (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64))(v7 + 24))(v16, 1LL, v17);
        v5 = v14;
        v10 = v13;
        v7 = v12;
        a1 = v18;
        result = 0LL;
      }
      v15 = *(_BYTE *)v5 ^ *(_BYTE *)(v10 + result);
      *(_BYTE *)v4 = v15;
      *(_BYTE *)(a1 + result++ + 16) = v15;
      if ( !i )
        break;
      ++v4;
      ++v5;
      v8 = *(_QWORD *)(v7 + 8);
    }
  }
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

//----- (0000000000028EDC) ----------------------------------------------------
int __fastcall ccmode_cfb_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, const void *a5)
{
  const void *v5; // r15@1
  __int64 v6; // r12@1
  size_t v7; // rdx@1
  unsigned __int64 v8; // rax@1
  signed __int64 v9; // r13@1
  void *v10; // rdi@1
  signed __int64 v12; // [sp+0h] [bp-40h]@2
  __int64 v13; // [sp+8h] [bp-38h]@1
  __int64 v14; // [sp+10h] [bp-30h]@1

  v5 = a5;
  v14 = a4;
  v13 = a3;
  v6 = *(_QWORD *)(a1 + 32);
  *(_QWORD *)a2 = v6;
  v7 = *(_QWORD *)(v6 + 8);
  v8 = (v7 + 7) & 0xFFFFFFFFFFFFFFF8LL;
  v9 = a2 + 16;
  v10 = (void *)(a2 + v8 + 16);
  if ( a5 )
  {
    v12 = a2 + v8 + 16;
    memcpy(v10, a5, v7);
  }
  else
  {
    v12 = a2 + v8 + 16;
    bzero(v10, v7);
  }
  (*(void (__fastcall **)(__int64, signed __int64, __int64, __int64))(v6 + 16))(
    v6,
    v9 + 2 * ((*(_QWORD *)(*(_QWORD *)a2 + 8LL) + 7LL) & 0x7FFFFFFFFFFFFFF8LL),
    v13,
    v14);
  *(_QWORD *)(a2 + 8) = 0LL;
  return (*(int (__fastcall **)(signed __int64, signed __int64, const void *, signed __int64))(v6 + 24))(
           v9 + 2 * ((*(_QWORD *)(*(_QWORD *)a2 + 8LL) + 7LL) & 0x7FFFFFFFFFFFFFF8LL),
           1LL,
           v5,
           v12);
}

//----- (0000000000028FA3) ----------------------------------------------------
signed __int64 __fastcall cczp_sqrt(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r13@1
  char *v4; // rbx@1
  __int64 v5; // r15@2
  __int64 v6; // r14@2
  signed __int64 result; // rax@2
  __int64 v8; // [sp+0h] [bp-40h]@1
  __int64 v9; // [sp+8h] [bp-38h]@1
  __int64 v10; // [sp+10h] [bp-30h]@1

  v9 = a3;
  v8 = a2;
  v10 = *(_QWORD *)off_69010[0];
  v3 = *(_QWORD *)a1;
  v4 = (char *)&v8 - ((8LL * *(_QWORD *)a1 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  ccn_set(*(_QWORD *)a1, v4, (const void *)(a1 + 16));
  if ( (*(_QWORD *)v4 & 3LL) == 3 )
  {
    ccn_add1(v3, (__int64)v4, (__int64)v4, 1LL);
    ccn_shift_right(v3, v4, v4, 2LL);
    v5 = v8;
    v6 = v9;
    cczp_power((__int64 *)a1, v8, v9, (__int64)v4);
    cczp_mul((signed __int64 *)a1, (__int64)v4, v5, (unsigned __int64 *)v5);
    result = (unsigned int)-((unsigned int)ccn_cmp(v3, (__int64)v4, v6) != 0);
  }
  else
  {
    result = *(_QWORD *)off_69010[0];
    if ( *(_QWORD *)off_69010[0] == v10 )
      result = 0xFFFFFFFFLL;
  }
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000290A7) ----------------------------------------------------
signed __int64 __usercall ccmode_ctr_crypt@<rax>(__int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, unsigned __int64 a4@<rsi>, __m128i a5@<xmm6>, __m128i a6@<xmm7>, __m128i a7@<xmm8>, __m128i a8@<xmm9>, __m128i a9@<xmm10>, __m128i a10@<xmm11>, __m128i a11@<xmm12>, __m128i a12@<xmm13>, __m128i a13@<xmm14>, __m128i a14@<xmm15>)
{
  __int64 v14; // r15@1
  __int64 v15; // r12@1
  signed __int64 v16; // r8@1
  __int64 v17; // r13@2
  __int64 v18; // rcx@2
  unsigned __int64 v19; // rdx@2
  unsigned __int64 v20; // r14@2
  unsigned __int64 v21; // rbx@5
  __int64 v22; // rax@5
  __int64 v23; // rax@6
  __m128i v24; // xmm4@6
  __m128i v25; // xmm5@6
  __int64 v26; // rax@9
  __int64 v27; // rax@10
  char v28; // cl@12
  signed __int64 result; // rax@13
  __int64 v30; // rcx@14
  signed __int64 v31; // rbx@16
  __int64 v32; // [sp+10h] [bp-40h]@2
  __int64 v33; // [sp+18h] [bp-38h]@2
  signed __int64 v34; // [sp+20h] [bp-30h]@2

  v14 = a2;
  v15 = a1;
  v16 = *(_QWORD *)(a3 + 8);
  if ( !a4 )
  {
LABEL_19:
    result = v16;
    goto LABEL_21;
  }
  v17 = *(_QWORD *)a3;
  v18 = *(_QWORD *)(*(_QWORD *)a3 + 8LL);
  v19 = (unsigned __int64)(v18 + 7) >> 3;
  v34 = a3 + 16;
  v33 = a3 + 8 * v19 + 16;
  v32 = a3 + 16 * v19 + 16;
  v20 = ((v18 + 7) & 0xFFFFFFFFFFFFFFF8LL) + a3 + 15;
  while ( 2 )
  {
    if ( v16 != v18 )
      goto LABEL_24;
    v21 = a4;
    LODWORD(v22) = cpuid_features();
    if ( _bittest((const unsigned __int64 *)&v22, 0x39u) )
    {
      LODWORD(v23) = cpuid_features();
      if ( _bittest((const unsigned __int64 *)&v23, 0x29u) )
      {
        if ( a4 >= 0x10 && *(_QWORD *)(v17 + 8) == 16LL )
        {
          ctr_crypt(a4 & 0xFFFFFFF0, v33, v15, v14, v32, v24, v25, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14);
          v26 = (unsigned int)a4 & 0xFFFFFFF0;
          v14 += v26;
          v15 += v26;
          v21 = a4 - v26;
        }
      }
    }
    (*(void (__fastcall **)(__int64, signed __int64, __int64, signed __int64))(v17 + 24))(v32, 1LL, v33, v34);
    v27 = *(_QWORD *)(v17 + 8);
    do
    {
      if ( !v27 )
        break;
      v28 = *(_BYTE *)(v20 + v27) + 1;
      *(_BYTE *)(v20 + v27--) = v28;
    }
    while ( !v28 );
    a4 = v21;
    v16 = 0LL;
    result = 0LL;
    if ( v21 )
    {
LABEL_24:
      v30 = 0LL;
      do
      {
        *(_BYTE *)(v14 + v30) = *(_BYTE *)(v15 + v30) ^ *(_BYTE *)(v34 + v16 + v30);
        if ( a4 - 1 == v30 )
        {
          result = v16 + v30 + 1;
          goto LABEL_21;
        }
        v31 = v16 + v30++ + 1;
      }
      while ( (unsigned __int64)v31 < *(_QWORD *)(v17 + 8) );
      a4 -= v30;
      if ( a4 )
      {
        v16 += v30;
        v15 += v30;
        v14 += v30;
        v18 = *(_QWORD *)(v17 + 8);
        continue;
      }
      v16 += v30;
      goto LABEL_19;
    }
    break;
  }
LABEL_21:
  *(_QWORD *)(a3 + 8) = result;
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);

//----- (000000000002920E) ----------------------------------------------------
__int64 __fastcall ccrsa_generate_931_key(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4, __int64 a5, __int64 a6)
{
  return ccrsa_generate_fips186_key(a1, a2, a3, a4, a5, a6);
}

//----- (0000000000029218) ----------------------------------------------------
__int64 __fastcall ccrsa_make_931_key(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9, __int64 a10, __int64 a11, __int64 a12, __int64 a13, __int64 a14, __int64 a15, __int64 a16, __int64 a17, void *a18, __int64 a19, void *a20, __int64 a21, void *a22, __int64 a23, void *a24)
{
  return ccrsa_make_fips186_key(
           a1,
           a2,
           a3,
           a4,
           a5,
           a6,
           a7,
           a8,
           a9,
           a10,
           a11,
           a12,
           a13,
           a14,
           a15,
           a16,
           a17,
           a18,
           a19,
           a20,
           a21,
           a22,
           a23,
           a24);
}

//----- (0000000000029222) ----------------------------------------------------
void *__fastcall ccmode_ctr_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, const void *a5)
{
  const void *v5; // r14@1
  __int64 v6; // rbx@1
  unsigned __int64 v7; // rax@1
  void *v8; // r15@1

  v5 = a5;
  v6 = *(_QWORD *)(a1 + 32);
  *(_QWORD *)a2 = v6;
  *(_QWORD *)(a2 + 8) = *(_QWORD *)(v6 + 8);
  v7 = (unsigned __int64)(*(_QWORD *)(v6 + 8) + 7LL) >> 3;
  v8 = (void *)(a2 + 8 * v7 + 16);
  (*(void (__fastcall **)(__int64, signed __int64))(v6 + 16))(v6, a2 + 16 * v7 + 16);
  return memcpy(v8, v5, *(_QWORD *)(v6 + 8));
}

//----- (0000000000029277) ----------------------------------------------------
char __fastcall ccmode_gcm_reset(__int64 a1)
{
  char result; // al@1

  cc_clear(0x10uLL, (void *)(a1 + 16));
  result = cc_clear(0x10uLL, (void *)(a1 + 64));
  *(_DWORD *)(a1 + 96) = 0;
  *(_DWORD *)(a1 + 92) = 0;
  *(_DWORD *)(a1 + 88) = 0;
  *(_QWORD *)(a1 + 104) = 0LL;
  *(_QWORD *)(a1 + 112) = 0LL;
  return result;
}

//----- (00000000000292C8) ----------------------------------------------------
__int64 __fastcall ccmode_ofb_crypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  __int64 v5; // rbx@1
  __int64 result; // rax@1
  __int64 v7; // r13@2
  __int64 v8; // rcx@2
  signed __int64 i; // r14@2
  char v10; // cl@6
  unsigned __int64 v11; // [sp+8h] [bp-38h]@2

  v4 = a4;
  v5 = a3;
  result = *(_QWORD *)(a1 + 8);
  if ( a2 )
  {
    v7 = *(_QWORD *)a1;
    v8 = *(_QWORD *)(*(_QWORD *)a1 + 8LL);
    v11 = a1 + ((v8 + 7) & 0xFFFFFFFFFFFFFFF8LL) + 16;
    for ( i = 1 - a2; ; ++i )
    {
      if ( result == v8 )
      {
        (*(void (__fastcall **)(unsigned __int64, signed __int64, signed __int64, signed __int64))(v7 + 24))(
          v11,
          1LL,
          a1 + 16,
          a1 + 16);
        result = 0LL;
      }
      v10 = *(_BYTE *)(a1 + result++ + 16);
      *(_BYTE *)v4 = *(_BYTE *)v5 ^ v10;
      if ( !i )
        break;
      ++v4;
      ++v5;
      v8 = *(_QWORD *)(v7 + 8);
    }
  }
  *(_QWORD *)(a1 + 8) = result;
  return result;
}

//----- (0000000000029365) ----------------------------------------------------
int __fastcall ccmode_ofb_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, const void *a5)
{
  __int64 v5; // r14@1
  __int64 v6; // r15@1
  __int64 v7; // rbx@1

  v5 = a4;
  v6 = a3;
  v7 = *(_QWORD *)(a1 + 32);
  *(_QWORD *)a2 = v7;
  memcpy((void *)(a2 + 16), a5, *(_QWORD *)(v7 + 8));
  *(_QWORD *)(a2 + 8) = *(_QWORD *)(v7 + 8);
  return (*(int (__fastcall **)(__int64, unsigned __int64, __int64, __int64))(v7 + 16))(
           v7,
           a2 + ((*(_QWORD *)(*(_QWORD *)a2 + 8LL) + 7LL) & 0xFFFFFFFFFFFFFFF8LL) + 16,
           v6,
           v5);
}

//----- (00000000000293C7) ----------------------------------------------------
signed __int64 __fastcall ccmode_xts_crypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5)
{
  __int64 v5; // rbx@1
  __int64 v6; // r14@1
  __int64 v7; // r15@1
  unsigned __int64 v8; // rcx@1
  signed __int64 result; // rax@1
  signed __int64 v10; // rax@4
  signed __int64 v11; // rax@6
  __int64 v12; // [sp+8h] [bp-38h]@3

  v5 = a5;
  v6 = a4;
  v7 = a3;
  v8 = a3 + *(_QWORD *)a2;
  result = 0LL;
  if ( v8 <= 0x100000 )
  {
    *(_QWORD *)a2 = v8;
    result = a2 + 8;
    if ( a3 )
    {
      v12 = *(_QWORD *)a1;
      do
      {
        v10 = 1LL;
        do
        {
          *(_QWORD *)(v5 + 8 * v10) = *(_QWORD *)(v6 + 8 * v10) ^ *(_QWORD *)(a2 + 8 * v10 + 8);
          --v10;
        }
        while ( v10 != -1 );
        (*(void (__fastcall **)(signed __int64, signed __int64, __int64, __int64))(v12 + 24))(a1 + 16, 1LL, v5, v5);
        v11 = 16LL;
        do
        {
          *(_QWORD *)(v5 + v11 - 8) ^= *(_QWORD *)(a2 + v11);
          v11 -= 8LL;
        }
        while ( v11 );
        ccmode_xts_mult_alpha(a2 + 8);
        v6 += 16LL;
        v5 += 16LL;
        --v7;
      }
      while ( v7 );
      result = a2 + 8;
    }
  }
  return result;
}

//----- (0000000000029483) ----------------------------------------------------
int __fastcall ccmode_xts_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  __int64 v6; // r15@1
  __int64 v7; // r12@1
  __int64 v8; // r13@1

  v5 = a5;
  v6 = a3;
  v7 = *(_QWORD *)(a1 + 48);
  v8 = *(_QWORD *)(a1 + 56);
  *(_QWORD *)a2 = v7;
  *(_QWORD *)(a2 + 8) = v8;
  (*(void (__fastcall **)(__int64, signed __int64))(v7 + 16))(v7, a2 + 16);
  return (*(int (__fastcall **)(__int64, unsigned __int64, __int64, __int64))(v8 + 16))(
           v7,
           a2 + ((**(_QWORD **)a2 + 7LL) & 0xFFFFFFFFFFFFFFF8LL) + 16,
           v6,
           v5);
}

//----- (00000000000294E5) ----------------------------------------------------
__int64 __fastcall ccmode_xts_mult_alpha(__int64 a1)
{
  __int64 result; // rax@1
  __int64 v2; // rcx@1
  char v3; // si@2

  result = 0LL;
  v2 = 0LL;
  do
  {
    v3 = result;
    LOBYTE(result) = (unsigned __int16)*(_BYTE *)(a1 + v2) >> 7;
    *(_BYTE *)(a1 + v2) = v3 | 2 * *(_BYTE *)(a1 + v2);
    ++v2;
  }
  while ( v2 != 16 );
  if ( (_BYTE)result )
    *(_BYTE *)a1 ^= 0x87u;
  return result;
}

//----- (0000000000029512) ----------------------------------------------------
int __fastcall ccmode_xts_set_tweak(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rax@1

  v3 = *(_QWORD *)(a1 + 8);
  *(_QWORD *)a2 = 0LL;
  return (*(int (__fastcall **)(unsigned __int64, signed __int64, __int64, __int64))(v3 + 24))(
           a1 + ((**(_QWORD **)a1 + 7LL) & 0xFFFFFFFFFFFFFFF8LL) + 16,
           1LL,
           a3,
           a2 + 8);
}

//----- (0000000000029544) ----------------------------------------------------
__int64 __fastcall ccder_decode_eckey(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, unsigned __int64 a7, unsigned __int64 a8)
{
  __int64 v8; // r14@1
  __int64 v9; // r15@1
  signed __int64 v10; // rax@1
  unsigned __int64 v11; // rax@1
  __int64 v12; // r12@1
  __int64 v13; // rax@2
  __int64 v14; // rcx@3
  signed __int64 v15; // rax@4
  __int64 v16; // rax@5
  signed __int64 v17; // rax@7
  __int64 v19; // [sp+8h] [bp-48h]@1
  __int64 v20; // [sp+10h] [bp-40h]@1
  __int64 v21; // [sp+18h] [bp-38h]@1
  unsigned __int64 v22; // [sp+20h] [bp-30h]@1

  v20 = a6;
  v8 = a5;
  v19 = a4;
  v9 = a3;
  v22 = a8;
  v21 = 0LL;
  v10 = ccder_decode_sequence_tl((__int64)&v22, a7, a8);
  v11 = ccder_decode_uint64(a1, v10, v22);
  v12 = 0LL;
  if ( *(_QWORD *)a1 == 1LL )
  {
    v13 = ccder_decode_tl(v11, v11, v22, 4LL, (__int64)&v21);
    v12 = 0LL;
    if ( v13 )
    {
      *(_QWORD *)v9 = v13;
      v14 = v21;
      *(_QWORD *)a2 = v21;
      v13 += v14;
      v12 = v13;
    }
    v15 = ccder_decode_tl(v13, v12, v22, -6917529027641081856LL, (__int64)&v21);
    if ( v15 )
    {
      v16 = ccder_decode_oid(v19, v15, v15 + v21);
      v12 = v16;
    }
    else
    {
      v16 = v19;
      *(_QWORD *)v19 = 0LL;
    }
    v17 = ccder_decode_tl(v16, v12, v22, -6917529027641081855LL, (__int64)&v21);
    if ( v17 )
    {
      v12 = ccder_decode_bitstring(v17, v17, v17 + v21, v20, v8);
    }
    else
    {
      *(_QWORD *)v20 = 0LL;
      *(_QWORD *)v8 = 0LL;
    }
  }
  return v12;
}

//----- (0000000000029672) ----------------------------------------------------
int __usercall ccmode_gcm_decrypt@<eax>(__int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, unsigned __int64 a4@<rsi>, __int128 a5@<xmm0>, __int128 a6@<xmm1>, __int128 a7@<xmm2>, __int128 a8@<xmm3>, __int128 a9@<xmm6>, __int128 a10@<xmm7>, __int128 a11@<xmm8>, __int128 a12@<xmm9>, __int128 a13@<xmm10>, __int128 a14@<xmm11>, __int128 a15@<xmm12>, __int128 a16@<xmm13>, __int128 a17@<xmm14>, __int128 a18@<xmm15>)
{
  __int64 v18; // r15@1
  unsigned __int64 v19; // r14@1
  __int64 v20; // rax@1
  __int64 v21; // r14@2
  unsigned __int64 v22; // r12@5
  __int64 v23; // r14@5
  signed __int64 v24; // rax@5
  bool v25; // zf@7
  signed __int64 v26; // rcx@8
  unsigned __int64 v27; // r13@10
  unsigned int v28; // er15@14
  __int64 v29; // rax@14
  __int128 v30; // xmm4@14
  __int128 v31; // xmm5@14
  __int64 v32; // r13@15
  __int64 v33; // r12@15
  unsigned __int64 v34; // r15@20
  __int64 v35; // rax@21
  __int64 v36; // r12@21
  __int64 v37; // rcx@21
  unsigned __int64 v38; // rdx@21
  signed __int64 v39; // rax@23
  signed __int64 v40; // rcx@25
  unsigned int v41; // eax@30
  unsigned __int64 v42; // r12@31
  __int64 v43; // r14@31
  __int64 v44; // r15@31
  signed __int64 v45; // rax@31
  signed __int64 v46; // rcx@33
  char v47; // si@35
  __int64 v49; // [sp+10h] [bp-50h]@11
  __int64 v50; // [sp+30h] [bp-30h]@13

  v18 = a1;
  v19 = a4;
  LODWORD(v20) = *(_DWORD *)(a3 + 92);
  if ( !(_DWORD)v20 )
  {
    v21 = a2;
    ccmode_gcm_gmac(0LL, a3, 0LL, (__m128i)a5, (__m128i)a6, (__m128i)a7, (__m128i)a8, (__m128i)a9, (__m128i)a10);
    a2 = v21;
    v19 = a4;
    LODWORD(v20) = *(_DWORD *)(a3 + 92);
  }
  if ( (_DWORD)v20 != 2 )
  {
    if ( (_DWORD)v20 != 1 )
      return v20;
    v22 = v19;
    v23 = a2;
    v24 = 47LL;
    if ( *(_DWORD *)(a3 + 96) )
    {
      *(_QWORD *)(a3 + 104) += 8LL * *(_DWORD *)(a3 + 96);
      ccmode_gcm_mult_h(a3, (__m128i *)(a3 + 16), (__m128i)a5, (__m128i)a6, (__m128i)a7, (__m128i)a8);
      v24 = 47LL;
    }
    do
    {
      v25 = (*(_BYTE *)(a3 + v24))++ == -1;
      if ( !v25 )
        break;
      v26 = v24-- - 33;
    }
    while ( (unsigned __int64)v26 > 0xB );
    LODWORD(v20) = (*(int (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(*(_QWORD *)(a3 + 80) + 24LL))(
                     a3 + 384,
                     1LL,
                     a3 + 32,
                     a3 + 64);
    *(_DWORD *)(a3 + 96) = 0;
    *(_DWORD *)(a3 + 92) = 2;
    *(_QWORD *)(a3 + 112) = 0LL;
    a2 = v23;
    v19 = v22;
  }
  v27 = 0LL;
  if ( !*(_DWORD *)(a3 + 96) )
  {
    v49 = a2;
    LODWORD(v20) = cpuid_features();
    if ( _bittest((const unsigned __int64 *)&v20, 0x39u)
      && (LODWORD(v20) = cpuid_features(), _bittest((const unsigned __int64 *)&v20, 0x29u)) )
    {
      v50 = v18;
      if ( v19 >= 0x10 )
      {
        v28 = v19 & 0xFFFFFFF0;
        LODWORD(v29) = cpuid_features();
        if ( _bittest((const unsigned __int64 *)&v29, 0x3Cu) )
        {
          v32 = v50;
          v33 = v49;
          gcmDecrypt_avx1(
            a3,
            v28,
            v50,
            v49,
            a3 + 384,
            a5,
            a6,
            a7,
            a8,
            v30,
            v31,
            a9,
            a10,
            a11,
            a12,
            a13,
            a14,
            a15,
            a16,
            a17,
            a18);
        }
        else
        {
          v32 = v50;
          v33 = v49;
          gcmDecrypt_SupplementalSSE3(
            a3,
            v28,
            v50,
            v49,
            a3 + 384,
            (__m128i)a5,
            (__m128i)a6,
            (__m128i)a7,
            (__m128i)a8,
            (__m128i)v30,
            (__m128i)v31,
            (__m128i)a9,
            (__m128i)a10,
            (__m128i)a11,
            (__m128i)a12,
            (__m128i)a13,
            (__m128i)a14,
            (__m128i)a15,
            (__m128i)a16,
            (__m128i)a17,
            (__m128i)a18);
        }
        v50 = v28 + v32;
        v49 = v28 + v33;
        *(_QWORD *)(a3 + 112) += 8 * v28;
        v19 -= v28;
        LODWORD(v20) = (*(int (__fastcall **)(__int64, signed __int64, __int64, __int64))(*(_QWORD *)(a3 + 80) + 24LL))(
                         a3 + 384,
                         1LL,
                         a3 + 32,
                         a3 + 64);
      }
    }
    else
    {
      v50 = v18;
    }
    v27 = (unsigned int)v19 & 0xFFFFFFF0;
    if ( (unsigned __int64)((unsigned int)v19 & 0xFFFFFFF0) )
    {
      v34 = 0LL;
      a2 = v49;
      do
      {
        v35 = a2 + v34;
        v36 = a2;
        v37 = v50 + v34;
        v38 = 0LL;
        do
        {
          *(_QWORD *)(a3 + v38 + 16) ^= *(_QWORD *)(v37 + v38);
          *(_QWORD *)(v35 + v38) = *(_QWORD *)(v37 + v38) ^ *(_QWORD *)(a3 + v38 + 64);
          v38 += 8LL;
        }
        while ( v38 < 0x10 );
        *(_QWORD *)(a3 + 112) += 128LL;
        ccmode_gcm_mult_h(a3, (__m128i *)(a3 + 16), (__m128i)a5, (__m128i)a6, (__m128i)a7, (__m128i)a8);
        v39 = 47LL;
        do
        {
          v25 = (*(_BYTE *)(a3 + v39))++ == -1;
          if ( !v25 )
            break;
          v40 = v39-- - 33;
        }
        while ( (unsigned __int64)v40 > 0xB );
        LODWORD(v20) = (*(int (__fastcall **)(__int64, signed __int64, __int64, __int64))(*(_QWORD *)(a3 + 80) + 24LL))(
                         a3 + 384,
                         1LL,
                         a3 + 32,
                         a3 + 64);
        v34 += 16LL;
        a2 = v36;
      }
      while ( v34 < v27 );
      v18 = v50;
    }
    else
    {
      v27 = 0LL;
      a2 = v49;
      v18 = v50;
    }
  }
  if ( v27 < v19 )
  {
    do
    {
      v41 = *(_DWORD *)(a3 + 96);
      if ( v41 == 16 )
      {
        v42 = v19;
        v43 = v18;
        v44 = a2;
        *(_QWORD *)(a3 + 112) += 128LL;
        ccmode_gcm_mult_h(a3, (__m128i *)(a3 + 16), (__m128i)a5, (__m128i)a6, (__m128i)a7, (__m128i)a8);
        v45 = 47LL;
        do
        {
          v25 = (*(_BYTE *)(a3 + v45))++ == -1;
          if ( !v25 )
            break;
          v46 = v45-- - 33;
        }
        while ( (unsigned __int64)v46 > 0xB );
        (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(*(_QWORD *)(a3 + 80)
                                                                                               + 24LL))(
          a3 + 384,
          1LL,
          a3 + 32,
          a3 + 64);
        *(_DWORD *)(a3 + 96) = 0;
        v41 = 0;
        a2 = v44;
        v18 = v43;
        v19 = v42;
      }
      v47 = *(_BYTE *)(v18 + v27);
      *(_BYTE *)(a2 + v27) = v47 ^ *(_BYTE *)(a3 + v41 + 64);
      v20 = *(_DWORD *)(a3 + 96);
      *(_DWORD *)(a3 + 96) = v20 + 1;
      *(_BYTE *)(a3 + v20 + 16) ^= v47;
      ++v27;
    }
    while ( v27 != v19 );
  }
  return v20;
}
// 4B8C8: using guessed type int cpuid_features(void);

//----- (00000000000299A0) ----------------------------------------------------
__int64 __fastcall ccn_add(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v5; // r8@1
  unsigned __int8 v6; // of@1
  signed __int64 i; // rdi@1
  unsigned __int8 v8; // cf@2
  __int64 v9; // rax@2
  __int64 v10; // rax@2
  __int64 v11; // rtt@2
  __int64 v12; // rax@2
  __int64 v13; // rax@2
  __int64 v14; // rtt@2
  __int64 v15; // rax@2
  __int64 v16; // rax@2
  __int64 v17; // rtt@2
  __int64 v18; // rax@4
  __int64 v19; // rax@4
  __int64 v20; // rtt@4

  _AH = 0;
  v5 = 0LL;
  v6 = __OFSUB__(a1, 4LL);
  for ( i = a1 - 4; !((i < 0) ^ v6); i -= 4LL )
  {
    v8 = (_AH & 1) != 0;
    v9 = *(_QWORD *)(a4 + 8 * v5);
    v11 = v8;
    v8 = __CFADD__(v8, v9);
    v10 = v11 + v9;
    v8 |= __CFADD__(*(_QWORD *)(a3 + 8 * v5), v10);
    *(_QWORD *)(a2 + 8 * v5) = *(_QWORD *)(a3 + 8 * v5) + v10;
    v12 = *(_QWORD *)(a4 + 8 * v5 + 8);
    v14 = v8;
    v8 = __CFADD__(v8, v12);
    v13 = v14 + v12;
    v8 |= __CFADD__(*(_QWORD *)(a3 + 8 * v5 + 8), v13);
    *(_QWORD *)(a2 + 8 * v5 + 8) = *(_QWORD *)(a3 + 8 * v5 + 8) + v13;
    v15 = *(_QWORD *)(a4 + 8 * v5 + 16);
    v17 = v8;
    v8 = __CFADD__(v8, v15);
    v16 = v17 + v15;
    v8 |= __CFADD__(*(_QWORD *)(a3 + 8 * v5 + 16), v16);
    *(_QWORD *)(a2 + 8 * v5 + 16) = *(_QWORD *)(a3 + 8 * v5 + 16) + v16;
    *(_QWORD *)(a2 + 8 * v5 + 24) = *(_QWORD *)(a3 + 8 * v5 + 24) + v8 + *(_QWORD *)(a4 + 8 * v5 + 24);
    __asm { lahf }
    v5 += 4LL;
    v6 = __OFSUB__(i, 4LL);
  }
  if ( (unsigned __int64)(i & 2) )
  {
    v8 = (_AH & 1) != 0;
    v18 = *(_QWORD *)(a4 + 8 * v5);
    v20 = v8;
    v8 = __CFADD__(v8, v18);
    v19 = v20 + v18;
    v8 |= __CFADD__(*(_QWORD *)(a3 + 8 * v5), v19);
    *(_QWORD *)(a2 + 8 * v5) = *(_QWORD *)(a3 + 8 * v5) + v19;
    *(_QWORD *)(a2 + 8 * v5 + 8) = *(_QWORD *)(a3 + 8 * v5 + 8) + v8 + *(_QWORD *)(a4 + 8 * v5 + 8);
    __asm { lahf }
    v5 += 2LL;
  }
  if ( (unsigned __int64)(i & 1) )
  {
    *(_QWORD *)(a2 + 8 * v5) = *(_QWORD *)(a3 + 8 * v5) + ((_AH & 1) != 0) + *(_QWORD *)(a4 + 8 * v5);
    __asm { lahf }
  }
  return (_AH & 1) != 0;
}

//----- (0000000000029A43) ----------------------------------------------------
signed __int64 __fastcall ccn_add1(__int64 a1, __int64 a2, __int64 a3, signed __int64 a4)
{
  __int64 v4; // rax@1
  unsigned __int8 v5; // cf@3

  v4 = 0LL;
  while ( a1 )
  {
    --a1;
    v5 = __CFADD__(*(_QWORD *)(a3 + v4), a4);
    *(_QWORD *)(a2 + v4) = *(_QWORD *)(a3 + v4) + a4;
    v4 += 8LL;
    a4 = 1LL;
    if ( !v5 )
    {
      a4 = 0LL;
      if ( a3 != a2 )
      {
        ccn_set(a1, (void *)(v4 + a2), (const void *)(v4 + a3));
        a4 = 0LL;
      }
      return a4;
    }
  }
  return a4;
}

//----- (0000000000029A7D) ----------------------------------------------------
signed __int64 __fastcall ccec_compact_import_priv_size(signed __int64 a1)
{
  signed __int64 result; // rax@1

  result = 192LL;
  if ( a1 > 131 )
  {
    if ( a1 == 132 )
      return 521LL;
  }
  else if ( a1 > 95 )
  {
    if ( a1 == 96 )
      return 384LL;
  }
  else
  {
    if ( a1 == 48 )
      return result;
    if ( a1 == 56 )
      return 224LL;
    if ( a1 == 64 )
      return 256LL;
  }
  return 0LL;
}

//----- (0000000000029AD6) ----------------------------------------------------
signed __int64 __fastcall ccec_compact_import_priv(unsigned __int64 *a1, unsigned __int64 a2, unsigned __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  unsigned __int64 v5; // r14@1
  signed __int64 result; // rax@3

  v4 = a4;
  v5 = a3;
  if ( (((unsigned __int64)(ccn_bitlen(*a1, (__int64)(a1 + 2)) + 7) >> 2) & 0x3FFFFFFFFFFFFFFELL) != a2
    || (*(_QWORD *)v4 = a1, (unsigned int)ccec_compact_import_pub((__int64 *)a1, a2 >> 1, v5, v4)) )
    result = 0xFFFFFFFFLL;
  else
    result = (unsigned int)-((unsigned int)ccn_read_uint(*a1, v4 + 24LL * **(_QWORD **)v4 + 16, a2 >> 1, (a2 >> 1) + v5) != 0);
  return result;
}

//----- (0000000000029B5F) ----------------------------------------------------
__int64 __usercall ccmode_ccm_decrypt_x86_64@<rax>(unsigned __int64 a1@<rdx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __m128i a6@<xmm0>, __m128i a7@<xmm1>, __m128i a8@<xmm2>, __m128i a9@<xmm3>)
{
  __int64 v9; // r14@1
  __int64 v10; // r13@1
  unsigned __int64 v11; // r15@1
  __int64 v12; // rbx@1
  __int64 v13; // r12@1
  __int64 result; // rax@1
  __int64 v15; // rax@6
  __int64 v16; // rax@7
  __m128i v17; // xmm4@7
  __m128i v18; // xmm5@7
  __int64 v19; // r8@8
  __int64 v20; // rax@8
  unsigned __int64 v21; // r13@8
  __int64 v22; // r14@11
  __int64 v23; // r12@11
  __int64 v24; // r12@11
  __m128i *v25; // rsi@16
  const __m128i *v26; // rcx@16
  unsigned __int64 v27; // r13@17
  const __m128i *v28; // r14@17
  __int64 v29; // ST10_8@17
  __int64 v30; // r12@17

  v9 = a5;
  v10 = a2;
  v11 = a1;
  v12 = a4;
  v13 = a3;
  result = *(_DWORD *)(a4 + 64);
  if ( (_DWORD)result == 2 )
    goto LABEL_6;
  if ( (_DWORD)result == 1 )
  {
    if ( *(_DWORD *)(a4 + 72) )
    {
      (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(*(_QWORD *)a3 + 24LL))(
        a3 + 8,
        1LL,
        a4 + 16,
        a4 + 16);
      *(_DWORD *)(a4 + 72) = 0;
    }
    *(_DWORD *)(a4 + 64) = 2;
LABEL_6:
    LODWORD(v15) = cpuid_features();
    if ( _bittest((const unsigned __int64 *)&v15, 0x39u) )
    {
      LODWORD(v16) = cpuid_features();
      if ( _bittest((const unsigned __int64 *)&v16, 0x29u) )
      {
        v19 = v9;
        v20 = v10;
        v21 = *(_DWORD *)(a4 + 68);
        if ( *(_DWORD *)(a4 + 68) )
        {
          if ( v21 > v11 )
            v21 = v11;
          v22 = v20;
          v23 = v19;
          ccmode_ccm_crypt(a3, a4, v21, v20, v19);
          ccmode_ccm_macdata(a3, a4, 0, v21, v23);
          v24 = v21 + v23;
          v11 -= v21;
          v10 = v21 + v22;
          v9 = v24;
          v13 = a3;
          if ( *(_DWORD *)(a4 + 68) )
            goto LABEL_18;
        }
        else
        {
          v10 = v20;
        }
        if ( *(_DWORD *)(v13 + 248) == 160 )
        {
          v25 = (__m128i *)v9;
          v26 = (const __m128i *)v10;
          if ( v11 >= 0x10 )
          {
            v27 = v11 >> 4;
            v28 = v26;
            v29 = v13;
            ccm128_decrypt(
              v11 >> 4,
              v26,
              v25,
              v13 + 8,
              (const __m128i *)v12,
              a6,
              a7,
              a8,
              a9,
              v17,
              v18,
              *(_DWORD *)(*(_QWORD *)v13 + 8LL) - 1 - *(_DWORD *)(v12 + 80));
            v11 &= 0xFu;
            v27 *= 16LL;
            v30 = (__int64)((char *)v25 + v27);
            v10 = (__int64)((char *)v28 + v27);
            v9 = v30;
            v13 = v29;
          }
        }
      }
    }
LABEL_18:
    ccmode_ccm_crypt(v13, v12, v11, v10, v9);
    result = ccmode_ccm_macdata(v13, v12, 0, v11, v9);
  }
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);

//----- (0000000000029CFF) ----------------------------------------------------
__int64 __fastcall ccrsa_export_pub(__int64 *a1, __int64 a2, __int64 a3)
{
  return ccder_encode_rsa_pub(a1, a3, a3 + a2) != a3;
}

//----- (0000000000029D29) ----------------------------------------------------
signed __int64 __fastcall ccn_bitlen(__int64 a1, __int64 a2)
{
  signed __int64 v2; // rcx@1
  signed __int64 result; // rax@1
  __int64 v4; // rdx@2
  unsigned __int64 v5; // rcx@3

  v2 = ccn_n(a1, a2);
  result = 0LL;
  if ( v2 )
  {
    v4 = *(_QWORD *)(a2 + 8 * v2 - 8);
    result = v2 << 6;
    if ( v4 >= 0 )
    {
      v5 = 0x8000000000000000LL;
      do
      {
        v5 >>= 1;
        --result;
      }
      while ( !(v5 & v4) );
    }
  }
  return result;
}

//----- (0000000000029D6E) ----------------------------------------------------
__int64 __usercall ccn_burn_stack@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>)
{
  __int64 v2; // rbx@1
  __int64 v4; // [sp+0h] [bp-10h]@1

  v4 = a1;
  v2 = off_69010[0];
  v4 = *(_QWORD *)off_69010[0];
  bzero((char *)&v4 - ((8 * a2 + 15) & 0xFFFFFFFFFFFFFFF0LL), 8 * a2);
  return *(_QWORD *)v2;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000029DBC) ----------------------------------------------------
signed __int64 __fastcall ccn_cmp(__int64 a1, __int64 a2, __int64 a3)
{
  signed __int64 result; // rax@1
  unsigned __int64 v4; // rax@2
  bool v5; // cf@2
  bool v6; // zf@2

  while ( 1 )
  {
    result = 0LL;
    if ( !a1 )
      break;
    v4 = *(_QWORD *)(a3 + 8 * a1 - 8);
    v5 = *(_QWORD *)(a2 + 8 * a1 - 8) < v4;
    v6 = *(_QWORD *)(a2 + 8 * a1-- - 8) == v4;
    if ( !v6 )
    {
      result = 0xFFFFFFFFLL;
      if ( !v5 && !v6 )
        result = 1LL;
      return result;
    }
  }
  return result;
}

//----- (0000000000029DE6) ----------------------------------------------------
__int64 __fastcall cccmac_block_update(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // rbx@1
  __int64 v5; // r14@1
  char v7; // [sp+0h] [bp-40h]@3
  __int64 v8; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v8 = *(_QWORD *)off_69010[0];
  if ( a3 && a4 )
  {
    do
    {
      (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, __int64, char *))(a1 + 24))(
        a2 + 32,
        a2 + *(_QWORD *)a1 + 32,
        1LL,
        v4,
        &v7);
      v4 += 16LL;
      --v5;
    }
    while ( v5 );
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000029E69) ----------------------------------------------------
__int64 __usercall ccsha256_vng_intel_avx2_compress@<rax>(__int64 _RDX@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __int128 _XMM0@<xmm0>, __int128 _XMM1@<xmm1>, __int128 _XMM2@<xmm2>, __int128 _XMM3@<xmm3>, __int128 _XMM4@<xmm4>, __int128 _XMM5@<xmm5>, __int128 _XMM6@<xmm6>, __int128 _XMM7@<xmm7>)
{
  signed __int64 v27; // rbx@1
  int v29; // er9@2
  int v30; // er10@2
  int v32; // er13@2
  int v33; // er14@2
  int v38; // er15@2
  int v41; // eax@2
  int v44; // eax@2
  int v47; // ecx@2
  int v51; // ebp@2
  int v59; // er14@2
  int v62; // eax@2
  int v64; // eax@2
  int v67; // ecx@2
  int v70; // ebp@2
  int v78; // er13@2
  int v81; // eax@2
  int v84; // eax@2
  int v87; // ecx@2
  int v91; // ebp@2
  int v99; // er12@2
  int v102; // eax@2
  int v105; // eax@2
  int v108; // ecx@2
  int v112; // ebp@2
  __int128 v118; // ST00_16@2
  int v123; // er11@2
  int v126; // eax@2
  int v130; // eax@2
  int v133; // ecx@2
  int v138; // ebp@2
  int v146; // er10@2
  int v149; // eax@2
  int v152; // eax@2
  int v155; // ecx@2
  int v159; // ebp@2
  int v166; // er9@2
  int v169; // eax@2
  int v172; // eax@2
  int v175; // ecx@2
  int v179; // ebp@2
  int v186; // er8@2
  int v189; // eax@2
  int v192; // eax@2
  int v195; // ecx@2
  int v198; // ebp@2
  __int128 v203; // ST10_16@2
  int v208; // er15@2
  int v211; // eax@2
  int v214; // eax@2
  int v217; // ecx@2
  int v221; // ebp@2
  int v229; // er14@2
  int v232; // eax@2
  int v234; // eax@2
  int v237; // ecx@2
  int v240; // ebp@2
  int v248; // er13@2
  int v251; // eax@2
  int v254; // eax@2
  int v257; // ecx@2
  int v261; // ebp@2
  int v269; // er12@2
  int v272; // eax@2
  int v275; // eax@2
  int v278; // ecx@2
  int v282; // ebp@2
  __int128 v288; // ST20_16@2
  int v293; // er11@2
  int v296; // eax@2
  int v300; // eax@2
  int v303; // ecx@2
  int v308; // ebp@2
  int v316; // er10@2
  int v319; // eax@2
  int v322; // eax@2
  int v325; // ecx@2
  int v329; // ebp@2
  int v336; // er9@2
  int v339; // eax@2
  int v342; // eax@2
  int v345; // ecx@2
  int v349; // ebp@2
  int v356; // er8@2
  int v359; // eax@2
  int v362; // eax@2
  int v365; // ecx@2
  int v368; // ebp@2
  __int128 v373; // ST30_16@2
  int v378; // er15@2
  int v381; // eax@2
  int v384; // eax@2
  int v387; // ecx@2
  int v391; // ebp@2
  int v399; // er14@2
  int v402; // eax@2
  int v404; // eax@2
  int v407; // ecx@2
  int v410; // ebp@2
  int v418; // er13@2
  int v421; // eax@2
  int v424; // eax@2
  int v427; // ecx@2
  int v431; // ebp@2
  int v439; // er12@2
  int v442; // eax@2
  int v445; // eax@2
  int v448; // ecx@2
  int v452; // ebp@2
  __int128 v458; // ST00_16@2
  int v463; // er11@2
  int v466; // eax@2
  int v470; // eax@2
  int v473; // ecx@2
  int v478; // ebp@2
  int v486; // er10@2
  int v489; // eax@2
  int v492; // eax@2
  int v495; // ecx@2
  int v499; // ebp@2
  int v506; // er9@2
  int v509; // eax@2
  int v512; // eax@2
  int v515; // ecx@2
  int v519; // ebp@2
  int v526; // er8@2
  int v529; // eax@2
  int v532; // eax@2
  int v535; // ecx@2
  int v538; // ebp@2
  __int128 v543; // ST10_16@2
  int v548; // er15@2
  int v551; // eax@2
  int v554; // eax@2
  int v557; // ecx@2
  int v561; // ebp@2
  int v569; // er14@2
  int v572; // eax@2
  int v574; // eax@2
  int v577; // ecx@2
  int v580; // ebp@2
  int v588; // er13@2
  int v591; // eax@2
  int v594; // eax@2
  int v597; // ecx@2
  int v601; // ebp@2
  int v609; // er12@2
  int v612; // eax@2
  int v615; // eax@2
  int v618; // ecx@2
  int v622; // ebp@2
  __int128 v628; // ST20_16@2
  int v633; // er11@2
  int v636; // eax@2
  int v640; // eax@2
  int v643; // ecx@2
  int v648; // ebp@2
  int v656; // er10@2
  int v659; // eax@2
  int v662; // eax@2
  int v665; // ecx@2
  int v669; // ebp@2
  int v676; // er9@2
  int v679; // eax@2
  int v682; // eax@2
  int v685; // ecx@2
  int v689; // ebp@2
  int v696; // er8@2
  int v699; // eax@2
  int v702; // eax@2
  int v705; // ecx@2
  int v708; // ebp@2
  __int128 v713; // ST30_16@2
  int v718; // er15@2
  int v721; // eax@2
  int v724; // eax@2
  int v727; // ecx@2
  int v731; // ebp@2
  int v739; // er14@2
  int v742; // eax@2
  int v744; // eax@2
  int v747; // ecx@2
  int v750; // ebp@2
  int v758; // er13@2
  int v761; // eax@2
  int v764; // eax@2
  int v767; // ecx@2
  int v771; // ebp@2
  int v779; // er12@2
  int v782; // eax@2
  int v785; // eax@2
  int v788; // ecx@2
  int v792; // ebp@2
  int v802; // er11@2
  int v805; // eax@2
  int v809; // eax@2
  int v812; // ecx@2
  int v817; // ebp@2
  int v825; // er10@2
  int v828; // eax@2
  int v831; // eax@2
  int v834; // ecx@2
  int v838; // ebp@2
  int v845; // er9@2
  int v848; // eax@2
  int v851; // eax@2
  int v854; // ecx@2
  int v858; // ebp@2
  int v865; // er8@2
  int v868; // eax@2
  int v871; // eax@2
  int v874; // ecx@2
  int v877; // ebp@2
  int v886; // er15@2
  int v889; // eax@2
  int v892; // eax@2
  int v895; // ecx@2
  int v899; // ebp@2
  int v907; // er14@2
  int v910; // eax@2
  int v912; // eax@2
  int v915; // ecx@2
  int v918; // ebp@2
  int v926; // er13@2
  int v929; // eax@2
  int v932; // eax@2
  int v935; // ecx@2
  int v939; // ebp@2
  int v947; // er12@2
  int v950; // eax@2
  int v953; // eax@2
  int v956; // ecx@2
  int v960; // ebp@2
  int v970; // er11@2
  int v973; // eax@2
  int v977; // eax@2
  int v980; // ecx@2
  int v985; // ebp@2
  int v993; // er10@2
  int v996; // eax@2
  int v999; // eax@2
  int v1002; // ecx@2
  int v1006; // ebp@2
  int v1013; // er9@2
  int v1016; // eax@2
  int v1019; // eax@2
  int v1022; // ecx@2
  int v1026; // ebp@2
  int v1032; // er8@2
  int v1035; // eax@2
  int v1037; // eax@2
  int v1040; // ecx@2
  int v1043; // ebp@2
  signed __int64 v1048; // rbx@2
  int v1052; // er15@3
  int v1054; // eax@3
  int v1056; // eax@3
  int v1058; // ecx@3
  int v1060; // ebp@3
  int v1065; // er14@3
  int v1067; // eax@3
  int v1069; // eax@3
  int v1071; // ecx@3
  int v1073; // ebp@3
  int v1078; // er13@3
  int v1080; // eax@3
  int v1082; // eax@3
  int v1084; // ecx@3
  int v1086; // ebp@3
  int v1092; // er12@3
  int v1094; // eax@3
  int v1096; // eax@3
  int v1098; // ecx@3
  int v1100; // ebp@3
  int v1106; // er11@3
  int v1108; // eax@3
  int v1110; // eax@3
  int v1112; // ecx@3
  int v1114; // ebp@3
  int v1119; // er10@3
  int v1121; // eax@3
  int v1123; // eax@3
  int v1125; // ecx@3
  int v1127; // ebp@3
  int v1132; // er9@3
  int v1134; // eax@3
  int v1136; // eax@3
  int v1138; // ecx@3
  int v1140; // ebp@3
  int v1146; // er8@3
  int v1148; // eax@3
  int v1150; // eax@3
  int v1152; // ecx@3
  int v1154; // ebp@3
  int v1160; // er15@3
  int v1162; // eax@3
  int v1164; // eax@3
  int v1166; // ecx@3
  int v1168; // ebp@3
  int v1173; // er14@3
  int v1175; // eax@3
  int v1177; // eax@3
  int v1179; // ecx@3
  int v1181; // ebp@3
  int v1186; // er13@3
  int v1188; // eax@3
  int v1190; // eax@3
  int v1192; // ecx@3
  int v1194; // ebp@3
  int v1200; // er12@3
  int v1202; // eax@3
  int v1204; // eax@3
  int v1206; // ecx@3
  int v1208; // ebp@3
  int v1214; // er11@3
  int v1216; // eax@3
  int v1218; // eax@3
  int v1220; // ecx@3
  int v1222; // ebp@3
  int v1227; // er10@3
  int v1229; // eax@3
  int v1231; // eax@3
  int v1233; // ecx@3
  int v1235; // ebp@3
  int v1240; // er9@3
  int v1242; // eax@3
  int v1244; // eax@3
  int v1246; // ecx@3
  int v1248; // ebp@3
  int v1253; // er8@3
  int v1255; // eax@3
  int v1257; // eax@3
  int v1259; // ecx@3
  int v1261; // ebp@3
  int v1262; // er12@3
  int v1263; // er8@3
  __int64 v1264; // rdi@3
  int v1267; // er15@4
  int v1269; // eax@4
  int v1271; // eax@4
  int v1273; // ecx@4
  int v1275; // ebp@4
  int v1280; // er14@4
  int v1282; // eax@4
  int v1284; // eax@4
  int v1286; // ecx@4
  int v1288; // ebp@4
  int v1293; // er13@4
  int v1295; // eax@4
  int v1297; // eax@4
  int v1299; // ecx@4
  int v1301; // ebp@4
  int v1306; // er12@4
  int v1308; // eax@4
  int v1310; // eax@4
  int v1312; // ecx@4
  int v1314; // ebp@4
  int v1319; // er11@4
  int v1321; // eax@4
  int v1323; // eax@4
  int v1325; // ecx@4
  int v1327; // ebp@4
  int v1332; // er10@4
  int v1334; // eax@4
  int v1336; // eax@4
  int v1338; // ecx@4
  int v1340; // ebp@4
  int v1345; // er9@4
  int v1347; // eax@4
  int v1349; // eax@4
  int v1351; // ecx@4
  int v1353; // ebp@4
  int v1358; // er8@4
  int v1360; // eax@4
  int v1362; // eax@4
  int v1364; // ecx@4
  int v1366; // ebp@4
  int v1371; // er15@4
  int v1373; // eax@4
  int v1375; // eax@4
  int v1377; // ecx@4
  int v1379; // ebp@4
  int v1384; // er14@4
  int v1386; // eax@4
  int v1388; // eax@4
  int v1390; // ecx@4
  int v1392; // ebp@4
  int v1397; // er13@4
  int v1399; // eax@4
  int v1401; // eax@4
  int v1403; // ecx@4
  int v1405; // ebp@4
  int v1410; // er12@4
  int v1412; // eax@4
  int v1414; // eax@4
  int v1416; // ecx@4
  int v1418; // ebp@4
  int v1423; // er11@4
  int v1425; // eax@4
  int v1427; // eax@4
  int v1429; // ecx@4
  int v1431; // ebp@4
  int v1436; // er10@4
  int v1438; // eax@4
  int v1440; // eax@4
  int v1442; // ecx@4
  int v1444; // ebp@4
  int v1449; // er9@4
  int v1451; // eax@4
  int v1453; // eax@4
  int v1455; // ecx@4
  int v1457; // ebp@4
  int v1462; // er8@4
  int v1464; // eax@4
  __int64 result; // rax@4
  int v1468; // ecx@4
  int v1470; // ebp@4
  int v1471; // er12@4
  int v1472; // er8@4
  __int64 v1473; // rdi@4
  __int128 v1483; // [sp+0h] [bp-118h]@1
  __int128 v1484; // [sp+0h] [bp-118h]@2
  __int128 v1486; // [sp+10h] [bp-108h]@1
  __int128 v1487; // [sp+10h] [bp-108h]@2
  __int128 v1489; // [sp+20h] [bp-F8h]@1
  __int128 v1490; // [sp+20h] [bp-F8h]@2
  __int128 v1492; // [sp+30h] [bp-E8h]@1
  __int128 v1493; // [sp+30h] [bp-E8h]@2
  __int64 v1511; // [sp+D0h] [bp-48h]@1
  __int64 v1512; // [sp+D8h] [bp-40h]@1

  v1511 = a3;
  v1512 = a2;
  __asm
  {
    vmovdqa [rsp+118h+var_C8], xmm0
    vmovdqa [rsp+118h+var_B8], xmm1
    vmovdqa [rsp+118h+var_A8], xmm2
    vmovdqa [rsp+118h+var_98], xmm3
    vmovdqa [rsp+118h+var_88], xmm4
    vmovdqa [rsp+118h+var_78], xmm5
    vmovdqa [rsp+118h+var_68], xmm6
    vmovdqa [rsp+118h+var_58], xmm7
  }
  _RAX = (__int64)qword_5EFC0;
  __asm
  {
    vmovdqa xmm0, xmmword ptr [rax]
    vmovdqa [rsp+118h+var_D8], xmm0
    vmovdqu xmm0, xmmword ptr [rdx]
    vmovdqu xmm1, xmmword ptr [rdx+10h]
    vmovdqu xmm2, xmmword ptr [rdx+20h]
    vmovdqu xmm3, xmmword ptr [rdx+30h]
  }
  _RDX = _RDX + 64;
  __asm
  {
    vpshufb xmm0, xmm0, [rsp+118h+var_D8]
    vpshufb xmm1, xmm1, [rsp+118h+var_D8]
    vpshufb xmm2, xmm2, [rsp+118h+var_D8]
    vpshufb xmm3, xmm3, [rsp+118h+var_D8]
    vpaddd  xmm4, xmm0, xmmword ptr [rbx]
    vpaddd  xmm5, xmm1, xmmword ptr [rbx+10h]
    vpaddd  xmm6, xmm2, xmmword ptr [rbx+20h]
    vpaddd  xmm7, xmm3, xmmword ptr [rbx+30h]
  }
  v27 = (signed __int64)&ccsha256_K[8];
  __asm
  {
    vmovdqa [rsp+118h+var_118], xmm4
    vmovdqa [rsp+118h+var_108], xmm5
    vmovdqa [rsp+118h+var_F8], xmm6
    vmovdqa [rsp+118h+var_E8], xmm7
  }
  while ( 1 )
  {
    _ER8 = *(_DWORD *)v1512;
    v29 = *(_DWORD *)(v1512 + 4);
    v30 = *(_DWORD *)(v1512 + 8);
    _ER12 = *(_DWORD *)(v1512 + 16);
    v32 = *(_DWORD *)(v1512 + 20);
    v33 = *(_DWORD *)(v1512 + 24);
    __asm
    {
      vpalignr xmm6, xmm3, xmm2, 4
      rorx    eax, r12d, 19h
      rorx    ecx, r12d, 0Bh
      vpalignr xmm4, xmm1, xmm0, 4
    }
    v38 = v1483 + *(_DWORD *)(v1512 + 28);
    __asm
    {
      rorx    edi, r8d, 0Dh
      vpaddd  xmm0, xmm0, xmm6
    }
    v41 = _ECX ^ _EAX;
    __asm
    {
      vpslld  xmm7, xmm4, 0Eh
      rorx    ecx, r12d, 6
    }
    v44 = _ECX ^ v41;
    __asm
    {
      vpsrld  xmm6, xmm4, 7
      rorx    ecx, r8d, 16h
    }
    v47 = _EDI ^ _ECX;
    __asm
    {
      vpsrld  xmm4, xmm4, 3
      rorx    edi, r8d, 2
      vpxor   xmm4, xmm4, xmm7
    }
    v51 = v44 + (v33 ^ _ER12 & (v33 ^ *(_DWORD *)(v1512 + 20)));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER11 = v51 + v38 + *(_DWORD *)(v1512 + 12);
    _ER15 = (v30 & *(_DWORD *)v1512 | v29 & (v30 | *(_DWORD *)v1512)) + v51 + (_EDI ^ v47) + v38;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    eax, r11d, 19h
      rorx    ecx, r11d, 0Bh
      vpsrld  xmm6, xmm6, 0Bh
    }
    v59 = DWORD1(v1483) + v33;
    __asm
    {
      rorx    edi, r15d, 0Dh
      vpxor   xmm4, xmm4, xmm7
    }
    v62 = _ECX ^ _EAX;
    __asm { rorx    ecx, r11d, 6 }
    v64 = _ECX ^ v62;
    __asm
    {
      rorx    ecx, r15d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v67 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r15d, 2
      vpaddd  xmm0, xmm0, xmm4
    }
    v70 = v64 + (v32 ^ _ER11 & (v32 ^ *(_DWORD *)(v1512 + 16)));
    __asm { vunpckhps xmm7, xmm3, xmm3 }
    _ER10 = v70 + v59 + v30;
    _ER14 = (v29 & _ER15 | *(_DWORD *)v1512 & (v29 | _ER15)) + v70 + (_EDI ^ v67) + v59;
    __asm
    {
      vpsrld  xmm4, xmm3, 0Ah
      vpsrlq  xmm6, xmm7, 11h
      rorx    eax, r10d, 19h
      rorx    ecx, r10d, 0Bh
    }
    v78 = DWORD2(v1483) + v32;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    edi, r14d, 0Dh
    }
    v81 = _ECX ^ _EAX;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    ecx, r10d, 6
    }
    v84 = _ECX ^ v81;
    __asm
    {
      rorx    ecx, r14d, 16h
      vpshufd xmm6, xmm6, 80h
    }
    v87 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r14d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrldq xmm4, xmm4, 8
    }
    v91 = v84 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    _ER9 = v91 + v78 + v29;
    _ER13 = (*(_DWORD *)v1512 & _ER14 | _ER15 & (*(_DWORD *)v1512 | _ER14)) + v91 + (_EDI ^ v87) + v78;
    __asm
    {
      vunpcklps xmm7, xmm0, xmm0
      rorx    eax, r9d, 19h
      rorx    ecx, r9d, 0Bh
      vpsrld  xmm4, xmm0, 0Ah
    }
    v99 = DWORD3(v1483) + _ER12;
    __asm
    {
      vpsrlq  xmm6, xmm7, 11h
      rorx    edi, r13d, 0Dh
    }
    v102 = _ECX ^ _EAX;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    ecx, r9d, 6
    }
    v105 = _ECX ^ v102;
    __asm
    {
      rorx    ecx, r13d, 16h
      vpxor   xmm6, xmm6, xmm7
    }
    v108 = _EDI ^ _ECX;
    __asm
    {
      vpshufd xmm6, xmm6, 8
      rorx    edi, r13d, 2
      vpxor   xmm4, xmm4, xmm6
    }
    v112 = v105 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
    __asm { vpslldq xmm4, xmm4, 8 }
    _ER8 = v112 + v99 + *(_DWORD *)v1512;
    __asm
    {
      vpaddd  xmm0, xmm0, xmm4
      vpaddd  xmm4, xmm0, xmmword ptr [rbx]
    }
    _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v112 + (_EDI ^ v108) + v99;
    __asm
    {
      vmovdqa [rsp+118h+var_118], xmm4
      vpalignr xmm6, xmm0, xmm3, 4
      rorx    eax, r8d, 19h
      rorx    ecx, r8d, 0Bh
      vpalignr xmm4, xmm2, xmm1, 4
    }
    v123 = v1486 + _ER11;
    __asm
    {
      rorx    edi, r12d, 0Dh
      vpslld  xmm7, xmm4, 0Eh
    }
    v126 = _ECX ^ _EAX;
    __asm
    {
      vpaddd  xmm1, xmm1, xmm6
      rorx    ecx, r8d, 6
      vpsrld  xmm6, xmm4, 7
    }
    v130 = _ECX ^ v126;
    __asm
    {
      rorx    ecx, r12d, 16h
      vpsrld  xmm4, xmm4, 3
    }
    v133 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r12d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm7
    }
    v138 = v130 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER15 = v138 + v123 + _ER15;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v138 + (_EDI ^ v133) + v123;
    __asm
    {
      rorx    eax, r15d, 19h
      vpxor   xmm4, xmm4, xmm7
      rorx    ecx, r15d, 0Bh
    }
    v146 = DWORD1(v1486) + _ER10;
    __asm
    {
      rorx    edi, r11d, 0Dh
      vpaddd  xmm1, xmm1, xmm4
    }
    v149 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r15d, 6
      vunpckhps xmm7, xmm0, xmm0
    }
    v152 = _ECX ^ v149;
    __asm
    {
      rorx    ecx, r11d, 16h
      vpsrld  xmm4, xmm0, 0Ah
    }
    v155 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r11d, 2
      vpsrlq  xmm6, xmm7, 11h
      vpsrlq  xmm7, xmm7, 13h
    }
    v159 = v152 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
    __asm { vpxor   xmm6, xmm6, xmm7 }
    _ER14 = v159 + v146 + _ER14;
    _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v159 + (_EDI ^ v155) + v146;
    __asm
    {
      vpshufd xmm6, xmm6, 80h
      rorx    eax, r14d, 19h
      rorx    ecx, r14d, 0Bh
    }
    v166 = DWORD2(v1486) + _ER9;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    edi, r10d, 0Dh
    }
    v169 = _ECX ^ _EAX;
    __asm
    {
      vpsrldq xmm4, xmm4, 8
      rorx    ecx, r14d, 6
    }
    v172 = _ECX ^ v169;
    __asm
    {
      rorx    ecx, r10d, 16h
      vpaddd  xmm1, xmm1, xmm4
    }
    v175 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r10d, 2
      vunpcklps xmm7, xmm1, xmm1
      vpsrld  xmm4, xmm1, 0Ah
    }
    v179 = v172 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
    __asm { vpsrlq  xmm6, xmm7, 11h }
    _ER13 = v179 + v166 + _ER13;
    _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v179 + (_EDI ^ v175) + v166;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    eax, r13d, 19h
      rorx    ecx, r13d, 0Bh
    }
    v186 = DWORD3(v1486) + _ER8;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    edi, r9d, 0Dh
    }
    v189 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r13d, 6
      vpshufd xmm6, xmm6, 8
    }
    v192 = _ECX ^ v189;
    __asm
    {
      rorx    ecx, r9d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v195 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r9d, 2
      vpslldq xmm4, xmm4, 8
    }
    v198 = v192 + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    _ER12 = v198 + v186 + _ER12;
    __asm { vpaddd  xmm4, xmm1, xmmword ptr [rbx] }
    _ER8 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v198 + (_EDI ^ v195) + v186;
    __asm
    {
      vmovdqa [rsp+118h+var_108], xmm4
      vpalignr xmm6, xmm1, xmm0, 4
      rorx    eax, r12d, 19h
      rorx    ecx, r12d, 0Bh
      vpalignr xmm4, xmm3, xmm2, 4
    }
    v208 = v1489 + _ER15;
    __asm
    {
      rorx    edi, r8d, 0Dh
      vpaddd  xmm2, xmm2, xmm6
    }
    v211 = _ECX ^ _EAX;
    __asm
    {
      vpslld  xmm7, xmm4, 0Eh
      rorx    ecx, r12d, 6
    }
    v214 = _ECX ^ v211;
    __asm
    {
      vpsrld  xmm6, xmm4, 7
      rorx    ecx, r8d, 16h
    }
    v217 = _EDI ^ _ECX;
    __asm
    {
      vpsrld  xmm4, xmm4, 3
      rorx    edi, r8d, 2
      vpxor   xmm4, xmm4, xmm7
    }
    v221 = v214 + (_ER14 ^ _ER12 & (_ER14 ^ _ER13));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER11 = v221 + v208 + _ER11;
    _ER15 = (_ER10 & _ER8 | _ER9 & (_ER10 | _ER8)) + v221 + (_EDI ^ v217) + v208;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    eax, r11d, 19h
      rorx    ecx, r11d, 0Bh
      vpsrld  xmm6, xmm6, 0Bh
    }
    v229 = DWORD1(v1489) + _ER14;
    __asm
    {
      rorx    edi, r15d, 0Dh
      vpxor   xmm4, xmm4, xmm7
    }
    v232 = _ECX ^ _EAX;
    __asm { rorx    ecx, r11d, 6 }
    v234 = _ECX ^ v232;
    __asm
    {
      rorx    ecx, r15d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v237 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r15d, 2
      vpaddd  xmm2, xmm2, xmm4
    }
    v240 = v234 + (_ER13 ^ _ER11 & (_ER13 ^ _ER12));
    __asm { vunpckhps xmm7, xmm1, xmm1 }
    _ER10 = v240 + v229 + _ER10;
    _ER14 = (_ER9 & _ER15 | _ER8 & (_ER9 | _ER15)) + v240 + (_EDI ^ v237) + v229;
    __asm
    {
      vpsrld  xmm4, xmm1, 0Ah
      vpsrlq  xmm6, xmm7, 11h
      rorx    eax, r10d, 19h
      rorx    ecx, r10d, 0Bh
    }
    v248 = DWORD2(v1489) + _ER13;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    edi, r14d, 0Dh
    }
    v251 = _ECX ^ _EAX;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    ecx, r10d, 6
    }
    v254 = _ECX ^ v251;
    __asm
    {
      rorx    ecx, r14d, 16h
      vpshufd xmm6, xmm6, 80h
    }
    v257 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r14d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrldq xmm4, xmm4, 8
    }
    v261 = v254 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    _ER9 = v261 + v248 + _ER9;
    _ER13 = (_ER8 & _ER14 | _ER15 & (_ER8 | _ER14)) + v261 + (_EDI ^ v257) + v248;
    __asm
    {
      vunpcklps xmm7, xmm2, xmm2
      rorx    eax, r9d, 19h
      rorx    ecx, r9d, 0Bh
      vpsrld  xmm4, xmm2, 0Ah
    }
    v269 = DWORD3(v1489) + _ER12;
    __asm
    {
      vpsrlq  xmm6, xmm7, 11h
      rorx    edi, r13d, 0Dh
    }
    v272 = _ECX ^ _EAX;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    ecx, r9d, 6
    }
    v275 = _ECX ^ v272;
    __asm
    {
      rorx    ecx, r13d, 16h
      vpxor   xmm6, xmm6, xmm7
    }
    v278 = _EDI ^ _ECX;
    __asm
    {
      vpshufd xmm6, xmm6, 8
      rorx    edi, r13d, 2
      vpxor   xmm4, xmm4, xmm6
    }
    v282 = v275 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
    __asm { vpslldq xmm4, xmm4, 8 }
    _ER8 = v282 + v269 + _ER8;
    __asm
    {
      vpaddd  xmm2, xmm2, xmm4
      vpaddd  xmm4, xmm2, xmmword ptr [rbx]
    }
    _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v282 + (_EDI ^ v278) + v269;
    __asm
    {
      vmovdqa [rsp+118h+var_F8], xmm4
      vpalignr xmm6, xmm2, xmm1, 4
      rorx    eax, r8d, 19h
      rorx    ecx, r8d, 0Bh
      vpalignr xmm4, xmm0, xmm3, 4
    }
    v293 = v1492 + _ER11;
    __asm
    {
      rorx    edi, r12d, 0Dh
      vpslld  xmm7, xmm4, 0Eh
    }
    v296 = _ECX ^ _EAX;
    __asm
    {
      vpaddd  xmm3, xmm3, xmm6
      rorx    ecx, r8d, 6
      vpsrld  xmm6, xmm4, 7
    }
    v300 = _ECX ^ v296;
    __asm
    {
      rorx    ecx, r12d, 16h
      vpsrld  xmm4, xmm4, 3
    }
    v303 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r12d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm7
    }
    v308 = v300 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER15 = v308 + v293 + _ER15;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v308 + (_EDI ^ v303) + v293;
    __asm
    {
      rorx    eax, r15d, 19h
      vpxor   xmm4, xmm4, xmm7
      rorx    ecx, r15d, 0Bh
    }
    v316 = DWORD1(v1492) + _ER10;
    __asm
    {
      rorx    edi, r11d, 0Dh
      vpaddd  xmm3, xmm3, xmm4
    }
    v319 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r15d, 6
      vunpckhps xmm7, xmm2, xmm2
    }
    v322 = _ECX ^ v319;
    __asm
    {
      rorx    ecx, r11d, 16h
      vpsrld  xmm4, xmm2, 0Ah
    }
    v325 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r11d, 2
      vpsrlq  xmm6, xmm7, 11h
      vpsrlq  xmm7, xmm7, 13h
    }
    v329 = v322 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
    __asm { vpxor   xmm6, xmm6, xmm7 }
    _ER14 = v329 + v316 + _ER14;
    _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v329 + (_EDI ^ v325) + v316;
    __asm
    {
      vpshufd xmm6, xmm6, 80h
      rorx    eax, r14d, 19h
      rorx    ecx, r14d, 0Bh
    }
    v336 = DWORD2(v1492) + _ER9;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    edi, r10d, 0Dh
    }
    v339 = _ECX ^ _EAX;
    __asm
    {
      vpsrldq xmm4, xmm4, 8
      rorx    ecx, r14d, 6
    }
    v342 = _ECX ^ v339;
    __asm
    {
      rorx    ecx, r10d, 16h
      vpaddd  xmm3, xmm3, xmm4
    }
    v345 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r10d, 2
      vunpcklps xmm7, xmm3, xmm3
      vpsrld  xmm4, xmm3, 0Ah
    }
    v349 = v342 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
    __asm { vpsrlq  xmm6, xmm7, 11h }
    _ER13 = v349 + v336 + _ER13;
    _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v349 + (_EDI ^ v345) + v336;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    eax, r13d, 19h
      rorx    ecx, r13d, 0Bh
    }
    v356 = DWORD3(v1492) + _ER8;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    edi, r9d, 0Dh
    }
    v359 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r13d, 6
      vpshufd xmm6, xmm6, 8
    }
    v362 = _ECX ^ v359;
    __asm
    {
      rorx    ecx, r9d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v365 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r9d, 2
      vpslldq xmm4, xmm4, 8
    }
    v368 = v362 + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    _ER12 = v368 + v356 + _ER12;
    __asm { vpaddd  xmm4, xmm3, xmmword ptr [rbx] }
    _ER8 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v368 + (_EDI ^ v365) + v356;
    __asm
    {
      vmovdqa [rsp+118h+var_E8], xmm4
      vpalignr xmm6, xmm3, xmm2, 4
      rorx    eax, r12d, 19h
      rorx    ecx, r12d, 0Bh
      vpalignr xmm4, xmm1, xmm0, 4
    }
    v378 = v118 + _ER15;
    __asm
    {
      rorx    edi, r8d, 0Dh
      vpaddd  xmm0, xmm0, xmm6
    }
    v381 = _ECX ^ _EAX;
    __asm
    {
      vpslld  xmm7, xmm4, 0Eh
      rorx    ecx, r12d, 6
    }
    v384 = _ECX ^ v381;
    __asm
    {
      vpsrld  xmm6, xmm4, 7
      rorx    ecx, r8d, 16h
    }
    v387 = _EDI ^ _ECX;
    __asm
    {
      vpsrld  xmm4, xmm4, 3
      rorx    edi, r8d, 2
      vpxor   xmm4, xmm4, xmm7
    }
    v391 = v384 + (_ER14 ^ _ER12 & (_ER14 ^ _ER13));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER11 = v391 + v378 + _ER11;
    _ER15 = (_ER10 & _ER8 | _ER9 & (_ER10 | _ER8)) + v391 + (_EDI ^ v387) + v378;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    eax, r11d, 19h
      rorx    ecx, r11d, 0Bh
      vpsrld  xmm6, xmm6, 0Bh
    }
    v399 = DWORD1(v118) + _ER14;
    __asm
    {
      rorx    edi, r15d, 0Dh
      vpxor   xmm4, xmm4, xmm7
    }
    v402 = _ECX ^ _EAX;
    __asm { rorx    ecx, r11d, 6 }
    v404 = _ECX ^ v402;
    __asm
    {
      rorx    ecx, r15d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v407 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r15d, 2
      vpaddd  xmm0, xmm0, xmm4
    }
    v410 = v404 + (_ER13 ^ _ER11 & (_ER13 ^ _ER12));
    __asm { vunpckhps xmm7, xmm3, xmm3 }
    _ER10 = v410 + v399 + _ER10;
    _ER14 = (_ER9 & _ER15 | _ER8 & (_ER9 | _ER15)) + v410 + (_EDI ^ v407) + v399;
    __asm
    {
      vpsrld  xmm4, xmm3, 0Ah
      vpsrlq  xmm6, xmm7, 11h
      rorx    eax, r10d, 19h
      rorx    ecx, r10d, 0Bh
    }
    v418 = DWORD2(v118) + _ER13;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    edi, r14d, 0Dh
    }
    v421 = _ECX ^ _EAX;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    ecx, r10d, 6
    }
    v424 = _ECX ^ v421;
    __asm
    {
      rorx    ecx, r14d, 16h
      vpshufd xmm6, xmm6, 80h
    }
    v427 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r14d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrldq xmm4, xmm4, 8
    }
    v431 = v424 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    _ER9 = v431 + v418 + _ER9;
    _ER13 = (_ER8 & _ER14 | _ER15 & (_ER8 | _ER14)) + v431 + (_EDI ^ v427) + v418;
    __asm
    {
      vunpcklps xmm7, xmm0, xmm0
      rorx    eax, r9d, 19h
      rorx    ecx, r9d, 0Bh
      vpsrld  xmm4, xmm0, 0Ah
    }
    v439 = DWORD3(v118) + _ER12;
    __asm
    {
      vpsrlq  xmm6, xmm7, 11h
      rorx    edi, r13d, 0Dh
    }
    v442 = _ECX ^ _EAX;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    ecx, r9d, 6
    }
    v445 = _ECX ^ v442;
    __asm
    {
      rorx    ecx, r13d, 16h
      vpxor   xmm6, xmm6, xmm7
    }
    v448 = _EDI ^ _ECX;
    __asm
    {
      vpshufd xmm6, xmm6, 8
      rorx    edi, r13d, 2
      vpxor   xmm4, xmm4, xmm6
    }
    v452 = v445 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
    __asm { vpslldq xmm4, xmm4, 8 }
    _ER8 = v452 + v439 + _ER8;
    __asm
    {
      vpaddd  xmm0, xmm0, xmm4
      vpaddd  xmm4, xmm0, xmmword ptr [rbx]
    }
    _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v452 + (_EDI ^ v448) + v439;
    __asm
    {
      vmovdqa [rsp+118h+var_118], xmm4
      vpalignr xmm6, xmm0, xmm3, 4
      rorx    eax, r8d, 19h
      rorx    ecx, r8d, 0Bh
      vpalignr xmm4, xmm2, xmm1, 4
    }
    v463 = v203 + _ER11;
    __asm
    {
      rorx    edi, r12d, 0Dh
      vpslld  xmm7, xmm4, 0Eh
    }
    v466 = _ECX ^ _EAX;
    __asm
    {
      vpaddd  xmm1, xmm1, xmm6
      rorx    ecx, r8d, 6
      vpsrld  xmm6, xmm4, 7
    }
    v470 = _ECX ^ v466;
    __asm
    {
      rorx    ecx, r12d, 16h
      vpsrld  xmm4, xmm4, 3
    }
    v473 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r12d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm7
    }
    v478 = v470 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER15 = v478 + v463 + _ER15;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v478 + (_EDI ^ v473) + v463;
    __asm
    {
      rorx    eax, r15d, 19h
      vpxor   xmm4, xmm4, xmm7
      rorx    ecx, r15d, 0Bh
    }
    v486 = DWORD1(v203) + _ER10;
    __asm
    {
      rorx    edi, r11d, 0Dh
      vpaddd  xmm1, xmm1, xmm4
    }
    v489 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r15d, 6
      vunpckhps xmm7, xmm0, xmm0
    }
    v492 = _ECX ^ v489;
    __asm
    {
      rorx    ecx, r11d, 16h
      vpsrld  xmm4, xmm0, 0Ah
    }
    v495 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r11d, 2
      vpsrlq  xmm6, xmm7, 11h
      vpsrlq  xmm7, xmm7, 13h
    }
    v499 = v492 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
    __asm { vpxor   xmm6, xmm6, xmm7 }
    _ER14 = v499 + v486 + _ER14;
    _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v499 + (_EDI ^ v495) + v486;
    __asm
    {
      vpshufd xmm6, xmm6, 80h
      rorx    eax, r14d, 19h
      rorx    ecx, r14d, 0Bh
    }
    v506 = DWORD2(v203) + _ER9;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    edi, r10d, 0Dh
    }
    v509 = _ECX ^ _EAX;
    __asm
    {
      vpsrldq xmm4, xmm4, 8
      rorx    ecx, r14d, 6
    }
    v512 = _ECX ^ v509;
    __asm
    {
      rorx    ecx, r10d, 16h
      vpaddd  xmm1, xmm1, xmm4
    }
    v515 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r10d, 2
      vunpcklps xmm7, xmm1, xmm1
      vpsrld  xmm4, xmm1, 0Ah
    }
    v519 = v512 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
    __asm { vpsrlq  xmm6, xmm7, 11h }
    _ER13 = v519 + v506 + _ER13;
    _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v519 + (_EDI ^ v515) + v506;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    eax, r13d, 19h
      rorx    ecx, r13d, 0Bh
    }
    v526 = DWORD3(v203) + _ER8;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    edi, r9d, 0Dh
    }
    v529 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r13d, 6
      vpshufd xmm6, xmm6, 8
    }
    v532 = _ECX ^ v529;
    __asm
    {
      rorx    ecx, r9d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v535 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r9d, 2
      vpslldq xmm4, xmm4, 8
    }
    v538 = v532 + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    _ER12 = v538 + v526 + _ER12;
    __asm { vpaddd  xmm4, xmm1, xmmword ptr [rbx] }
    _ER8 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v538 + (_EDI ^ v535) + v526;
    __asm
    {
      vmovdqa [rsp+118h+var_108], xmm4
      vpalignr xmm6, xmm1, xmm0, 4
      rorx    eax, r12d, 19h
      rorx    ecx, r12d, 0Bh
      vpalignr xmm4, xmm3, xmm2, 4
    }
    v548 = v288 + _ER15;
    __asm
    {
      rorx    edi, r8d, 0Dh
      vpaddd  xmm2, xmm2, xmm6
    }
    v551 = _ECX ^ _EAX;
    __asm
    {
      vpslld  xmm7, xmm4, 0Eh
      rorx    ecx, r12d, 6
    }
    v554 = _ECX ^ v551;
    __asm
    {
      vpsrld  xmm6, xmm4, 7
      rorx    ecx, r8d, 16h
    }
    v557 = _EDI ^ _ECX;
    __asm
    {
      vpsrld  xmm4, xmm4, 3
      rorx    edi, r8d, 2
      vpxor   xmm4, xmm4, xmm7
    }
    v561 = v554 + (_ER14 ^ _ER12 & (_ER14 ^ _ER13));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER11 = v561 + v548 + _ER11;
    _ER15 = (_ER10 & _ER8 | _ER9 & (_ER10 | _ER8)) + v561 + (_EDI ^ v557) + v548;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    eax, r11d, 19h
      rorx    ecx, r11d, 0Bh
      vpsrld  xmm6, xmm6, 0Bh
    }
    v569 = DWORD1(v288) + _ER14;
    __asm
    {
      rorx    edi, r15d, 0Dh
      vpxor   xmm4, xmm4, xmm7
    }
    v572 = _ECX ^ _EAX;
    __asm { rorx    ecx, r11d, 6 }
    v574 = _ECX ^ v572;
    __asm
    {
      rorx    ecx, r15d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v577 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r15d, 2
      vpaddd  xmm2, xmm2, xmm4
    }
    v580 = v574 + (_ER13 ^ _ER11 & (_ER13 ^ _ER12));
    __asm { vunpckhps xmm7, xmm1, xmm1 }
    _ER10 = v580 + v569 + _ER10;
    _ER14 = (_ER9 & _ER15 | _ER8 & (_ER9 | _ER15)) + v580 + (_EDI ^ v577) + v569;
    __asm
    {
      vpsrld  xmm4, xmm1, 0Ah
      vpsrlq  xmm6, xmm7, 11h
      rorx    eax, r10d, 19h
      rorx    ecx, r10d, 0Bh
    }
    v588 = DWORD2(v288) + _ER13;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    edi, r14d, 0Dh
    }
    v591 = _ECX ^ _EAX;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    ecx, r10d, 6
    }
    v594 = _ECX ^ v591;
    __asm
    {
      rorx    ecx, r14d, 16h
      vpshufd xmm6, xmm6, 80h
    }
    v597 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r14d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrldq xmm4, xmm4, 8
    }
    v601 = v594 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    _ER9 = v601 + v588 + _ER9;
    _ER13 = (_ER8 & _ER14 | _ER15 & (_ER8 | _ER14)) + v601 + (_EDI ^ v597) + v588;
    __asm
    {
      vunpcklps xmm7, xmm2, xmm2
      rorx    eax, r9d, 19h
      rorx    ecx, r9d, 0Bh
      vpsrld  xmm4, xmm2, 0Ah
    }
    v609 = DWORD3(v288) + _ER12;
    __asm
    {
      vpsrlq  xmm6, xmm7, 11h
      rorx    edi, r13d, 0Dh
    }
    v612 = _ECX ^ _EAX;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    ecx, r9d, 6
    }
    v615 = _ECX ^ v612;
    __asm
    {
      rorx    ecx, r13d, 16h
      vpxor   xmm6, xmm6, xmm7
    }
    v618 = _EDI ^ _ECX;
    __asm
    {
      vpshufd xmm6, xmm6, 8
      rorx    edi, r13d, 2
      vpxor   xmm4, xmm4, xmm6
    }
    v622 = v615 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
    __asm { vpslldq xmm4, xmm4, 8 }
    _ER8 = v622 + v609 + _ER8;
    __asm
    {
      vpaddd  xmm2, xmm2, xmm4
      vpaddd  xmm4, xmm2, xmmword ptr [rbx]
    }
    _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v622 + (_EDI ^ v618) + v609;
    __asm
    {
      vmovdqa [rsp+118h+var_F8], xmm4
      vpalignr xmm6, xmm2, xmm1, 4
      rorx    eax, r8d, 19h
      rorx    ecx, r8d, 0Bh
      vpalignr xmm4, xmm0, xmm3, 4
    }
    v633 = v373 + _ER11;
    __asm
    {
      rorx    edi, r12d, 0Dh
      vpslld  xmm7, xmm4, 0Eh
    }
    v636 = _ECX ^ _EAX;
    __asm
    {
      vpaddd  xmm3, xmm3, xmm6
      rorx    ecx, r8d, 6
      vpsrld  xmm6, xmm4, 7
    }
    v640 = _ECX ^ v636;
    __asm
    {
      rorx    ecx, r12d, 16h
      vpsrld  xmm4, xmm4, 3
    }
    v643 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r12d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm7
    }
    v648 = v640 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER15 = v648 + v633 + _ER15;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v648 + (_EDI ^ v643) + v633;
    __asm
    {
      rorx    eax, r15d, 19h
      vpxor   xmm4, xmm4, xmm7
      rorx    ecx, r15d, 0Bh
    }
    v656 = DWORD1(v373) + _ER10;
    __asm
    {
      rorx    edi, r11d, 0Dh
      vpaddd  xmm3, xmm3, xmm4
    }
    v659 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r15d, 6
      vunpckhps xmm7, xmm2, xmm2
    }
    v662 = _ECX ^ v659;
    __asm
    {
      rorx    ecx, r11d, 16h
      vpsrld  xmm4, xmm2, 0Ah
    }
    v665 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r11d, 2
      vpsrlq  xmm6, xmm7, 11h
      vpsrlq  xmm7, xmm7, 13h
    }
    v669 = v662 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
    __asm { vpxor   xmm6, xmm6, xmm7 }
    _ER14 = v669 + v656 + _ER14;
    _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v669 + (_EDI ^ v665) + v656;
    __asm
    {
      vpshufd xmm6, xmm6, 80h
      rorx    eax, r14d, 19h
      rorx    ecx, r14d, 0Bh
    }
    v676 = DWORD2(v373) + _ER9;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    edi, r10d, 0Dh
    }
    v679 = _ECX ^ _EAX;
    __asm
    {
      vpsrldq xmm4, xmm4, 8
      rorx    ecx, r14d, 6
    }
    v682 = _ECX ^ v679;
    __asm
    {
      rorx    ecx, r10d, 16h
      vpaddd  xmm3, xmm3, xmm4
    }
    v685 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r10d, 2
      vunpcklps xmm7, xmm3, xmm3
      vpsrld  xmm4, xmm3, 0Ah
    }
    v689 = v682 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
    __asm { vpsrlq  xmm6, xmm7, 11h }
    _ER13 = v689 + v676 + _ER13;
    _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v689 + (_EDI ^ v685) + v676;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    eax, r13d, 19h
      rorx    ecx, r13d, 0Bh
    }
    v696 = DWORD3(v373) + _ER8;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    edi, r9d, 0Dh
    }
    v699 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r13d, 6
      vpshufd xmm6, xmm6, 8
    }
    v702 = _ECX ^ v699;
    __asm
    {
      rorx    ecx, r9d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v705 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r9d, 2
      vpslldq xmm4, xmm4, 8
    }
    v708 = v702 + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    _ER12 = v708 + v696 + _ER12;
    __asm { vpaddd  xmm4, xmm3, xmmword ptr [rbx] }
    _ER8 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v708 + (_EDI ^ v705) + v696;
    __asm
    {
      vmovdqa [rsp+118h+var_E8], xmm4
      vpalignr xmm6, xmm3, xmm2, 4
      rorx    eax, r12d, 19h
      rorx    ecx, r12d, 0Bh
      vpalignr xmm4, xmm1, xmm0, 4
    }
    v718 = v458 + _ER15;
    __asm
    {
      rorx    edi, r8d, 0Dh
      vpaddd  xmm0, xmm0, xmm6
    }
    v721 = _ECX ^ _EAX;
    __asm
    {
      vpslld  xmm7, xmm4, 0Eh
      rorx    ecx, r12d, 6
    }
    v724 = _ECX ^ v721;
    __asm
    {
      vpsrld  xmm6, xmm4, 7
      rorx    ecx, r8d, 16h
    }
    v727 = _EDI ^ _ECX;
    __asm
    {
      vpsrld  xmm4, xmm4, 3
      rorx    edi, r8d, 2
      vpxor   xmm4, xmm4, xmm7
    }
    v731 = v724 + (_ER14 ^ _ER12 & (_ER14 ^ _ER13));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER11 = v731 + v718 + _ER11;
    _ER15 = (_ER10 & _ER8 | _ER9 & (_ER10 | _ER8)) + v731 + (_EDI ^ v727) + v718;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    eax, r11d, 19h
      rorx    ecx, r11d, 0Bh
      vpsrld  xmm6, xmm6, 0Bh
    }
    v739 = DWORD1(v458) + _ER14;
    __asm
    {
      rorx    edi, r15d, 0Dh
      vpxor   xmm4, xmm4, xmm7
    }
    v742 = _ECX ^ _EAX;
    __asm { rorx    ecx, r11d, 6 }
    v744 = _ECX ^ v742;
    __asm
    {
      rorx    ecx, r15d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v747 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r15d, 2
      vpaddd  xmm0, xmm0, xmm4
    }
    v750 = v744 + (_ER13 ^ _ER11 & (_ER13 ^ _ER12));
    __asm { vunpckhps xmm7, xmm3, xmm3 }
    _ER10 = v750 + v739 + _ER10;
    _ER14 = (_ER9 & _ER15 | _ER8 & (_ER9 | _ER15)) + v750 + (_EDI ^ v747) + v739;
    __asm
    {
      vpsrld  xmm4, xmm3, 0Ah
      vpsrlq  xmm6, xmm7, 11h
      rorx    eax, r10d, 19h
      rorx    ecx, r10d, 0Bh
    }
    v758 = DWORD2(v458) + _ER13;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    edi, r14d, 0Dh
    }
    v761 = _ECX ^ _EAX;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    ecx, r10d, 6
    }
    v764 = _ECX ^ v761;
    __asm
    {
      rorx    ecx, r14d, 16h
      vpshufd xmm6, xmm6, 80h
    }
    v767 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r14d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrldq xmm4, xmm4, 8
    }
    v771 = v764 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
    __asm { vpaddd  xmm0, xmm0, xmm4 }
    _ER9 = v771 + v758 + _ER9;
    _ER13 = (_ER8 & _ER14 | _ER15 & (_ER8 | _ER14)) + v771 + (_EDI ^ v767) + v758;
    __asm
    {
      vunpcklps xmm7, xmm0, xmm0
      rorx    eax, r9d, 19h
      rorx    ecx, r9d, 0Bh
      vpsrld  xmm4, xmm0, 0Ah
    }
    v779 = DWORD3(v458) + _ER12;
    __asm
    {
      vpsrlq  xmm6, xmm7, 11h
      rorx    edi, r13d, 0Dh
    }
    v782 = _ECX ^ _EAX;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    ecx, r9d, 6
    }
    v785 = _ECX ^ v782;
    __asm
    {
      rorx    ecx, r13d, 16h
      vpxor   xmm6, xmm6, xmm7
    }
    v788 = _EDI ^ _ECX;
    __asm
    {
      vpshufd xmm6, xmm6, 8
      rorx    edi, r13d, 2
      vpxor   xmm4, xmm4, xmm6
    }
    v792 = v785 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
    __asm { vpslldq xmm4, xmm4, 8 }
    _ER8 = v792 + v779 + _ER8;
    __asm
    {
      vpaddd  xmm0, xmm0, xmm4
      vpaddd  xmm4, xmm0, xmmword ptr [rbx]
    }
    _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v792 + (_EDI ^ v788) + v779;
    __asm
    {
      vmovdqa [rsp+118h+var_118], xmm4
      vpalignr xmm6, xmm0, xmm3, 4
      rorx    eax, r8d, 19h
      rorx    ecx, r8d, 0Bh
      vpalignr xmm4, xmm2, xmm1, 4
    }
    v802 = v543 + _ER11;
    __asm
    {
      rorx    edi, r12d, 0Dh
      vpslld  xmm7, xmm4, 0Eh
    }
    v805 = _ECX ^ _EAX;
    __asm
    {
      vpaddd  xmm1, xmm1, xmm6
      rorx    ecx, r8d, 6
      vpsrld  xmm6, xmm4, 7
    }
    v809 = _ECX ^ v805;
    __asm
    {
      rorx    ecx, r12d, 16h
      vpsrld  xmm4, xmm4, 3
    }
    v812 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r12d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm7
    }
    v817 = v809 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER15 = v817 + v802 + _ER15;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v817 + (_EDI ^ v812) + v802;
    __asm
    {
      rorx    eax, r15d, 19h
      vpxor   xmm4, xmm4, xmm7
      rorx    ecx, r15d, 0Bh
    }
    v825 = DWORD1(v543) + _ER10;
    __asm
    {
      rorx    edi, r11d, 0Dh
      vpaddd  xmm1, xmm1, xmm4
    }
    v828 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r15d, 6
      vunpckhps xmm7, xmm0, xmm0
    }
    v831 = _ECX ^ v828;
    __asm
    {
      rorx    ecx, r11d, 16h
      vpsrld  xmm4, xmm0, 0Ah
    }
    v834 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r11d, 2
      vpsrlq  xmm6, xmm7, 11h
      vpsrlq  xmm7, xmm7, 13h
    }
    v838 = v831 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
    __asm { vpxor   xmm6, xmm6, xmm7 }
    _ER14 = v838 + v825 + _ER14;
    _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v838 + (_EDI ^ v834) + v825;
    __asm
    {
      vpshufd xmm6, xmm6, 80h
      rorx    eax, r14d, 19h
      rorx    ecx, r14d, 0Bh
    }
    v845 = DWORD2(v543) + _ER9;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    edi, r10d, 0Dh
    }
    v848 = _ECX ^ _EAX;
    __asm
    {
      vpsrldq xmm4, xmm4, 8
      rorx    ecx, r14d, 6
    }
    v851 = _ECX ^ v848;
    __asm
    {
      rorx    ecx, r10d, 16h
      vpaddd  xmm1, xmm1, xmm4
    }
    v854 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r10d, 2
      vunpcklps xmm7, xmm1, xmm1
      vpsrld  xmm4, xmm1, 0Ah
    }
    v858 = v851 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
    __asm { vpsrlq  xmm6, xmm7, 11h }
    _ER13 = v858 + v845 + _ER13;
    _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v858 + (_EDI ^ v854) + v845;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    eax, r13d, 19h
      rorx    ecx, r13d, 0Bh
    }
    v865 = DWORD3(v543) + _ER8;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    edi, r9d, 0Dh
    }
    v868 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r13d, 6
      vpshufd xmm6, xmm6, 8
    }
    v871 = _ECX ^ v868;
    __asm
    {
      rorx    ecx, r9d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v874 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r9d, 2
      vpslldq xmm4, xmm4, 8
    }
    v877 = v871 + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
    __asm { vpaddd  xmm1, xmm1, xmm4 }
    _ER12 = v877 + v865 + _ER12;
    __asm { vpaddd  xmm4, xmm1, xmmword ptr [rbx] }
    _ER8 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v877 + (_EDI ^ v874) + v865;
    __asm
    {
      vmovdqa [rsp+118h+var_108], xmm4
      vpalignr xmm6, xmm1, xmm0, 4
      rorx    eax, r12d, 19h
      rorx    ecx, r12d, 0Bh
      vpalignr xmm4, xmm3, xmm2, 4
    }
    v886 = v628 + _ER15;
    __asm
    {
      rorx    edi, r8d, 0Dh
      vpaddd  xmm2, xmm2, xmm6
    }
    v889 = _ECX ^ _EAX;
    __asm
    {
      vpslld  xmm7, xmm4, 0Eh
      rorx    ecx, r12d, 6
    }
    v892 = _ECX ^ v889;
    __asm
    {
      vpsrld  xmm6, xmm4, 7
      rorx    ecx, r8d, 16h
    }
    v895 = _EDI ^ _ECX;
    __asm
    {
      vpsrld  xmm4, xmm4, 3
      rorx    edi, r8d, 2
      vpxor   xmm4, xmm4, xmm7
    }
    v899 = v892 + (_ER14 ^ _ER12 & (_ER14 ^ _ER13));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER11 = v899 + v886 + _ER11;
    _ER15 = (_ER10 & _ER8 | _ER9 & (_ER10 | _ER8)) + v899 + (_EDI ^ v895) + v886;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    eax, r11d, 19h
      rorx    ecx, r11d, 0Bh
      vpsrld  xmm6, xmm6, 0Bh
    }
    v907 = DWORD1(v628) + _ER14;
    __asm
    {
      rorx    edi, r15d, 0Dh
      vpxor   xmm4, xmm4, xmm7
    }
    v910 = _ECX ^ _EAX;
    __asm { rorx    ecx, r11d, 6 }
    v912 = _ECX ^ v910;
    __asm
    {
      rorx    ecx, r15d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v915 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r15d, 2
      vpaddd  xmm2, xmm2, xmm4
    }
    v918 = v912 + (_ER13 ^ _ER11 & (_ER13 ^ _ER12));
    __asm { vunpckhps xmm7, xmm1, xmm1 }
    _ER10 = v918 + v907 + _ER10;
    _ER14 = (_ER9 & _ER15 | _ER8 & (_ER9 | _ER15)) + v918 + (_EDI ^ v915) + v907;
    __asm
    {
      vpsrld  xmm4, xmm1, 0Ah
      vpsrlq  xmm6, xmm7, 11h
      rorx    eax, r10d, 19h
      rorx    ecx, r10d, 0Bh
    }
    v926 = DWORD2(v628) + _ER13;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    edi, r14d, 0Dh
    }
    v929 = _ECX ^ _EAX;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    ecx, r10d, 6
    }
    v932 = _ECX ^ v929;
    __asm
    {
      rorx    ecx, r14d, 16h
      vpshufd xmm6, xmm6, 80h
    }
    v935 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r14d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrldq xmm4, xmm4, 8
    }
    v939 = v932 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
    __asm { vpaddd  xmm2, xmm2, xmm4 }
    _ER9 = v939 + v926 + _ER9;
    _ER13 = (_ER8 & _ER14 | _ER15 & (_ER8 | _ER14)) + v939 + (_EDI ^ v935) + v926;
    __asm
    {
      vunpcklps xmm7, xmm2, xmm2
      rorx    eax, r9d, 19h
      rorx    ecx, r9d, 0Bh
      vpsrld  xmm4, xmm2, 0Ah
    }
    v947 = DWORD3(v628) + _ER12;
    __asm
    {
      vpsrlq  xmm6, xmm7, 11h
      rorx    edi, r13d, 0Dh
    }
    v950 = _ECX ^ _EAX;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    ecx, r9d, 6
    }
    v953 = _ECX ^ v950;
    __asm
    {
      rorx    ecx, r13d, 16h
      vpxor   xmm6, xmm6, xmm7
    }
    v956 = _EDI ^ _ECX;
    __asm
    {
      vpshufd xmm6, xmm6, 8
      rorx    edi, r13d, 2
      vpxor   xmm4, xmm4, xmm6
    }
    v960 = v953 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
    __asm { vpslldq xmm4, xmm4, 8 }
    _ER8 = v960 + v947 + _ER8;
    __asm
    {
      vpaddd  xmm2, xmm2, xmm4
      vpaddd  xmm4, xmm2, xmmword ptr [rbx]
    }
    _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v960 + (_EDI ^ v956) + v947;
    __asm
    {
      vmovdqa [rsp+118h+var_F8], xmm4
      vpalignr xmm6, xmm2, xmm1, 4
      rorx    eax, r8d, 19h
      rorx    ecx, r8d, 0Bh
      vpalignr xmm4, xmm0, xmm3, 4
    }
    v970 = v713 + _ER11;
    __asm
    {
      rorx    edi, r12d, 0Dh
      vpslld  xmm7, xmm4, 0Eh
    }
    v973 = _ECX ^ _EAX;
    __asm
    {
      vpaddd  xmm3, xmm3, xmm6
      rorx    ecx, r8d, 6
      vpsrld  xmm6, xmm4, 7
    }
    v977 = _ECX ^ v973;
    __asm
    {
      rorx    ecx, r12d, 16h
      vpsrld  xmm4, xmm4, 3
    }
    v980 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r12d, 2
      vpxor   xmm4, xmm4, xmm6
      vpsrld  xmm6, xmm6, 0Bh
      vpxor   xmm4, xmm4, xmm7
    }
    v985 = v977 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
    __asm { vpslld  xmm7, xmm7, 0Bh }
    _ER15 = v985 + v970 + _ER15;
    __asm { vpxor   xmm4, xmm4, xmm6 }
    _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v985 + (_EDI ^ v980) + v970;
    __asm
    {
      rorx    eax, r15d, 19h
      vpxor   xmm4, xmm4, xmm7
      rorx    ecx, r15d, 0Bh
    }
    v993 = DWORD1(v713) + _ER10;
    __asm
    {
      rorx    edi, r11d, 0Dh
      vpaddd  xmm3, xmm3, xmm4
    }
    v996 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r15d, 6
      vunpckhps xmm7, xmm2, xmm2
    }
    v999 = _ECX ^ v996;
    __asm
    {
      rorx    ecx, r11d, 16h
      vpsrld  xmm4, xmm2, 0Ah
    }
    v1002 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r11d, 2
      vpsrlq  xmm6, xmm7, 11h
      vpsrlq  xmm7, xmm7, 13h
    }
    v1006 = v999 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
    __asm { vpxor   xmm6, xmm6, xmm7 }
    _ER14 = v1006 + v993 + _ER14;
    _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v1006 + (_EDI ^ v1002) + v993;
    __asm
    {
      vpshufd xmm6, xmm6, 80h
      rorx    eax, r14d, 19h
      rorx    ecx, r14d, 0Bh
    }
    v1013 = DWORD2(v713) + _ER9;
    __asm
    {
      vpxor   xmm4, xmm4, xmm6
      rorx    edi, r10d, 0Dh
    }
    v1016 = _ECX ^ _EAX;
    __asm
    {
      vpsrldq xmm4, xmm4, 8
      rorx    ecx, r14d, 6
    }
    v1019 = _ECX ^ v1016;
    __asm
    {
      rorx    ecx, r10d, 16h
      vpaddd  xmm3, xmm3, xmm4
    }
    v1022 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r10d, 2
      vunpcklps xmm7, xmm3, xmm3
      vpsrld  xmm4, xmm3, 0Ah
    }
    v1026 = v1019 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
    __asm { vpsrlq  xmm6, xmm7, 11h }
    _ER13 = v1026 + v1013 + _ER13;
    _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v1026 + (_EDI ^ v1022) + v1013;
    __asm
    {
      vpsrlq  xmm7, xmm7, 13h
      rorx    eax, r13d, 19h
      rorx    ecx, r13d, 0Bh
    }
    v1032 = DWORD3(v713) + _ER8;
    __asm
    {
      vpxor   xmm6, xmm6, xmm7
      rorx    edi, r9d, 0Dh
    }
    v1035 = _ECX ^ _EAX;
    __asm
    {
      rorx    ecx, r13d, 6
      vpshufd xmm6, xmm6, 8
    }
    v1037 = _ECX ^ v1035;
    __asm
    {
      rorx    ecx, r9d, 16h
      vpxor   xmm4, xmm4, xmm6
    }
    v1040 = _EDI ^ _ECX;
    __asm
    {
      rorx    edi, r9d, 2
      vpslldq xmm4, xmm4, 8
    }
    v1043 = v1037 + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
    __asm { vpaddd  xmm3, xmm3, xmm4 }
    _ER12 = v1043 + v1032 + _ER12;
    __asm { vpaddd  xmm4, xmm3, xmmword ptr [rbx] }
    _ER8 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v1043 + (_EDI ^ v1040) + v1032;
    __asm { vmovdqa [rsp+118h+var_E8], xmm4 }
    v1048 = v27 - 64;
    --v1511;
    if ( !v1511 )
      break;
    __asm
    {
      vmovdqu xmm0, xmmword ptr [rdx]
      rorx    eax, r12d, 19h
      rorx    ecx, r12d, 0Bh
    }
    v1052 = v1484 + _ER15;
    __asm { rorx    edi, r8d, 0Dh }
    v1054 = _ECX ^ _EAX;
    __asm { rorx    ecx, r12d, 6 }
    v1056 = _ECX ^ v1054;
    __asm { rorx    ecx, r8d, 16h }
    v1058 = _EDI ^ _ECX;
    __asm { rorx    edi, r8d, 2 }
    v1060 = v1056 + (_ER14 ^ _ER12 & (_ER14 ^ _ER13));
    _ER11 = v1060 + v1052 + _ER11;
    _ER15 = (_ER10 & _ER8 | _ER9 & (_ER10 | _ER8)) + v1060 + (_EDI ^ v1058) + v1052;
    __asm
    {
      vpshufb xmm0, xmm0, [rsp+118h+var_D8]
      rorx    eax, r11d, 19h
      rorx    ecx, r11d, 0Bh
    }
    v1065 = DWORD1(v1484) + _ER14;
    __asm { rorx    edi, r15d, 0Dh }
    v1067 = _ECX ^ _EAX;
    __asm { rorx    ecx, r11d, 6 }
    v1069 = _ECX ^ v1067;
    __asm { rorx    ecx, r15d, 16h }
    v1071 = _EDI ^ _ECX;
    __asm { rorx    edi, r15d, 2 }
    v1073 = v1069 + (_ER13 ^ _ER11 & (_ER13 ^ _ER12));
    _ER10 = v1073 + v1065 + _ER10;
    _ER14 = (_ER9 & _ER15 | _ER8 & (_ER9 | _ER15)) + v1073 + (_EDI ^ v1071) + v1065;
    __asm
    {
      rorx    eax, r10d, 19h
      rorx    ecx, r10d, 0Bh
    }
    v1078 = DWORD2(v1484) + _ER13;
    __asm { rorx    edi, r14d, 0Dh }
    v1080 = _ECX ^ _EAX;
    __asm { rorx    ecx, r10d, 6 }
    v1082 = _ECX ^ v1080;
    __asm { rorx    ecx, r14d, 16h }
    v1084 = _EDI ^ _ECX;
    __asm { rorx    edi, r14d, 2 }
    v1086 = v1082 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
    _ER9 = v1086 + v1078 + _ER9;
    _ER13 = (_ER8 & _ER14 | _ER15 & (_ER8 | _ER14)) + v1086 + (_EDI ^ v1084) + v1078;
    __asm
    {
      vpaddd  xmm4, xmm0, xmmword ptr [rbx]
      rorx    eax, r9d, 19h
      rorx    ecx, r9d, 0Bh
    }
    v1092 = DWORD3(v1484) + _ER12;
    __asm { rorx    edi, r13d, 0Dh }
    v1094 = _ECX ^ _EAX;
    __asm { rorx    ecx, r9d, 6 }
    v1096 = _ECX ^ v1094;
    __asm { rorx    ecx, r13d, 16h }
    v1098 = _EDI ^ _ECX;
    __asm { rorx    edi, r13d, 2 }
    v1100 = v1096 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
    _ER8 = v1100 + v1092 + _ER8;
    _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v1100 + (_EDI ^ v1098) + v1092;
    __asm
    {
      vmovdqa [rsp+118h+var_118], xmm4
      vmovdqu xmm1, xmmword ptr [rdx+10h]
      rorx    eax, r8d, 19h
      rorx    ecx, r8d, 0Bh
    }
    v1106 = v1487 + _ER11;
    __asm { rorx    edi, r12d, 0Dh }
    v1108 = _ECX ^ _EAX;
    __asm { rorx    ecx, r8d, 6 }
    v1110 = _ECX ^ v1108;
    __asm { rorx    ecx, r12d, 16h }
    v1112 = _EDI ^ _ECX;
    __asm { rorx    edi, r12d, 2 }
    v1114 = v1110 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
    _ER15 = v1114 + v1106 + _ER15;
    _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v1114 + (_EDI ^ v1112) + v1106;
    __asm
    {
      vpshufb xmm1, xmm1, [rsp+118h+var_D8]
      rorx    eax, r15d, 19h
      rorx    ecx, r15d, 0Bh
    }
    v1119 = DWORD1(v1487) + _ER10;
    __asm { rorx    edi, r11d, 0Dh }
    v1121 = _ECX ^ _EAX;
    __asm { rorx    ecx, r15d, 6 }
    v1123 = _ECX ^ v1121;
    __asm { rorx    ecx, r11d, 16h }
    v1125 = _EDI ^ _ECX;
    __asm { rorx    edi, r11d, 2 }
    v1127 = v1123 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
    _ER14 = v1127 + v1119 + _ER14;
    _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v1127 + (_EDI ^ v1125) + v1119;
    __asm
    {
      rorx    eax, r14d, 19h
      rorx    ecx, r14d, 0Bh
    }
    v1132 = DWORD2(v1487) + _ER9;
    __asm { rorx    edi, r10d, 0Dh }
    v1134 = _ECX ^ _EAX;
    __asm { rorx    ecx, r14d, 6 }
    v1136 = _ECX ^ v1134;
    __asm { rorx    ecx, r10d, 16h }
    v1138 = _EDI ^ _ECX;
    __asm { rorx    edi, r10d, 2 }
    v1140 = v1136 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
    _ER13 = v1140 + v1132 + _ER13;
    _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v1140 + (_EDI ^ v1138) + v1132;
    __asm
    {
      vpaddd  xmm4, xmm1, xmmword ptr [rbx+10h]
      rorx    eax, r13d, 19h
      rorx    ecx, r13d, 0Bh
    }
    v1146 = DWORD3(v1487) + _ER8;
    __asm { rorx    edi, r9d, 0Dh }
    v1148 = _ECX ^ _EAX;
    __asm { rorx    ecx, r13d, 6 }
    v1150 = _ECX ^ v1148;
    __asm { rorx    ecx, r9d, 16h }
    v1152 = _EDI ^ _ECX;
    __asm { rorx    edi, r9d, 2 }
    v1154 = v1150 + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
    _ER12 = v1154 + v1146 + _ER12;
    _ER8 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v1154 + (_EDI ^ v1152) + v1146;
    __asm
    {
      vmovdqa [rsp+118h+var_108], xmm4
      vmovdqu xmm2, xmmword ptr [rdx+20h]
      rorx    eax, r12d, 19h
      rorx    ecx, r12d, 0Bh
    }
    v1160 = v1490 + _ER15;
    __asm { rorx    edi, r8d, 0Dh }
    v1162 = _ECX ^ _EAX;
    __asm { rorx    ecx, r12d, 6 }
    v1164 = _ECX ^ v1162;
    __asm { rorx    ecx, r8d, 16h }
    v1166 = _EDI ^ _ECX;
    __asm { rorx    edi, r8d, 2 }
    v1168 = v1164 + (_ER14 ^ _ER12 & (_ER14 ^ _ER13));
    _ER11 = v1168 + v1160 + _ER11;
    _ER15 = (_ER10 & _ER8 | _ER9 & (_ER10 | _ER8)) + v1168 + (_EDI ^ v1166) + v1160;
    __asm
    {
      vpshufb xmm2, xmm2, [rsp+118h+var_D8]
      rorx    eax, r11d, 19h
      rorx    ecx, r11d, 0Bh
    }
    v1173 = DWORD1(v1490) + _ER14;
    __asm { rorx    edi, r15d, 0Dh }
    v1175 = _ECX ^ _EAX;
    __asm { rorx    ecx, r11d, 6 }
    v1177 = _ECX ^ v1175;
    __asm { rorx    ecx, r15d, 16h }
    v1179 = _EDI ^ _ECX;
    __asm { rorx    edi, r15d, 2 }
    v1181 = v1177 + (_ER13 ^ _ER11 & (_ER13 ^ _ER12));
    _ER10 = v1181 + v1173 + _ER10;
    _ER14 = (_ER9 & _ER15 | _ER8 & (_ER9 | _ER15)) + v1181 + (_EDI ^ v1179) + v1173;
    __asm
    {
      rorx    eax, r10d, 19h
      rorx    ecx, r10d, 0Bh
    }
    v1186 = DWORD2(v1490) + _ER13;
    __asm { rorx    edi, r14d, 0Dh }
    v1188 = _ECX ^ _EAX;
    __asm { rorx    ecx, r10d, 6 }
    v1190 = _ECX ^ v1188;
    __asm { rorx    ecx, r14d, 16h }
    v1192 = _EDI ^ _ECX;
    __asm { rorx    edi, r14d, 2 }
    v1194 = v1190 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
    _ER9 = v1194 + v1186 + _ER9;
    _ER13 = (_ER8 & _ER14 | _ER15 & (_ER8 | _ER14)) + v1194 + (_EDI ^ v1192) + v1186;
    __asm
    {
      vpaddd  xmm4, xmm2, xmmword ptr [rbx+20h]
      rorx    eax, r9d, 19h
      rorx    ecx, r9d, 0Bh
    }
    v1200 = DWORD3(v1490) + _ER12;
    __asm { rorx    edi, r13d, 0Dh }
    v1202 = _ECX ^ _EAX;
    __asm { rorx    ecx, r9d, 6 }
    v1204 = _ECX ^ v1202;
    __asm { rorx    ecx, r13d, 16h }
    v1206 = _EDI ^ _ECX;
    __asm { rorx    edi, r13d, 2 }
    v1208 = v1204 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
    _ER8 = v1208 + v1200 + _ER8;
    _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v1208 + (_EDI ^ v1206) + v1200;
    __asm
    {
      vmovdqa [rsp+118h+var_F8], xmm4
      vmovdqu xmm3, xmmword ptr [rdx+30h]
      rorx    eax, r8d, 19h
      rorx    ecx, r8d, 0Bh
    }
    v1214 = v1493 + _ER11;
    __asm { rorx    edi, r12d, 0Dh }
    v1216 = _ECX ^ _EAX;
    __asm { rorx    ecx, r8d, 6 }
    v1218 = _ECX ^ v1216;
    __asm { rorx    ecx, r12d, 16h }
    v1220 = _EDI ^ _ECX;
    __asm { rorx    edi, r12d, 2 }
    v1222 = v1218 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
    _ER15 = v1222 + v1214 + _ER15;
    _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v1222 + (_EDI ^ v1220) + v1214;
    __asm
    {
      vpshufb xmm3, xmm3, [rsp+118h+var_D8]
      rorx    eax, r15d, 19h
      rorx    ecx, r15d, 0Bh
    }
    v1227 = DWORD1(v1493) + _ER10;
    __asm { rorx    edi, r11d, 0Dh }
    v1229 = _ECX ^ _EAX;
    __asm { rorx    ecx, r15d, 6 }
    v1231 = _ECX ^ v1229;
    __asm { rorx    ecx, r11d, 16h }
    v1233 = _EDI ^ _ECX;
    __asm { rorx    edi, r11d, 2 }
    v1235 = v1231 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
    _ER14 = v1235 + v1227 + _ER14;
    _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v1235 + (_EDI ^ v1233) + v1227;
    __asm
    {
      rorx    eax, r14d, 19h
      rorx    ecx, r14d, 0Bh
    }
    v1240 = DWORD2(v1493) + _ER9;
    __asm { rorx    edi, r10d, 0Dh }
    v1242 = _ECX ^ _EAX;
    __asm { rorx    ecx, r14d, 6 }
    v1244 = _ECX ^ v1242;
    __asm { rorx    ecx, r10d, 16h }
    v1246 = _EDI ^ _ECX;
    __asm { rorx    edi, r10d, 2 }
    v1248 = v1244 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
    _ER13 = v1248 + v1240 + _ER13;
    _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v1248 + (_EDI ^ v1246) + v1240;
    __asm
    {
      vpaddd  xmm4, xmm3, xmmword ptr [rbx+30h]
      rorx    eax, r13d, 19h
      rorx    ecx, r13d, 0Bh
    }
    v1253 = DWORD3(v1493) + _ER8;
    __asm { rorx    edi, r9d, 0Dh }
    v1255 = _ECX ^ _EAX;
    __asm { rorx    ecx, r13d, 6 }
    v1257 = _ECX ^ v1255;
    __asm { rorx    ecx, r9d, 16h }
    v1259 = _EDI ^ _ECX;
    __asm { rorx    edi, r9d, 2 }
    v1261 = v1257 + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
    v1262 = v1261 + v1253 + _ER12;
    v1263 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v1261 + (_EDI ^ v1259) + v1253;
    __asm { vmovdqa [rsp+118h+var_E8], xmm4 }
    v27 = v1048 + 64;
    _RDX += 64LL;
    v1264 = v1512;
    *(_DWORD *)v1264 += v1263;
    *(_DWORD *)(v1264 + 4) += _ER9;
    *(_DWORD *)(v1264 + 8) += _ER10;
    *(_DWORD *)(v1264 + 12) += _ER11;
    *(_DWORD *)(v1264 + 16) += v1262;
    *(_DWORD *)(v1264 + 20) += _ER13;
    *(_DWORD *)(v1264 + 24) += _ER14;
    *(_DWORD *)(v1264 + 28) += _ER15;
  }
  __asm
  {
    rorx    eax, r12d, 19h
    rorx    ecx, r12d, 0Bh
  }
  v1267 = v1484 + _ER15;
  __asm { rorx    edi, r8d, 0Dh }
  v1269 = _ECX ^ _EAX;
  __asm { rorx    ecx, r12d, 6 }
  v1271 = _ECX ^ v1269;
  __asm { rorx    ecx, r8d, 16h }
  v1273 = _EDI ^ _ECX;
  __asm { rorx    edi, r8d, 2 }
  v1275 = v1271 + (_ER14 ^ _ER12 & (_ER14 ^ _ER13));
  _ER11 = v1275 + v1267 + _ER11;
  _ER15 = (_ER10 & _ER8 | _ER9 & (_ER10 | _ER8)) + v1275 + (_EDI ^ v1273) + v1267;
  __asm
  {
    rorx    eax, r11d, 19h
    rorx    ecx, r11d, 0Bh
  }
  v1280 = DWORD1(v1484) + _ER14;
  __asm { rorx    edi, r15d, 0Dh }
  v1282 = _ECX ^ _EAX;
  __asm { rorx    ecx, r11d, 6 }
  v1284 = _ECX ^ v1282;
  __asm { rorx    ecx, r15d, 16h }
  v1286 = _EDI ^ _ECX;
  __asm { rorx    edi, r15d, 2 }
  v1288 = v1284 + (_ER13 ^ _ER11 & (_ER13 ^ _ER12));
  _ER10 = v1288 + v1280 + _ER10;
  _ER14 = (_ER9 & _ER15 | _ER8 & (_ER9 | _ER15)) + v1288 + (_EDI ^ v1286) + v1280;
  __asm
  {
    rorx    eax, r10d, 19h
    rorx    ecx, r10d, 0Bh
  }
  v1293 = DWORD2(v1484) + _ER13;
  __asm { rorx    edi, r14d, 0Dh }
  v1295 = _ECX ^ _EAX;
  __asm { rorx    ecx, r10d, 6 }
  v1297 = _ECX ^ v1295;
  __asm { rorx    ecx, r14d, 16h }
  v1299 = _EDI ^ _ECX;
  __asm { rorx    edi, r14d, 2 }
  v1301 = v1297 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
  _ER9 = v1301 + v1293 + _ER9;
  _ER13 = (_ER8 & _ER14 | _ER15 & (_ER8 | _ER14)) + v1301 + (_EDI ^ v1299) + v1293;
  __asm
  {
    rorx    eax, r9d, 19h
    rorx    ecx, r9d, 0Bh
  }
  v1306 = DWORD3(v1484) + _ER12;
  __asm { rorx    edi, r13d, 0Dh }
  v1308 = _ECX ^ _EAX;
  __asm { rorx    ecx, r9d, 6 }
  v1310 = _ECX ^ v1308;
  __asm { rorx    ecx, r13d, 16h }
  v1312 = _EDI ^ _ECX;
  __asm { rorx    edi, r13d, 2 }
  v1314 = v1310 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
  _ER8 = v1314 + v1306 + _ER8;
  _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v1314 + (_EDI ^ v1312) + v1306;
  __asm
  {
    rorx    eax, r8d, 19h
    rorx    ecx, r8d, 0Bh
  }
  v1319 = v1487 + _ER11;
  __asm { rorx    edi, r12d, 0Dh }
  v1321 = _ECX ^ _EAX;
  __asm { rorx    ecx, r8d, 6 }
  v1323 = _ECX ^ v1321;
  __asm { rorx    ecx, r12d, 16h }
  v1325 = _EDI ^ _ECX;
  __asm { rorx    edi, r12d, 2 }
  v1327 = v1323 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
  _ER15 = v1327 + v1319 + _ER15;
  _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v1327 + (_EDI ^ v1325) + v1319;
  __asm
  {
    rorx    eax, r15d, 19h
    rorx    ecx, r15d, 0Bh
  }
  v1332 = DWORD1(v1487) + _ER10;
  __asm { rorx    edi, r11d, 0Dh }
  v1334 = _ECX ^ _EAX;
  __asm { rorx    ecx, r15d, 6 }
  v1336 = _ECX ^ v1334;
  __asm { rorx    ecx, r11d, 16h }
  v1338 = _EDI ^ _ECX;
  __asm { rorx    edi, r11d, 2 }
  v1340 = v1336 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
  _ER14 = v1340 + v1332 + _ER14;
  _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v1340 + (_EDI ^ v1338) + v1332;
  __asm
  {
    rorx    eax, r14d, 19h
    rorx    ecx, r14d, 0Bh
  }
  v1345 = DWORD2(v1487) + _ER9;
  __asm { rorx    edi, r10d, 0Dh }
  v1347 = _ECX ^ _EAX;
  __asm { rorx    ecx, r14d, 6 }
  v1349 = _ECX ^ v1347;
  __asm { rorx    ecx, r10d, 16h }
  v1351 = _EDI ^ _ECX;
  __asm { rorx    edi, r10d, 2 }
  v1353 = v1349 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
  _ER13 = v1353 + v1345 + _ER13;
  _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v1353 + (_EDI ^ v1351) + v1345;
  __asm
  {
    rorx    eax, r13d, 19h
    rorx    ecx, r13d, 0Bh
  }
  v1358 = DWORD3(v1487) + _ER8;
  __asm { rorx    edi, r9d, 0Dh }
  v1360 = _ECX ^ _EAX;
  __asm { rorx    ecx, r13d, 6 }
  v1362 = _ECX ^ v1360;
  __asm { rorx    ecx, r9d, 16h }
  v1364 = _EDI ^ _ECX;
  __asm { rorx    edi, r9d, 2 }
  v1366 = v1362 + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
  _ER12 = v1366 + v1358 + _ER12;
  _ER8 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v1366 + (_EDI ^ v1364) + v1358;
  __asm
  {
    rorx    eax, r12d, 19h
    rorx    ecx, r12d, 0Bh
  }
  v1371 = v1490 + _ER15;
  __asm { rorx    edi, r8d, 0Dh }
  v1373 = _ECX ^ _EAX;
  __asm { rorx    ecx, r12d, 6 }
  v1375 = _ECX ^ v1373;
  __asm { rorx    ecx, r8d, 16h }
  v1377 = _EDI ^ _ECX;
  __asm { rorx    edi, r8d, 2 }
  v1379 = v1375 + (_ER14 ^ _ER12 & (_ER14 ^ _ER13));
  _ER11 = v1379 + v1371 + _ER11;
  _ER15 = (_ER10 & _ER8 | _ER9 & (_ER10 | _ER8)) + v1379 + (_EDI ^ v1377) + v1371;
  __asm
  {
    rorx    eax, r11d, 19h
    rorx    ecx, r11d, 0Bh
  }
  v1384 = DWORD1(v1490) + _ER14;
  __asm { rorx    edi, r15d, 0Dh }
  v1386 = _ECX ^ _EAX;
  __asm { rorx    ecx, r11d, 6 }
  v1388 = _ECX ^ v1386;
  __asm { rorx    ecx, r15d, 16h }
  v1390 = _EDI ^ _ECX;
  __asm { rorx    edi, r15d, 2 }
  v1392 = v1388 + (_ER13 ^ _ER11 & (_ER13 ^ _ER12));
  _ER10 = v1392 + v1384 + _ER10;
  _ER14 = (_ER9 & _ER15 | _ER8 & (_ER9 | _ER15)) + v1392 + (_EDI ^ v1390) + v1384;
  __asm
  {
    rorx    eax, r10d, 19h
    rorx    ecx, r10d, 0Bh
  }
  v1397 = DWORD2(v1490) + _ER13;
  __asm { rorx    edi, r14d, 0Dh }
  v1399 = _ECX ^ _EAX;
  __asm { rorx    ecx, r10d, 6 }
  v1401 = _ECX ^ v1399;
  __asm { rorx    ecx, r14d, 16h }
  v1403 = _EDI ^ _ECX;
  __asm { rorx    edi, r14d, 2 }
  v1405 = v1401 + (_ER12 ^ _ER10 & (_ER12 ^ _ER11));
  _ER9 = v1405 + v1397 + _ER9;
  _ER13 = (_ER8 & _ER14 | _ER15 & (_ER8 | _ER14)) + v1405 + (_EDI ^ v1403) + v1397;
  __asm
  {
    rorx    eax, r9d, 19h
    rorx    ecx, r9d, 0Bh
  }
  v1410 = DWORD3(v1490) + _ER12;
  __asm { rorx    edi, r13d, 0Dh }
  v1412 = _ECX ^ _EAX;
  __asm { rorx    ecx, r9d, 6 }
  v1414 = _ECX ^ v1412;
  __asm { rorx    ecx, r13d, 16h }
  v1416 = _EDI ^ _ECX;
  __asm { rorx    edi, r13d, 2 }
  v1418 = v1414 + (_ER11 ^ _ER9 & (_ER11 ^ _ER10));
  _ER8 = v1418 + v1410 + _ER8;
  _ER12 = (_ER15 & _ER13 | _ER14 & (_ER15 | _ER13)) + v1418 + (_EDI ^ v1416) + v1410;
  __asm
  {
    rorx    eax, r8d, 19h
    rorx    ecx, r8d, 0Bh
  }
  v1423 = v1493 + _ER11;
  __asm { rorx    edi, r12d, 0Dh }
  v1425 = _ECX ^ _EAX;
  __asm { rorx    ecx, r8d, 6 }
  v1427 = _ECX ^ v1425;
  __asm { rorx    ecx, r12d, 16h }
  v1429 = _EDI ^ _ECX;
  __asm { rorx    edi, r12d, 2 }
  v1431 = v1427 + (_ER10 ^ _ER8 & (_ER10 ^ _ER9));
  _ER15 = v1431 + v1423 + _ER15;
  _ER11 = (_ER14 & _ER12 | _ER13 & (_ER14 | _ER12)) + v1431 + (_EDI ^ v1429) + v1423;
  __asm
  {
    rorx    eax, r15d, 19h
    rorx    ecx, r15d, 0Bh
  }
  v1436 = DWORD1(v1493) + _ER10;
  __asm { rorx    edi, r11d, 0Dh }
  v1438 = _ECX ^ _EAX;
  __asm { rorx    ecx, r15d, 6 }
  v1440 = _ECX ^ v1438;
  __asm { rorx    ecx, r11d, 16h }
  v1442 = _EDI ^ _ECX;
  __asm { rorx    edi, r11d, 2 }
  v1444 = v1440 + (_ER9 ^ _ER15 & (_ER9 ^ _ER8));
  _ER14 = v1444 + v1436 + _ER14;
  _ER10 = (_ER13 & _ER11 | _ER12 & (_ER13 | _ER11)) + v1444 + (_EDI ^ v1442) + v1436;
  __asm
  {
    rorx    eax, r14d, 19h
    rorx    ecx, r14d, 0Bh
  }
  v1449 = DWORD2(v1493) + _ER9;
  __asm { rorx    edi, r10d, 0Dh }
  v1451 = _ECX ^ _EAX;
  __asm { rorx    ecx, r14d, 6 }
  v1453 = _ECX ^ v1451;
  __asm { rorx    ecx, r10d, 16h }
  v1455 = _EDI ^ _ECX;
  __asm { rorx    edi, r10d, 2 }
  v1457 = v1453 + (_ER8 ^ _ER14 & (_ER8 ^ _ER15));
  _ER13 = v1457 + v1449 + _ER13;
  _ER9 = (_ER12 & _ER10 | _ER11 & (_ER12 | _ER10)) + v1457 + (_EDI ^ v1455) + v1449;
  __asm
  {
    rorx    eax, r13d, 19h
    rorx    ecx, r13d, 0Bh
  }
  v1462 = DWORD3(v1493) + _ER8;
  __asm { rorx    edi, r9d, 0Dh }
  v1464 = _ECX ^ _EAX;
  __asm { rorx    ecx, r13d, 6 }
  result = _ECX ^ (unsigned int)v1464;
  __asm { rorx    ecx, r9d, 16h }
  v1468 = _EDI ^ _ECX;
  __asm { rorx    edi, r9d, 2 }
  v1470 = result + (_ER15 ^ _ER13 & (_ER15 ^ _ER14));
  v1471 = v1470 + v1462 + _ER12;
  v1472 = (_ER11 & _ER9 | _ER10 & (_ER11 | _ER9)) + v1470 + (_EDI ^ v1468) + v1462;
  v1473 = v1512;
  *(_DWORD *)v1473 += v1472;
  *(_DWORD *)(v1473 + 4) += _ER9;
  *(_DWORD *)(v1473 + 8) += _ER10;
  *(_DWORD *)(v1473 + 12) += _ER11;
  *(_DWORD *)(v1473 + 16) += v1471;
  *(_DWORD *)(v1473 + 20) += _ER13;
  *(_DWORD *)(v1473 + 24) += _ER14;
  *(_DWORD *)(v1473 + 28) += _ER15;
  __asm
  {
    vmovdqa xmm0, [rsp+118h+var_C8]
    vmovdqa xmm1, [rsp+118h+var_B8]
    vmovdqa xmm2, [rsp+118h+var_A8]
    vmovdqa xmm3, [rsp+118h+var_98]
    vmovdqa xmm4, [rsp+118h+var_88]
    vmovdqa xmm5, [rsp+118h+var_78]
    vmovdqa xmm6, [rsp+118h+var_68]
    vmovdqa xmm7, [rsp+118h+var_58]
  }
  return result;
}
// 5EFC0: using guessed type __int64 qword_5EFC0[2];
// 66F00: using guessed type __int64 ccsha256_K[32];

//----- (000000000002C6BE) ----------------------------------------------------
__int64 *ccaes_ccm_encrypt_mode()
{
  __int64 (*v0)[2]; // rax@1

  v0 = ccaes_ecb_encrypt_mode();
  ccm_aes_encrypt = (((*v0)[1] + 15) & 0xFFFFFFFFFFFFFFF8LL) + (((*v0)[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BBE0 = 96LL;
  qword_6BBE8 = 1LL;
  qword_6BBF0 = (__int64)ccmode_ccm_init;
  qword_6BBF8 = (__int64)ccmode_ccm_set_iv;
  qword_6BC00 = (__int64)ccmode_ccm_cbcmac;
  qword_6BC08 = (__int64)ccmode_ccm_encrypt_x86_64;
  qword_6BC10 = (__int64)ccmode_ccm_finalize;
  qword_6BC18 = (__int64)ccmode_ccm_reset;
  qword_6BC20 = (__int64)v0;
  return &ccm_aes_encrypt;
}
// 6BBD8: using guessed type __int64 ccm_aes_encrypt;
// 6BBE0: using guessed type __int64 qword_6BBE0;
// 6BBE8: using guessed type __int64 qword_6BBE8;
// 6BBF0: using guessed type __int64 qword_6BBF0;
// 6BBF8: using guessed type __int64 qword_6BBF8;
// 6BC00: using guessed type __int64 qword_6BC00;
// 6BC08: using guessed type __int64 qword_6BC08;
// 6BC10: using guessed type __int64 qword_6BC10;
// 6BC18: using guessed type __int64 qword_6BC18;
// 6BC20: using guessed type __int64 qword_6BC20;

//----- (000000000002C765) ----------------------------------------------------
signed __int64 __fastcall cchkdf(__int64 a1, size_t a2, const void *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, const void *a7, unsigned __int64 a8, void *a9)
{
  const void *v9; // rbx@1
  unsigned __int64 v10; // r9@1
  __int64 v11; // r12@1
  size_t v12; // r15@1
  unsigned __int64 v13; // rax@1
  char *v14; // r14@1
  __int64 v15; // r13@1
  __int64 v16; // rax@1
  __int64 v17; // rsi@3
  unsigned __int64 v18; // rcx@3
  signed __int64 v19; // rdi@3
  signed __int64 result; // rax@3
  signed __int64 v21; // rbx@4
  __int64 v22; // r13@5
  __int64 i; // r12@5
  __int64 v24; // r14@7
  signed __int64 v25; // rax@7
  unsigned __int64 v26; // rdx@7
  const void *v27; // r12@7
  signed __int64 v28; // rax@7
  signed __int64 v29; // rax@7
  __int64 v30; // r14@7
  size_t v31; // rdx@7
  size_t v32; // r13@7
  size_t v33; // rax@9
  __int64 v34; // rcx@11
  __int64 v35; // [sp+0h] [bp-80h]@1
  __int64 v36; // [sp+8h] [bp-78h]@5
  __int64 v37; // [sp+10h] [bp-70h]@4
  const void *v38; // [sp+18h] [bp-68h]@4
  void *v39; // [sp+20h] [bp-60h]@1
  unsigned __int64 v40; // [sp+28h] [bp-58h]@1
  void *v41; // [sp+30h] [bp-50h]@1
  size_t v42; // [sp+38h] [bp-48h]@2
  void *v43; // [sp+40h] [bp-40h]@2
  char v44; // [sp+4Fh] [bp-31h]@7
  __int64 v45; // [sp+50h] [bp-30h]@1

  v40 = a6;
  v9 = a3;
  v10 = a2;
  v11 = off_69010[0];
  v45 = *(_QWORD *)off_69010[0];
  v12 = *(_QWORD *)a1;
  v13 = (*(_QWORD *)a1 + 15LL) & 0xFFFFFFFFFFFFFFF0LL;
  v14 = (char *)&v35 - v13;
  v15 = (__int64)((char *)&v35 - v13);
  v39 = (char *)&v35 - v13;
  v16 = (((*(_QWORD *)(a1 + 16) + 2LL * *(_QWORD *)(a1 + 8) + 19) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v41 = (char *)&v35 - v16;
  if ( !a4 )
  {
    v43 = (void *)a1;
    v42 = a2;
    bzero(v14, v12);
    v10 = v42;
    a1 = (__int64)v43;
    a5 = v14;
    a4 = v12;
  }
  cchmac(v16, a5, v10, a1, a4, v9, v15);
  v17 = a1;
  v18 = *(_QWORD *)a1;
  v19 = a8 / *(_QWORD *)a1 - ((a8 % *(_QWORD *)a1 < 1) - 1LL);
  result = 0xFFFFFFFFLL;
  if ( (unsigned __int64)v19 <= 0xFF )
  {
    v38 = (const void *)v15;
    v21 = 1LL;
    v37 = v19;
    if ( v19 )
    {
      v43 = a9;
      v42 = a8;
      v22 = v12;
      v36 = v12;
      for ( i = 0LL; ; i = v36 )
      {
        v24 = (__int64)v41;
        v25 = cchmac_init(v17, (__int64)v41, v18, v38);
        v26 = i;
        v27 = v39;
        LODWORD(v28) = cchmac_update(v25, v26, v39, v17, v24);
        LODWORD(v29) = cchmac_update(v28, v40, a7, v17, v24);
        v44 = v21;
        cchmac_update(v29, 1uLL, &v44, v17, v24);
        cchmac_final(v17, v24, (__int64)v27);
        v30 = v37;
        v31 = v22;
        v32 = v42;
        if ( v21 == v37 )
          v31 = v42;
        memcpy(v43, v27, v31);
        ++v21;
        v33 = v32;
        v22 = v36;
        if ( v21 > (unsigned __int64)v30 )
          break;
        v18 = *(_QWORD *)v17;
        v43 = (char *)v43 + v36;
        v42 = v33 - v36;
      }
    }
    cc_clear(*(_QWORD *)(v17 + 16) + 2LL * *(_QWORD *)(v17 + 8) + 12, v41);
    v11 = off_69010[0];
    result = 0LL;
  }
  v34 = *(_QWORD *)v11;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002C964) ----------------------------------------------------
int __usercall ccn_print@<eax>(__int64 a1@<rax>, __int64 a2@<rdi>, __int64 a3@<rsi>)
{
  __int64 i; // rbx@1
  __int64 v5; // [sp-8h] [bp-20h]@1

  v5 = a1;
  for ( i = a2; i; --i )
    LODWORD(a1) = printf("%.016llx", *(_QWORD *)(a3 + 8 * i - 8), v5);
  return a1;
}

//----- (000000000002C99F) ----------------------------------------------------
int __usercall ccn_lprint@<eax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>)
{
  __int64 v3; // r14@1
  __int64 v4; // rbx@1

  v3 = a2;
  v4 = a3;
  printf("%s", a1);
  if ( a3 )
  {
    do
      printf("%.016llx", *(_QWORD *)(v3 + 8 * v4-- - 8));
    while ( v4 );
  }
  return printf("\n");
}

//----- (000000000002C9F5) ----------------------------------------------------
__int64 __fastcall ccn_divmod(__int64 a1, __int64 a2, void *a3, unsigned __int64 a4, const void *a5)
{
  void *v5; // r14@1
  __int64 v6; // r12@1
  unsigned __int64 v7; // rbx@1
  void *v8; // rdx@1
  unsigned __int64 v9; // rax@1
  char *v10; // r15@1
  char *v11; // r14@1
  void *v12; // r15@4
  __int64 v14; // [sp+0h] [bp-40h]@1
  unsigned __int64 v15; // [sp+8h] [bp-38h]@1
  __int64 v16; // [sp+10h] [bp-30h]@1

  v15 = a4;
  v5 = a3;
  v6 = a2;
  v16 = *(_QWORD *)off_69010[0];
  v7 = (unsigned __int64)((char *)&v14 - ((16 * a1 + 39) & 0xFFFFFFFFFFFFFFF0LL));
  *(_QWORD *)v7 = a1;
  ccn_set(a1, (void *)(v7 + 16), a5);
  cczp_init(v7);
  v8 = v5;
  v9 = (8 * a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v10 = (char *)&v14 - v9;
  v11 = (char *)&v14 - v9;
  if ( !v8 )
  {
    bzero((char *)&v14 - v9, 8 * a1);
    v8 = v10;
  }
  if ( !a2 )
  {
    v12 = v8;
    bzero(v11, 8 * a1);
    v8 = v12;
    v6 = (__int64)v11;
  }
  cczp_div(v7, v6, v8, v15);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002CAE0) ----------------------------------------------------
__int64 __fastcall ccn_trailing_zeros(unsigned __int64 a1, __int64 a2)
{
  __int64 result; // rax@1
  unsigned __int64 v3; // rdx@2
  unsigned __int64 v4; // rcx@3
  bool v5; // zf@6

  LODWORD(result) = 0;
  if ( a1 )
  {
    v3 = 0LL;
    while ( 1 )
    {
      v4 = *(_QWORD *)(a2 + 8 * v3);
      if ( v4 )
        break;
      ++v3;
      LODWORD(result) = result + 64;
      if ( v3 >= a1 )
        return (unsigned int)result;
    }
    do
    {
      LODWORD(result) = *((_BYTE *)ccn_trailing_zeros_nibble2zeros + (v4 & 0xF)) + (_DWORD)result;
      v5 = (v4 & 0xF) == 0;
      v4 >>= 4;
    }
    while ( v5 );
  }
  return (unsigned int)result;
}
// 5EFD0: using guessed type __int64 ccn_trailing_zeros_nibble2zeros[2];

//----- (000000000002CB2B) ----------------------------------------------------
__int64 __fastcall ccn_gcdn(unsigned __int64 a1, void *a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 v6; // r15@1
  __int64 v7; // rbx@1
  __int64 v8; // r14@1
  signed __int64 v9; // r12@1
  __int64 v10; // rax@1
  unsigned __int64 v11; // rbx@1
  unsigned __int64 v12; // r15@3
  unsigned __int64 v13; // rax@3
  __int64 v14; // rbx@3
  char *v15; // r13@3
  __int64 v16; // r12@3
  signed __int64 v17; // r15@3
  signed int v18; // eax@4
  __int64 v19; // r14@7
  __int64 v20; // rbx@11
  signed __int64 v21; // rax@14
  __int64 v22; // rax@14
  unsigned __int64 v23; // rcx@16
  unsigned __int64 v24; // r13@19
  __int64 v25; // rdi@19
  __int64 v27; // [sp+0h] [bp-70h]@3
  void *v28; // [sp+8h] [bp-68h]@3
  __int64 *v29; // [sp+10h] [bp-60h]@3
  unsigned __int64 v30; // [sp+18h] [bp-58h]@3
  __int64 v31; // [sp+20h] [bp-50h]@3
  __int64 v32; // [sp+28h] [bp-48h]@3
  void *v33; // [sp+30h] [bp-40h]@1
  __int64 v34; // [sp+38h] [bp-38h]@1
  __int64 v35; // [sp+40h] [bp-30h]@1

  v6 = a6;
  v7 = a5;
  v34 = a4;
  v33 = a2;
  v8 = off_69010[0];
  v35 = *(_QWORD *)off_69010[0];
  v9 = ccn_n(a3, a4);
  v10 = ccn_n(v7, v6);
  v11 = v10;
  if ( !v9 )
  {
    ccn_set(v10, v33, (const void *)v6);
    v24 = a1 - v11;
    v25 = (__int64)((char *)v33 + 8 * v11);
LABEL_21:
    bzero((void *)v25, 8 * v24);
    return *(_QWORD *)v8;
  }
  if ( !v10 )
  {
    ccn_set(v9, v33, (const void *)v34);
    v24 = a1 - v9;
    v25 = (__int64)((char *)v33 + 8 * v9);
    goto LABEL_21;
  }
  v32 = ccn_trailing_zeros(v9, v34);
  v28 = (void *)v6;
  v31 = ccn_trailing_zeros(v11, v6);
  v29 = &v27;
  v12 = v11;
  v13 = (8 * a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v14 = (__int64)((char *)&v27 - v13);
  v30 = a1;
  v15 = (char *)&v27 - v13;
  ccn_shift_right_multi(v9, (char *)&v27 - v13, (void *)v34, v32);
  bzero((void *)(v14 + 8 * v9), 8 * (a1 - v9));
  v16 = ccn_n(v9, v14);
  ccn_shift_right_multi(v12, v15, v28, v31);
  bzero(&v15[8 * v12], 8 * (a1 - v12));
  v17 = ccn_n(v12, (__int64)v15);
  while ( 1 )
  {
    v18 = 1;
    if ( v16 <= (unsigned __int64)v17 )
    {
      v18 = -1;
      if ( v16 >= (unsigned __int64)v17 )
      {
        v18 = ccn_cmp(v16, v14, (__int64)v15);
        if ( !v18 )
          break;
      }
    }
    v19 = v14;
    if ( v18 > 0 )
      v19 = (__int64)v15;
    if ( v18 > 0 )
      v15 = (char *)v14;
    v20 = v17;
    if ( v18 > 0 )
    {
      v20 = v16;
      if ( v18 > 0 )
        v16 = v17;
    }
    ccn_sub(v20, (__int64)v15, (__int64)v15, v19);
    v21 = ccn_n(v20, (__int64)v15);
    v17 = v21;
    v22 = ccn_trailing_zeros(v21, (__int64)v15);
    v14 = v19;
    if ( v22 )
    {
      ccn_shift_right_multi(v17, v15, v15, v22);
      v17 = ccn_n(v17, (__int64)v15);
      v14 = v19;
    }
  }
  v23 = v32;
  if ( v32 > (unsigned __int64)v31 )
    v23 = v31;
  ccn_shift_left_multi(v30, v33, (__int64)v15, v23);
  v8 = off_69010[0];
  return *(_QWORD *)v8;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002CD5E) ----------------------------------------------------
__int64 __fastcall ccn_gcd(unsigned __int64 a1, void *a2, __int64 a3, __int64 a4)
{
  return ccn_gcdn(a1, a2, a1, a3, a1, a4);
}

//----- (000000000002CD77) ----------------------------------------------------
__int64 __fastcall ccecies_encrypt_gcm(__int64 **a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5, const void *a6, __int64 a7, __int64 a8, __int64 a9, __int64 a10)
{
  __int64 v10; // r14@1
  unsigned __int64 v11; // rax@1
  unsigned __int64 v12; // r12@1
  __int64 result; // rax@2
  const void *v14; // [sp+30h] [bp-40h]@1
  unsigned __int64 v15; // [sp+38h] [bp-38h]@1
  __int64 v16; // [sp+40h] [bp-30h]@1

  v14 = a6;
  v15 = a5;
  v16 = a4;
  v10 = a3;
  v11 = ccecies_pub_key_size(a1, a2);
  v12 = v11 + v10 + *(_DWORD *)(a2 + 28);
  if ( v12 > *(_QWORD *)a9 )
  {
    result = 0xFFFFFFFFLL;
  }
  else
  {
    result = ccecies_encrypt_gcm_composite(a1, a2, a10, a10 + v11, a10 + v11 + v10, v10, v16, v15, v14, a7, a8);
    *(_QWORD *)a9 = v12;
  }
  return result;
}

//----- (000000000002CE18) ----------------------------------------------------
__int64 __fastcall ccn_lcm(unsigned __int64 a1, void *a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r13@1
  __int64 v5; // rbx@1
  size_t v6; // r14@1
  int v7; // eax@1
  __int64 v8; // r12@1
  __int64 v9; // r13@5
  void *v10; // rbx@5
  char *v11; // rbx@5
  unsigned __int64 *v12; // r14@5
  unsigned __int64 v13; // rcx@5
  __int64 v14; // rbx@5
  __int64 v16; // [sp+0h] [bp-40h]@1
  void *v17; // [sp+8h] [bp-38h]@1
  __int64 v18; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v17 = a2;
  v18 = *(_QWORD *)off_69010[0];
  v6 = 16 * a1;
  v16 = (__int64)(&v16 - 2 * a1);
  v7 = ccn_cmp(a1, a3, a4);
  v8 = v4;
  if ( v7 < 0 )
    v8 = v5;
  if ( v7 < 0 )
    v5 = v4;
  v9 = v5;
  v10 = v17;
  bzero(v17, v6);
  ccn_gcd(a1, v10, v9, v8);
  v11 = (char *)&v16 - v6;
  ccn_set(a1, (char *)&v16 - v6, (const void *)v8);
  bzero((char *)&v16 + 8 * a1 - v6, 8 * a1);
  v12 = (unsigned __int64 *)v16;
  v13 = (unsigned __int64)v11;
  v14 = (__int64)v17;
  ccn_divmod(a1, v16, 0LL, v13, v17);
  ccn_mul(a1, v14, v9, v12);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002CF0B) ----------------------------------------------------
void __fastcall ccn_mul_ws(signed __int64 a1, __int64 a2, __int64 a3, unsigned __int64 *a4)
{
  ccn_mul(a1, a2, a3, a4);
}

//----- (000000000002CF20) ----------------------------------------------------
void __fastcall ccn_mul(signed __int64 a1, __int64 a2, __int64 a3, unsigned __int64 *a4)
{
  __int64 v4; // r10@2
  unsigned __int64 v5; // r11@2
  unsigned __int128 v6; // ax@2
  signed __int64 v7; // r8@3
  unsigned __int128 v8; // ax@4
  unsigned __int8 v9; // cf@4
  signed __int64 v10; // r9@5
  unsigned __int64 v11; // r11@6
  signed __int64 v12; // r8@6
  unsigned __int128 v13; // ax@7
  __int64 v14; // rtt@7
  signed __int64 i; // rax@8
  unsigned __int128 v16; // ax@12

  if ( a1 > 0 )
  {
    v4 = a3;
    v5 = *a4;
    v6 = *a4 * (unsigned __int128)*(_QWORD *)a3;
    *(_QWORD *)a2 = v6;
    *(_QWORD *)(a2 + 8) = *((_QWORD *)&v6 + 1);
    if ( a1 != 1 )
    {
      v7 = 1LL;
      do
      {
        v8 = v5 * (unsigned __int128)*(_QWORD *)(v4 + 8 * v7);
        v9 = __CFADD__((_QWORD)v8, *(_QWORD *)(a2 + 8 * v7));
        *(_QWORD *)(a2 + 8 * v7) += v8;
        *(_QWORD *)(a2 + 8 * v7++ + 8) = v9 + *((_QWORD *)&v8 + 1);
      }
      while ( a1 > v7 );
      v10 = 1LL;
      do
      {
        v11 = a4[v10];
        a2 += 8LL;
        *(_QWORD *)(a2 + 8 * a1) = 0LL;
        v12 = 1LL;
        do
        {
          v13 = v11 * (unsigned __int128)*(_QWORD *)(v4 + 8 * v12 - 8);
          v9 = __CFADD__((_QWORD)v13, *(_QWORD *)(a2 + 8 * v12 - 8));
          *(_QWORD *)(a2 + 8 * v12 - 8) += v13;
          v14 = v9;
          v9 = __CFADD__(v9, *(_QWORD *)(a2 + 8 * v12)) | __CFADD__(
                                                            *((_QWORD *)&v13 + 1),
                                                            v9 + *(_QWORD *)(a2 + 8 * v12));
          *(_QWORD *)(a2 + 8 * v12) += *((_QWORD *)&v13 + 1) + v14;
          if ( v9 )
          {
            for ( i = v12; ; ++i )
            {
              v9 = __CFADD__((*(_QWORD *)(a2 + 8 * i + 8))++, 1LL);
              if ( !v9 )
                break;
            }
          }
          ++v12;
        }
        while ( a1 > v12 );
        v16 = v11 * (unsigned __int128)*(_QWORD *)(v4 + 8 * v12 - 8);
        v9 = __CFADD__((_QWORD)v16, *(_QWORD *)(a2 + 8 * v12 - 8));
        *(_QWORD *)(a2 + 8 * v12 - 8) += v16;
        *(_QWORD *)(a2 + 8 * v12) += *((_QWORD *)&v16 + 1) + v9;
        ++v10;
      }
      while ( a1 > v10 );
    }
  }
}

//----- (000000000002CFD2) ----------------------------------------------------
signed __int64 __fastcall ccn_n(__int64 a1, __int64 a2)
{
  signed __int64 v2; // rdi@1
  signed __int64 result; // rax@2
  bool v4; // zf@3

  v2 = a1 + 1;
  do
  {
    result = 0LL;
    if ( v2 == 1 )
      break;
    v4 = *(_QWORD *)(a2 + 8 * v2-- - 16) == 0LL;
    result = v2;
  }
  while ( v4 );
  return result;
}

//----- (000000000002CFF5) ----------------------------------------------------
int __fastcall ccn_random_bits(__int64 a1, __int64 a2, __int64 a3)
{
  unsigned __int64 v3; // rbx@1
  int result; // eax@1

  v3 = (unsigned __int64)(a1 + 63) >> 6;
  result = (*(int (__fastcall **)(__int64, unsigned __int64, __int64))a3)(a3, 8 * v3, a2);
  if ( !result )
  {
    if ( (unsigned __int64)(a1 & 0x3F) )
      *(_QWORD *)(a2 + 8 * v3 - 8) &= 0xFFFFFFFFFFFFFFFFLL >> (64 - ((unsigned __int8)a1 & 0x3Fu));
  }
  return result;
}

//----- (000000000002D04C) ----------------------------------------------------
signed __int64 __fastcall ccn_read_uint(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4)
{
  unsigned __int64 v4; // r8@1
  unsigned __int64 v5; // r9@1
  signed __int64 result; // rax@6
  signed __int64 v7; // rdx@8
  signed __int64 v8; // r11@9
  __int64 v9; // rcx@9
  unsigned __int64 v10; // rbx@9
  __int64 v11; // r10@9
  signed __int64 v12; // rax@14
  unsigned __int64 v13; // rdi@14

  v4 = a4;
  v5 = 0LL;
  if ( !a3 )
  {
LABEL_13:
    if ( v5 < a1 )
    {
      v12 = a2 + 8 * v5;
      v13 = a1 - v5;
      do
      {
        *(_QWORD *)v12 = 0LL;
        v12 += 8LL;
        --v13;
      }
      while ( v13 );
    }
    return 0LL;
  }
  v5 = 0LL;
  while ( !*(_BYTE *)v4 )
  {
    ++v4;
    --a3;
    if ( !a3 )
      goto LABEL_13;
  }
  result = 0xFFFFFFFFLL;
  if ( a3 <= 8 * a1 )
  {
    v5 = 0LL;
    if ( (signed __int64)a3 > 0 )
    {
      v7 = v4 + a3;
      v5 = 0LL;
      do
      {
        v8 = v7 - 1;
        v9 = 0LL;
        v10 = 0LL;
        v11 = 0LL;
        do
        {
          v7 = v8;
          if ( v8 < v4 )
            break;
          v11 |= (unsigned __int64)*(_BYTE *)v8 << v9;
          ++v10;
          v9 += 8LL;
          --v8;
        }
        while ( v10 < 8 );
        *(_QWORD *)(a2 + 8 * v5++) = v11;
      }
      while ( v7 > v4 );
    }
    goto LABEL_13;
  }
  return result;
}

//----- (000000000002D0EB) ----------------------------------------------------
__int64 __fastcall fe_mul(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // STA0_8@1
  __int64 v4; // ST2D8_8@1
  __int64 v5; // ST288_8@1
  __int64 v6; // r15@1
  __int64 v7; // ST2E8_8@1
  __int64 v8; // ST298_8@1
  __int64 v9; // r13@1
  __int64 v10; // ST2F0_8@1
  int v11; // ST48_4@1
  __int64 v12; // rax@1
  __int64 v13; // ST218_8@1
  __int64 v14; // ST2C8_8@1
  __int64 v15; // ST60_8@1
  __int64 v16; // ST70_8@1
  __int64 v17; // ST80_8@1
  __int64 v18; // ST58_8@1
  __int64 v19; // ST68_8@1
  __int64 v20; // ST78_8@1
  __int64 v21; // ST88_8@1
  __int64 v22; // ST90_8@1
  __int64 v23; // ST98_8@1
  __int64 v24; // ST20_8@1
  __int64 v25; // ST138_8@1
  __int64 v26; // r14@1
  __int64 v27; // ST150_8@1
  __int64 v28; // ST28_8@1
  int v29; // ST44_4@1
  int v30; // ST34_4@1
  __int64 v31; // r10@1
  __int64 v32; // ST280_8@1
  __int64 v33; // ST200_8@1
  __int64 v34; // ST260_8@1
  __int64 v35; // ST2B0_8@1
  __int64 v36; // ST180_8@1
  __int64 v37; // ST108_8@1
  __int64 v38; // ST1D8_8@1
  __int64 v39; // ST208_8@1
  __int64 v40; // ST240_8@1
  __int64 v41; // ST2B8_8@1
  __int64 v42; // rdx@1
  __int64 v43; // ST228_8@1
  __int64 v44; // ST118_8@1
  __int64 v45; // ST258_8@1
  __int64 v46; // ST290_8@1
  __int64 v47; // ST268_8@1
  __int64 v48; // ST110_8@1
  __int64 v49; // ST1A0_8@1
  __int64 v50; // STE0_8@1
  __int64 v51; // ST158_8@1
  __int64 v52; // rdi@1
  __int64 v53; // ST1C8_8@1
  __int64 v54; // rbx@1
  __int64 v55; // STF0_8@1
  __int64 v56; // STB8_8@1
  __int64 v57; // ST1D0_8@1
  __int64 v58; // ST278_8@1
  __int64 v59; // ST2E8_8@1
  __int64 v60; // ST168_8@1
  __int64 v61; // ST160_8@1
  __int64 v62; // ST238_8@1
  __int64 v63; // ST270_8@1
  __int64 v64; // ST1B0_8@1
  __int64 v65; // ST1B8_8@1
  __int64 v66; // ST1F0_8@1
  __int64 v67; // ST220_8@1
  __int64 v68; // ST230_8@1
  __int64 v69; // r11@1
  __int64 v70; // ST2D8_8@1
  __int64 v71; // ST140_8@1
  __int64 v72; // rcx@1
  __int64 v73; // ST128_8@1
  __int64 v74; // ST178_8@1
  __int64 v75; // STA8_8@1
  signed __int64 v76; // r10@1
  signed __int64 v77; // rcx@1
  __int64 v78; // r13@1
  unsigned __int64 v79; // ST200_8@1
  signed __int64 v80; // r15@1
  __int64 v81; // r9@1
  signed __int64 v82; // rsi@1
  unsigned __int64 v83; // r15@1
  signed __int64 v84; // rbx@1
  signed __int64 v85; // rax@1
  signed __int64 v86; // rdi@1
  __int64 v87; // ST238_8@1
  signed __int64 v88; // rdx@1
  signed __int64 v89; // r8@1
  signed __int64 v90; // rcx@1
  __int64 v91; // rax@1
  signed __int64 v92; // rbx@1
  signed __int64 v93; // rcx@1
  __int64 v94; // r14@1
  signed __int64 v95; // rax@1
  __int64 v96; // r12@1
  signed __int64 v97; // rdx@1
  unsigned __int64 v98; // rax@1
  signed __int64 v99; // rcx@1
  __int64 v100; // rdi@1
  signed __int64 v101; // rdx@1
  signed __int64 v102; // rbx@1
  signed __int64 v103; // rdx@1
  __int64 result; // rax@1

  v3 = a1;
  v4 = *(_DWORD *)(a3 + 4);
  v5 = *(_DWORD *)(a3 + 12);
  v6 = *(_DWORD *)(a3 + 12);
  v7 = *(_DWORD *)(a3 + 16);
  v8 = *(_DWORD *)(a3 + 24);
  v9 = *(_DWORD *)(a3 + 24);
  v10 = *(_DWORD *)(a3 + 28);
  v11 = 19 * *(_DWORD *)(a3 + 36);
  v12 = *(_DWORD *)a2;
  v13 = *(_DWORD *)a3;
  v14 = *(_DWORD *)(a3 + 32);
  v15 = v12 * v4;
  v16 = v12 * *(_DWORD *)(a3 + 8);
  v17 = v12 * v6;
  v18 = v12 * v7;
  v19 = v12 * *(_DWORD *)(a3 + 20);
  v20 = v12 * v9;
  v21 = v12 * v10;
  v22 = v12 * v14;
  v23 = v12 * *(_DWORD *)(a3 + 36);
  v24 = *(_DWORD *)(a2 + 4);
  v25 = *(_DWORD *)(a2 + 12);
  v26 = *(_DWORD *)(a2 + 20);
  v27 = *(_DWORD *)(a2 + 28);
  v28 = *(_DWORD *)(a2 + 36);
  v29 = 19 * v4;
  v30 = 2 * v26;
  v31 = *(_DWORD *)a3;
  v32 = *(_DWORD *)(a3 + 8);
  v33 = *(_DWORD *)(a3 + 12);
  v34 = *(_DWORD *)(a3 + 20);
  v35 = *(_DWORD *)(a3 + 24);
  v36 = *(_DWORD *)(a2 + 8);
  v37 = *(_DWORD *)a3;
  v38 = *(_DWORD *)(a3 + 8);
  v39 = *(_DWORD *)(a3 + 12);
  v40 = *(_DWORD *)(a3 + 16);
  v41 = *(_DWORD *)(a3 + 24);
  v42 = 19 * (signed int)v14;
  v43 = v6;
  v44 = *(_DWORD *)(a2 + 16);
  v45 = v6;
  v46 = v7;
  v47 = v6;
  v48 = *(_DWORD *)(a2 + 24);
  v49 = 19 * (signed int)v7;
  v50 = 19 * (signed int)v7;
  v51 = 19 * (signed int)v9;
  v52 = 19 * (signed int)v7;
  v53 = 19 * (signed int)v9;
  v54 = 19 * (signed int)v32;
  v55 = 19 * (signed int)v7;
  v56 = 19 * (signed int)v9;
  v57 = v24 * v7;
  v58 = v25 * v7;
  v59 = v26 * v7;
  v60 = v42 * v26;
  v61 = v28 * v42;
  v62 = 2 * (signed int)v24 * v4;
  v63 = v36 * v4;
  v64 = 2 * (signed int)v25 * v4;
  v65 = v44 * v4;
  v66 = 2 * (signed int)v26 * v4;
  v67 = v48 * v4;
  v68 = 2 * (signed int)v27 * v4;
  v69 = *(_DWORD *)(a2 + 32);
  v70 = v69 * v4;
  v71 = v69 * v42;
  v72 = 2 * (signed int)v28;
  v73 = v72 * 19 * (signed int)v6;
  v74 = v72 * 19 * (signed int)v10;
  v75 = v72 * v11;
  v76 = v48 * v42
      + 2 * (signed int)v27 * (signed __int64)(19 * (signed int)v10)
      + v69 * v56
      + v72 * 19 * (signed int)v34
      + v18
      + 2 * (signed int)v24 * v33
      + v36 * v38
      + v64
      + v44 * v31;
  v77 = v36 * v42
      + 2 * (signed int)v25 * (signed __int64)(19 * (signed int)v10)
      + v44 * v51
      + 2 * (signed int)v26 * (signed __int64)(19 * (signed int)v34)
      + v48 * v50
      + 2 * (signed int)v27 * (signed __int64)(19 * (signed int)v6)
      + v69 * v54
      + v12 * v13
      + v72 * v29
      + v11 * (signed __int64)(2 * (signed int)v24);
  v78 = v11 * v36
      + v42 * v25
      + v44 * 19 * (signed int)v10
      + v26 * v51
      + v48 * 19 * (signed int)v34
      + v27 * v52
      + v69 * 19 * (signed int)v6
      + v28 * v54
      + v15
      + v24 * v13;
  v79 = v77 - ((v77 + 0x2000000) & 0xFFFFFFFFFC000000LL);
  v80 = v76 + v11 * (signed __int64)(2 * (signed int)v26);
  v81 = v11 * v48
      + v42 * v27
      + v69 * 19 * (signed int)v10
      + v28 * v56
      + v19
      + v57
      + v36 * v39
      + v25 * v32
      + v65
      + v26 * v13;
  v82 = v80 + 0x2000000;
  v83 = v80 - ((v80 + 0x2000000) & 0xFFFFFFFFFC000000LL);
  v84 = (v77 + 0x2000000) >> 26;
  v85 = v84 + v78 + 0x1000000;
  v86 = v11 * (signed __int64)(2 * (signed int)v25)
      + v44 * v42
      + 2 * (signed int)v26 * (signed __int64)(19 * (signed int)v10)
      + v48 * v51
      + 2 * (signed int)v27 * (signed __int64)(19 * (signed int)v34)
      + v69 * v55
      + v73
      + v16
      + v62
      + v36 * v37;
  v87 = (_DWORD)v78 + (_DWORD)v84 - ((unsigned int)v85 & 0xFE000000);
  v82 >>= 26;
  v88 = v82 + v81 + 0x1000000;
  v89 = v11 * (signed __int64)(2 * (signed int)v27)
      + v71
      + v74
      + v20
      + 2 * (signed int)v24 * v34
      + v36 * v40
      + 2 * (signed int)v25 * v43
      + v44 * v32
      + v66
      + v48 * v13;
  LODWORD(v78) = v81 + v82 - (v88 & 0xFE000000);
  LODWORD(v76) = v86 + (v85 >> 25);
  v90 = (v85 >> 25) + v86 + 0x2000000;
  v91 = v11 * v44
      + v60
      + v48 * 19 * (signed int)v10
      + v27 * v53
      + v69 * 19 * (signed int)v34
      + v28 * v49
      + v17
      + v24 * v32
      + v63
      + v25 * v13;
  v92 = v90;
  LODWORD(v76) = v76 - (v90 & 0xFC000000);
  v93 = (v88 >> 25) + v89 + 0x2000000;
  v94 = v11 * v69 + v61 + v21 + v24 * v35 + v36 * v34 + v58 + v44 * v45 + v26 * v32 + v67 + v27 * v13;
  LODWORD(v81) = v89 + (v88 >> 25) - (v93 & 0xFC000000);
  v92 >>= 26;
  LODWORD(v89) = v91 + v92;
  v95 = v92 + v91 + 0x1000000;
  LODWORD(v89) = v89 - (v95 & 0xFE000000);
  v96 = v75
      + v22
      + 2 * (signed int)v24 * v10
      + v36 * v41
      + 2 * (signed int)v25 * v34
      + v44 * v46
      + v30 * v47
      + v48 * v32
      + v68
      + v69 * v13;
  v97 = (v93 >> 26) + v94 + 0x1000000;
  LODWORD(v69) = v94 + (v93 >> 26) - (v97 & 0xFE000000);
  v95 >>= 25;
  LODWORD(v94) = v95 + v83;
  v98 = v95 + v83 + 0x2000000;
  LODWORD(v82) = v78 + (v98 >> 26);
  LODWORD(v94) = v94 - (v98 & 0xFC000000);
  v97 >>= 25;
  LODWORD(v83) = v96 + v97;
  v99 = v97 + v96 + 0x2000000;
  v100 = v23 + v24 * v14 + v36 * v10 + v25 * v8 + v44 * v34 + v59 + v48 * v5 + v27 * v32 + v70 + v28 * v13;
  v101 = v99 >> 26;
  LODWORD(v83) = v83 - (v99 & 0xFC000000);
  LODWORD(v99) = v100 + v101;
  v102 = v101 + v100 + 0x1000000;
  v103 = 19 * (v102 >> 25);
  result = v87;
  *(_DWORD *)v3 = v103 + v79 - ((v103 + v79 + 0x2000000) & 0xFC000000);
  *(_DWORD *)(v3 + 4) = v87 + ((v103 + v79 + 0x2000000) >> 26);
  *(_DWORD *)(v3 + 8) = v76;
  *(_DWORD *)(v3 + 12) = v89;
  *(_DWORD *)(v3 + 16) = v94;
  *(_DWORD *)(v3 + 20) = v82;
  *(_DWORD *)(v3 + 24) = v81;
  *(_DWORD *)(v3 + 28) = v69;
  *(_DWORD *)(v3 + 32) = v83;
  *(_DWORD *)(v3 + 36) = v99 - (v102 & 0xFE000000);
  return result;
}

//----- (000000000002DEB4) ----------------------------------------------------
__int64 __fastcall fe_sq(__int64 a1, __int64 a2)
{
  __int64 v2; // ST48_8@1
  __int64 v3; // rax@1
  __int64 v4; // ST38_8@1
  __int64 v5; // r9@1
  __int64 v6; // r11@1
  __int64 v7; // r12@1
  __int64 v8; // ST18_8@1
  __int64 v9; // ST80_8@1
  __int64 v10; // ST20_8@1
  __int64 v11; // STC8_8@1
  __int64 v12; // ST50_8@1
  __int64 v13; // rdx@1
  __int64 v14; // STD0_8@1
  __int64 v15; // ST78_8@1
  __int64 v16; // r13@1
  __int64 v17; // rcx@1
  __int64 v18; // r14@1
  __int64 v19; // ST10_8@1
  __int64 v20; // ST160_8@1
  __int64 v21; // ST08_8@1
  __int64 v22; // ST70_8@1
  __int64 v23; // STA0_8@1
  __int64 v24; // ST00_8@1
  __int64 v25; // ST108_8@1
  __int64 v26; // STF0_8@1
  __int64 v27; // STF8_8@1
  __int64 v28; // ST118_8@1
  __int64 v29; // ST110_8@1
  __int64 v30; // ST158_8@1
  __int64 v31; // ST150_8@1
  __int64 v32; // ST40_8@1
  __int64 v33; // STD8_8@1
  __int64 v34; // STC0_8@1
  __int64 v35; // STE0_8@1
  __int64 v36; // ST120_8@1
  __int64 v37; // ST138_8@1
  __int64 v38; // ST170_8@1
  __int64 v39; // ST168_8@1
  __int64 v40; // r9@1
  __int64 v41; // r10@1
  __int64 v42; // ST100_8@1
  __int64 v43; // ST148_8@1
  __int64 v44; // rax@1
  __int64 v45; // ST18_8@1
  __int64 v46; // ST140_8@1
  __int64 v47; // ST30_8@1
  __int64 v48; // ST130_8@1
  __int64 v49; // rsi@1
  __int64 v50; // ST88_8@1
  __int64 v51; // STB8_8@1
  __int64 v52; // r13@1
  __int64 v53; // rbx@1
  __int64 v54; // STB0_8@1
  __int64 v55; // rax@1
  __int64 v56; // STD0_8@1
  __int64 v57; // rdx@1
  __int64 v58; // r11@1
  __int64 v59; // rcx@1
  signed __int64 v60; // rax@1
  __int64 v61; // rsi@1
  signed __int64 v62; // rdx@1
  unsigned __int64 v63; // ST108_8@1
  signed __int64 v64; // rax@1
  __int64 v65; // r10@1
  signed __int64 v66; // r14@1
  unsigned __int64 v67; // r9@1
  int v68; // edi@1
  signed __int64 v69; // rax@1
  __int64 v70; // rcx@1
  signed __int64 v71; // rdx@1
  signed __int64 v72; // rax@1
  __int64 v73; // rbx@1
  signed __int64 v74; // rsi@1
  int v75; // er8@1
  int v76; // er15@1
  signed __int64 v77; // rax@1
  __int64 v78; // rcx@1
  signed __int64 v79; // rdi@1
  int v80; // er15@1
  signed __int64 v81; // rax@1
  __int64 v82; // rsi@1
  signed __int64 v83; // rdx@1
  signed __int64 v84; // rax@1
  signed __int64 v85; // rcx@1
  signed __int64 v86; // rax@1
  __int64 v87; // rsi@1
  signed __int64 v88; // rdx@1
  unsigned __int64 v89; // rcx@1
  __int64 result; // rax@1
  int v91; // er8@1
  signed __int64 v92; // rsi@1
  __int64 v93; // rdi@1
  signed __int64 v94; // rdx@1

  v2 = a1;
  v3 = *(_DWORD *)a2;
  v4 = v3;
  v5 = *(_DWORD *)(a2 + 4);
  v6 = *(_DWORD *)(a2 + 8);
  v7 = *(_DWORD *)(a2 + 12);
  v8 = *(_DWORD *)(a2 + 20);
  v9 = *(_DWORD *)(a2 + 24);
  v10 = *(_DWORD *)(a2 + 28);
  v11 = *(_DWORD *)(a2 + 32);
  v12 = *(_DWORD *)(a2 + 36);
  v13 = 2 * (signed int)v3;
  v14 = 2 * (signed int)v5;
  v15 = 2 * (signed int)v7;
  v16 = 2 * (signed int)v8;
  v17 = 2 * (signed int)v10;
  v18 = 2 * (signed int)v6;
  v19 = 38 * (signed int)v12;
  v20 = *(_DWORD *)(a2 + 24);
  v21 = *(_DWORD *)(a2 + 16);
  v22 = 2 * (signed int)v21;
  v23 = 19 * (signed int)v9;
  v24 = 19 * (signed int)v11;
  v25 = v5 * v13;
  v26 = v13 * v6;
  v27 = v13 * v21;
  v28 = v13 * v8;
  v29 = v13 * v20;
  v30 = v13 * v10;
  v31 = v13 * v11;
  v32 = v12 * v13;
  v33 = v14 * v5;
  v34 = v14 * v15;
  v35 = v14 * v21;
  v36 = v14 * v16;
  v37 = v14 * v20;
  v38 = v14 * v17;
  v39 = v14 * v11;
  v40 = (signed int)v19 * v14;
  v41 = v7 * v18;
  v42 = v18 * v21;
  v43 = v8 * v22;
  v44 = v8 * 38 * (signed int)v8;
  v45 = v18 * v8;
  v46 = v18 * v20;
  v47 = v18 * v10;
  v48 = v15 * v16;
  v49 = v16 * v23;
  v50 = v16 * 38 * (signed int)v10;
  v51 = v16 * v24;
  v52 = (signed int)v19 * v16;
  v53 = v17 * v24;
  v54 = (signed int)v19 * v17;
  v55 = v24 * v18 + v15 * 38 * (signed int)v10 + v22 * v23 + v4 * v4 + v44;
  v56 = v13 * v7 + v6 * v14;
  v57 = v6 * v19;
  v58 = v24 * 2 * (signed int)v9 + v10 * 38 * (signed int)v10 + v27 + v34 + v6 * v6;
  v59 = v55 + v40;
  v60 = v40 + v55 + 0x2000000;
  v61 = v57 + v15 * v24 + v21 * 38 * (signed int)v10 + v25 + v49;
  v62 = v60;
  v63 = v59 - (v60 & 0xFFFFFFFFFC000000LL);
  v64 = v52 + v58 + 0x2000000;
  v65 = (signed int)v19 * v9 + v53 + v28 + v35 + v41;
  v66 = v64;
  v67 = v58 + v52 - (v64 & 0xFFFFFFFFFC000000LL);
  v62 >>= 26;
  v68 = v61 + v62;
  v69 = v62 + v61 + 0x1000000;
  v70 = (signed int)v19 * v15 + v24 * v22 + v50 + v9 * v23 + v26 + v33;
  v71 = v69;
  LODWORD(v28) = v68 - (v69 & 0xFE000000);
  v66 >>= 26;
  v72 = v66 + v65 + 0x1000000;
  v73 = v54 + v11 * v24 + v29 + v36 + v42 + v15 * v7;
  v74 = v72;
  v75 = v65 + v66 - (v72 & 0xFE000000);
  v71 >>= 25;
  v76 = v70 + v71;
  v77 = v71 + v70 + 0x2000000;
  v78 = (signed int)v19 * v21 + v51 + v9 * 38 * (signed int)v10 + v56;
  v79 = v77;
  v80 = v76 - (v77 & 0xFC000000);
  v74 >>= 25;
  LODWORD(v65) = v73 + v74;
  v81 = v74 + v73 + 0x2000000;
  v82 = (signed int)v19 * v11 + v30 + v37 + v45 + v21 * v15;
  v83 = v81;
  LODWORD(v65) = v65 - (v81 & 0xFC000000);
  v79 >>= 26;
  LODWORD(v58) = v78 + v79;
  v84 = v79 + v78 + 0x1000000;
  v85 = v84;
  LODWORD(v58) = v58 - (v84 & 0xFE000000);
  v83 >>= 26;
  LODWORD(v66) = v82 + v83;
  v86 = v83 + v82 + 0x1000000;
  v87 = v12 * (signed int)v19 + v31 + v38 + v48 + v46 + v21 * v21;
  v88 = v86;
  LODWORD(v66) = v66 - (v86 & 0xFE000000);
  v85 >>= 25;
  LODWORD(v7) = v85 + v67;
  v89 = v85 + v67 + 0x2000000;
  result = v75 + (unsigned int)(v89 >> 26);
  v88 >>= 25;
  v91 = v87 + v88;
  v92 = v88 + v87 + 0x2000000;
  v93 = v32 + v39 + v47 + v15 * v20 + v43;
  v94 = 19 * (((v92 >> 26) + v93 + 0x1000000) >> 25);
  *(_DWORD *)v2 = v94 + v63 - ((v94 + v63 + 0x2000000) & 0xFC000000);
  *(_DWORD *)(v2 + 4) = v28 + ((v94 + v63 + 0x2000000) >> 26);
  *(_DWORD *)(v2 + 8) = v80;
  *(_DWORD *)(v2 + 12) = v58;
  *(_DWORD *)(v2 + 16) = v7 - (v89 & 0xFC000000);
  *(_DWORD *)(v2 + 20) = result;
  *(_DWORD *)(v2 + 24) = v65;
  *(_DWORD *)(v2 + 28) = v66;
  *(_DWORD *)(v2 + 32) = v91 - (v92 & 0xFC000000);
  *(_DWORD *)(v2 + 36) = v93 + (v92 >> 26) - (((v92 >> 26) + v93 + 0x1000000) & 0xFE000000);
  return result;
}

//----- (000000000002E65F) ----------------------------------------------------
__int64 __fastcall fe_tobytes(__int64 a1, __int64 a2)
{
  int v2; // ebx@1
  int v3; // eax@1
  int v4; // er15@1
  int v5; // er12@1
  int v6; // er14@1
  int v7; // er11@1
  int v8; // er10@1
  int v9; // er9@1
  int v10; // er8@1
  int v11; // edx@1
  unsigned int v12; // ecx@1
  signed int v13; // esi@1
  unsigned int v14; // edx@1
  signed int v15; // eax@1
  unsigned int v16; // esi@1
  signed int v17; // er15@1
  unsigned int v18; // eax@1
  signed int v19; // er12@1
  unsigned int v20; // er15@1
  signed int v21; // er14@1
  unsigned int v22; // er12@1
  signed int v23; // er11@1
  unsigned int v24; // er14@1
  signed int v25; // er10@1
  unsigned int v26; // er11@1
  unsigned int v27; // er9@1
  unsigned int v28; // er10@1
  __int64 result; // rax@1

  v2 = *(_DWORD *)(a2 + 4);
  v3 = *(_DWORD *)(a2 + 8);
  v4 = *(_DWORD *)(a2 + 12);
  v5 = *(_DWORD *)(a2 + 16);
  v6 = *(_DWORD *)(a2 + 20);
  v7 = *(_DWORD *)(a2 + 24);
  v8 = *(_DWORD *)(a2 + 28);
  v9 = *(_DWORD *)(a2 + 32);
  v10 = *(_DWORD *)(a2 + 36);
  v11 = v2
      + ((*(_DWORD *)a2
        + 19
        * ((v10
          + ((v9
            + ((v8
              + ((v7
                + ((v6
                  + ((v5
                    + ((v4 + ((v3 + ((v2 + ((*(_DWORD *)a2 + ((19 * v10 + 0x1000000) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26);
  v12 = *(_DWORD *)a2
      + 19
      * ((v10
        + ((v9
          + ((v8
            + ((v7
              + ((v6
                + ((v5
                  + ((v4 + ((v3 + ((v2 + ((*(_DWORD *)a2 + ((19 * v10 + 0x1000000) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)
      - ((*(_DWORD *)a2
        + 19
        * ((v10
          + ((v9
            + ((v8
              + ((v7
                + ((v6
                  + ((v5
                    + ((v4 + ((v3 + ((v2 + ((*(_DWORD *)a2 + ((19 * v10 + 0x1000000) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) & 0xFC000000);
  v13 = v3
      + ((v2
        + ((*(_DWORD *)a2
          + 19
          * ((v10
            + ((v9
              + ((v8
                + ((v7
                  + ((v6
                    + ((v5
                      + ((v4 + ((v3 + ((v2 + ((*(_DWORD *)a2 + ((19 * v10 + 0x1000000) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25)) >> 26)) >> 25);
  v14 = v11 - (v11 & 0xFE000000);
  v15 = v4 + (v13 >> 26);
  v16 = v13 - (v13 & 0xFC000000);
  v17 = v5 + (v15 >> 25);
  v18 = v15 - (v15 & 0xFE000000);
  v19 = v6 + (v17 >> 26);
  v20 = v17 - (v17 & 0xFC000000);
  v21 = v7 + (v19 >> 25);
  v22 = v19 - (v19 & 0xFE000000);
  v23 = v8 + (v21 >> 26);
  v24 = v21 - (v21 & 0xFC000000);
  v25 = v9 + (v23 >> 25);
  v26 = v23 - (v23 & 0xFE000000);
  v27 = v10 + (v25 >> 26);
  v28 = v25 - (v25 & 0xFC000000);
  *(_BYTE *)a1 = v12;
  *(_BYTE *)(a1 + 1) = BYTE1(v12);
  *(_BYTE *)(a1 + 2) = v12 >> 16;
  *(_BYTE *)(a1 + 3) = BYTE3(v12) | 4 * v14;
  *(_BYTE *)(a1 + 4) = v14 >> 6;
  *(_BYTE *)(a1 + 5) = v14 >> 14;
  *(_BYTE *)(a1 + 6) = (v14 >> 22) | 8 * v16;
  *(_BYTE *)(a1 + 7) = v16 >> 5;
  *(_BYTE *)(a1 + 8) = v16 >> 13;
  *(_BYTE *)(a1 + 9) = (v16 >> 21) | 32 * v18;
  *(_BYTE *)(a1 + 10) = v18 >> 3;
  *(_BYTE *)(a1 + 11) = v18 >> 11;
  *(_BYTE *)(a1 + 12) = (v18 >> 19) | ((_BYTE)v20 << 6);
  *(_BYTE *)(a1 + 13) = v20 >> 2;
  *(_BYTE *)(a1 + 14) = v20 >> 10;
  *(_BYTE *)(a1 + 15) = v20 >> 18;
  *(_BYTE *)(a1 + 16) = v22;
  *(_BYTE *)(a1 + 17) = BYTE1(v22);
  *(_BYTE *)(a1 + 18) = v22 >> 16;
  *(_BYTE *)(a1 + 19) = BYTE3(v22) | 2 * v24;
  *(_BYTE *)(a1 + 20) = v24 >> 7;
  *(_BYTE *)(a1 + 21) = v24 >> 15;
  *(_BYTE *)(a1 + 22) = (v24 >> 23) | 8 * v26;
  *(_BYTE *)(a1 + 23) = v26 >> 5;
  *(_BYTE *)(a1 + 24) = v26 >> 13;
  *(_BYTE *)(a1 + 25) = (v26 >> 21) | 16 * v28;
  *(_BYTE *)(a1 + 26) = v28 >> 4;
  *(_BYTE *)(a1 + 27) = v28 >> 12;
  *(_BYTE *)(a1 + 28) = (v28 >> 20) | ((_BYTE)v27 << 6);
  result = v27 >> 2;
  *(_BYTE *)(a1 + 29) = result;
  *(_BYTE *)(a1 + 30) = v27 >> 10;
  *(_BYTE *)(a1 + 31) = (v27 & 0x1FFFFFF) >> 18;
  return result;
}

//----- (000000000002E8CF) ----------------------------------------------------
__int64 __fastcall ge_double_scalarmult_vartime(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // r15@1
  __int64 v6; // r13@1
  signed int v7; // edx@1
  __int64 v8; // rax@1
  signed __int64 v9; // rcx@8
  __int64 v10; // rbx@8
  signed __int64 v11; // r12@9
  signed int *v12; // rax@9
  __int64 v13; // r12@9
  signed int v14; // ebx@9
  signed __int64 v15; // rbx@12
  int v16; // STB8_4@12
  int v17; // ST58_4@12
  int v18; // ST60_4@12
  int v19; // ST68_4@12
  int v20; // ST70_4@12
  int v21; // ST78_4@12
  int v22; // ST80_4@12
  int v23; // ST88_4@12
  int v24; // ST90_4@12
  int v25; // ST98_4@12
  signed int v26; // ebx@13
  __int64 v27; // r12@14
  signed __int64 v28; // r15@16
  signed int *v30; // [sp+C0h] [bp-9A0h]@10
  int v31; // [sp+C8h] [bp-998h]@8
  char *v32; // [sp+D0h] [bp-990h]@8
  char v33; // [sp+D8h] [bp-988h]@1
  int v34; // [sp+178h] [bp-8E8h]@1
  int v35; // [sp+17Ch] [bp-8E4h]@12
  int v36; // [sp+180h] [bp-8E0h]@12
  int v37; // [sp+184h] [bp-8DCh]@12
  int v38; // [sp+188h] [bp-8D8h]@12
  int v39; // [sp+18Ch] [bp-8D4h]@12
  int v40; // [sp+190h] [bp-8D0h]@12
  int v41; // [sp+194h] [bp-8CCh]@12
  int v42; // [sp+198h] [bp-8C8h]@12
  int v43; // [sp+19Ch] [bp-8C4h]@12
  int v44; // [sp+1A0h] [bp-8C0h]@12
  int v45; // [sp+1A4h] [bp-8BCh]@12
  int v46; // [sp+1A8h] [bp-8B8h]@12
  int v47; // [sp+1ACh] [bp-8B4h]@12
  int v48; // [sp+1B0h] [bp-8B0h]@12
  int v49; // [sp+1B4h] [bp-8ACh]@12
  int v50; // [sp+1B8h] [bp-8A8h]@12
  int v51; // [sp+1BCh] [bp-8A4h]@12
  int v52; // [sp+1C0h] [bp-8A0h]@12
  int v53; // [sp+1C4h] [bp-89Ch]@12
  int v54; // [sp+1C8h] [bp-898h]@12
  int v55; // [sp+1CCh] [bp-894h]@16
  int v56; // [sp+1D0h] [bp-890h]@16
  int v57; // [sp+1D4h] [bp-88Ch]@16
  int v58; // [sp+1D8h] [bp-888h]@16
  int v59; // [sp+1DCh] [bp-884h]@16
  int v60; // [sp+1E0h] [bp-880h]@16
  int v61; // [sp+1E4h] [bp-87Ch]@16
  int v62; // [sp+1E8h] [bp-878h]@16
  int v63; // [sp+1ECh] [bp-874h]@16
  char v64; // [sp+1F0h] [bp-870h]@12
  int v65; // [sp+218h] [bp-848h]@1
  int v66; // [sp+21Ch] [bp-844h]@12
  int v67; // [sp+220h] [bp-840h]@12
  int v68; // [sp+224h] [bp-83Ch]@12
  int v69; // [sp+228h] [bp-838h]@12
  int v70; // [sp+22Ch] [bp-834h]@12
  int v71; // [sp+230h] [bp-830h]@12
  int v72; // [sp+234h] [bp-82Ch]@12
  int v73; // [sp+238h] [bp-828h]@12
  int v74; // [sp+23Ch] [bp-824h]@12
  int v75; // [sp+240h] [bp-820h]@12
  int v76; // [sp+244h] [bp-81Ch]@12
  int v77; // [sp+248h] [bp-818h]@12
  int v78; // [sp+24Ch] [bp-814h]@12
  int v79; // [sp+250h] [bp-810h]@12
  int v80; // [sp+254h] [bp-80Ch]@12
  int v81; // [sp+258h] [bp-808h]@12
  int v82; // [sp+25Ch] [bp-804h]@12
  int v83; // [sp+260h] [bp-800h]@12
  int v84; // [sp+264h] [bp-7FCh]@12
  int v85; // [sp+268h] [bp-7F8h]@12
  int v86; // [sp+26Ch] [bp-7F4h]@12
  int v87; // [sp+270h] [bp-7F0h]@12
  int v88; // [sp+274h] [bp-7ECh]@12
  int v89; // [sp+278h] [bp-7E8h]@12
  int v90; // [sp+27Ch] [bp-7E4h]@12
  int v91; // [sp+280h] [bp-7E0h]@12
  int v92; // [sp+284h] [bp-7DCh]@12
  int v93; // [sp+288h] [bp-7D8h]@12
  int v94; // [sp+28Ch] [bp-7D4h]@12
  int v95; // [sp+290h] [bp-7D0h]@12
  int v96; // [sp+294h] [bp-7CCh]@12
  int v97; // [sp+298h] [bp-7C8h]@12
  int v98; // [sp+29Ch] [bp-7C4h]@12
  int v99; // [sp+2A0h] [bp-7C0h]@12
  int v100; // [sp+2A4h] [bp-7BCh]@12
  int v101; // [sp+2A8h] [bp-7B8h]@12
  int v102; // [sp+2ACh] [bp-7B4h]@12
  int v103; // [sp+2B0h] [bp-7B0h]@12
  int v104; // [sp+2B4h] [bp-7ACh]@12
  char v105; // [sp+2B8h] [bp-7A8h]@1
  char v106[40]; // [sp+330h] [bp-730h]@1
  char v107[40]; // [sp+358h] [bp-708h]@12
  char v108[40]; // [sp+380h] [bp-6E0h]@12
  char v109[40]; // [sp+3A8h] [bp-6B8h]@12
  char v110; // [sp+3D0h] [bp-690h]@1
  char v111; // [sp+470h] [bp-5F0h]@1
  char v112; // [sp+510h] [bp-550h]@1
  char v113; // [sp+5B0h] [bp-4B0h]@1
  char v114; // [sp+650h] [bp-410h]@1
  char v115; // [sp+6F0h] [bp-370h]@1
  char v116; // [sp+790h] [bp-2D0h]@1
  char v117[255]; // [sp+830h] [bp-230h]@1
  char v118; // [sp+92Fh] [bp-131h]@3
  char v119[255]; // [sp+930h] [bp-130h]@1
  char v120; // [sp+A2Fh] [bp-31h]@2
  __int64 v121; // [sp+A30h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v121 = *(_QWORD *)off_69010[0];
  ge_slide((__int64)v119, a2);
  ge_slide((__int64)v117, v4);
  ge_p3_to_cached((__int64)v106, v5);
  ge_p3_to_p2((__int64)&v105, v5);
  ge_p2_dbl((__int64)&v65, (__int64)&v105);
  ge_p1p1_to_p3((__int64)&v33, (__int64)&v65);
  ge_add((__int64)&v65, (__int64)&v33, (__int64)v106);
  ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
  ge_p3_to_cached((__int64)&v110, (__int64)&v34);
  ge_add((__int64)&v65, (__int64)&v33, (__int64)&v110);
  ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
  ge_p3_to_cached((__int64)&v111, (__int64)&v34);
  ge_add((__int64)&v65, (__int64)&v33, (__int64)&v111);
  ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
  ge_p3_to_cached((__int64)&v112, (__int64)&v34);
  ge_add((__int64)&v65, (__int64)&v33, (__int64)&v112);
  ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
  ge_p3_to_cached((__int64)&v113, (__int64)&v34);
  ge_add((__int64)&v65, (__int64)&v33, (__int64)&v113);
  ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
  ge_p3_to_cached((__int64)&v114, (__int64)&v34);
  ge_add((__int64)&v65, (__int64)&v33, (__int64)&v114);
  ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
  ge_p3_to_cached((__int64)&v115, (__int64)&v34);
  v6 = a1;
  ge_add((__int64)&v65, (__int64)&v33, (__int64)&v115);
  ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
  ge_p3_to_cached((__int64)&v116, (__int64)&v34);
  *(_DWORD *)a1 = 0;
  *(_DWORD *)(a1 + 4) = 0;
  *(_DWORD *)(a1 + 8) = 0;
  *(_DWORD *)(a1 + 12) = 0;
  *(_DWORD *)(a1 + 16) = 0;
  *(_DWORD *)(a1 + 20) = 0;
  *(_DWORD *)(a1 + 24) = 0;
  *(_DWORD *)(a1 + 28) = 0;
  *(_DWORD *)(a1 + 32) = 0;
  *(_DWORD *)(a1 + 36) = 0;
  *(_DWORD *)(a1 + 40) = 1;
  *(_DWORD *)(a1 + 44) = 0;
  *(_DWORD *)(a1 + 48) = 0;
  *(_DWORD *)(a1 + 52) = 0;
  *(_DWORD *)(a1 + 56) = 0;
  *(_DWORD *)(a1 + 60) = 0;
  *(_DWORD *)(a1 + 64) = 0;
  *(_DWORD *)(a1 + 68) = 0;
  *(_DWORD *)(a1 + 72) = 0;
  *(_DWORD *)(a1 + 76) = 0;
  *(_DWORD *)(a1 + 80) = 1;
  *(_QWORD *)(a1 + 84) = 0LL;
  *(_QWORD *)(a1 + 92) = 0LL;
  *(_QWORD *)(a1 + 100) = 0LL;
  *(_QWORD *)(a1 + 108) = 0LL;
  *(_DWORD *)(a1 + 116) = 0;
  v7 = 255;
  v8 = 0LL;
  while ( !*(&v120 + v8) )
  {
    if ( !*(&v118 + v8) )
    {
      --v7;
      --v8;
      if ( (signed int)v8 + 256 > 0 )
        continue;
    }
    goto LABEL_7;
  }
  v7 = v8 + 255;
LABEL_7:
  if ( v7 >= 0 )
  {
    v31 = v7 + 1;
    v9 = (signed __int64)&v119[v7];
    v32 = &v117[v7];
    v10 = (__int64)&v65;
    do
    {
      v11 = v9;
      ge_p2_dbl(v10, v6);
      v12 = (signed int *)v11;
      v13 = v10;
      v14 = *(_BYTE *)v12;
      if ( v14 <= 0 )
      {
        v30 = v12;
        if ( (char)v14 < 0 )
        {
          ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
          v65 = v34 + v44;
          v66 = v35 + v45;
          v67 = v36 + v46;
          v68 = v37 + v47;
          v69 = v38 + v48;
          v70 = v39 + v49;
          v71 = v40 + v50;
          v72 = v41 + v51;
          v73 = v42 + v52;
          v74 = v43 + v53;
          v75 = v44 - v34;
          v76 = v45 - v35;
          v77 = v46 - v36;
          v78 = v47 - v37;
          v79 = v48 - v38;
          v80 = v49 - v39;
          v81 = v50 - v40;
          v82 = v51 - v41;
          v83 = v52 - v42;
          v84 = v53 - v43;
          v15 = 160LL * (v14 / -2);
          fe_mul((__int64)&v85, (__int64)&v65, (__int64)&v107[v15]);
          fe_mul((__int64)&v75, (__int64)&v75, (__int64)&v106[v15]);
          fe_mul((__int64)&v95, (__int64)&v109[v15], (__int64)&v64);
          fe_mul((__int64)&v65, (__int64)&v54, (__int64)&v108[v15]);
          v16 = v65;
          v17 = v66;
          v18 = v67;
          v19 = v68;
          v20 = v69;
          v21 = v70;
          v22 = v71;
          v23 = v72;
          v24 = v73;
          v25 = v74;
          v65 = v85 - v75;
          v66 = v86 - v76;
          v67 = v87 - v77;
          v68 = v88 - v78;
          v69 = v89 - v79;
          v70 = v90 - v80;
          v71 = v91 - v81;
          v72 = v92 - v82;
          v73 = v93 - v83;
          v74 = v94 - v84;
          v75 += v85;
          v76 += v86;
          v77 += v87;
          v78 += v88;
          v79 += v89;
          v80 += v90;
          v81 += v91;
          v82 += v92;
          v83 += v93;
          v84 += v94;
          v85 = 2 * v16 - v95;
          v86 = 2 * v17 - v96;
          v87 = 2 * v18 - v97;
          v88 = 2 * v19 - v98;
          v89 = 2 * v20 - v99;
          v90 = 2 * v21 - v100;
          v91 = 2 * v22 - v101;
          v92 = 2 * v23 - v102;
          v93 = 2 * v24 - v103;
          v94 = 2 * v25 - v104;
          v95 += 2 * v16;
          v96 += 2 * v17;
          v97 += 2 * v18;
          v98 += 2 * v19;
          v99 += 2 * v20;
          v100 += 2 * v21;
          v6 = a1;
          v101 += 2 * v22;
          v102 += 2 * v23;
          v103 += 2 * v24;
          v104 += 2 * v25;
        }
      }
      else
      {
        v30 = v12;
        ge_p1p1_to_p3((__int64)&v34, v13);
        ge_add(v13, (__int64)&v34, (__int64)&v106[160 * (v14 / 2)]);
      }
      v26 = *v32;
      if ( v26 <= 0 )
      {
        v27 = (__int64)&v65;
        if ( (char)v26 < 0 )
        {
          ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
          v65 = v34 + v44;
          v66 = v35 + v45;
          v67 = v36 + v46;
          v68 = v37 + v47;
          v69 = v38 + v48;
          v70 = v39 + v49;
          v71 = v40 + v50;
          v72 = v41 + v51;
          v73 = v42 + v52;
          v74 = v43 + v53;
          v75 = v44 - v34;
          v76 = v45 - v35;
          v77 = v46 - v36;
          v78 = v47 - v37;
          v79 = v48 - v38;
          v80 = v49 - v39;
          v81 = v50 - v40;
          v82 = v51 - v41;
          v83 = v52 - v42;
          v84 = v53 - v43;
          v28 = 15LL * (v26 / -2);
          fe_mul((__int64)&v85, (__int64)&v65, (__int64)&Bi[v28 + 5]);
          fe_mul((__int64)&v75, (__int64)&v75, (__int64)&Bi[v28]);
          fe_mul((__int64)&v95, (__int64)&Bi[v28 + 10], (__int64)&v64);
          v65 = v85 - v75;
          v66 = v86 - v76;
          v67 = v87 - v77;
          v68 = v88 - v78;
          v69 = v89 - v79;
          v70 = v90 - v80;
          v71 = v91 - v81;
          v72 = v92 - v82;
          v73 = v93 - v83;
          v74 = v94 - v84;
          v75 += v85;
          v76 += v86;
          v77 += v87;
          v78 += v88;
          v79 += v89;
          v80 += v90;
          v81 += v91;
          v82 += v92;
          v83 += v93;
          v84 += v94;
          v27 = (__int64)&v65;
          v85 = 2 * v54 - v95;
          v86 = 2 * v55 - v96;
          v87 = 2 * v56 - v97;
          v88 = 2 * v57 - v98;
          v89 = 2 * v58 - v99;
          v90 = 2 * v59 - v100;
          v91 = 2 * v60 - v101;
          v92 = 2 * v61 - v102;
          v93 = 2 * v62 - v103;
          v94 = 2 * v63 - v104;
          v95 += 2 * v54;
          v96 += 2 * v55;
          v97 += 2 * v56;
          v98 += 2 * v57;
          v6 = a1;
          v99 += 2 * v58;
          v100 += 2 * v59;
          v101 += 2 * v60;
          v102 += 2 * v61;
          v103 += 2 * v62;
          v104 += 2 * v63;
        }
      }
      else
      {
        v27 = (__int64)&v65;
        ge_p1p1_to_p3((__int64)&v34, (__int64)&v65);
        ge_madd((__int64)&v65, (__int64)&v34, (__int64)&Bi[15 * (v26 / 2)]);
      }
      ge_p1p1_to_p2(v6, v27);
      --v31;
      v9 = (signed __int64)((char *)v30 - 1);
      --v32;
      v10 = v27;
    }
    while ( v31 > 0 );
  }
  return *(_QWORD *)off_69010[0];
}
// 5EFE0: using guessed type __int64 Bi[120];
// 69010: using guessed type __int64 off_69010[2];
// 2E8CF: using guessed type char var_130[255];
// 2E8CF: using guessed type char var_230[255];
// 2E8CF: using guessed type char var_730[40];
// 2E8CF: using guessed type char var_708[40];
// 2E8CF: using guessed type char var_6B8[40];
// 2E8CF: using guessed type char var_6E0[40];

//----- (000000000002FA56) ----------------------------------------------------
char __fastcall ge_slide(__int64 a1, __int64 a2)
{
  __int64 v2; // r9@1
  signed __int64 v3; // r8@1
  __int64 v4; // rdx@1
  signed __int64 v5; // rax@2
  signed __int64 v6; // rcx@3
  signed __int64 v7; // r10@3
  signed __int64 v8; // rdx@4
  int v9; // esi@6

  v2 = 0LL;
  v3 = 1LL;
  v4 = 0LL;
  do
  {
    LOBYTE(v5) = ((unsigned int)*(_BYTE *)(a2 + ((signed int)v4 >> 3)) >> (v4 & 7)) & 1;
    *(_BYTE *)(a1 + v4++) = v5;
  }
  while ( v4 != 256 );
  do
  {
    v6 = 1LL;
    v7 = v3;
    if ( *(_BYTE *)(a1 + v2) )
    {
      do
      {
        v8 = v6 + v2;
        if ( (signed int)v6 + (signed int)v2 > 255 )
          break;
        if ( *(_BYTE *)(a1 + v8) )
        {
          LODWORD(v5) = *(_BYTE *)(a1 + v2);
          v9 = *(_BYTE *)(a1 + v8) << v6;
          if ( (signed int)v5 + v9 > 15 )
          {
            LODWORD(v5) = v5 - v9;
            if ( (signed int)v5 < -15 )
              break;
            *(_BYTE *)(a1 + v2) = v5;
            v5 = v7;
            while ( *(_BYTE *)(a1 + v5) )
            {
              *(_BYTE *)(a1 + v5++) = 0;
              if ( (signed int)v5 >= 256 )
                goto LABEL_14;
            }
            *(_BYTE *)(a1 + v5) = 1;
          }
          else
          {
            *(_BYTE *)(a1 + v2) = v5 + v9;
            *(_BYTE *)(a1 + v8) = 0;
          }
        }
LABEL_14:
        ++v6;
        ++v7;
      }
      while ( (signed int)v6 < 7 );
    }
    ++v2;
    ++v3;
  }
  while ( v2 != 256 );
  return v5;
}

//----- (000000000002FB0A) ----------------------------------------------------
__int64 __fastcall ge_p3_to_cached(__int64 a1, __int64 a2)
{
  __int64 v2; // rbx@1
  __int64 v3; // r14@1
  __int64 v4; // r8@1
  __int64 v5; // rdx@1
  __int64 v6; // rsi@1
  __int64 v7; // rdi@1
  int v8; // ecx@1

  v2 = a2;
  v3 = a1;
  fe_add(a1, a2 + 40, a2);
  fe_sub(a1 + 40, a2 + 40, a2);
  v4 = *(_QWORD *)(a2 + 84);
  v5 = *(_QWORD *)(a2 + 92);
  v6 = *(_QWORD *)(a2 + 100);
  v7 = *(_QWORD *)(v2 + 108);
  v8 = *(_DWORD *)(v2 + 116);
  *(_DWORD *)(v3 + 80) = *(_DWORD *)(v2 + 80);
  *(_QWORD *)(v3 + 92) = v5;
  *(_QWORD *)(v3 + 84) = v4;
  *(_QWORD *)(v3 + 108) = v7;
  *(_QWORD *)(v3 + 100) = v6;
  *(_DWORD *)(v3 + 116) = v8;
  return fe_mul(v3 + 120, v2 + 120, (__int64)d2);
}
// 66C00: using guessed type __int64 d2[6];

//----- (000000000002FB8A) ----------------------------------------------------
__int64 __fastcall ge_p1p1_to_p3(__int64 a1, __int64 a2)
{
  fe_mul(a1, a2, a2 + 120);
  fe_mul(a1 + 40, a2 + 40, a2 + 80);
  fe_mul(a1 + 80, a2 + 80, a2 + 120);
  return fe_mul(a1 + 120, a2, a2 + 40);
}

//----- (000000000002FBF0) ----------------------------------------------------
__int64 __fastcall ge_add(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r12@1
  __int64 v4; // rbx@1
  char v6; // [sp+10h] [bp-60h]@1
  __int64 v7; // [sp+40h] [bp-30h]@1

  v3 = a3;
  v4 = a1;
  v7 = *(_QWORD *)off_69010[0];
  fe_add(a1, a2 + 40, a2);
  fe_sub(a1 + 40, a2 + 40, a2);
  fe_mul(v4 + 80, v4, v3);
  fe_mul(v4 + 40, v4 + 40, v3 + 40);
  fe_mul(a1 + 120, v3 + 120, a2 + 120);
  fe_mul(a1, a2 + 80, v3 + 80);
  fe_add((__int64)&v6, v4, v4);
  fe_sub(a1, a1 + 80, a1 + 40);
  fe_add(a1 + 40, a1 + 80, a1 + 40);
  fe_add(a1 + 80, (__int64)&v6, a1 + 120);
  fe_sub(a1 + 120, (__int64)&v6, a1 + 120);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000002FCF9) ----------------------------------------------------
__int64 __fastcall ge_p2_dbl(__int64 a1, __int64 a2)
{
  __int64 v2; // r14@1
  __int64 v3; // ST58_8@1
  __int64 v4; // ST60_8@1
  __int64 v5; // ST70_8@1
  __int64 v6; // ST68_8@1
  __int64 v7; // ST50_8@1
  __int64 v8; // rcx@1
  __int64 v9; // r12@1
  __int64 v10; // STD8_8@1
  __int64 v11; // r15@1
  __int64 v12; // ST30_8@1
  __int64 v13; // STB8_8@1
  __int64 v14; // ST38_8@1
  __int64 v15; // STC8_8@1
  __int64 v16; // ST78_8@1
  __int64 v17; // STC0_8@1
  __int64 v18; // r8@1
  __int64 v19; // r10@1
  __int64 v20; // STB0_8@1
  __int64 v21; // ST28_8@1
  __int64 v22; // r9@1
  __int64 v23; // ST20_8@1
  __int64 v24; // STE8_8@1
  __int64 v25; // rsi@1
  __int64 v26; // ST18_8@1
  __int64 v27; // STA0_8@1
  __int64 v28; // r13@1
  __int64 v29; // ST80_8@1
  __int64 v30; // ST120_8@1
  __int64 v31; // ST130_8@1
  __int64 v32; // ST140_8@1
  __int64 v33; // ST100_8@1
  __int64 v34; // ST128_8@1
  __int64 v35; // STD0_8@1
  __int64 v36; // STE0_8@1
  __int64 v37; // ST188_8@1
  __int64 v38; // rcx@1
  __int64 v39; // ST30_8@1
  __int64 v40; // ST118_8@1
  __int64 v41; // ST40_8@1
  __int64 v42; // rax@1
  __int64 v43; // rcx@1
  __int64 v44; // r14@1
  __int64 v45; // r9@1
  __int64 v46; // STF0_8@1
  __int64 v47; // r12@1
  __int64 v48; // rdi@1
  __int64 v49; // r15@1
  __int64 v50; // ST180_8@1
  __int64 v51; // r13@1
  __int64 v52; // ST188_8@1
  signed __int64 v53; // rax@1
  unsigned __int64 v54; // r10@1
  signed __int64 v55; // rdx@1
  unsigned __int64 v56; // rbx@1
  signed __int64 v57; // r8@1
  signed __int64 v58; // rsi@1
  signed __int64 v59; // rax@1
  signed __int64 v60; // rdx@1
  signed __int64 v61; // rax@1
  signed __int64 v62; // r9@1
  signed __int64 v63; // rdi@1
  signed __int64 v64; // rax@1
  int v65; // er11@1
  signed __int64 v66; // r14@1
  signed __int64 v67; // r15@1
  signed __int64 v68; // rdi@1
  unsigned __int64 v69; // rdi@1
  signed __int64 v70; // rsi@1
  signed __int64 v71; // rdi@1
  char v73; // [sp+190h] [bp-60h]@1
  __int64 v74; // [sp+1C0h] [bp-30h]@1

  v2 = a2;
  v3 = a2;
  v4 = a1;
  v74 = *(_QWORD *)off_69010[0];
  fe_sq(a1, a2);
  v5 = a1 + 80;
  v6 = a2 + 40;
  fe_sq(a1 + 80, a2 + 40);
  v7 = *(_DWORD *)(a2 + 80);
  v8 = *(_DWORD *)(a2 + 84);
  v9 = *(_DWORD *)(a2 + 88);
  v10 = *(_DWORD *)(a2 + 92);
  v11 = *(_DWORD *)(a2 + 96);
  v12 = *(_DWORD *)(a2 + 100);
  v13 = *(_DWORD *)(a2 + 104);
  v14 = *(_DWORD *)(a2 + 108);
  v15 = *(_DWORD *)(a2 + 112);
  v16 = *(_DWORD *)(a2 + 116);
  v17 = 2 * (signed int)v7;
  v18 = 2 * (signed int)v8;
  v19 = 2 * (signed int)v10;
  v20 = 2 * (signed int)v12;
  v21 = 2 * (signed int)v14;
  v22 = 2 * (signed int)v9;
  v23 = 38 * (signed int)v16;
  v24 = 2 * (signed int)v12;
  v25 = 19 * (signed int)v15;
  v26 = *(_DWORD *)(v2 + 96);
  v27 = 2 * (signed int)v11;
  v28 = 19 * (signed int)v13;
  v29 = 38 * (signed int)v14;
  v30 = v8 * (signed int)v17;
  v31 = v17 * *(_DWORD *)(v2 + 92);
  v32 = v17 * v12;
  v33 = v18 * v8;
  v34 = v18 * v20;
  v35 = v10 * v22;
  v36 = v22 * v26;
  v37 = v12 * v27;
  v38 = v12 * 38 * (signed int)v12;
  v39 = v22 * v12;
  v40 = v22 * v13;
  v41 = v22 * v14;
  v42 = *(_DWORD *)(v2 + 96);
  v43 = v23 * v18 + v25 * v22 + v19 * v29 + v27 * v28 + v7 * v7 + v38;
  v44 = v9 * v23 + v19 * v25 + v42 * v29 + v30 + v20 * v28;
  v45 = v23 * v19 + v25 * v27 + v20 * v29 + v13 * v28 + v17 * v9 + v33;
  v46 = v23 * v42 + v20 * v25 + v13 * v29 + v31 + v9 * v18;
  v47 = v23 * v20 + v25 * 2 * (signed int)v13 + v14 * v29 + v17 * v11 + v18 * v19 + v9 * v9;
  v48 = v23 * v13 + v21 * v25 + v32 + v18 * v11 + v35;
  v49 = v23 * v21 + v15 * v25 + v17 * v13 + v34 + v36 + v19 * v10;
  v50 = v23 * v15 + v17 * v14 + v18 * v13 + v39 + v42 * v19;
  v51 = v16 * v23 + v17 * v15 + v18 * v21 + v19 * v24 + v40 + v26 * v26;
  v52 = v16 * v17 + v18 * v15 + v41 + v19 * v13 + v37;
  v53 = (2 * v43 + 0x2000000) >> 26;
  v54 = 2 * v43 - ((2 * v43 + 0x2000000) & 0xFFFFFFFFFC000000LL);
  v55 = 2 * v47 + 0x2000000;
  v56 = 2 * v47 - (v55 & 0xFFFFFFFFFC000000LL);
  v57 = v53 + 2 * v44;
  v58 = v55 >> 26;
  v59 = v53 + 2 * v44 + 0x1000000;
  LODWORD(v57) = v57 - (v59 & 0xFE000000);
  v60 = v59 >> 25;
  v61 = v58 + 2 * v48 + 0x1000000;
  LODWORD(v43) = v58 + 2 * v48 - (v61 & 0xFE000000);
  v62 = v60 + 2 * v45;
  v63 = v61 >> 25;
  v64 = (v61 >> 25) + 2 * v49 + 0x2000000;
  v65 = v63 + 2 * v49 - (v64 & 0xFC000000);
  v66 = ((v62 + 0x2000000) >> 26) + 2 * v46;
  v67 = (v64 >> 26) + 2 * v50;
  v68 = (v66 + 0x1000000) >> 25;
  LODWORD(v47) = v68 + v56;
  v69 = v68 + v56 + 0x2000000;
  LODWORD(v56) = v43 + (v69 >> 26);
  LODWORD(v47) = v47 - (v69 & 0xFC000000);
  v70 = ((((v67 + 0x1000000) >> 25) + 2 * v51 + 0x2000000) >> 26) + 2 * v52;
  v71 = 19 * ((v70 + 0x1000000) >> 25);
  *(_DWORD *)(v4 + 120) = v71 + v54 - ((v71 + v54 + 0x2000000) & 0xFC000000);
  *(_DWORD *)(v4 + 124) = v57 + ((v71 + v54 + 0x2000000) >> 26);
  *(_DWORD *)(v4 + 128) = v62 - ((v62 + 0x2000000) & 0xFC000000);
  *(_DWORD *)(v4 + 132) = v66 - ((v66 + 0x1000000) & 0xFE000000);
  *(_DWORD *)(v4 + 136) = v47;
  *(_DWORD *)(v4 + 140) = v56;
  *(_DWORD *)(v4 + 144) = v65;
  *(_DWORD *)(v4 + 148) = v67 - ((v67 + 0x1000000) & 0xFE000000);
  *(_DWORD *)(v4 + 152) = ((v67 + 0x1000000) >> 25)
                        + 2 * v51
                        - ((((v67 + 0x1000000) >> 25) + 2 * v51 + 0x2000000) & 0xFC000000);
  *(_DWORD *)(v4 + 156) = v70 - ((v70 + 0x1000000) & 0xFE000000);
  fe_add(v4 + 40, v3, v6);
  fe_sq((__int64)&v73, v4 + 40);
  fe_add(v4 + 40, v5, v4);
  fe_sub(v5, v5, v4);
  fe_sub(v4, (__int64)&v73, v4 + 40);
  fe_sub(v4 + 120, v4 + 120, v5);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000305CA) ----------------------------------------------------
__int64 __fastcall ge_madd(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r13@1
  __int64 v4; // ST08_8@1
  __int64 v5; // r14@1
  char v7; // [sp+10h] [bp-60h]@1
  __int64 v8; // [sp+40h] [bp-30h]@1

  v3 = a3;
  v4 = a3;
  v5 = a1;
  v8 = *(_QWORD *)off_69010[0];
  fe_add(a1, a2 + 40, a2);
  fe_sub(a1 + 40, a2 + 40, a2);
  fe_mul(v5 + 80, v5, v3);
  fe_mul(v5 + 40, v5 + 40, v3 + 40);
  fe_mul(a1 + 120, v4 + 80, a2 + 120);
  fe_add((__int64)&v7, a2 + 80, a2 + 80);
  fe_sub(v5, v5 + 80, v5 + 40);
  fe_add(v5 + 40, v5 + 80, v5 + 40);
  fe_add(v5 + 80, (__int64)&v7, v5 + 120);
  fe_sub(v5 + 120, (__int64)&v7, v5 + 120);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000306C3) ----------------------------------------------------
__int64 __fastcall ge_p1p1_to_p2(__int64 a1, __int64 a2)
{
  fe_mul(a1, a2, a2 + 120);
  fe_mul(a1 + 40, a2 + 40, a2 + 80);
  return fe_mul(a1 + 80, a2 + 80, a2 + 120);
}

//----- (000000000003070F) ----------------------------------------------------
signed __int64 __fastcall ge_frombytes_negate_vartime(__int64 a1, __int64 a2)
{
  unsigned __int64 v2; // r8@1
  unsigned __int64 v3; // r9@1
  unsigned __int64 v4; // r11@1
  unsigned __int64 v5; // r15@1
  unsigned __int64 v6; // r10@1
  unsigned __int64 v7; // r13@1
  unsigned __int64 v8; // r14@1
  unsigned __int64 v9; // rbx@1
  unsigned __int64 v10; // r12@1
  unsigned __int64 v11; // rdx@1
  __int64 v12; // rax@1
  unsigned __int64 v13; // rcx@1
  int v14; // esi@1
  unsigned __int64 v15; // rdi@1
  int v16; // ST58_4@1
  unsigned __int64 v17; // r11@1
  unsigned __int64 v18; // r11@1
  unsigned __int64 v19; // rdi@1
  unsigned __int64 v20; // r14@1
  unsigned __int64 v21; // rsi@1
  unsigned __int64 v22; // rdi@1
  __int64 v23; // r12@1
  signed int v24; // ebx@1
  signed int v25; // ebx@3
  signed int v26; // ebx@5
  signed int v27; // ebx@7
  signed int v28; // ebx@9
  signed int v29; // ebx@11
  signed int v30; // ebx@13
  int v31; // er13@15
  int v32; // er14@15
  int v33; // er12@15
  int v34; // ecx@16
  signed __int64 result; // rax@16
  __int64 v36; // rcx@21
  __int64 v37; // [sp+10h] [bp-200h]@1
  int v38; // [sp+18h] [bp-1F8h]@15
  int v39; // [sp+1Ch] [bp-1F4h]@15
  int v40; // [sp+20h] [bp-1F0h]@15
  int v41; // [sp+24h] [bp-1ECh]@15
  int v42; // [sp+28h] [bp-1E8h]@15
  int v43; // [sp+2Ch] [bp-1E4h]@15
  int v44; // [sp+30h] [bp-1E0h]@15
  int v45; // [sp+34h] [bp-1DCh]@15
  int v46; // [sp+38h] [bp-1D8h]@15
  int v47; // [sp+3Ch] [bp-1D4h]@15
  int v48; // [sp+40h] [bp-1D0h]@15
  int v49; // [sp+44h] [bp-1CCh]@15
  int v50; // [sp+48h] [bp-1C8h]@15
  int v51; // [sp+4Ch] [bp-1C4h]@15
  __int64 v52; // [sp+50h] [bp-1C0h]@1
  int v53; // [sp+58h] [bp-1B8h]@15
  int v54; // [sp+60h] [bp-1B0h]@15
  int v55; // [sp+64h] [bp-1ACh]@15
  int v56; // [sp+68h] [bp-1A8h]@15
  int v57; // [sp+6Ch] [bp-1A4h]@15
  int v58; // [sp+70h] [bp-1A0h]@15
  int v59; // [sp+74h] [bp-19Ch]@15
  int v60; // [sp+78h] [bp-198h]@15
  int v61; // [sp+7Ch] [bp-194h]@15
  int v62; // [sp+80h] [bp-190h]@15
  int v63; // [sp+84h] [bp-18Ch]@15
  int v64; // [sp+90h] [bp-180h]@15
  int v65; // [sp+94h] [bp-17Ch]@15
  int v66; // [sp+98h] [bp-178h]@15
  int v67; // [sp+9Ch] [bp-174h]@15
  int v68; // [sp+A0h] [bp-170h]@15
  int v69; // [sp+A4h] [bp-16Ch]@15
  int v70; // [sp+A8h] [bp-168h]@15
  int v71; // [sp+ACh] [bp-164h]@15
  int v72; // [sp+B0h] [bp-160h]@15
  int v73; // [sp+B4h] [bp-15Ch]@15
  char v74; // [sp+C0h] [bp-150h]@1
  char v75; // [sp+F0h] [bp-120h]@1
  int v76; // [sp+120h] [bp-F0h]@1
  int v77; // [sp+124h] [bp-ECh]@15
  int v78; // [sp+128h] [bp-E8h]@15
  int v79; // [sp+12Ch] [bp-E4h]@15
  int v80; // [sp+130h] [bp-E0h]@15
  int v81; // [sp+134h] [bp-DCh]@15
  int v82; // [sp+138h] [bp-D8h]@15
  int v83; // [sp+13Ch] [bp-D4h]@15
  int v84; // [sp+140h] [bp-D0h]@15
  int v85; // [sp+144h] [bp-CCh]@15
  char v86; // [sp+150h] [bp-C0h]@5
  char v87; // [sp+180h] [bp-90h]@1
  char v88; // [sp+1B0h] [bp-60h]@1
  __int64 v89; // [sp+1E0h] [bp-30h]@1

  v37 = a2;
  v52 = a1;
  v89 = *(_QWORD *)off_69010[0];
  v2 = *(_BYTE *)a2 | ((unsigned __int64)*(_BYTE *)(a2 + 1) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 2) << 16) | ((unsigned __int64)*(_BYTE *)(a2 + 3) << 24);
  v3 = (*(_BYTE *)(a2 + 4) | ((unsigned __int64)*(_BYTE *)(a2 + 5) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 6) << 16)) << 6;
  v4 = *(_BYTE *)(a2 + 7) | ((unsigned __int64)*(_BYTE *)(a2 + 8) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 9) << 16);
  v5 = *(_BYTE *)(a2 + 10) | ((unsigned __int64)*(_BYTE *)(a2 + 11) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 12) << 16);
  v6 = *(_BYTE *)(a2 + 13) | ((unsigned __int64)*(_BYTE *)(a2 + 14) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 15) << 16);
  v7 = *(_BYTE *)(a2 + 16) | ((unsigned __int64)*(_BYTE *)(a2 + 17) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 18) << 16) | ((unsigned __int64)*(_BYTE *)(a2 + 19) << 24);
  v8 = *(_BYTE *)(a2 + 20) | ((unsigned __int64)*(_BYTE *)(a2 + 21) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 22) << 16);
  v9 = *(_BYTE *)(a2 + 23) | ((unsigned __int64)*(_BYTE *)(a2 + 24) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 25) << 16);
  v10 = *(_BYTE *)(a2 + 26) | ((unsigned __int64)*(_BYTE *)(a2 + 27) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 28) << 16);
  v11 = (v3 + 0x1000000) >> 25;
  v12 = 4 * (*(_BYTE *)(a2 + 29) | (*(_BYTE *)(a2 + 30) << 8) | (*(_BYTE *)(a2 + 31) << 16)) & 0x1FFFFFC;
  v13 = (unsigned __int64)(v12 + 0x1000000) >> 25;
  v14 = 19 * v13 + v2;
  v15 = 19 * v13 + v2 + 0x2000000;
  LODWORD(v2) = v3 - ((_DWORD)v11 << 25) + (v15 >> 26);
  v16 = v14 - (v15 & 0xFC000000);
  v4 *= 32LL;
  LODWORD(v3) = v4 + v11;
  v17 = v4 + v11 + 0x2000000;
  LODWORD(v11) = (v17 >> 26) + 8 * v5 - ((unsigned int)((8 * v5 + 0x1000000) >> 25) << 25);
  LODWORD(v3) = v3 - (v17 & 0xFC000000);
  v18 = ((8 * v5 + 0x1000000) >> 25) + 4 * v6;
  v19 = (v7 + 0x1000000) >> 25;
  LODWORD(v7) = ((v18 + 0x2000000) >> 26) + v7 - ((_DWORD)v19 << 25);
  v8 <<= 7;
  LODWORD(v6) = v8 + v19;
  v20 = v8 + v19 + 0x2000000;
  v9 *= 32LL;
  v21 = (v9 + 0x1000000) >> 25;
  LODWORD(v9) = (v20 >> 26) + v9 - ((_DWORD)v21 << 25);
  LODWORD(v6) = v6 - (v20 & 0xFC000000);
  v10 *= 16LL;
  LODWORD(v20) = v10 + v21;
  v22 = v10 + v21 + 0x2000000;
  v23 = v52;
  *(_DWORD *)(v52 + 40) = v16;
  *(_DWORD *)(v52 + 44) = v2;
  *(_DWORD *)(v52 + 48) = v3;
  *(_DWORD *)(v52 + 52) = v11;
  *(_DWORD *)(v52 + 56) = v18 - ((v18 + 0x2000000) & 0xFC000000);
  *(_DWORD *)(v52 + 60) = v7;
  *(_DWORD *)(v52 + 64) = v6;
  *(_DWORD *)(v52 + 68) = v9;
  *(_DWORD *)(v52 + 72) = v20 - (v22 & 0xFC000000);
  *(_DWORD *)(v52 + 76) = (v22 >> 26) + v12 - ((_DWORD)v13 << 25);
  *(_DWORD *)(v52 + 80) = 1;
  *(_QWORD *)(v52 + 84) = 0LL;
  *(_QWORD *)(v52 + 92) = 0LL;
  *(_QWORD *)(v52 + 100) = 0LL;
  *(_QWORD *)(v52 + 108) = 0LL;
  *(_DWORD *)(v52 + 116) = 0;
  fe_sq((__int64)&v76, v52 + 40);
  fe_mul((__int64)&v75, (__int64)&v76, (__int64)d);
  fe_sub((__int64)&v76, (__int64)&v76, v52 + 80);
  fe_add((__int64)&v75, (__int64)&v75, v52 + 80);
  fe_sq((__int64)&v74, (__int64)&v75);
  fe_mul((__int64)&v74, (__int64)&v74, (__int64)&v75);
  fe_sq(v52, (__int64)&v74);
  fe_mul(v23, v23, (__int64)&v75);
  fe_mul(v23, v23, (__int64)&v76);
  fe_sq((__int64)&v88, v52);
  fe_sq((__int64)&v87, (__int64)&v88);
  fe_sq((__int64)&v87, (__int64)&v87);
  fe_mul((__int64)&v87, v52, (__int64)&v87);
  fe_mul((__int64)&v88, (__int64)&v88, (__int64)&v87);
  fe_sq((__int64)&v88, (__int64)&v88);
  fe_mul((__int64)&v88, (__int64)&v87, (__int64)&v88);
  fe_sq((__int64)&v87, (__int64)&v88);
  v24 = 4;
  do
  {
    fe_sq((__int64)&v87, (__int64)&v87);
    --v24;
  }
  while ( v24 );
  fe_mul((__int64)&v88, (__int64)&v87, (__int64)&v88);
  fe_sq((__int64)&v87, (__int64)&v88);
  v25 = 9;
  do
  {
    fe_sq((__int64)&v87, (__int64)&v87);
    --v25;
  }
  while ( v25 );
  fe_mul((__int64)&v87, (__int64)&v87, (__int64)&v88);
  fe_sq((__int64)&v86, (__int64)&v87);
  v26 = 19;
  do
  {
    fe_sq((__int64)&v86, (__int64)&v86);
    --v26;
  }
  while ( v26 );
  fe_mul((__int64)&v87, (__int64)&v86, (__int64)&v87);
  fe_sq((__int64)&v87, (__int64)&v87);
  v27 = 9;
  do
  {
    fe_sq((__int64)&v87, (__int64)&v87);
    --v27;
  }
  while ( v27 );
  fe_mul((__int64)&v88, (__int64)&v87, (__int64)&v88);
  fe_sq((__int64)&v87, (__int64)&v88);
  v28 = 49;
  do
  {
    fe_sq((__int64)&v87, (__int64)&v87);
    --v28;
  }
  while ( v28 );
  fe_mul((__int64)&v87, (__int64)&v87, (__int64)&v88);
  fe_sq((__int64)&v86, (__int64)&v87);
  v29 = 99;
  do
  {
    fe_sq((__int64)&v86, (__int64)&v86);
    --v29;
  }
  while ( v29 );
  fe_mul((__int64)&v87, (__int64)&v86, (__int64)&v87);
  fe_sq((__int64)&v87, (__int64)&v87);
  v30 = 49;
  do
  {
    fe_sq((__int64)&v87, (__int64)&v87);
    --v30;
  }
  while ( v30 );
  fe_mul((__int64)&v88, (__int64)&v87, (__int64)&v88);
  fe_sq((__int64)&v88, (__int64)&v88);
  fe_sq((__int64)&v88, (__int64)&v88);
  fe_mul(v52, (__int64)&v88, v52);
  fe_mul(v52, v52, (__int64)&v74);
  fe_mul(v52, v52, (__int64)&v76);
  fe_sq((__int64)&v64, v52);
  fe_mul((__int64)&v64, (__int64)&v64, (__int64)&v75);
  v43 = v67;
  v41 = v66;
  v45 = v69;
  v42 = v68;
  v53 = v71;
  v44 = v70;
  v49 = v76;
  v48 = v77;
  v51 = v79;
  v47 = v78;
  v50 = v81;
  v46 = v80;
  v54 = v64 - v76;
  v40 = v82;
  v55 = v65 - v77;
  v56 = v66 - v78;
  v57 = v67 - v79;
  v58 = v68 - v80;
  v59 = v69 - v81;
  v60 = v70 - v82;
  v31 = v83;
  v61 = v71 - v83;
  v39 = v72;
  v32 = v84;
  v62 = v72 - v84;
  v38 = v73;
  v33 = v85;
  v63 = v73 - v85;
  fe_tobytes((__int64)&v88, (__int64)&v54);
  if ( (unsigned int)crypto_verify_32((__int64)&v88, (__int64)zero) )
  {
    v54 = v64 + v49;
    v55 = v65 + v48;
    v56 = v41 + v47;
    v57 = v43 + v51;
    v58 = v42 + v46;
    v59 = v45 + v50;
    v60 = v44 + v40;
    v61 = v53 + v31;
    v62 = v39 + v32;
    v63 = v38 + v33;
    fe_tobytes((__int64)&v88, (__int64)&v54);
    v34 = crypto_verify_32((__int64)&v88, (__int64)zero);
    result = 0xFFFFFFFFLL;
    if ( v34 )
      goto LABEL_21;
    fe_mul(v52, v52, (__int64)sqrtm1);
  }
  fe_tobytes((__int64)&v88, v52);
  if ( (v88 & 1) == (unsigned int)*(_BYTE *)(v37 + 31) >> 7 )
    fe_neg(v52, v52);
  fe_mul(v52 + 120, v52, v52 + 40);
  result = 0LL;
LABEL_21:
  v36 = *(_QWORD *)off_69010[0];
  return result;
}
// 5F3A0: using guessed type __int64 d[6];
// 5F3D0: using guessed type __int64 sqrtm1[6];
// 66C30: using guessed type __int64 zero[4];
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000030FC8) ----------------------------------------------------
__int64 __fastcall fe_sub(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 result; // rax@1
  int v4; // ecx@1
  int v5; // er8@1
  int v6; // er9@1
  int v7; // er10@1
  int v8; // er11@1
  int v9; // ebx@1
  int v10; // er14@1
  int v11; // er15@1
  int v12; // esi@1

  result = (unsigned int)(*(_DWORD *)a2 - *(_DWORD *)a3);
  v4 = *(_DWORD *)(a2 + 4) - *(_DWORD *)(a3 + 4);
  v5 = *(_DWORD *)(a2 + 8) - *(_DWORD *)(a3 + 8);
  v6 = *(_DWORD *)(a2 + 12) - *(_DWORD *)(a3 + 12);
  v7 = *(_DWORD *)(a2 + 16) - *(_DWORD *)(a3 + 16);
  v8 = *(_DWORD *)(a2 + 20) - *(_DWORD *)(a3 + 20);
  v9 = *(_DWORD *)(a2 + 24) - *(_DWORD *)(a3 + 24);
  v10 = *(_DWORD *)(a2 + 28) - *(_DWORD *)(a3 + 28);
  v11 = *(_DWORD *)(a2 + 32) - *(_DWORD *)(a3 + 32);
  v12 = *(_DWORD *)(a2 + 36) - *(_DWORD *)(a3 + 36);
  *(_DWORD *)a1 = result;
  *(_DWORD *)(a1 + 4) = v4;
  *(_DWORD *)(a1 + 8) = v5;
  *(_DWORD *)(a1 + 12) = v6;
  *(_DWORD *)(a1 + 16) = v7;
  *(_DWORD *)(a1 + 20) = v8;
  *(_DWORD *)(a1 + 24) = v9;
  *(_DWORD *)(a1 + 28) = v10;
  *(_DWORD *)(a1 + 32) = v11;
  *(_DWORD *)(a1 + 36) = v12;
  return result;
}

//----- (0000000000031041) ----------------------------------------------------
__int64 __fastcall fe_add(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 result; // rax@1
  int v4; // ecx@1
  int v5; // er8@1
  int v6; // er9@1
  int v7; // er10@1
  int v8; // er11@1
  int v9; // ebx@1
  int v10; // er14@1
  int v11; // er15@1
  int v12; // edx@1

  result = (unsigned int)(*(_DWORD *)a2 + *(_DWORD *)a3);
  v4 = *(_DWORD *)(a2 + 4) + *(_DWORD *)(a3 + 4);
  v5 = *(_DWORD *)(a2 + 8) + *(_DWORD *)(a3 + 8);
  v6 = *(_DWORD *)(a2 + 12) + *(_DWORD *)(a3 + 12);
  v7 = *(_DWORD *)(a2 + 16) + *(_DWORD *)(a3 + 16);
  v8 = *(_DWORD *)(a2 + 20) + *(_DWORD *)(a3 + 20);
  v9 = *(_DWORD *)(a2 + 24) + *(_DWORD *)(a3 + 24);
  v10 = *(_DWORD *)(a2 + 28) + *(_DWORD *)(a3 + 28);
  v11 = *(_DWORD *)(a2 + 32) + *(_DWORD *)(a3 + 32);
  v12 = *(_DWORD *)(a2 + 36) + *(_DWORD *)(a3 + 36);
  *(_DWORD *)a1 = result;
  *(_DWORD *)(a1 + 4) = v4;
  *(_DWORD *)(a1 + 8) = v5;
  *(_DWORD *)(a1 + 12) = v6;
  *(_DWORD *)(a1 + 16) = v7;
  *(_DWORD *)(a1 + 20) = v8;
  *(_DWORD *)(a1 + 24) = v9;
  *(_DWORD *)(a1 + 28) = v10;
  *(_DWORD *)(a1 + 32) = v11;
  *(_DWORD *)(a1 + 36) = v12;
  return result;
}

//----- (00000000000310BA) ----------------------------------------------------
__int64 __fastcall fe_neg(__int64 a1, __int64 a2)
{
  int v2; // er10@1
  int v3; // er11@1
  int v4; // er14@1
  int v5; // er15@1
  __int64 result; // rax@1
  int v7; // ebx@1
  int v8; // ecx@1
  int v9; // edx@1
  int v10; // er8@1

  v2 = -*(_DWORD *)(a2 + 4);
  v3 = -*(_DWORD *)(a2 + 8);
  v4 = -*(_DWORD *)(a2 + 12);
  v5 = -*(_DWORD *)(a2 + 16);
  result = (unsigned int)-*(_DWORD *)(a2 + 20);
  v7 = -*(_DWORD *)(a2 + 24);
  v8 = -*(_DWORD *)(a2 + 28);
  v9 = -*(_DWORD *)(a2 + 32);
  v10 = -*(_DWORD *)(a2 + 36);
  *(_DWORD *)a1 = -*(_DWORD *)a2;
  *(_DWORD *)(a1 + 4) = v2;
  *(_DWORD *)(a1 + 8) = v3;
  *(_DWORD *)(a1 + 12) = v4;
  *(_DWORD *)(a1 + 16) = v5;
  *(_DWORD *)(a1 + 20) = result;
  *(_DWORD *)(a1 + 24) = v7;
  *(_DWORD *)(a1 + 28) = v8;
  *(_DWORD *)(a1 + 32) = v9;
  *(_DWORD *)(a1 + 36) = v10;
  return result;
}

//----- (000000000003112A) ----------------------------------------------------
__int64 __fastcall ge_p3_to_p2(__int64 a1, __int64 a2)
{
  __int64 v2; // r8@1
  __int64 v3; // r9@1
  __int64 v4; // rax@1
  __int64 v5; // rdx@1
  __int64 v6; // r9@1
  __int64 v7; // rdx@1
  __int64 v8; // r8@1
  __int64 v9; // rcx@1
  int v10; // er10@1
  __int64 result; // rax@1
  __int64 v12; // r9@1
  __int64 v13; // rdx@1
  __int64 v14; // r8@1
  __int64 v15; // rcx@1
  int v16; // esi@1

  v2 = *(_QWORD *)a2;
  v3 = *(_QWORD *)(a2 + 16);
  v4 = *(_QWORD *)(a2 + 24);
  v5 = *(_QWORD *)(a2 + 32);
  *(_QWORD *)(a1 + 8) = *(_QWORD *)(a2 + 8);
  *(_QWORD *)a1 = v2;
  *(_QWORD *)(a1 + 24) = v4;
  *(_QWORD *)(a1 + 16) = v3;
  *(_QWORD *)(a1 + 32) = v5;
  v6 = *(_QWORD *)(a2 + 44);
  v7 = *(_QWORD *)(a2 + 52);
  v8 = *(_QWORD *)(a2 + 60);
  v9 = *(_QWORD *)(a2 + 68);
  v10 = *(_DWORD *)(a2 + 76);
  *(_DWORD *)(a1 + 40) = *(_DWORD *)(a2 + 40);
  *(_QWORD *)(a1 + 52) = v7;
  *(_QWORD *)(a1 + 44) = v6;
  *(_QWORD *)(a1 + 68) = v9;
  *(_QWORD *)(a1 + 60) = v8;
  *(_DWORD *)(a1 + 76) = v10;
  result = *(_DWORD *)(a2 + 80);
  v12 = *(_QWORD *)(a2 + 84);
  v13 = *(_QWORD *)(a2 + 92);
  v14 = *(_QWORD *)(a2 + 100);
  v15 = *(_QWORD *)(a2 + 108);
  v16 = *(_DWORD *)(a2 + 116);
  *(_DWORD *)(a1 + 80) = result;
  *(_QWORD *)(a1 + 92) = v13;
  *(_QWORD *)(a1 + 84) = v12;
  *(_QWORD *)(a1 + 108) = v15;
  *(_QWORD *)(a1 + 100) = v14;
  *(_DWORD *)(a1 + 116) = v16;
  return result;
}

//----- (00000000000311B0) ----------------------------------------------------
__int64 __fastcall ge_p3_tobytes(__int64 a1, __int64 a2)
{
  char v3; // [sp+0h] [bp-E0h]@1
  char v4; // [sp+30h] [bp-B0h]@1
  char v5; // [sp+60h] [bp-80h]@1
  char v6; // [sp+90h] [bp-50h]@1
  __int64 v7; // [sp+B0h] [bp-30h]@1

  v7 = *(_QWORD *)off_69010[0];
  fe_invert((__int64)&v5, a2 + 80);
  fe_mul((__int64)&v4, a2, (__int64)&v5);
  fe_mul((__int64)&v3, a2 + 40, (__int64)&v5);
  fe_tobytes(a1, (__int64)&v3);
  fe_tobytes((__int64)&v6, (__int64)&v4);
  *(_BYTE *)(a1 + 31) ^= v6 << 7;
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000003125E) ----------------------------------------------------
__int64 __fastcall fe_invert(__int64 a1, __int64 a2)
{
  signed int v2; // ebx@1
  signed int v3; // ebx@3
  signed int v4; // ebx@5
  signed int v5; // ebx@7
  signed int v6; // ebx@9
  signed int v7; // ebx@11
  signed int v8; // ebx@13
  signed int v9; // ebx@15
  char v11; // [sp+0h] [bp-F0h]@5
  char v12; // [sp+30h] [bp-C0h]@1
  char v13; // [sp+60h] [bp-90h]@1
  char v14; // [sp+90h] [bp-60h]@1
  __int64 v15; // [sp+C0h] [bp-30h]@1

  v15 = *(_QWORD *)off_69010[0];
  fe_sq((__int64)&v14, a2);
  fe_sq((__int64)&v13, (__int64)&v14);
  fe_sq((__int64)&v13, (__int64)&v13);
  fe_mul((__int64)&v13, a2, (__int64)&v13);
  fe_mul((__int64)&v14, (__int64)&v14, (__int64)&v13);
  fe_sq((__int64)&v12, (__int64)&v14);
  fe_mul((__int64)&v13, (__int64)&v13, (__int64)&v12);
  fe_sq((__int64)&v12, (__int64)&v13);
  v2 = 4;
  do
  {
    fe_sq((__int64)&v12, (__int64)&v12);
    --v2;
  }
  while ( v2 );
  fe_mul((__int64)&v13, (__int64)&v12, (__int64)&v13);
  fe_sq((__int64)&v12, (__int64)&v13);
  v3 = 9;
  do
  {
    fe_sq((__int64)&v12, (__int64)&v12);
    --v3;
  }
  while ( v3 );
  fe_mul((__int64)&v12, (__int64)&v12, (__int64)&v13);
  fe_sq((__int64)&v11, (__int64)&v12);
  v4 = 19;
  do
  {
    fe_sq((__int64)&v11, (__int64)&v11);
    --v4;
  }
  while ( v4 );
  fe_mul((__int64)&v12, (__int64)&v11, (__int64)&v12);
  fe_sq((__int64)&v12, (__int64)&v12);
  v5 = 9;
  do
  {
    fe_sq((__int64)&v12, (__int64)&v12);
    --v5;
  }
  while ( v5 );
  fe_mul((__int64)&v13, (__int64)&v12, (__int64)&v13);
  fe_sq((__int64)&v12, (__int64)&v13);
  v6 = 49;
  do
  {
    fe_sq((__int64)&v12, (__int64)&v12);
    --v6;
  }
  while ( v6 );
  fe_mul((__int64)&v12, (__int64)&v12, (__int64)&v13);
  fe_sq((__int64)&v11, (__int64)&v12);
  v7 = 99;
  do
  {
    fe_sq((__int64)&v11, (__int64)&v11);
    --v7;
  }
  while ( v7 );
  fe_mul((__int64)&v12, (__int64)&v11, (__int64)&v12);
  fe_sq((__int64)&v12, (__int64)&v12);
  v8 = 49;
  do
  {
    fe_sq((__int64)&v12, (__int64)&v12);
    --v8;
  }
  while ( v8 );
  fe_mul((__int64)&v13, (__int64)&v12, (__int64)&v13);
  fe_sq((__int64)&v13, (__int64)&v13);
  v9 = 4;
  do
  {
    fe_sq((__int64)&v13, (__int64)&v13);
    --v9;
  }
  while ( v9 );
  fe_mul(a1, (__int64)&v13, (__int64)&v14);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000314E0) ----------------------------------------------------
__int64 __fastcall ge_scalarmult_base(__int64 a1, __int64 a2)
{
  __int64 v2; // r12@1
  __int64 v3; // rax@1
  __int64 v4; // rcx@1
  unsigned __int8 v5; // bl@2
  int v6; // ecx@3
  char v7; // dl@4
  signed __int64 v8; // rbx@5
  __int64 v9; // rbx@7
  char v11; // [sp+8h] [bp-278h]@6
  char v12; // [sp+80h] [bp-200h]@7
  char v13; // [sp+F8h] [bp-188h]@6
  char v14; // [sp+198h] [bp-E8h]@7
  char v15; // [sp+210h] [bp-70h]@2
  char v16[62]; // [sp+211h] [bp-6Fh]@2
  char v17; // [sp+24Fh] [bp-31h]@5
  __int64 v18; // [sp+250h] [bp-30h]@1

  v2 = off_69010[0];
  v18 = *(_QWORD *)off_69010[0];
  v3 = 0LL;
  v4 = 0LL;
  do
  {
    v5 = *(_BYTE *)(a2 + v4);
    *(&v15 + 2 * v4) = *(_BYTE *)(a2 + v4) & 0xF;
    v16[2 * v4++] = v5 >> 4;
  }
  while ( v4 != 32 );
  v6 = 0;
  do
  {
    v7 = v6 + *(&v15 + v3);
    v6 = (((v6 + (unsigned __int8)*(&v15 + v3)) << 24) + 0x8000000) >> 28;
    *(&v15 + v3++) = v7 - 16 * v6;
  }
  while ( v3 != 63 );
  v17 += v6;
  *(_DWORD *)a1 = 0;
  *(_DWORD *)(a1 + 4) = 0;
  *(_DWORD *)(a1 + 8) = 0;
  *(_DWORD *)(a1 + 12) = 0;
  *(_DWORD *)(a1 + 16) = 0;
  *(_DWORD *)(a1 + 20) = 0;
  *(_DWORD *)(a1 + 24) = 0;
  *(_DWORD *)(a1 + 28) = 0;
  *(_DWORD *)(a1 + 32) = 0;
  *(_DWORD *)(a1 + 36) = 0;
  *(_DWORD *)(a1 + 40) = 1;
  *(_DWORD *)(a1 + 44) = 0;
  *(_DWORD *)(a1 + 48) = 0;
  *(_DWORD *)(a1 + 52) = 0;
  *(_DWORD *)(a1 + 56) = 0;
  *(_DWORD *)(a1 + 60) = 0;
  *(_DWORD *)(a1 + 64) = 0;
  *(_DWORD *)(a1 + 68) = 0;
  *(_DWORD *)(a1 + 72) = 0;
  *(_DWORD *)(a1 + 76) = 0;
  *(_DWORD *)(a1 + 80) = 1;
  *(_DWORD *)(a1 + 84) = 0;
  *(_DWORD *)(a1 + 88) = 0;
  *(_DWORD *)(a1 + 92) = 0;
  *(_DWORD *)(a1 + 96) = 0;
  *(_DWORD *)(a1 + 100) = 0;
  *(_DWORD *)(a1 + 104) = 0;
  *(_DWORD *)(a1 + 108) = 0;
  *(_DWORD *)(a1 + 112) = 0;
  *(_DWORD *)(a1 + 116) = 0;
  *(_DWORD *)(a1 + 120) = 0;
  *(_QWORD *)(a1 + 124) = 0LL;
  *(_QWORD *)(a1 + 132) = 0LL;
  *(_QWORD *)(a1 + 140) = 0LL;
  *(_QWORD *)(a1 + 148) = 0LL;
  *(_DWORD *)(a1 + 156) = 0;
  v8 = 1LL;
  do
  {
    ge_select((__int64)&v11, (signed int)v8 / 2, *(&v15 + v8));
    ge_madd((__int64)&v13, a1, (__int64)&v11);
    ge_p1p1_to_p3(a1, (__int64)&v13);
    v8 += 2LL;
  }
  while ( (signed int)v8 < 64 );
  ge_p3_to_p2((__int64)&v14, a1);
  ge_p2_dbl((__int64)&v13, (__int64)&v14);
  ge_p1p1_to_p2((__int64)&v12, (__int64)&v13);
  ge_p2_dbl((__int64)&v13, (__int64)&v12);
  ge_p1p1_to_p2((__int64)&v12, (__int64)&v13);
  ge_p2_dbl((__int64)&v13, (__int64)&v12);
  ge_p1p1_to_p2((__int64)&v12, (__int64)&v13);
  ge_p2_dbl((__int64)&v13, (__int64)&v12);
  ge_p1p1_to_p3(a1, (__int64)&v13);
  v9 = 0LL;
  do
  {
    ge_select((__int64)&v11, (signed int)v9 / 2, *(&v15 + v9));
    ge_madd((__int64)&v13, a1, (__int64)&v11);
    ge_p1p1_to_p3(a1, (__int64)&v13);
    v9 += 2LL;
  }
  while ( (signed int)v9 < 64 );
  return *(_QWORD *)v2;
}
// 69010: using guessed type __int64 off_69010[2];
// 314E0: using guessed type char var_6F[62];

//----- (00000000000317AB) ----------------------------------------------------
__int64 __fastcall ge_select(__int64 a1, int a2, int a3)
{
  __int64 v3; // rbx@1
  unsigned __int64 v4; // r14@1
  char v5; // r12@1
  __int64 v6; // ST08_8@1
  signed __int64 v7; // r13@1
  int v8; // ecx@1
  int v9; // edx@1
  int v10; // esi@1
  int v11; // edi@1
  int v12; // er8@1
  int v13; // er9@1
  int v14; // er10@1
  int v15; // er11@1
  int v16; // er15@1
  __int64 v17; // r8@1
  __int64 v18; // rdx@1
  __int64 v19; // rsi@1
  __int64 v20; // rdi@1
  int v21; // ecx@1
  int v23; // [sp+10h] [bp-A0h]@1
  int v24; // [sp+14h] [bp-9Ch]@1
  int v25; // [sp+18h] [bp-98h]@1
  int v26; // [sp+1Ch] [bp-94h]@1
  int v27; // [sp+20h] [bp-90h]@1
  int v28; // [sp+24h] [bp-8Ch]@1
  int v29; // [sp+28h] [bp-88h]@1
  int v30; // [sp+2Ch] [bp-84h]@1
  int v31; // [sp+30h] [bp-80h]@1
  int v32; // [sp+34h] [bp-7Ch]@1
  int v33; // [sp+38h] [bp-78h]@1
  __int64 v34; // [sp+3Ch] [bp-74h]@1
  __int64 v35; // [sp+44h] [bp-6Ch]@1
  __int64 v36; // [sp+4Ch] [bp-64h]@1
  __int64 v37; // [sp+54h] [bp-5Ch]@1
  int v38; // [sp+5Ch] [bp-54h]@1
  char v39; // [sp+60h] [bp-50h]@1

  v3 = a1;
  v4 = (unsigned __int64)a3 >> 63;
  v5 = a3 - 2 * (a3 & -(char)v4);
  *(_DWORD *)a1 = 1;
  *(_DWORD *)(a1 + 4) = 0;
  *(_DWORD *)(a1 + 8) = 0;
  *(_DWORD *)(a1 + 12) = 0;
  *(_DWORD *)(a1 + 16) = 0;
  *(_DWORD *)(a1 + 20) = 0;
  *(_DWORD *)(a1 + 24) = 0;
  *(_DWORD *)(a1 + 28) = 0;
  *(_DWORD *)(a1 + 32) = 0;
  *(_DWORD *)(a1 + 36) = 0;
  *(_DWORD *)(a1 + 40) = 1;
  *(_DWORD *)(a1 + 44) = 0;
  *(_DWORD *)(a1 + 48) = 0;
  *(_DWORD *)(a1 + 52) = 0;
  *(_DWORD *)(a1 + 56) = 0;
  *(_DWORD *)(a1 + 60) = 0;
  *(_DWORD *)(a1 + 64) = 0;
  *(_DWORD *)(a1 + 68) = 0;
  *(_DWORD *)(a1 + 72) = 0;
  *(_DWORD *)(a1 + 76) = 0;
  v6 = a1 + 80;
  *(_DWORD *)(a1 + 80) = 0;
  *(_QWORD *)(a1 + 84) = 0LL;
  *(_QWORD *)(a1 + 92) = 0LL;
  *(_QWORD *)(a1 + 100) = 0LL;
  *(_QWORD *)(a1 + 108) = 0LL;
  *(_DWORD *)(a1 + 116) = 0;
  v7 = 120LL * a2;
  ge_cmov(a1, (__int64)&base[v7], ((unsigned int)(unsigned __int8)((a3 - 2 * (a3 & -(char)v4)) ^ 1) - 1) >> 31);
  ge_cmov(a1, (__int64)&base[v7 + 15], ((unsigned int)(unsigned __int8)(v5 ^ 2) - 1) >> 31);
  ge_cmov(a1, (__int64)&base[v7 + 30], ((unsigned int)(unsigned __int8)(v5 ^ 3) - 1) >> 31);
  ge_cmov(a1, (__int64)&base[v7 + 45], ((unsigned int)(unsigned __int8)(v5 ^ 4) - 1) >> 31);
  ge_cmov(a1, (__int64)&base[v7 + 60], ((unsigned int)(unsigned __int8)(v5 ^ 5) - 1) >> 31);
  ge_cmov(a1, (__int64)&base[v7 + 75], ((unsigned int)(unsigned __int8)(v5 ^ 6) - 1) >> 31);
  ge_cmov(a1, (__int64)&base[v7 + 90], ((unsigned int)(unsigned __int8)(v5 ^ 7) - 1) >> 31);
  ge_cmov(a1, (__int64)&base[v7 + 105], ((unsigned int)(unsigned __int8)(v5 ^ 8) - 1) >> 31);
  v8 = *(_DWORD *)(a1 + 44);
  v9 = *(_DWORD *)(a1 + 48);
  v10 = *(_DWORD *)(a1 + 52);
  v11 = *(_DWORD *)(a1 + 56);
  v12 = *(_DWORD *)(v3 + 60);
  v13 = *(_DWORD *)(v3 + 64);
  v14 = *(_DWORD *)(v3 + 68);
  v15 = *(_DWORD *)(v3 + 72);
  v16 = *(_DWORD *)(v3 + 76);
  v23 = *(_DWORD *)(v3 + 40);
  v24 = v8;
  v25 = v9;
  v26 = v10;
  v27 = v11;
  v28 = v12;
  v29 = v13;
  v30 = v14;
  v31 = v15;
  v32 = v16;
  v17 = *(_QWORD *)(v3 + 4);
  v18 = *(_QWORD *)(v3 + 12);
  v19 = *(_QWORD *)(v3 + 20);
  v20 = *(_QWORD *)(v3 + 28);
  v21 = *(_DWORD *)(v3 + 36);
  v33 = *(_DWORD *)v3;
  v35 = v18;
  v34 = v17;
  v37 = v20;
  v36 = v19;
  v38 = v21;
  fe_neg((__int64)&v39, v6);
  return ge_cmov(v3, (__int64)&v23, v4);
}
// 5F400: using guessed type __int64 base[512];

//----- (0000000000031A49) ----------------------------------------------------
__int64 __fastcall ge_tobytes(__int64 a1, __int64 a2)
{
  char v3; // [sp+0h] [bp-E0h]@1
  char v4; // [sp+30h] [bp-B0h]@1
  char v5; // [sp+60h] [bp-80h]@1
  char v6; // [sp+90h] [bp-50h]@1
  __int64 v7; // [sp+B0h] [bp-30h]@1

  v7 = *(_QWORD *)off_69010[0];
  fe_invert((__int64)&v5, a2 + 80);
  fe_mul((__int64)&v4, a2, (__int64)&v5);
  fe_mul((__int64)&v3, a2 + 40, (__int64)&v5);
  fe_tobytes(a1, (__int64)&v3);
  fe_tobytes((__int64)&v6, (__int64)&v4);
  *(_BYTE *)(a1 + 31) ^= v6 << 7;
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000031AF7) ----------------------------------------------------
unsigned __int64 __fastcall sc_muladd(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r8@1
  __int64 v5; // STE0_8@1
  unsigned __int64 v6; // ST2C0_8@1
  unsigned __int64 v7; // ST300_8@1
  unsigned __int64 v8; // ST2E0_8@1
  int v9; // ST2B8_4@1
  int v10; // ST288_4@1
  unsigned __int64 v11; // ST290_8@1
  unsigned __int64 v12; // ST2F8_8@1
  int v13; // er11@1
  unsigned __int64 v14; // ST2D0_8@1
  unsigned __int64 v15; // ST2A8_8@1
  int v16; // ST2D8_4@1
  int v17; // ST2B0_4@1
  unsigned __int64 v18; // ST268_8@1
  unsigned __int64 v19; // STC8_8@1
  int v20; // STD8_4@1
  int v21; // STB8_4@1
  int v22; // STD0_4@1
  int v23; // STB0_4@1
  int v24; // STA8_4@1
  int v25; // STA0_4@1
  unsigned __int64 v26; // ST98_8@1
  unsigned __int64 v27; // ST90_8@1
  int v28; // STC0_4@1
  int v29; // ST88_4@1
  unsigned __int64 v30; // ST80_8@1
  __int64 v31; // ST1E8_8@1
  __int64 v32; // ST2E8_8@1
  __int64 v33; // r8@1
  __int64 v34; // ST298_8@1
  unsigned __int64 v35; // ST78_8@1
  __int64 v36; // ST280_8@1
  __int64 v37; // ST270_8@1
  __int64 v38; // ST2A0_8@1
  __int64 v39; // ST2F0_8@1
  unsigned __int64 v40; // ST70_8@1
  __int64 v41; // ST310_8@1
  __int64 v42; // rsi@1
  __int64 v43; // r14@1
  __int64 v44; // rax@1
  __int64 v45; // ST2C8_8@1
  __int64 v46; // ST318_8@1
  unsigned __int64 v47; // ST68_8@1
  unsigned __int64 v48; // ST60_8@1
  __int64 v49; // rbx@1
  __int64 v50; // rdx@1
  unsigned __int64 v51; // rcx@1
  __int64 v52; // rax@1
  __int64 v53; // ST2D8_8@1
  unsigned __int64 v54; // rcx@1
  __int64 v55; // rax@1
  unsigned __int64 v56; // ST58_8@1
  __int64 v57; // ST238_8@1
  __int64 v58; // ST240_8@1
  __int64 v59; // ST248_8@1
  __int64 v60; // ST250_8@1
  __int64 v61; // ST260_8@1
  __int64 v62; // ST258_8@1
  unsigned __int64 v63; // ST1E8_8@1
  __int64 v64; // ST208_8@1
  __int64 v65; // ST210_8@1
  __int64 v66; // ST228_8@1
  __int64 v67; // ST230_8@1
  __int64 v68; // ST218_8@1
  __int64 v69; // ST1D0_8@1
  __int64 v70; // r9@1
  unsigned __int64 v71; // ST2A0_8@1
  __int64 v72; // ST1C8_8@1
  __int64 v73; // ST188_8@1
  __int64 v74; // ST1F8_8@1
  __int64 v75; // ST200_8@1
  __int64 v76; // ST220_8@1
  __int64 v77; // r10@1
  unsigned __int64 v78; // ST2C8_8@1
  unsigned __int64 v79; // ST138_8@1
  unsigned __int64 v80; // ST1A8_8@1
  unsigned __int64 v81; // ST1B8_8@1
  unsigned __int64 v82; // ST1F0_8@1
  unsigned __int64 v83; // ST38_8@1
  unsigned __int64 v84; // r14@1
  __int64 v85; // rcx@1
  unsigned __int64 v86; // ST2D0_8@1
  __int64 v87; // ST158_8@1
  __int64 v88; // ST170_8@1
  __int64 v89; // ST1B0_8@1
  __int64 v90; // ST1D8_8@1
  __int64 v91; // ST30_8@1
  unsigned __int64 v92; // ST2D8_8@1
  __int64 v93; // ST258_8@1
  __int64 v94; // ST1D0_8@1
  __int64 v95; // ST40_8@1
  unsigned __int64 v96; // ST48_8@1
  __int64 v97; // ST50_8@1
  unsigned __int64 v98; // ST2B0_8@1
  __int64 v99; // ST250_8@1
  unsigned __int64 v100; // r12@1
  __int64 v101; // ST230_8@1
  unsigned __int64 v102; // ST100_8@1
  unsigned __int64 v103; // r10@1
  __int64 v104; // ST160_8@1
  __int64 v105; // ST200_8@1
  unsigned __int64 v106; // ST128_8@1
  __int64 v107; // ST120_8@1
  __int64 v108; // ST1A0_8@1
  unsigned __int64 v109; // ST1F0_8@1
  unsigned __int64 v110; // ST150_8@1
  __int64 v111; // ST148_8@1
  __int64 v112; // ST1C0_8@1
  __int64 v113; // ST1D8_8@1
  unsigned __int64 v114; // ST190_8@1
  __int64 v115; // ST180_8@1
  unsigned __int64 v116; // ST308_8@1
  unsigned __int64 v117; // ST1E0_8@1
  unsigned __int64 v118; // ST288_8@1
  unsigned __int64 v119; // ST300_8@1
  __int64 v120; // ST198_8@1
  unsigned __int64 v121; // rsi@1
  unsigned __int64 v122; // rbx@1
  unsigned __int64 v123; // rax@1
  unsigned __int64 v124; // ST178_8@1
  unsigned __int64 v125; // ST238_8@1
  unsigned __int64 v126; // STC8_8@1
  unsigned __int64 v127; // r9@1
  unsigned __int64 v128; // ST240_8@1
  unsigned __int64 v129; // r11@1
  unsigned __int64 v130; // ST248_8@1
  unsigned __int64 v131; // r12@1
  signed __int64 v132; // rax@1
  signed __int64 v133; // ST228_8@1
  unsigned __int64 v134; // r10@1
  unsigned __int64 v135; // ST260_8@1
  unsigned __int64 v136; // ST2F8_8@1
  unsigned __int64 v137; // ST2C0_8@1
  unsigned __int64 v138; // r14@1
  unsigned __int64 v139; // ST258_8@1
  unsigned __int64 v140; // ST2F0_8@1
  unsigned __int64 v141; // ST220_8@1
  unsigned __int64 v142; // ST318_8@1
  signed __int64 v143; // rax@1
  unsigned __int64 v144; // ST2A8_8@1
  signed __int64 v145; // r13@1
  unsigned __int64 v146; // r8@1
  unsigned __int64 v147; // rdx@1
  unsigned __int64 v148; // rax@1
  unsigned __int64 v149; // ST310_8@1
  unsigned __int64 v150; // r15@1
  unsigned __int64 v151; // rbx@1
  unsigned __int64 v152; // ST2D0_8@1
  unsigned __int64 v153; // ST2E8_8@1
  unsigned __int64 v154; // ST2B8_8@1
  unsigned __int64 v155; // rdi@1
  signed __int64 v156; // ST2D8_8@1
  unsigned __int64 v157; // ST2C8_8@1
  signed __int64 v158; // ST2E0_8@1
  signed __int64 v159; // ST308_8@1
  signed __int64 v160; // ST270_8@1
  signed __int64 v161; // ST208_8@1
  unsigned __int64 v162; // ST278_8@1
  signed __int64 v163; // ST298_8@1
  signed __int64 v164; // ST2A0_8@1
  signed __int64 v165; // ST218_8@1
  signed __int64 v166; // ST2F8_8@1
  signed __int64 v167; // rbx@1
  signed __int64 v168; // rsi@1
  unsigned __int64 v169; // ST2F0_8@1
  signed __int64 v170; // r14@1
  signed __int64 v171; // r13@1
  unsigned __int64 v172; // r14@1
  unsigned __int64 v173; // rax@1
  unsigned __int64 v174; // r15@1
  unsigned __int64 v175; // r12@1
  unsigned __int64 v176; // r8@1
  unsigned __int64 v177; // ST300_8@1
  unsigned __int64 v178; // r10@1
  unsigned __int64 v179; // rcx@1
  unsigned __int64 v180; // r9@1
  unsigned __int64 v181; // ST318_8@1
  unsigned __int64 v182; // rbx@1
  unsigned __int64 v183; // r11@1
  unsigned __int64 v184; // r15@1
  unsigned __int64 v185; // rsi@1
  signed __int64 v186; // rdi@1
  unsigned __int64 v187; // rdx@1
  unsigned __int64 v188; // rbx@1
  signed __int64 v189; // ST2F0_8@1
  signed __int64 v190; // rax@1
  unsigned __int64 v191; // ST2C0_8@1
  signed __int64 v192; // ST2F8_8@1
  unsigned __int64 v193; // r9@1
  unsigned __int64 v194; // r11@1
  signed __int64 v195; // r15@1
  unsigned __int64 v196; // r13@1
  unsigned __int64 v197; // r15@1
  unsigned __int64 v198; // r12@1
  unsigned __int64 v199; // r14@1
  unsigned __int64 v200; // ST310_8@1
  signed __int64 v201; // ST2A0_8@1
  unsigned __int64 v202; // rsi@1
  unsigned __int64 v203; // rcx@1
  signed __int64 v204; // r8@1
  unsigned __int64 v205; // rax@1
  signed __int64 v206; // rdi@1
  signed __int64 v207; // r10@1
  signed __int64 v208; // ST2B8_8@1
  unsigned __int64 v209; // ST2C8_8@1
  signed __int64 v210; // ST2D0_8@1
  signed __int64 v211; // ST2E8_8@1
  signed __int64 v212; // rdx@1
  signed __int64 v213; // ST2D8_8@1
  signed __int64 v214; // ST300_8@1
  unsigned __int64 v215; // rax@1
  signed __int64 v216; // rdi@1
  unsigned __int64 v217; // r15@1
  signed __int64 v218; // rdi@1
  unsigned __int64 v219; // rcx@1
  unsigned __int64 v220; // ST308_8@1
  unsigned __int64 v221; // r13@1
  signed __int64 v222; // ST2B8_8@1
  signed __int64 v223; // ST2C0_8@1
  signed __int64 v224; // r15@1
  unsigned __int64 v225; // ST2E0_8@1
  signed __int64 v226; // r9@1
  unsigned __int64 v227; // r10@1
  unsigned __int64 v228; // r12@1
  unsigned __int64 v229; // ST2F0_8@1
  signed __int64 v230; // rax@1
  signed __int64 v231; // rdx@1
  unsigned __int64 v232; // ST308_8@1
  signed __int64 v233; // rax@1
  signed __int64 v234; // r14@1
  signed __int64 v235; // rdx@1
  signed __int64 v236; // rcx@1
  signed __int64 v237; // rdi@1
  signed __int64 v238; // r11@1
  signed __int64 v239; // rbx@1
  signed __int64 v240; // rsi@1
  unsigned __int64 v241; // ST300_8@1
  unsigned __int64 v242; // r15@1
  unsigned __int64 v243; // r10@1
  signed __int64 v244; // r12@1
  signed __int64 v245; // rsi@1
  unsigned __int64 v246; // r9@1
  unsigned __int64 v247; // ST318_8@1
  signed __int64 v248; // rsi@1
  signed __int64 v249; // rax@1
  unsigned __int64 v250; // r14@1
  unsigned __int64 v251; // rdx@1
  unsigned __int64 v252; // rcx@1
  unsigned __int64 v253; // r13@1
  unsigned __int64 v254; // r11@1
  signed __int64 v255; // rdi@1
  signed __int64 v256; // rsi@1
  unsigned __int64 v257; // rdi@1
  signed __int64 v258; // rdx@1
  unsigned __int64 v259; // rsi@1
  signed __int64 v260; // r8@1
  unsigned __int64 v261; // rdx@1
  signed __int64 v262; // r14@1
  unsigned __int64 v263; // r8@1
  signed __int64 v264; // r13@1
  unsigned __int64 v265; // r14@1
  signed __int64 v266; // r11@1
  unsigned __int64 v267; // r13@1
  signed __int64 v268; // rbx@1
  unsigned __int64 v269; // r11@1
  signed __int64 v270; // r10@1
  signed __int64 v271; // r12@1
  unsigned __int64 v272; // r10@1
  unsigned __int64 v273; // r15@1
  unsigned __int64 v274; // r12@1
  unsigned __int64 result; // rax@1

  v4 = a4;
  v5 = a1;
  v6 = *(_BYTE *)a2 | ((unsigned __int64)*(_BYTE *)(a2 + 1) << 8) | ((unsigned __int64)(*(_BYTE *)(a2 + 2) & 0x1F) << 16);
  v7 = *(_BYTE *)(a2 + 18) | ((unsigned __int64)*(_BYTE *)(a2 + 19) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 20) << 16);
  v8 = *(_BYTE *)(a2 + 21) | ((unsigned __int64)*(_BYTE *)(a2 + 22) << 8) | ((unsigned __int64)(*(_BYTE *)(a2 + 23) & 0x1F) << 16);
  v9 = (unsigned __int8)(*(_BYTE *)(a2 + 23) & 0xE0) | (*(_BYTE *)(a2 + 24) << 8) | (*(_BYTE *)(a2 + 25) << 16) | (*(_BYTE *)(a2 + 26) << 24);
  v10 = (unsigned __int8)(*(_BYTE *)(a2 + 26) & 0xFC) | (*(_BYTE *)(a2 + 27) << 8) | (*(_BYTE *)(a2 + 28) << 16);
  v11 = *(_BYTE *)(a2 + 28) | ((unsigned __int64)*(_BYTE *)(a2 + 29) << 8) | ((unsigned __int64)*(_BYTE *)(a2 + 30) << 16) | ((unsigned __int64)*(_BYTE *)(a2 + 31) << 24);
  v13 = (unsigned __int8)(*(_BYTE *)(a3 + 10) & 0xF0) | (*(_BYTE *)(a3 + 11) << 8) | (*(_BYTE *)(a3 + 12) << 16) | (*(_BYTE *)(a3 + 13) << 24);
  v14 = *(_BYTE *)(a3 + 18) | ((unsigned __int64)*(_BYTE *)(a3 + 19) << 8) | ((unsigned __int64)*(_BYTE *)(a3 + 20) << 16);
  v15 = *(_BYTE *)(a3 + 21) | ((unsigned __int64)*(_BYTE *)(a3 + 22) << 8) | ((unsigned __int64)(*(_BYTE *)(a3 + 23) & 0x1F) << 16);
  v16 = (unsigned __int8)(*(_BYTE *)(a3 + 23) & 0xE0) | (*(_BYTE *)(a3 + 24) << 8) | (*(_BYTE *)(a3 + 25) << 16) | (*(_BYTE *)(a3 + 26) << 24);
  v17 = (unsigned __int8)(*(_BYTE *)(a3 + 26) & 0xFC) | (*(_BYTE *)(a3 + 27) << 8) | (*(_BYTE *)(a3 + 28) << 16);
  v18 = *(_BYTE *)(a3 + 28) | ((unsigned __int64)*(_BYTE *)(a3 + 29) << 8) | ((unsigned __int64)*(_BYTE *)(a3 + 30) << 16) | ((unsigned __int64)*(_BYTE *)(a3 + 31) << 24);
  v19 = *(_BYTE *)v4 | ((unsigned __int64)*(_BYTE *)(v4 + 1) << 8) | ((unsigned __int64)(*(_BYTE *)(v4 + 2) & 0x1F) << 16);
  v20 = (unsigned __int8)(*(_BYTE *)(a4 + 2) & 0xE0) | (*(_BYTE *)(v4 + 3) << 8) | (*(_BYTE *)(v4 + 4) << 16) | (*(_BYTE *)(v4 + 5) << 24);
  v21 = *(_BYTE *)(v4 + 5) | (*(_BYTE *)(v4 + 6) << 8) | (*(_BYTE *)(v4 + 7) << 16);
  v22 = (unsigned __int8)(*(_BYTE *)(v4 + 7) & 0x80) | (*(_BYTE *)(v4 + 8) << 8) | (*(_BYTE *)(v4 + 9) << 16) | (*(_BYTE *)(v4 + 10) << 24);
  v23 = *(_BYTE *)(a4 + 10) | (*(_BYTE *)(v4 + 11) << 8) | (*(_BYTE *)(v4 + 12) << 16) | (*(_BYTE *)(v4 + 13) << 24);
  v24 = (unsigned __int8)(*(_BYTE *)(v4 + 13) & 0xFE) | (*(_BYTE *)(v4 + 14) << 8) | (*(_BYTE *)(v4 + 15) << 16);
  v25 = *(_BYTE *)(v4 + 15) | (*(_BYTE *)(v4 + 16) << 8) | (*(_BYTE *)(v4 + 17) << 16) | (*(_BYTE *)(v4 + 18) << 24);
  v26 = *(_BYTE *)(a4 + 18) | ((unsigned __int64)*(_BYTE *)(v4 + 19) << 8) | ((unsigned __int64)*(_BYTE *)(v4 + 20) << 16);
  v27 = *(_BYTE *)(v4 + 21) | ((unsigned __int64)*(_BYTE *)(v4 + 22) << 8) | ((unsigned __int64)(*(_BYTE *)(v4 + 23) & 0x1F) << 16);
  v28 = (unsigned __int8)(*(_BYTE *)(a4 + 23) & 0xE0) | (*(_BYTE *)(v4 + 24) << 8) | (*(_BYTE *)(v4 + 25) << 16) | (*(_BYTE *)(v4 + 26) << 24);
  v29 = *(_BYTE *)(v4 + 26) | (*(_BYTE *)(v4 + 27) << 8) | (*(_BYTE *)(v4 + 28) << 16);
  v30 = *(_BYTE *)(v4 + 28) | ((unsigned __int64)*(_BYTE *)(v4 + 29) << 8) | ((unsigned __int64)*(_BYTE *)(v4 + 30) << 16) | ((unsigned __int64)*(_BYTE *)(v4 + 31) << 24);
  v31 = (((unsigned __int8)(*(_BYTE *)(a3 + 2) & 0xE0) | (*(_BYTE *)(a3 + 3) << 8) | (*(_BYTE *)(a3 + 4) << 16) | ((unsigned int)*(_BYTE *)(a3 + 5) << 24)) >> 5) & 0x1FFFFF;
  v12 = *(_BYTE *)a3 | ((unsigned __int64)*(_BYTE *)(a3 + 1) << 8) | ((unsigned __int64)(*(_BYTE *)(a3 + 2) & 0x1F) << 16);
  v32 = (((unsigned __int8)(*(_BYTE *)(a2 + 2) & 0xE0) | (*(_BYTE *)(a2 + 3) << 8) | (*(_BYTE *)(a2 + 4) << 16) | ((unsigned int)*(_BYTE *)(a2 + 5) << 24)) >> 5) & 0x1FFFFF;
  v33 = (((unsigned __int8)(*(_BYTE *)(a2 + 5) & 0xFC) | (*(_BYTE *)(a2 + 6) << 8) | ((unsigned int)*(_BYTE *)(a2 + 7) << 16)) >> 2) & 0x1FFFFF;
  v34 = (((unsigned __int8)(*(_BYTE *)(a3 + 5) & 0xFC) | (*(_BYTE *)(a3 + 6) << 8) | ((unsigned int)*(_BYTE *)(a3 + 7) << 16)) >> 2) & 0x1FFFFF;
  v35 = v6 * v34 + v32 * v31 + v33 * v12;
  v36 = (((unsigned __int8)(*(_BYTE *)(a2 + 7) & 0x80) | (*(_BYTE *)(a2 + 8) << 8) | (*(_BYTE *)(a2 + 9) << 16) | ((unsigned int)*(_BYTE *)(a2 + 10) << 24)) >> 7) & 0x1FFFFF;
  v37 = (((unsigned __int8)(*(_BYTE *)(a2 + 10) & 0xF0) | (*(_BYTE *)(a2 + 11) << 8) | (*(_BYTE *)(a2 + 12) << 16) | ((unsigned int)*(_BYTE *)(a2 + 13) << 24)) >> 4) & 0x1FFFFF;
  v38 = (((unsigned __int8)(*(_BYTE *)(a3 + 7) & 0x80) | (*(_BYTE *)(a3 + 8) << 8) | (*(_BYTE *)(a3 + 9) << 16) | ((unsigned int)*(_BYTE *)(a3 + 10) << 24)) >> 7) & 0x1FFFFF;
  v39 = ((unsigned int)v13 >> 4) & 0x1FFFFF;
  v40 = v6 * v39 + v32 * v38 + v33 * v34 + v36 * v31 + v37 * v12;
  v41 = (((unsigned __int8)(*(_BYTE *)(a2 + 13) & 0xFE) | (*(_BYTE *)(a2 + 14) << 8) | ((unsigned int)*(_BYTE *)(a2 + 15) << 16)) >> 1) & 0x1FFFFF;
  v42 = (((unsigned __int8)(*(_BYTE *)(a2 + 15) & 0xC0) | (*(_BYTE *)(a2 + 16) << 8) | (*(_BYTE *)(a2 + 17) << 16) | ((unsigned int)*(_BYTE *)(a2 + 18) << 24)) >> 6) & 0x1FFFFF;
  v43 = ((unsigned int)v13 >> 4) & 0x1FFFFF;
  v44 = (((unsigned __int8)(*(_BYTE *)(a3 + 13) & 0xFE) | (*(_BYTE *)(a3 + 14) << 8) | ((unsigned int)*(_BYTE *)(a3 + 15) << 16)) >> 1) & 0x1FFFFF;
  v45 = v44;
  v46 = (((unsigned __int8)(*(_BYTE *)(a3 + 15) & 0xC0) | (*(_BYTE *)(a3 + 16) << 8) | (*(_BYTE *)(a3 + 17) << 16) | ((unsigned int)*(_BYTE *)(a3 + 18) << 24)) >> 6) & 0x1FFFFF;
  v47 = v6 * v46 + v32 * v44 + v33 * v43 + v36 * v38 + v37 * v34 + v41 * v31 + v42 * v12;
  v48 = v6 * v15
      + v32 * (v14 >> 3)
      + v33 * v46
      + v36 * v44
      + v37 * v43
      + v41 * v38
      + v42 * v34
      + (v7 >> 3) * v31
      + v8 * v12;
  v49 = ((unsigned int)v9 >> 5) & 0x1FFFFF;
  v50 = ((unsigned int)v10 >> 2) & 0x1FFFFF;
  v51 = v33 * v15
      + v36 * (v14 >> 3)
      + v37 * v46
      + v41 * v44
      + v42 * v39
      + (v7 >> 3) * v38
      + v8 * v34
      + v49 * v31
      + v50 * v12;
  v52 = ((unsigned int)v16 >> 5) & 0x1FFFFF;
  v53 = v52;
  v54 = v32 * v52 + v51;
  v55 = ((unsigned int)v17 >> 2) & 0x1FFFFF;
  v56 = v6 * v55 + v54;
  v57 = v31;
  v58 = v31;
  v59 = v31;
  v60 = v31;
  v61 = v31;
  v62 = v31;
  v11 >>= 7;
  v63 = v33 * v55
      + v36 * v53
      + v37 * v15
      + v41 * (v14 >> 3)
      + v42 * v46
      + (v7 >> 3) * v45
      + v8 * v39
      + v49 * v38
      + v50 * v34
      + v11 * v31;
  v64 = v38;
  v65 = v38;
  v66 = v38;
  v67 = v38;
  v68 = v38;
  v69 = v38;
  v70 = ((unsigned int)v10 >> 2) & 0x1FFFFF;
  v71 = v37 * v55 + v41 * v53 + v42 * v15 + (v7 >> 3) * (v14 >> 3) + v8 * v46 + v49 * v45 + v50 * v39 + v11 * v38;
  v72 = v45;
  v73 = v45;
  v74 = v45;
  v75 = v45;
  v76 = v45;
  v77 = v45;
  v78 = v42 * v55 + (v7 >> 3) * v53 + v8 * v15 + v49 * (v14 >> 3) + v70 * v46 + v11 * v45;
  v79 = v14 >> 3;
  v80 = v14 >> 3;
  v81 = v14 >> 3;
  v82 = v14 >> 3;
  v83 = v14 >> 3;
  v84 = v14 >> 3;
  v85 = v53;
  v86 = v8 * v55 + v49 * v53 + v70 * v15 + v11 * (v14 >> 3);
  v87 = v53;
  v88 = v53;
  v89 = v53;
  v90 = v53;
  v91 = v53;
  v92 = v50 * v55 + v11 * v53;
  v93 = v70 * v62;
  v94 = v70 * v69;
  v95 = v70 * v77;
  v96 = v70 * v84;
  v97 = v70 * v85;
  v98 = (v18 >> 7) * v70 + v11 * v55;
  v99 = v42 * v60;
  v100 = (v7 >> 3) * v12;
  v101 = v42 * v67;
  v102 = (v7 >> 3) * v34;
  v103 = v49 * v12;
  v104 = v32 * v55;
  v105 = v42 * v75;
  v106 = (v7 >> 3) * v39;
  v107 = v49 * v34;
  v108 = v36 * v55;
  v109 = v42 * v82;
  v110 = (v7 >> 3) * v46;
  v111 = v49 * v39;
  v112 = v41 * v55;
  v113 = v42 * v90;
  v114 = (v7 >> 3) * v15;
  v115 = v49 * v46;
  v116 = (v18 >> 7) * v42;
  v117 = (v7 >> 3) * v55;
  v118 = v49 * v15;
  v119 = (v18 >> 7) * (v7 >> 3);
  v120 = v49 * v55;
  v121 = (v18 >> 7) * v49;
  v122 = v11 * (v18 >> 7);
  v123 = (v19 + v6 * v12 + 0x100000) >> 21;
  v124 = v123 + v6 * v57 + v32 * v12;
  v125 = v19 + v6 * v12 - (v123 << 21);
  v126 = v35 + (((unsigned int)v21 >> 2) & 0x1FFFFF);
  v127 = v6 * v64 + v32 * v34 + v33 * v58 + v36 * v12;
  v128 = v40 + (((unsigned int)v23 >> 4) & 0x1FFFFF);
  v129 = (((unsigned int)v24 >> 1) & 0x1FFFFF) + v6 * v72 + v32 * v39 + v33 * v65 + v36 * v34 + v37 * v59 + v41 * v12;
  v130 = v47 + (((unsigned int)v25 >> 6) & 0x1FFFFF);
  v131 = (v26 >> 3) + v6 * v79 + v32 * v46 + v33 * v73 + v36 * v39 + v37 * v66 + v41 * v34 + v99 + v100;
  v132 = v48 + v27 + 0x100000;
  v133 = v132;
  v134 = (v132 >> 21)
       + v6 * v87
       + v32 * v15
       + v33 * v80
       + v36 * v46
       + v37 * v74
       + v41 * v39
       + v101
       + v102
       + v8 * v61
       + v103;
  v135 = v56 + (((unsigned int)v29 >> 2) & 0x1FFFFF);
  v136 = (v30 >> 7)
       + (v18 >> 7) * v6
       + v104
       + v33 * v88
       + v36 * v15
       + v37 * v81
       + v41 * v46
       + v105
       + v106
       + v8 * v68
       + v107
       + v93
       + v11 * v12;
  v137 = v63 + (v18 >> 7) * v32;
  v138 = (v18 >> 7) * v33 + v108 + v37 * v89 + v41 * v15 + v109 + v110 + v8 * v76 + v111 + v94 + v11 * v34;
  v139 = v71 + (v18 >> 7) * v36;
  v140 = (v18 >> 7) * v37 + v112 + v113 + v114 + v8 * v83 + v115 + v95 + v11 * v39;
  v141 = v78 + (v18 >> 7) * v41;
  v142 = v116 + v117 + v8 * v91 + v118 + v96 + v11 * v46;
  v143 = v86 + v119 + 0x100000;
  v144 = (v18 >> 7) * v8 + v120 + v97 + v11 * v15;
  v145 = v143;
  v146 = v86 + v119 - (v143 & 0xFFFFFFFFFFE00000LL);
  v147 = (v92 + v121 + 0x100000) >> 21;
  v148 = v92 + v121 - (v147 << 21);
  v149 = (v122 + 0x100000) >> 21;
  v150 = v122 - (v149 << 21);
  v151 = v124 + (((unsigned int)v20 >> 5) & 0x1FFFFF);
  v152 = (v151 + 0x100000) >> 21;
  v153 = v151 - (v152 << 21);
  v154 = (v126 + 0x100000) >> 21;
  v155 = v154 + v127 + (((unsigned int)v22 >> 7) & 0x1FFFFF);
  v156 = v155 + 0x100000;
  v157 = v155 - ((v155 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v158 = v129 + ((signed __int64)(v128 + 0x100000) >> 21);
  v159 = ((signed __int64)(v128 + 0x100000) >> 21) + v129 + 0x100000;
  v160 = v131 + ((signed __int64)(v130 + 0x100000) >> 21);
  v161 = ((signed __int64)(v130 + 0x100000) >> 21) + v131 + 0x100000;
  v162 = v134 + (((unsigned int)v28 >> 5) & 0x1FFFFF);
  v163 = v136 + ((signed __int64)(v135 + 0x100000) >> 21);
  v164 = v138 + ((signed __int64)(v137 + 0x100000) >> 21);
  v165 = ((signed __int64)(v137 + 0x100000) >> 21) + v138 + 0x100000;
  v166 = v140 + ((signed __int64)(v139 + 0x100000) >> 21);
  v167 = v142 + ((signed __int64)(v141 + 0x100000) >> 21);
  v168 = v146 + ((v167 + 0x100000) >> 21);
  v169 = v167 - ((v167 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v170 = v144 + (v145 >> 21);
  v171 = v148 + ((v170 + 0x100000) >> 21);
  v172 = v170 - ((v170 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v173 = (v98 + v147 + 0x100000) >> 21;
  v174 = v173 + v150;
  v175 = v98 + v147 - (v173 << 21);
  v176 = 136657 * v174 + (v165 >> 21) + v139 + -997805LL * v149 - ((v139 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v177 = v130 + (v159 >> 21) + 666643 * v168 - ((v130 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v178 = (v161 >> 21) + v48 + v27 + 470296 * v172 + 666643 * v171 + 654183 * v168 - (v133 & 0xFFFFFFFFFFE00000LL);
  v179 = v135
       + ((signed __int64)(v162 + 0x100000) >> 21)
       + -997805LL * v172
       + 470296 * v175
       + 136657 * v168
       + 666643 * v174
       + 654183 * v171
       - ((v135 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v180 = v160 + 666643 * v172 + 470296 * v168 - (v161 & 0xFFFFFFFFFFE00000LL);
  v181 = ((signed __int64)(v178 + 0x100000) >> 21)
       + v162
       + 654183 * v172
       + 666643 * v175
       + 470296 * v171
       + -997805 * v168
       - ((v162 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v182 = v174;
  v183 = ((signed __int64)(v179 + 0x100000) >> 21)
       + v163
       + 136657 * v172
       + 654183 * v175
       + -683901 * v168
       + -997805 * v171
       + 666643 * v149
       + 470296 * v174
       - ((v163 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v184 = -683901LL * v172
       + -997805LL * v175
       + 136657 * v171
       + 654183 * v174
       + v137
       + 470296 * v149
       - ((v137 + 0x100000) & 0xFFFFFFFFFFE00000LL)
       + ((v163 + 0x100000) >> 21);
  v185 = 136657 * v175 + -683901 * v171 + -997805LL * v182 + v164 + 654183 * v149 - (v165 & 0xFFFFFFFFFFE00000LL);
  v186 = -683901LL * v175 + v176 + 0x100000;
  v187 = -683901LL * v182 + v166 + 136657 * v149 - ((v166 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v188 = v141 + -683901LL * v149 - ((v141 + 0x100000) & 0xFFFFFFFFFFE00000LL) + ((v166 + 0x100000) >> 21);
  v189 = v169 + ((signed __int64)(v188 + 0x100000) >> 21);
  v190 = (signed __int64)(v177 + 0x100000) >> 21;
  v191 = v180 + v190;
  v192 = v180 + v190 + 0x100000;
  v193 = v184 - ((v184 + 0x100000) & 0xFFFFFFFFFFE00000LL) + ((signed __int64)(v183 + 0x100000) >> 21);
  v194 = v183 - ((v183 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v195 = v185 + ((signed __int64)(v184 + 0x100000) >> 21);
  v196 = v176 + -683901LL * v175 - (v186 & 0xFFFFFFFFFFE00000LL) + ((v195 + 0x100000) >> 21);
  v197 = v195 - ((v195 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v198 = v188 - ((v188 + 0x100000) & 0xFFFFFFFFFFE00000LL) + ((signed __int64)((v186 >> 21) + v187 + 0x100000) >> 21);
  v199 = v187 + (v186 >> 21) - (((v186 >> 21) + v187 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v200 = v179 + ((signed __int64)(v181 + 0x100000) >> 21) + -683901 * v189 - ((v179 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v201 = v157 + 666643 * v199;
  v202 = (v192 >> 21)
       + v178
       + -683901LL * v199
       + -997805 * v189
       + 136657 * v198
       - ((v178 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v203 = 136657 * v196
       + v177
       + -997805LL * v199
       + 470296 * v189
       + 654183 * v198
       - ((v177 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v204 = 666643 * v196 + 654183 * v193 + v126 + v152 - (v154 << 21);
  v205 = 654183 * v196
       + 136657 * v193
       + 470296 * v199
       + v128
       + (v156 >> 21)
       + 666643 * v198
       - ((v128 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v206 = 666643 * v193 + v125;
  v207 = 666643 * v197 + v153 + 470296 * v193;
  v208 = v206 + 0x100000;
  v209 = v206 - ((v206 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v210 = v204 + 470296 * v197;
  v211 = 470296 * v197 + v204 + 0x100000;
  v212 = 654183 * v197 + 470296 * v196 + v201 + -997805LL * v193;
  v213 = v205 + -997805LL * v197;
  v214 = -997805LL * v197 + v205 + 0x100000;
  v215 = 136657 * v197
       + -997805LL * v196
       + -683901LL * v193
       + v158
       + 654183 * v199
       + 666643 * v189
       + 470296 * v198
       - (v159 & 0xFFFFFFFFFFE00000LL);
  v216 = -683901LL * v197;
  v217 = v203 + v216;
  v218 = v216 + v203 + 0x100000;
  v219 = -683901LL * v196 + v191 + 136657 * v199 + 654183 * v189 + -997805LL * v198 - (v192 & 0xFFFFFFFFFFE00000LL);
  v220 = v217 - (v218 & 0xFFFFFFFFFFE00000LL);
  v221 = ((signed __int64)(v202 + 0x100000) >> 21)
       + v181
       + 136657 * v189
       + -683901LL * v198
       - ((v181 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v222 = v207 + (v208 >> 21);
  v223 = v212 + (v211 >> 21);
  v224 = (v211 >> 21) + v212 + 0x100000;
  v225 = v215 + (v214 >> 21);
  v226 = (v214 >> 21) + v215 + 0x100000;
  v227 = v220 + (v226 >> 21);
  v228 = v202 - ((v202 + 0x100000) & 0xFFFFFFFFFFE00000LL) + ((signed __int64)((v218 >> 21) + v219 + 0x100000) >> 21);
  v229 = v219 + (v218 >> 21) - (((v218 >> 21) + v219 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v230 = (signed __int64)(v200 + 0x100000) >> 21;
  v231 = v194 + v230 + 0x100000;
  v232 = v194 + v230 - (v231 & 0xFFFFFFFFFFE00000LL);
  v233 = v231 >> 21;
  v234 = v209 + 666643 * (v231 >> 21);
  v235 = (v234 >> 21) + v222 + 470296 * (v231 >> 21) - ((v222 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v236 = (v235 >> 21) + ((v222 + 0x100000) >> 21) + v210 + 654183 * v233 - (v211 & 0xFFFFFFFFFFE00000LL);
  v237 = (v236 >> 21) + v223 + -997805 * v233 - (v224 & 0xFFFFFFFFFFE00000LL);
  v238 = (v237 >> 21) + (v224 >> 21) + v213 + 136657 * v233 - (v214 & 0xFFFFFFFFFFE00000LL);
  v239 = (v238 >> 21) + v225 + -683901 * v233 - (v226 & 0xFFFFFFFFFFE00000LL);
  v240 = v227 + (v239 >> 21);
  v241 = v240 - (v240 & 0xFFFFFFFFFFE00000LL);
  v242 = v228 + ((signed __int64)(v229 + (v240 >> 21)) >> 21);
  v243 = v229 + (v240 >> 21) - ((v229 + (v240 >> 21)) & 0xFFFFFFFFFFE00000LL);
  v244 = v221
       - ((v221 + 0x100000) & 0xFFFFFFFFFFE00000LL)
       + ((signed __int64)(v228 + ((signed __int64)(v229 + (v240 >> 21)) >> 21)) >> 21);
  v245 = (v244 >> 21) + v200 + ((signed __int64)(v221 + 0x100000) >> 21) - ((v200 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v246 = v232 + (v245 >> 21);
  v247 = v245 - (v245 & 0xFFFFFFFFFFE00000LL);
  v248 = (signed __int64)(v232 + (v245 >> 21)) >> 21;
  v249 = v234 - (v234 & 0xFFFFFFFFFFE00000LL) + 666643 * v248;
  v250 = v235 - (v235 & 0xFFFFFFFFFFE00000LL) + 470296 * v248;
  v251 = v236 - (v236 & 0xFFFFFFFFFFE00000LL) + 654183 * v248;
  v252 = v237 - (v237 & 0xFFFFFFFFFFE00000LL) + -997805 * v248;
  v253 = v238 - (v238 & 0xFFFFFFFFFFE00000LL) + 136657 * v248;
  v254 = v239 - (v239 & 0xFFFFFFFFFFE00000LL) + -683901 * v248;
  v255 = v250 + (v249 >> 21);
  LODWORD(v249) = v249 - (v249 & 0xFFE00000);
  v256 = v251 + (v255 >> 21);
  v257 = v255 - (v255 & 0xFFFFFFFFFFE00000LL);
  v258 = v252 + (v256 >> 21);
  v259 = v256 - (v256 & 0xFFFFFFFFFFE00000LL);
  v260 = v253 + (v258 >> 21);
  v261 = v258 - (v258 & 0xFFFFFFFFFFE00000LL);
  v262 = v254 + (v260 >> 21);
  v263 = v260 - (v260 & 0xFFFFFFFFFFE00000LL);
  v264 = v241 + (v262 >> 21);
  v265 = v262 - (v262 & 0xFFFFFFFFFFE00000LL);
  v266 = v243 + (v264 >> 21);
  v267 = v264 - (v264 & 0xFFFFFFFFFFE00000LL);
  v268 = v242 - (v242 & 0xFFFFFFFFFFE00000LL) + (v266 >> 21);
  v269 = v266 - (v266 & 0xFFFFFFFFFFE00000LL);
  v270 = v244 - (v244 & 0xFFFFFFFFFFE00000LL) + (v268 >> 21);
  LODWORD(v268) = v268 - (v268 & 0xFFE00000);
  v271 = v247 + (v270 >> 21);
  v272 = v270 - (v270 & 0xFFFFFFFFFFE00000LL);
  v273 = v246 - (v246 & 0xFFFFFFFFFFE00000LL) + (v271 >> 21);
  v274 = v271 - (v271 & 0xFFFFFFFFFFE00000LL);
  *(_BYTE *)v5 = v249;
  *(_BYTE *)(v5 + 1) = BYTE1(v249);
  *(_BYTE *)(v5 + 2) = 32 * v257 | ((unsigned int)v249 >> 16);
  *(_BYTE *)(v5 + 3) = v257 >> 3;
  *(_BYTE *)(v5 + 4) = v257 >> 11;
  *(_BYTE *)(v5 + 5) = 4 * v259 | ((unsigned int)v257 >> 19);
  *(_BYTE *)(v5 + 6) = v259 >> 6;
  *(_BYTE *)(v5 + 7) = ((unsigned int)v259 >> 14) | ((_BYTE)v261 << 7);
  *(_BYTE *)(v5 + 8) = v261 >> 1;
  *(_BYTE *)(v5 + 9) = v261 >> 9;
  *(_BYTE *)(v5 + 10) = ((unsigned int)v261 >> 17) | 16 * v263;
  *(_BYTE *)(v5 + 11) = v263 >> 4;
  *(_BYTE *)(v5 + 12) = v263 >> 12;
  *(_BYTE *)(v5 + 13) = ((unsigned int)v263 >> 20) | 2 * v265;
  *(_BYTE *)(v5 + 14) = v265 >> 7;
  *(_BYTE *)(v5 + 15) = ((unsigned int)v265 >> 15) | ((_BYTE)v267 << 6);
  *(_BYTE *)(v5 + 16) = v267 >> 2;
  *(_BYTE *)(v5 + 17) = v267 >> 10;
  *(_BYTE *)(v5 + 18) = ((unsigned int)v267 >> 18) | 8 * v269;
  *(_BYTE *)(v5 + 19) = v269 >> 5;
  *(_BYTE *)(v5 + 20) = v269 >> 13;
  *(_BYTE *)(v5 + 21) = v268;
  *(_BYTE *)(v5 + 22) = BYTE1(v268);
  *(_BYTE *)(v5 + 23) = ((unsigned int)v268 >> 16) | 32 * v272;
  *(_BYTE *)(v5 + 24) = v272 >> 3;
  *(_BYTE *)(v5 + 25) = v272 >> 11;
  *(_BYTE *)(v5 + 26) = ((unsigned int)v272 >> 19) | 4 * v274;
  *(_BYTE *)(v5 + 27) = v274 >> 6;
  *(_BYTE *)(v5 + 28) = ((unsigned int)v274 >> 14) | ((_BYTE)v273 << 7);
  *(_BYTE *)(v5 + 29) = v273 >> 1;
  result = v273 >> 9;
  *(_BYTE *)(v5 + 30) = v273 >> 9;
  *(_BYTE *)(v5 + 31) = v273 >> 17;
  return result;
}

//----- (0000000000033D8F) ----------------------------------------------------
unsigned __int64 __fastcall sc_reduce(__int64 a1)
{
  __int64 v1; // ST60_8@1
  unsigned __int64 v2; // ST88_8@1
  int v3; // ST80_4@1
  int v4; // ST70_4@1
  int v5; // ST78_4@1
  int v6; // ST68_4@1
  int v7; // ST58_4@1
  unsigned __int64 v8; // ST98_8@1
  int v9; // ST50_4@1
  int v10; // ST48_4@1
  int v11; // ST40_4@1
  unsigned __int64 v12; // ST38_8@1
  unsigned __int64 v13; // ST30_8@1
  int v14; // ST28_4@1
  unsigned __int64 v15; // r9@1
  __int64 v16; // r14@1
  __int64 v17; // rdx@1
  signed __int64 v18; // ST08_8@1
  __int64 v19; // r12@1
  __int64 v20; // r8@1
  signed __int64 v21; // rax@1
  __int64 v22; // r10@1
  unsigned __int64 v23; // rcx@1
  signed __int64 v24; // rbx@1
  __int64 v25; // r11@1
  unsigned __int64 v26; // rdi@1
  signed __int64 v27; // r15@1
  signed __int64 v28; // ST18_8@1
  signed __int64 v29; // ST20_8@1
  unsigned __int64 v30; // ST90_8@1
  signed __int64 v31; // rcx@1
  signed __int64 v32; // ST10_8@1
  signed __int64 v33; // ST00_8@1
  signed __int64 v34; // r10@1
  signed __int64 v35; // rsi@1
  signed __int64 v36; // rdi@1
  signed __int64 v37; // rax@1
  signed __int64 v38; // r14@1
  unsigned __int64 v39; // ST50_8@1
  signed __int64 v40; // rbx@1
  signed __int64 v41; // rdx@1
  signed __int64 v42; // rdi@1
  signed __int64 v43; // r13@1
  signed __int64 v44; // ST30_8@1
  signed __int64 v45; // rsi@1
  unsigned __int64 v46; // r12@1
  unsigned __int64 v47; // ST40_8@1
  signed __int64 v48; // ST98_8@1
  signed __int64 v49; // r11@1
  unsigned __int64 v50; // ST50_8@1
  signed __int64 v51; // r8@1
  unsigned __int64 v52; // rbx@1
  unsigned __int64 v53; // r8@1
  unsigned __int64 v54; // r10@1
  unsigned __int64 v55; // r14@1
  signed __int64 v56; // ST48_8@1
  unsigned __int64 v57; // ST58_8@1
  signed __int64 v58; // r12@1
  signed __int64 v59; // rax@1
  signed __int64 v60; // rcx@1
  signed __int64 v61; // r15@1
  signed __int64 v62; // r9@1
  unsigned __int64 v63; // ST38_8@1
  signed __int64 v64; // ST80_8@1
  signed __int64 v65; // ST28_8@1
  signed __int64 v66; // rax@1
  signed __int64 v67; // ST88_8@1
  signed __int64 v68; // rdi@1
  signed __int64 v69; // r8@1
  signed __int64 v70; // rbx@1
  signed __int64 v71; // rcx@1
  unsigned __int64 v72; // r10@1
  signed __int64 v73; // ST20_8@1
  signed __int64 v74; // r9@1
  signed __int64 v75; // ST30_8@1
  signed __int64 v76; // r15@1
  signed __int64 v77; // ST48_8@1
  signed __int64 v78; // r11@1
  unsigned __int64 v79; // r13@1
  signed __int64 v80; // rdx@1
  signed __int64 v81; // rax@1
  signed __int64 v82; // rdx@1
  unsigned __int64 v83; // ST70_8@1
  unsigned __int64 v84; // ST40_8@1
  unsigned __int64 v85; // rbx@1
  unsigned __int64 v86; // ST98_8@1
  signed __int64 v87; // rdi@1
  unsigned __int64 v88; // ST78_8@1
  signed __int64 v89; // rcx@1
  unsigned __int64 v90; // rdi@1
  unsigned __int64 v91; // rsi@1
  signed __int64 v92; // rax@1
  unsigned __int64 v93; // r12@1
  signed __int64 v94; // r9@1
  signed __int64 v95; // r11@1
  signed __int64 v96; // r10@1
  signed __int64 v97; // r13@1
  signed __int64 v98; // r8@1
  signed __int64 v99; // rdx@1
  signed __int64 v100; // rbx@1
  unsigned __int64 v101; // ST90_8@1
  unsigned __int64 v102; // ST98_8@1
  signed __int64 v103; // rdx@1
  signed __int64 v104; // rcx@1
  signed __int64 v105; // rbx@1
  signed __int64 v106; // rdi@1
  signed __int64 v107; // r14@1
  unsigned __int64 v108; // r15@1
  unsigned __int64 v109; // r9@1
  signed __int64 v110; // rdx@1
  signed __int64 v111; // rax@1
  unsigned __int64 v112; // rdx@1
  signed __int64 v113; // rsi@1
  unsigned __int64 v114; // rax@1
  signed __int64 v115; // r12@1
  unsigned __int64 v116; // rsi@1
  signed __int64 v117; // r14@1
  unsigned __int64 v118; // r12@1
  signed __int64 v119; // r15@1
  unsigned __int64 v120; // r14@1
  signed __int64 v121; // r11@1
  unsigned __int64 v122; // r15@1
  signed __int64 v123; // rbx@1
  unsigned __int64 v124; // r11@1
  signed __int64 v125; // r10@1
  signed __int64 v126; // r9@1
  unsigned __int64 v127; // r10@1
  unsigned __int64 v128; // r8@1
  unsigned __int64 v129; // r9@1
  unsigned __int64 result; // rax@1

  v1 = a1;
  v2 = *(_BYTE *)a1 | ((unsigned __int64)*(_BYTE *)(a1 + 1) << 8) | ((unsigned __int64)(*(_BYTE *)(a1 + 2) & 0x1F) << 16);
  v3 = (unsigned __int8)(*(_BYTE *)(a1 + 2) & 0xE0) | (*(_BYTE *)(a1 + 3) << 8) | (*(_BYTE *)(a1 + 4) << 16) | (*(_BYTE *)(a1 + 5) << 24);
  v4 = *(_BYTE *)(a1 + 5) | (*(_BYTE *)(a1 + 6) << 8) | (*(_BYTE *)(a1 + 7) << 16);
  v5 = *(_BYTE *)(a1 + 7) | (*(_BYTE *)(a1 + 8) << 8) | (*(_BYTE *)(a1 + 9) << 16) | (*(_BYTE *)(a1 + 10) << 24);
  v6 = *(_BYTE *)(a1 + 10) | (*(_BYTE *)(a1 + 11) << 8) | (*(_BYTE *)(a1 + 12) << 16) | (*(_BYTE *)(a1 + 13) << 24);
  v7 = (unsigned __int8)(*(_BYTE *)(a1 + 13) & 0xFE) | (*(_BYTE *)(a1 + 14) << 8) | (*(_BYTE *)(a1 + 15) << 16);
  v8 = *(_BYTE *)(a1 + 18) | ((unsigned __int64)*(_BYTE *)(a1 + 19) << 8) | ((unsigned __int64)*(_BYTE *)(a1 + 20) << 16);
  v9 = (unsigned __int8)(*(_BYTE *)(a1 + 23) & 0xE0) | (*(_BYTE *)(a1 + 24) << 8) | (*(_BYTE *)(a1 + 25) << 16) | (*(_BYTE *)(a1 + 26) << 24);
  v10 = *(_BYTE *)(a1 + 28) | (*(_BYTE *)(a1 + 29) << 8) | (*(_BYTE *)(a1 + 30) << 16) | (*(_BYTE *)(a1 + 31) << 24);
  v11 = (unsigned __int8)(*(_BYTE *)(a1 + 34) & 0xFE) | (*(_BYTE *)(a1 + 35) << 8) | (*(_BYTE *)(a1 + 36) << 16);
  v12 = *(_BYTE *)(a1 + 39) | ((unsigned __int64)*(_BYTE *)(a1 + 40) << 8) | ((unsigned __int64)*(_BYTE *)(a1 + 41) << 16);
  v13 = *(_BYTE *)(a1 + 42) | ((unsigned __int64)*(_BYTE *)(a1 + 43) << 8) | ((unsigned __int64)(*(_BYTE *)(a1 + 44) & 0x1F) << 16);
  v14 = (unsigned __int8)(*(_BYTE *)(a1 + 44) & 0xE0) | (*(_BYTE *)(a1 + 45) << 8) | (*(_BYTE *)(a1 + 46) << 16) | (*(_BYTE *)(a1 + 47) << 24);
  v15 = *(_BYTE *)(a1 + 60) | ((unsigned __int64)*(_BYTE *)(a1 + 61) << 8) | ((unsigned __int64)*(_BYTE *)(a1 + 62) << 16) | ((unsigned __int64)*(_BYTE *)(a1 + 63) << 24);
  v16 = (((unsigned __int8)(*(_BYTE *)(a1 + 55) & 0xFE) | (*(_BYTE *)(a1 + 56) << 8) | ((unsigned int)*(_BYTE *)(a1 + 57) << 16)) >> 1) & 0x1FFFFF;
  v17 = (((unsigned __int8)(*(_BYTE *)(a1 + 57) & 0xC0) | (*(_BYTE *)(a1 + 58) << 8) | (*(_BYTE *)(a1 + 59) << 16) | ((unsigned int)*(_BYTE *)(a1 + 60) << 24)) >> 6) & 0x1FFFFF;
  v18 = 136657 * v17
      + (((*(_BYTE *)(a1 + 36) | (*(_BYTE *)(a1 + 37) << 8) | (*(_BYTE *)(a1 + 38) << 16) | ((unsigned int)*(_BYTE *)(a1 + 39) << 24)) >> 6) & 0x1FFFFF)
      + -683901 * v16;
  v19 = (((unsigned __int8)(*(_BYTE *)(a1 + 49) & 0x80) | (*(_BYTE *)(a1 + 50) << 8) | (*(_BYTE *)(a1 + 51) << 16) | ((unsigned int)*(_BYTE *)(a1 + 52) << 24)) >> 7) & 0x1FFFFF;
  v20 = (((unsigned __int8)(*(_BYTE *)(a1 + 52) & 0xF0) | (*(_BYTE *)(a1 + 53) << 8) | (*(_BYTE *)(a1 + 54) << 16) | ((unsigned int)*(_BYTE *)(a1 + 55) << 24)) >> 4) & 0x1FFFFF;
  v21 = 654183 * v17
      + -997805 * v16
      + 136657 * v20
      + (((*(_BYTE *)(a1 + 31) | (*(_BYTE *)(a1 + 32) << 8) | (*(_BYTE *)(a1 + 33) << 16) | ((unsigned int)*(_BYTE *)(a1 + 34) << 24)) >> 4) & 0x1FFFFF)
      + -683901 * v19;
  v22 = ((*(_BYTE *)(a1 + 47) | (*(_BYTE *)(a1 + 48) << 8) | ((unsigned int)*(_BYTE *)(a1 + 49) << 16)) >> 2) & 0x1FFFFF;
  v23 = 470296 * v19
      + (*(_BYTE *)(a1 + 21) | ((unsigned __int64)*(_BYTE *)(a1 + 22) << 8) | ((unsigned __int64)(*(_BYTE *)(a1 + 23) & 0x1F) << 16))
      + 654183 * v22;
  v24 = 470296 * v16
      + 654183 * v20
      + -997805 * v19
      + (((*(_BYTE *)(a1 + 26) | (*(_BYTE *)(a1 + 27) << 8) | ((unsigned int)*(_BYTE *)(a1 + 28) << 16)) >> 2) & 0x1FFFFF)
      + 136657 * v22;
  v25 = ((*(_BYTE *)(a1 + 15) | (*(_BYTE *)(a1 + 16) << 8) | (*(_BYTE *)(a1 + 17) << 16) | ((unsigned int)*(_BYTE *)(a1 + 18) << 24)) >> 6) & 0x1FFFFF;
  v26 = (unsigned __int64)(666643 * v22 + v25 + 0x100000) >> 21;
  v27 = v26 + (v8 >> 3) + 470296 * v22;
  v28 = 666643 * v22 + v25 - (v26 << 21);
  v29 = v23 + 666643 * v20;
  v30 = 666643 * v20 + v23 + 0x100000;
  v31 = 666643 * v16 + 470296 * v20 + 654183 * v19 + (((unsigned int)v9 >> 5) & 0x1FFFFF) + -997805 * v22;
  v32 = v24 + 666643 * v17;
  v33 = 666643 * v17 + v24 + 0x100000;
  v15 >>= 3;
  v34 = 666643 * v15
      + 470296 * v17
      + 654183 * v16
      + -997805 * v20
      + 136657 * v19
      + (((unsigned int)v10 >> 7) & 0x1FFFFF)
      + -683901 * v22;
  v35 = v21 + 470296 * v15;
  v36 = 470296 * v15 + v21 + 0x100000;
  v37 = 654183 * v15 + -997805 * v17 + 136657 * v16 + (((unsigned int)v11 >> 1) & 0x1FFFFF) + -683901 * v20;
  v38 = v36;
  v39 = v35 - (v36 & 0xFFFFFFFFFFE00000LL);
  v40 = v18 + -997805LL * v15;
  v41 = 136657 * v15 + (v12 >> 3) + -683901 * v17;
  v42 = v40 + 0x100000;
  v43 = -683901LL * v15 + v13;
  v44 = (((unsigned int)v14 >> 5) & 0x1FFFFF) + ((v43 + 0x100000) >> 21);
  v45 = 666643 * v19;
  v46 = (unsigned __int64)(666643 * v19 + v27 + 0x100000) >> 21;
  v47 = v27 + v45 - (v46 << 21);
  v30 >>= 21;
  v48 = v30 + v31 + 0x100000;
  v49 = v39 + (((v33 >> 21) + v34 + 0x100000) >> 21);
  v50 = v34 + (v33 >> 21) - (((v33 >> 21) + v34 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v51 = v37 + (v38 >> 21);
  v52 = v40 - ((v40 + 0x100000) & 0xFFFFFFFFFFE00000LL) + ((v51 + 0x100000) >> 21);
  v53 = v51 - ((v51 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v42 >>= 21;
  v54 = v43 - ((v43 + 0x100000) & 0xFFFFFFFFFFE00000LL) + ((v42 + v41 + 0x100000) >> 21);
  v55 = v41 + v42 - ((v42 + v41 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v56 = (((unsigned int)v7 >> 1) & 0x1FFFFF) + 666643 * v44;
  v57 = v31 + v30 - (v48 & 0xFFFFFFFFFFE00000LL) + 136657 * v44 + -683901LL * v54;
  v58 = 136657 * v54 + -997805 * v44 + v29 + v46 - (v30 << 21);
  v59 = 666643 * v52 + (((unsigned int)v4 >> 2) & 0x1FFFFF) + 654183 * v49;
  v60 = 666643 * v49 + v2;
  v61 = 666643 * v53 + (((unsigned int)v3 >> 5) & 0x1FFFFF) + 470296 * v49;
  v62 = v60 + 0x100000;
  v63 = v60 - ((v60 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v64 = v59 + 470296 * v53;
  v65 = 470296 * v53 + v59 + 0x100000;
  v66 = 654183 * v53 + 666643 * v55 + 470296 * v52 + (((unsigned int)v5 >> 7) & 0x1FFFFF) + -997805 * v49;
  v67 = 470296 * v55
      + 654183 * v52
      + 666643 * v54
      + (((unsigned int)v6 >> 4) & 0x1FFFFF)
      + 136657 * v49
      + -997805LL * v53;
  v68 = 136657 * v53 + 654183 * v55 + -997805LL * v52 + 470296 * v54 + v56 + -683901 * v49;
  v69 = -997805LL * v55 + v28 + 470296 * v44 + 654183 * v54 + 136657 * v52 + -683901LL * v53;
  v70 = 136657 * v55 + v47 + 654183 * v44 + -997805LL * v54 + -683901LL * v52;
  v71 = -683901LL * v55 + v58 + 0x100000;
  v72 = v32 + (v48 >> 21) - (v33 & 0xFFFFFFFFFFE00000LL) + -683901 * v44;
  v62 >>= 21;
  v73 = v61 + v62;
  v74 = v62 + v61 + 0x100000;
  v75 = v66 + (v65 >> 21);
  v76 = (v65 >> 21) + v66 + 0x100000;
  v77 = v68 + ((v67 + 0x100000) >> 21);
  v78 = ((v67 + 0x100000) >> 21) + v68 + 0x100000;
  v79 = v69 - ((v69 + 0x100000) & 0xFFFFFFFFFFE00000LL) + (v78 >> 21);
  v80 = (v69 + 0x100000) >> 21;
  v81 = v70 + v80;
  v82 = v80 + v70 + 0x100000;
  v83 = v58 + -683901LL * v55 - (v71 & 0xFFFFFFFFFFE00000LL) + (v82 >> 21);
  v84 = v81 - (v82 & 0xFFFFFFFFFFE00000LL);
  v85 = v57 + (v71 >> 21);
  v86 = v72 - ((v72 + 0x100000) & 0xFFFFFFFFFFE00000LL) + ((signed __int64)(v85 + 0x100000) >> 21);
  v87 = ((signed __int64)(v72 + 0x100000) >> 21) + v50;
  v88 = v87 - ((v87 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v89 = (v87 + 0x100000) >> 21;
  v90 = ((signed __int64)(v63 + 666643 * v89) >> 21)
      + v73
      + 470296 * v89
      - (v74 & 0xFFFFFFFFFFE00000LL)
      - ((((signed __int64)(v63 + 666643 * v89) >> 21) + v73 + 470296 * v89 - (v74 & 0xFFFFFFFFFFE00000LL)) & 0xFFFFFFFFFFE00000LL);
  v91 = ((signed __int64)(((signed __int64)(v63 + 666643 * v89) >> 21)
                        + v73
                        + 470296 * v89
                        - (v74 & 0xFFFFFFFFFFE00000LL)) >> 21)
      + (v74 >> 21)
      + v64
      + 654183 * v89
      - (v65 & 0xFFFFFFFFFFE00000LL)
      - ((((signed __int64)(((signed __int64)(v63 + 666643 * v89) >> 21)
                          + v73
                          + 470296 * v89
                          - (v74 & 0xFFFFFFFFFFE00000LL)) >> 21)
        + (v74 >> 21)
        + v64
        + 654183 * v89
        - (v65 & 0xFFFFFFFFFFE00000LL)) & 0xFFFFFFFFFFE00000LL);
  v92 = ((signed __int64)(((signed __int64)(((signed __int64)(((signed __int64)(v63 + 666643 * v89) >> 21)
                                                            + v73
                                                            + 470296 * v89
                                                            - (v74 & 0xFFFFFFFFFFE00000LL)) >> 21)
                                          + (v74 >> 21)
                                          + v64
                                          + 654183 * v89
                                          - (v65 & 0xFFFFFFFFFFE00000LL)) >> 21)
                        + v75
                        + -997805 * v89
                        - (v76 & 0xFFFFFFFFFFE00000LL)) >> 21)
      + (v76 >> 21)
      + v67
      + 136657 * v89
      - ((v67 + 0x100000) & 0xFFFFFFFFFFE00000LL);
  v93 = ((signed __int64)(((signed __int64)(((signed __int64)(v63 + 666643 * v89) >> 21)
                                          + v73
                                          + 470296 * v89
                                          - (v74 & 0xFFFFFFFFFFE00000LL)) >> 21)
                        + (v74 >> 21)
                        + v64
                        + 654183 * v89
                        - (v65 & 0xFFFFFFFFFFE00000LL)) >> 21)
      + v75
      + -997805 * v89
      - (v76 & 0xFFFFFFFFFFE00000LL)
      - ((((signed __int64)(((signed __int64)(((signed __int64)(v63 + 666643 * v89) >> 21)
                                            + v73
                                            + 470296 * v89
                                            - (v74 & 0xFFFFFFFFFFE00000LL)) >> 21)
                          + (v74 >> 21)
                          + v64
                          + 654183 * v89
                          - (v65 & 0xFFFFFFFFFFE00000LL)) >> 21)
        + v75
        + -997805 * v89
        - (v76 & 0xFFFFFFFFFFE00000LL)) & 0xFFFFFFFFFFE00000LL);
  v94 = (v92 >> 21) + v77 + -683901 * v89 - (v78 & 0xFFFFFFFFFFE00000LL);
  v95 = v79 + (v94 >> 21);
  v96 = v84 + (v95 >> 21);
  v97 = v83 + (v96 >> 21);
  v98 = v85 - ((v85 + 0x100000) & 0xFFFFFFFFFFE00000LL) + (v97 >> 21);
  v99 = v86 + (v98 >> 21);
  v100 = v88 + (v99 >> 21);
  v101 = v99 - (v99 & 0xFFFFFFFFFFE00000LL);
  v102 = v100 - (v100 & 0xFFFFFFFFFFE00000LL);
  v103 = v100 >> 21;
  v104 = v63 + 666643 * v89 - ((v63 + 666643 * v89) & 0xFFFFFFFFFFE00000LL) + 666643 * (v100 >> 21);
  v105 = v90 + 470296 * (v100 >> 21);
  v106 = v91 + 654183 * v103;
  v107 = v93 + -997805 * v103;
  v108 = v92 - (v92 & 0xFFFFFFFFFFE00000LL) + 136657 * v103;
  v109 = v94 - (v94 & 0xFFFFFFFFFFE00000LL) + -683901 * v103;
  v110 = v105 + (v104 >> 21);
  LODWORD(v104) = v104 - (v104 & 0xFFE00000);
  v111 = v106 + (v110 >> 21);
  v112 = v110 - (v110 & 0xFFFFFFFFFFE00000LL);
  v113 = v107 + (v111 >> 21);
  v114 = v111 - (v111 & 0xFFFFFFFFFFE00000LL);
  v115 = v108 + (v113 >> 21);
  v116 = v113 - (v113 & 0xFFFFFFFFFFE00000LL);
  v117 = v109 + (v115 >> 21);
  v118 = v115 - (v115 & 0xFFFFFFFFFFE00000LL);
  v119 = v95 - (v95 & 0xFFFFFFFFFFE00000LL) + (v117 >> 21);
  v120 = v117 - (v117 & 0xFFFFFFFFFFE00000LL);
  v121 = v96 - (v96 & 0xFFFFFFFFFFE00000LL) + (v119 >> 21);
  v122 = v119 - (v119 & 0xFFFFFFFFFFE00000LL);
  v123 = v97 - (v97 & 0xFFFFFFFFFFE00000LL) + (v121 >> 21);
  v124 = v121 - (v121 & 0xFFFFFFFFFFE00000LL);
  v125 = v98 - (v98 & 0xFFFFFFFFFFE00000LL) + (v123 >> 21);
  LODWORD(v123) = v123 - (v123 & 0xFFE00000);
  v126 = v101 + (v125 >> 21);
  v127 = v125 - (v125 & 0xFFFFFFFFFFE00000LL);
  v128 = v102 + (v126 >> 21);
  v129 = v126 - (v126 & 0xFFFFFFFFFFE00000LL);
  *(_BYTE *)v1 = v104;
  *(_BYTE *)(v1 + 1) = BYTE1(v104);
  *(_BYTE *)(v1 + 2) = 32 * v112 | ((unsigned int)v104 >> 16);
  *(_BYTE *)(v1 + 3) = v112 >> 3;
  *(_BYTE *)(v1 + 4) = v112 >> 11;
  *(_BYTE *)(v1 + 5) = 4 * v114 | ((unsigned int)v112 >> 19);
  *(_BYTE *)(v1 + 6) = v114 >> 6;
  *(_BYTE *)(v1 + 7) = ((unsigned int)v114 >> 14) | ((_BYTE)v116 << 7);
  *(_BYTE *)(v1 + 8) = v116 >> 1;
  *(_BYTE *)(v1 + 9) = v116 >> 9;
  *(_BYTE *)(v1 + 10) = ((unsigned int)v116 >> 17) | 16 * v118;
  *(_BYTE *)(v1 + 11) = v118 >> 4;
  *(_BYTE *)(v1 + 12) = v118 >> 12;
  *(_BYTE *)(v1 + 13) = ((unsigned int)v118 >> 20) | 2 * v120;
  *(_BYTE *)(v1 + 14) = v120 >> 7;
  *(_BYTE *)(v1 + 15) = ((unsigned int)v120 >> 15) | ((_BYTE)v122 << 6);
  *(_BYTE *)(v1 + 16) = v122 >> 2;
  *(_BYTE *)(v1 + 17) = v122 >> 10;
  *(_BYTE *)(v1 + 18) = ((unsigned int)v122 >> 18) | 8 * v124;
  *(_BYTE *)(v1 + 19) = v124 >> 5;
  *(_BYTE *)(v1 + 20) = v124 >> 13;
  *(_BYTE *)(v1 + 21) = v123;
  *(_BYTE *)(v1 + 22) = BYTE1(v123);
  *(_BYTE *)(v1 + 23) = ((unsigned int)v123 >> 16) | 32 * v127;
  *(_BYTE *)(v1 + 24) = v127 >> 3;
  *(_BYTE *)(v1 + 25) = v127 >> 11;
  *(_BYTE *)(v1 + 26) = ((unsigned int)v127 >> 19) | 4 * v129;
  *(_BYTE *)(v1 + 27) = v129 >> 6;
  *(_BYTE *)(v1 + 28) = ((unsigned int)v129 >> 14) | ((_BYTE)v128 << 7);
  *(_BYTE *)(v1 + 29) = v128 >> 1;
  result = v128 >> 9;
  *(_BYTE *)(v1 + 30) = v128 >> 9;
  *(_BYTE *)(v1 + 31) = v128 >> 17;
  return result;
}

//----- (0000000000034C61) ----------------------------------------------------
__int64 __fastcall crypto_verify_32(__int64 a1, __int64 a2)
{
  char v2; // al@1

  v2 = *(_BYTE *)a1 ^ *(_BYTE *)a2 | *(_BYTE *)(a1 + 1) ^ *(_BYTE *)(a2 + 1) | *(_BYTE *)(a1 + 2) ^ *(_BYTE *)(a2 + 2) | *(_BYTE *)(a1 + 3) ^ *(_BYTE *)(a2 + 3) | *(_BYTE *)(a1 + 4) ^ *(_BYTE *)(a2 + 4) | *(_BYTE *)(a1 + 5) ^ *(_BYTE *)(a2 + 5) | *(_BYTE *)(a1 + 6) ^ *(_BYTE *)(a2 + 6) | *(_BYTE *)(a1 + 7) ^ *(_BYTE *)(a2 + 7) | *(_BYTE *)(a1 + 8) ^ *(_BYTE *)(a2 + 8) | *(_BYTE *)(a1 + 9) ^ *(_BYTE *)(a2 + 9) | *(_BYTE *)(a1 + 10) ^ *(_BYTE *)(a2 + 10) | *(_BYTE *)(a1 + 11) ^ *(_BYTE *)(a2 + 11) | *(_BYTE *)(a1 + 12) ^ *(_BYTE *)(a2 + 12) | *(_BYTE *)(a1 + 13) ^ *(_BYTE *)(a2 + 13) | *(_BYTE *)(a1 + 14) ^ *(_BYTE *)(a2 + 14) | *(_BYTE *)(a1 + 15) ^ *(_BYTE *)(a2 + 15) | *(_BYTE *)(a1 + 16) ^ *(_BYTE *)(a2 + 16) | *(_BYTE *)(a1 + 17) ^ *(_BYTE *)(a2 + 17) | *(_BYTE *)(a1 + 18) ^ *(_BYTE *)(a2 + 18) | *(_BYTE *)(a1 + 19) ^ *(_BYTE *)(a2 + 19) | *(_BYTE *)(a1 + 20) ^ *(_BYTE *)(a2 + 20);
  return ((((unsigned int)(unsigned __int8)(v2 | *(_BYTE *)(a1 + 21) ^ *(_BYTE *)(a2 + 21) | *(_BYTE *)(a1 + 22) ^ *(_BYTE *)(a2 + 22) | *(_BYTE *)(a1 + 23) ^ *(_BYTE *)(a2 + 23) | *(_BYTE *)(a1 + 24) ^ *(_BYTE *)(a2 + 24) | *(_BYTE *)(a1 + 25) ^ *(_BYTE *)(a2 + 25) | *(_BYTE *)(a1 + 26) ^ *(_BYTE *)(a2 + 26) | *(_BYTE *)(a1 + 27) ^ *(_BYTE *)(a2 + 27) | *(_BYTE *)(a1 + 28) ^ *(_BYTE *)(a2 + 28) | *(_BYTE *)(a1 + 29) ^ *(_BYTE *)(a2 + 29) | *(_BYTE *)(a1 + 30) ^ *(_BYTE *)(a2 + 30) | *(_BYTE *)(a1 + 31) ^ *(_BYTE *)(a2 + 31))
          + 511) >> 8) & 1)
       - 1;
}

//----- (0000000000034D73) ----------------------------------------------------
__int64 __fastcall ge_cmov(__int64 a1, __int64 a2, int a3)
{
  int v3; // er14@1

  v3 = a3;
  fe_cmov(a1, a2, a3);
  fe_cmov(a1 + 40, a2 + 40, v3);
  return fe_cmov(a1 + 80, a2 + 80, v3);
}

//----- (0000000000034DBB) ----------------------------------------------------
__int64 __fastcall fe_cmov(__int64 a1, __int64 a2, int a3)
{
  int v3; // ST04_4@1
  int v4; // ST08_4@1
  int v5; // ST0C_4@1
  int v6; // ST10_4@1
  int v7; // ST14_4@1
  int v8; // ST18_4@1
  int v9; // ST1C_4@1
  int v10; // er12@1
  int v11; // er13@1
  int v12; // eax@1
  int v13; // ecx@1
  int v14; // er8@1
  int v15; // er9@1
  int v16; // er10@1
  int v17; // er11@1
  int v18; // ebx@1
  int v19; // er14@1
  int v20; // er15@1
  int v21; // esi@1
  int v22; // edx@1
  __int64 result; // rax@1

  v3 = *(_DWORD *)(a1 + 4);
  v4 = *(_DWORD *)(a1 + 8);
  v5 = *(_DWORD *)(a1 + 12);
  v6 = *(_DWORD *)(a1 + 16);
  v7 = *(_DWORD *)(a1 + 20);
  v8 = *(_DWORD *)(a1 + 24);
  v9 = *(_DWORD *)(a1 + 28);
  v10 = *(_DWORD *)(a1 + 32);
  v11 = *(_DWORD *)(a1 + 36);
  v12 = *(_DWORD *)a1 ^ *(_DWORD *)a2;
  v13 = v3 ^ *(_DWORD *)(a2 + 4);
  v14 = v4 ^ *(_DWORD *)(a2 + 8);
  v15 = v5 ^ *(_DWORD *)(a2 + 12);
  v16 = v6 ^ *(_DWORD *)(a2 + 16);
  v17 = v7 ^ *(_DWORD *)(a2 + 20);
  v18 = v8 ^ *(_DWORD *)(a2 + 24);
  v19 = v9 ^ *(_DWORD *)(a2 + 28);
  v20 = v10 ^ *(_DWORD *)(a2 + 32);
  v21 = v11 ^ *(_DWORD *)(a2 + 36);
  v22 = -a3;
  result = *(_DWORD *)a1 ^ v22 & (unsigned int)v12;
  *(_DWORD *)a1 = result;
  *(_DWORD *)(a1 + 4) = v3 ^ v22 & v13;
  *(_DWORD *)(a1 + 8) = v4 ^ v22 & v14;
  *(_DWORD *)(a1 + 12) = v5 ^ v22 & v15;
  *(_DWORD *)(a1 + 16) = v6 ^ v22 & v16;
  *(_DWORD *)(a1 + 20) = v7 ^ v22 & v17;
  *(_DWORD *)(a1 + 24) = v8 ^ v22 & v18;
  *(_DWORD *)(a1 + 28) = v9 ^ v22 & v19;
  *(_DWORD *)(a1 + 32) = v10 ^ v22 & v20;
  *(_DWORD *)(a1 + 36) = v11 ^ v21 & v22;
  return result;
}

//----- (0000000000034EBA) ----------------------------------------------------
void *__fastcall ccn_set(__int64 a1, void *a2, const void *a3)
{
  return memcpy(a2, a3, 8 * a1);
}

//----- (0000000000034ED5) ----------------------------------------------------
__int64 __fastcall cc_cmp_safe(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 result; // rax@1

  for ( LOBYTE(result) = a1 == 0; a1; --a1 )
    LOBYTE(result) = *(_BYTE *)a2++ ^ *(_BYTE *)a3++ | result;
  return (unsigned __int8)result;
}

//----- (0000000000034EF7) ----------------------------------------------------
int __fastcall ccrsa_eme_pkcs1v15_encode(__int64 a1, __int64 a2, void *a3, size_t a4, const void *a5)
{
  size_t v5; // r14@1
  unsigned __int64 v6; // rbx@1
  size_t v7; // r13@1
  unsigned __int64 v8; // rcx@1
  unsigned __int64 v9; // rax@2
  size_t v10; // r12@4
  int result; // eax@4
  signed __int64 v12; // rax@5
  signed __int64 v13; // rcx@6
  signed __int64 v14; // r14@7
  unsigned __int64 v15; // r14@12
  size_t v16; // r12@12
  unsigned __int64 v17; // rax@13
  bool v22; // cf@14
  signed __int64 v25; // [sp+8h] [bp-58h]@7
  const void *v26; // [sp+18h] [bp-48h]@1
  size_t v27; // [sp+20h] [bp-40h]@4
  unsigned __int64 v28; // [sp+28h] [bp-38h]@1
  signed __int64 v29; // [sp+30h] [bp-30h]@4

  v26 = a5;
  v5 = a4;
  v6 = (unsigned __int64)a3;
  v28 = a2 + 7;
  v7 = ((a2 + 7) & 0xFFFFFFFFFFFFFFF8LL) - a2;
  bzero(a3, v7);
  if ( v6 < v6 + v7 )
  {
    v9 = ((a2 + 7) & 0xFFFFFFFFFFFFFFF8LL) - a2;
    v8 = v6;
    do
    {
      *(_BYTE *)v8++ = 0;
      --v9;
    }
    while ( v9 );
  }
  v10 = a2 - v5;
  v27 = v5;
  v29 = a2 - v5 - 3;
  result = (*(int (__fastcall **)(__int64, signed __int64, unsigned __int64, unsigned __int64))a1)(
             a1,
             v29,
             v6 + v7 + 2,
             v8);
  if ( !result )
  {
    v12 = 0LL;
    if ( v10 != 3 )
    {
      v13 = v7 + 2;
      do
      {
        v25 = v12;
        v14 = v6 + v13 + v12;
        while ( !*(_BYTE *)v14 )
        {
          result = (*(int (__fastcall **)(__int64, signed __int64, signed __int64))a1)(a1, 1LL, v14);
          if ( result )
            return result;
        }
        v12 = v25 + 1;
        v13 = v7 + 2;
      }
      while ( v25 + 1 < (unsigned __int64)v29 );
    }
    v15 = v28 >> 3;
    v16 = v7 + v10;
    *(_BYTE *)(v6 + v7) = 0;
    *(_BYTE *)(v6 + v7 + 1) = 2;
    *(_BYTE *)(v6 + v16 - 1) = 0;
    memcpy((void *)(v6 + v16), v26, v27);
    if ( (signed __int64)((v28 >> 3) - 1) > 0 )
    {
      v17 = ((a2 + 7) & 0xFFFFFFFFFFFFFFF8LL) + v6 - 16;
      do
      {
        _RCX = *(_QWORD *)v6;
        __asm { bswap   rcx }
        _RDX = *(_QWORD *)(v17 + 8);
        __asm { bswap   rdx }
        *(_QWORD *)v6 = _RDX;
        *(_QWORD *)(v17 + 8) = _RCX;
        v6 += 8LL;
        v22 = v6 < v17;
        v17 -= 8LL;
      }
      while ( v22 );
    }
    result = 0;
    if ( v15 & 1 )
    {
      _RCX = *(_QWORD *)v6;
      __asm { bswap   rcx }
      *(_QWORD *)v6 = _RCX;
    }
  }
  return result;
}

//----- (0000000000035043) ----------------------------------------------------
int __usercall ccmode_gcm_finalize@<eax>(signed __int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __m128i a5@<xmm0>, __m128i a6@<xmm1>, __m128i a7@<xmm2>, __m128i a8@<xmm3>)
{
  __int64 v8; // r12@1
  signed __int64 v15; // rax@4
  signed __int64 v16; // rbx@6
  signed __int64 v17; // rcx@7

  v8 = a2;
  if ( *(_DWORD *)(a3 + 92) == 2 )
  {
    if ( *(_DWORD *)(a3 + 96) )
    {
      *(_QWORD *)(a3 + 112) += 8LL * *(_DWORD *)(a3 + 96);
      ccmode_gcm_mult_h(a3, (__m128i *)(a3 + 16), a5, a6, a7, a8);
    }
    _RAX = *(_QWORD *)(a3 + 104);
    __asm { bswap   rax }
    *(_QWORD *)(a3 + 64) = _RAX;
    __asm { bswap   rax }
    _RAX = *(_QWORD *)(a3 + 112);
    __asm { bswap   rax }
    *(_QWORD *)(a3 + 72) = _RAX;
    __asm { bswap   rax }
    v15 = 64LL;
    do
    {
      *(_BYTE *)(a3 + v15 - 48) ^= *(_BYTE *)(a3 + v15);
      ++v15;
    }
    while ( v15 != 80 );
    v16 = a3 + 16;
    ccmode_gcm_mult_h(a3, (__m128i *)(a3 + 16), a5, a6, a7, a8);
    LODWORD(a1) = (*(int (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(*(_QWORD *)(a3 + 80) + 24LL))(
                    a3 + 384,
                    1LL,
                    a3 + 48,
                    a3 + 64);
    if ( a4 )
    {
      v17 = 15LL;
      if ( (unsigned __int64)(a4 - 1) > 0xF )
        v17 = a4 - 1;
      a1 = a4 - v17 + 15;
      do
      {
        *(_BYTE *)v8++ = *(_BYTE *)(v16 + 48) ^ *(_BYTE *)v16;
        ++v16;
        --a1;
      }
      while ( a1 );
    }
  }
  return a1;
}

//----- (0000000000035129) ----------------------------------------------------
__int64 __fastcall ccrsa_sign_oaep(void *a1, __int64 a2, __int64 a3, size_t a4, __int64 a5, __int64 a6, void *a7)
{
  __int64 v7; // r14@1
  __int64 v8; // rbx@1
  __int64 v9; // rax@1
  signed int v10; // er13@1
  __int64 v11; // r14@2
  unsigned __int64 v12; // rax@3
  size_t v13; // r12@3
  __int64 result; // rax@6
  __int64 v15; // [sp+10h] [bp-60h]@2
  __int64 v16; // [sp+18h] [bp-58h]@3
  __int64 v17; // [sp+20h] [bp-50h]@1
  __int64 v18; // [sp+28h] [bp-48h]@1
  size_t v19; // [sp+30h] [bp-40h]@1
  __int64 v20; // [sp+38h] [bp-38h]@1
  __int64 v21; // [sp+40h] [bp-30h]@1

  v7 = a6;
  v20 = a5;
  v19 = a4;
  v18 = a3;
  v17 = a2;
  v21 = *(_QWORD *)off_69010[0];
  v8 = *(_QWORD *)a1;
  v9 = ccn_write_uint_size(*(_QWORD *)a1, (__int64)((char *)a1 + 16));
  v10 = -2;
  if ( *(_QWORD *)v7 >= (unsigned __int64)v9 )
  {
    *(_QWORD *)v7 = v9;
    v11 = v9;
    v10 = ccrsa_oaep_encode_parameter(
            v17,
            v18,
            v9,
            (char *)&v15 - ((8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL),
            v19,
            0LL,
            0LL,
            0LL);
    if ( !v10 )
    {
      ccrsa_priv_crypt(
        (__int64 *)a1 + 4 * *(_QWORD *)a1 + 3,
        (__int64)((char *)&v15 - ((8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL)),
        (char *)&v15 - ((8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL));
      v16 = v8;
      v12 = ccn_write_uint_size(v8, (__int64)((char *)&v15 - ((8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL)));
      v10 = 0;
      v13 = v11 - v12;
      if ( v11 <= v12 )
        v13 = 0LL;
      bzero(a7, v13);
      ccn_write_uint(
        v16,
        (__int64)((char *)&v15 - ((8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL)),
        v11 - v13,
        (__int64)((char *)a7 + v13));
    }
  }
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v21 )
    result = (unsigned int)v10;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000003525D) ----------------------------------------------------
__int64 __fastcall ccn_shift_left(__int64 a1, __int64 a2, __int64 a3, char a4)
{
  __int64 result; // rax@1
  __int64 v5; // r10@2
  __int64 v6; // r11@3

  result = 0LL;
  if ( a1 )
  {
    v5 = *(_QWORD *)(a3 + 8 * a1 - 8);
    result = *(_QWORD *)(a3 + 8 * a1 - 8) >> (64 - a4);
    if ( a1 == 1 )
    {
      v6 = *(_QWORD *)(a3 + 8 * a1 - 8);
    }
    else
    {
      do
      {
        v6 = *(_QWORD *)(a3 + 8 * a1 - 16);
        *(_QWORD *)(a2 + 8 * a1 - 8) = (v5 << a4) | (*(_QWORD *)(a3 + 8 * a1 - 16) >> (64 - a4));
        --a1;
        v5 = v6;
      }
      while ( a1 != 1 );
    }
    *(_QWORD *)a2 = v6 << a4;
  }
  return result;
}

//----- (00000000000352C5) ----------------------------------------------------
void __fastcall ccn_shift_left_multi(unsigned __int64 a1, void *a2, __int64 a3, unsigned __int64 a4)
{
  unsigned __int64 v4; // rbx@1
  unsigned __int64 v5; // rcx@2

  v4 = a4 >> 6;
  if ( a4 >> 6 >= a1 )
  {
    v4 = a1;
    goto LABEL_5;
  }
  v5 = a4 & 0x3F;
  if ( v5 )
  {
    ccn_shift_left(a1 - v4, (__int64)((char *)a2 + 8 * v4), a3, v5);
    goto LABEL_5;
  }
  if ( v4 || a2 != (void *)a3 )
  {
    memmove((char *)a2 + 8 * v4, (const void *)a3, 8 * (a1 - v4));
LABEL_5:
    if ( v4 )
      bzero(a2, 8 * v4);
  }
}

//----- (0000000000035337) ----------------------------------------------------
__int64 __fastcall ccrsa_decrypt_eme_pkcs1v15(__int64 *a1, unsigned __int64 *a2, __int64 a3, unsigned __int64 a4, unsigned __int64 a5)
{
  unsigned __int64 v5; // r14@1
  unsigned __int64 v6; // r12@1
  __int64 result; // rax@1
  int v8; // ecx@2
  __int64 v9; // rcx@5
  __int64 v10; // [sp+0h] [bp-50h]@2
  __int64 v11; // [sp+8h] [bp-48h]@1
  unsigned __int64 v12; // [sp+10h] [bp-40h]@1
  unsigned __int64 v13; // [sp+18h] [bp-38h]@1
  __int64 v14; // [sp+20h] [bp-30h]@1

  v13 = a5;
  v12 = a4;
  v11 = a3;
  v14 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v6 = ccn_write_uint_size(*a1, (__int64)(a1 + 2));
  result = 4294967294LL;
  if ( *a2 >= v6 )
  {
    *a2 = v6;
    v8 = ccn_read_uint(v5, (__int64)((char *)&v10 - ((8 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL)), v12, v13);
    result = 0xFFFFFFFFLL;
    if ( !v8 )
    {
      result = ccrsa_priv_crypt(
                 &a1[4 * *a1 + 3],
                 (__int64)((char *)&v10 - ((8 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL)),
                 (char *)&v10 - ((8 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL));
      if ( !(_DWORD)result )
        result = ccrsa_eme_pkcs1v15_decode(
                   a2,
                   v11,
                   v6,
                   (unsigned __int64)((char *)&v10 - ((8 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL)));
    }
  }
  v9 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000003540D) ----------------------------------------------------
__int64 __fastcall ccn_shift_right(__int64 a1, void *a2, const void *a3, __int64 a4)
{
  __int64 result; // rax@1
  unsigned __int64 v5; // r10@3
  signed __int64 v6; // rbx@3
  signed __int64 v7; // rdx@4
  signed __int64 v8; // r11@4
  void *v9; // r15@4
  unsigned __int64 v10; // r14@5

  result = 0LL;
  if ( a1 )
  {
    if ( a4 )
    {
      v5 = *(_QWORD *)a3;
      result = *(_QWORD *)a3 << (64 - (unsigned __int8)a4);
      v6 = 0LL;
      if ( (unsigned __int64)a1 >= 2 )
      {
        v7 = (signed __int64)((char *)a3 + 8);
        v8 = a1 - 1;
        v9 = a2;
        do
        {
          v10 = v5;
          v5 = *(_QWORD *)v7;
          *(_QWORD *)v9 = (v10 >> a4) | (*(_QWORD *)v7 << (64 - (unsigned __int8)a4));
          v7 += 8LL;
          v9 = (char *)v9 + 8;
          --v8;
        }
        while ( v8 );
        v6 = a1 - 1;
      }
      *((_QWORD *)a2 + v6) = v5 >> a4;
    }
    else
    {
      ccn_set(a1, a2, a3);
      result = 0LL;
    }
  }
  return result;
}

//----- (000000000003549A) ----------------------------------------------------
void __fastcall ccn_shift_right_multi(unsigned __int64 a1, void *a2, void *a3, unsigned __int64 a4)
{
  void *v4; // r14@1
  unsigned __int64 v5; // rbx@1
  unsigned __int64 v6; // rsi@1
  __int64 v7; // rcx@2

  v4 = a2;
  v5 = a4 >> 6;
  v6 = a1;
  if ( a4 >> 6 >= a1 )
    goto LABEL_8;
  v7 = a4 & 0x3F;
  if ( v7 )
  {
    ccn_shift_right(a1 - v5, v4, (char *)a3 + 8 * v5, v7);
LABEL_7:
    v6 = v5;
LABEL_8:
    if ( v6 )
      bzero((char *)v4 + 8 * (a1 - v6), 8 * v6);
    return;
  }
  if ( v5 || v4 != a3 )
  {
    ccn_set(a1 - v5, v4, (char *)a3 + 8 * v5);
    goto LABEL_7;
  }
}

//----- (0000000000035520) ----------------------------------------------------
__int64 __fastcall ccn_sub(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v5; // r8@1
  unsigned __int8 v6; // of@1
  signed __int64 i; // rdi@1
  bool v8; // cf@2
  unsigned __int64 v9; // rax@2
  unsigned __int64 v10; // rtt@2
  unsigned __int64 v11; // rax@2
  unsigned __int64 v12; // rtt@2
  unsigned __int64 v13; // rax@2
  unsigned __int64 v14; // rtt@2
  unsigned __int64 v15; // rax@4
  unsigned __int64 v16; // rtt@4

  _AH = 0;
  v5 = 0LL;
  v6 = __OFSUB__(a1, 4LL);
  for ( i = a1 - 4; !((i < 0) ^ v6); i -= 4LL )
  {
    v8 = (_AH & 1) != 0;
    v9 = *(_QWORD *)(a3 + 8 * v5);
    v10 = v8 + *(_QWORD *)(a4 + 8 * v5);
    v8 = v9 < v10;
    *(_QWORD *)(a2 + 8 * v5) = v9 - v10;
    v11 = *(_QWORD *)(a3 + 8 * v5 + 8);
    v12 = v8 + *(_QWORD *)(a4 + 8 * v5 + 8);
    v8 = v11 < v12;
    *(_QWORD *)(a2 + 8 * v5 + 8) = v11 - v12;
    v13 = *(_QWORD *)(a3 + 8 * v5 + 16);
    v14 = v8 + *(_QWORD *)(a4 + 8 * v5 + 16);
    v8 = v13 < v14;
    *(_QWORD *)(a2 + 8 * v5 + 16) = v13 - v14;
    *(_QWORD *)(a2 + 8 * v5 + 24) = *(_QWORD *)(a3 + 8 * v5 + 24) - (v8 + *(_QWORD *)(a4 + 8 * v5 + 24));
    __asm { lahf }
    v5 += 4LL;
    v6 = __OFSUB__(i, 4LL);
  }
  if ( (unsigned __int64)(i & 2) )
  {
    v8 = (_AH & 1) != 0;
    v15 = *(_QWORD *)(a3 + 8 * v5);
    v16 = v8 + *(_QWORD *)(a4 + 8 * v5);
    v8 = v15 < v16;
    *(_QWORD *)(a2 + 8 * v5) = v15 - v16;
    *(_QWORD *)(a2 + 8 * v5 + 8) = *(_QWORD *)(a3 + 8 * v5 + 8) - (v8 + *(_QWORD *)(a4 + 8 * v5 + 8));
    __asm { lahf }
    v5 += 2LL;
  }
  if ( (unsigned __int64)(i & 1) )
  {
    *(_QWORD *)(a2 + 8 * v5) = *(_QWORD *)(a3 + 8 * v5) - (((_AH & 1) != 0) + *(_QWORD *)(a4 + 8 * v5));
    __asm { lahf }
  }
  return (_AH & 1) != 0;
}

//----- (00000000000355C3) ----------------------------------------------------
signed __int64 __fastcall ccn_sub1(__int64 a1, __int64 a2, __int64 a3, signed __int64 a4)
{
  __int64 v4; // r8@1
  unsigned __int64 v5; // rax@3
  bool v6; // cf@3

  v4 = 0LL;
  while ( a1 )
  {
    --a1;
    v5 = *(_QWORD *)(a3 + v4);
    v6 = v5 < a4;
    *(_QWORD *)(a2 + v4) = v5 - a4;
    v4 += 8LL;
    a4 = 1LL;
    if ( !v6 )
    {
      a4 = 0LL;
      if ( a3 != a2 )
      {
        ccn_set(a1, (void *)(v4 + a2), (const void *)(v4 + a3));
        a4 = 0LL;
      }
      return a4;
    }
  }
  return a4;
}

//----- (0000000000035601) ----------------------------------------------------
signed __int64 __fastcall ccrsa_emsa_pkcs1v15_encode(unsigned __int64 a1, __int64 a2, size_t a3, const void *a4, __int64 a5)
{
  __int64 v5; // r13@1
  int v6; // er14@1
  int v7; // esi@1
  signed __int64 result; // rax@1
  size_t v9; // rbx@2
  const void *v10; // ST10_8@2
  size_t v11; // r15@2
  const void *v12; // ST00_8@2
  size_t v13; // r14@2
  signed __int64 v14; // rax@2

  v5 = a2;
  v6 = *(_BYTE *)(a5 + 1);
  v7 = a3 + v6 + 10;
  result = 0xFFFFFFFFLL;
  if ( (unsigned int)(unsigned __int8)(a3 + v6 + 10) + 11 <= a1 )
  {
    v9 = a1 - 3 - (unsigned __int8)v7;
    *(_WORD *)v5 = 256;
    v10 = a4;
    v11 = a3;
    v12 = (const void *)a5;
    memset((void *)(v5 + 2), 255, v9);
    *(_BYTE *)(v9 + v5 + 2) = 0;
    *(_BYTE *)(v9 + v5 + 3) = 48;
    *(_BYTE *)(v9 + v5 + 4) = v11 + v6 + 8;
    *(_BYTE *)(v9 + v5 + 5) = 48;
    *(_BYTE *)(v9 + v5 + 6) = v6 + 4;
    v13 = (unsigned int)(v6 + 2);
    memcpy((void *)(v5 + v9 + 7), v12, v13);
    v14 = v9 + v13 + 7;
    *(_BYTE *)(v5 + v14) = 5;
    *(_BYTE *)(v5 + v14 + 1) = 0;
    *(_BYTE *)(v5 + v14 + 2) = 4;
    *(_BYTE *)(v5 + v14 + 3) = v11;
    memcpy((void *)(v5 + v14 + 4), v10, v11);
    result = 0LL;
  }
  return result;
}

//----- (00000000000356E9) ----------------------------------------------------
unsigned __int64 __fastcall ccn_write_int_size(__int64 a1, __int64 a2)
{
  signed __int64 v2; // rax@1

  v2 = ccn_bitlen(a1, a2);
  return (((unsigned __int8)v2 & 7u) < 1uLL) + ((unsigned __int64)(v2 + 7) >> 3);
}

//----- (000000000003570B) ----------------------------------------------------
unsigned __int64 __fastcall ccn_write_int(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  signed __int64 v5; // r14@1

  v4 = a4;
  v5 = a3;
  if ( !(ccn_bitlen(a1, a2) & 7) )
  {
    *(_BYTE *)v4 = 0;
    --v5;
    ++v4;
  }
  return ccn_write_uint(a1, a2, v5, v4);
}

//----- (000000000003574E) ----------------------------------------------------
__int64 __fastcall ccrsa_generate_fips186_key(unsigned __int64 a1, __int64 a2, unsigned __int64 a3, unsigned __int64 a4, __int64 a5, __int64 a6)
{
  unsigned __int64 v6; // r13@1
  unsigned __int64 v7; // r15@1
  unsigned __int64 v8; // rax@1
  __int64 v9; // r12@1
  char *v10; // r14@1
  int v11; // eax@1
  signed int v12; // ecx@1
  bool v13; // zf@1
  __int64 v14; // rax@1
  __int64 v15; // rcx@2
  __int64 v16; // r8@2
  __int64 v17; // r13@2
  __int64 v18; // r15@2
  signed int v19; // eax@2
  __int64 v20; // r8@2
  __int64 *v21; // r12@2
  const void *v22; // rcx@3
  __int64 v23; // r15@3
  bool v24; // al@4
  size_t v25; // rbx@6
  signed int v26; // er15@6
  __int64 result; // rax@7
  __int64 v28; // [sp+10h] [bp-60h]@1
  __int64 v29; // [sp+18h] [bp-58h]@1
  __int64 v30; // [sp+20h] [bp-50h]@1
  unsigned __int64 v31; // [sp+28h] [bp-48h]@1
  unsigned __int64 v32; // [sp+30h] [bp-40h]@1
  __int64 v33; // [sp+38h] [bp-38h]@1
  __int64 v34; // [sp+40h] [bp-30h]@1

  v29 = a6;
  v30 = a5;
  v31 = a3;
  v33 = a2;
  v32 = a1;
  v34 = *(_QWORD *)off_69010[0];
  v6 = (a1 + 63) >> 6;
  v7 = (((a1 >> 1) + 63) >> 6) + 1;
  *(_QWORD *)a2 = v6;
  v8 = (16 * v7 + 39) & 0xFFFFFFFFFFFFFFF0LL;
  v9 = (__int64)((char *)&v28 - v8);
  v10 = (char *)&v28 - v8;
  v11 = ccn_read_uint(v6, 16 * v6 + a2 + 24, a3, a4);
  v12 = -1;
  v13 = v11 == 0;
  v14 = off_69010[0];
  if ( v13 )
  {
    v15 = (v31 + 7) >> 3;
    v16 = 16LL * *(_QWORD *)v33 + v33 + 24;
    *(_QWORD *)v33 = v6;
    v17 = v15;
    *(_QWORD *)v10 = v7;
    *(_QWORD *)v9 = v7;
    v18 = v16;
    v19 = ccrsa_generate_fips186_prime_factors(v32, v9, (__int64)v10, v15, v16, v30, v29);
    v20 = v9;
    v21 = (__int64 *)v33;
    if ( !v19 )
    {
      v22 = (const void *)v18;
      v23 = v20;
      v19 = ccrsa_crt_make_fips186_key(v32, v33, v17, v22, v20, (__int64)v10);
      v20 = v23;
      if ( !v19 )
      {
        v24 = ccrsa_pairwise_consistency_check(v21);
        v20 = v23;
        v13 = v24 == 0;
        v19 = -12;
        if ( !v13 )
          v19 = 0;
      }
    }
    v25 = 16 * ((16 * ((((a1 >> 1) + 63) >> 6) + 1) + 39) >> 4);
    v26 = v19;
    cc_clear(v25, (void *)v20);
    cc_clear(v25, v10);
    v12 = v26;
    v14 = off_69010[0];
  }
  result = *(_QWORD *)v14;
  if ( result == v34 )
    result = (unsigned int)v12;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000358C9) ----------------------------------------------------
__int64 __fastcall ccrsa_generate_fips186_prime_factors(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7)
{
  __int64 v7; // rcx@1
  signed int v8; // ebx@1
  __int64 v9; // r14@2
  __int64 v10; // r12@5
  unsigned __int64 v11; // rax@5
  char *v12; // r13@5
  unsigned __int64 v13; // r15@6
  size_t v14; // r12@11
  __int64 result; // rax@12
  unsigned __int64 v16; // [sp+10h] [bp-70h]@5
  __int64 *v17; // [sp+18h] [bp-68h]@6
  __int64 *v18; // [sp+20h] [bp-60h]@6
  __int64 v19; // [sp+28h] [bp-58h]@6
  void *v20; // [sp+30h] [bp-50h]@6
  unsigned __int64 v21; // [sp+38h] [bp-48h]@6
  __int64 v22; // [sp+40h] [bp-40h]@1
  __int64 v23; // [sp+48h] [bp-38h]@1
  __int64 v24; // [sp+50h] [bp-30h]@1

  v22 = a4;
  v23 = a3;
  v7 = off_69010[0];
  v24 = *(_QWORD *)off_69010[0];
  v8 = -7;
  if ( a1 >= 0x400 )
  {
    v9 = 101LL;
    if ( a1 + 1 >= 0x402 )
    {
      v9 = 171LL;
      if ( a1 + 1 < 0x802 )
        v9 = 141LL;
    }
    v10 = (((a1 + 1) >> 1) + 63) >> 6;
    *(_QWORD *)v23 = v10;
    *(_QWORD *)a2 = v10;
    v11 = (16 * v10 + 39) & 0xFFFFFFFFFFFFFFF0LL;
    v12 = (char *)&v16 - v11;
    *(unsigned __int64 *)((char *)&v16 - v11) = v10;
    *(unsigned __int64 *)((char *)&v16 - v11) = v10;
    v8 = -8;
    if ( *(_BYTE *)a5 & 1 )
    {
      v17 = (__int64 *)&v16;
      v21 = a1;
      v19 = a5;
      v18 = (__int64 *)a2;
      v20 = (char *)&v16 - v11;
      v16 = (a1 + 1) >> 1;
      v8 = ccrsa_generate_probable_prime(v16, a2, (__int64)((char *)&v16 - v11), v9, v9, v22, a5, a6);
      v13 = 0LL;
      if ( !v8 )
      {
        v21 -= v16;
        do
        {
          v8 = ccrsa_generate_probable_prime(v21, v23, (__int64)v12, v9, v9, v22, v19, a7);
          if ( v8 )
            break;
          v8 = 0;
          if ( cczp_check_delta_100bits(v10, v18, v23, (__int64)v20, (__int64)v12) )
            break;
          ++v13;
          v8 = -14;
        }
        while ( v13 <= 0x63 );
      }
      v14 = 2 * v10 + 24;
      cc_clear(v14, v20);
      cc_clear(v14, v12);
      v7 = off_69010[0];
    }
  }
  result = *(_QWORD *)v7;
  if ( *(_QWORD *)v7 == v24 )
    result = (unsigned int)v8;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000035A76) ----------------------------------------------------
__int64 __fastcall ccrsa_crt_make_fips186_key(__int64 a1, __int64 a2, __int64 a3, const void *a4, __int64 a5, __int64 a6)
{
  __int64 v6; // rbx@1
  __int64 v7; // r13@1
  __int64 v8; // r12@1
  __int64 v9; // r15@1
  signed __int64 v10; // r14@1
  signed int v11; // esi@1
  __int64 v12; // r15@2
  __int64 v13; // r14@2
  signed __int64 v14; // rcx@2
  unsigned int v15; // eax@3
  __int64 v16; // rbx@6
  __int64 v17; // r14@6
  __int64 v18; // r13@10
  __int64 v19; // rbx@10
  __int64 v20; // r15@10
  __int64 v21; // r14@10
  __int64 v22; // rbx@10
  __int64 v23; // rax@10
  __int64 v24; // r13@10
  __int64 v25; // r14@10
  __int64 v26; // r12@11
  __int64 v27; // r15@11
  __int64 v28; // rbx@11
  signed __int64 v29; // rax@15
  __int64 v30; // rdx@16
  __int64 v31; // rcx@16
  unsigned __int64 v32; // rbx@18
  __int64 v33; // r14@18
  __int64 v34; // r12@18
  unsigned __int64 v35; // rbx@18
  __int64 v36; // r12@18
  unsigned __int64 v37; // rdi@18
  __int64 v38; // r14@18
  __int64 v39; // r15@18
  __int64 v40; // r12@18
  __int64 v41; // rcx@18
  const void *v42; // rbx@18
  int v43; // eax@20
  __int64 result; // rax@22
  __int64 v45; // [sp+0h] [bp-90h]@2
  const void *v46; // [sp+8h] [bp-88h]@1
  __int64 v47; // [sp+10h] [bp-80h]@1
  __int64 v48; // [sp+18h] [bp-78h]@1
  __int64 v49; // [sp+20h] [bp-70h]@1
  __int64 v50; // [sp+28h] [bp-68h]@1
  __int64 v51; // [sp+30h] [bp-60h]@2
  __int64 v52; // [sp+38h] [bp-58h]@1
  __int64 v53; // [sp+40h] [bp-50h]@1
  __int64 v54; // [sp+48h] [bp-48h]@2
  __int64 v55; // [sp+50h] [bp-40h]@1
  __int64 v56; // [sp+58h] [bp-38h]@1
  __int64 v57; // [sp+60h] [bp-30h]@1

  v6 = a6;
  v50 = a6;
  v55 = a5;
  v46 = a4;
  v7 = a3;
  v8 = a2;
  v53 = a1;
  v9 = off_69010[0];
  v57 = *(_QWORD *)off_69010[0];
  v56 = *(_QWORD *)a2;
  v49 = *(_QWORD *)a5;
  v52 = a5 + 16;
  v10 = ccn_bitlen(v49, a5 + 16);
  v48 = *(_QWORD *)v6;
  v47 = v6 + 16;
  v11 = -5;
  if ( v10 + ccn_bitlen(v48, v6 + 16) <= (unsigned __int64)(v56 << 6) )
  {
    v54 = v8 + 32 * v56 + 24;
    v45 = 32 * v56 | 0x18;
    v51 = v8 + 16;
    v12 = *(_QWORD *)v8;
    ccn_set(v7, (void *)(v8 + 16LL * *(_QWORD *)v8 + 24), v46);
    bzero((void *)(v8 + 8 * (v7 + 2 * v12 + 3)), 8 * (v56 - v7));
    v13 = ccn_n(v49, v52);
    v14 = ccn_n(v48, v47);
    if ( v13 <= (unsigned __int64)v14 )
    {
      LOBYTE(v15) = 1;
      if ( v13 >= (unsigned __int64)v14 )
        v15 = (unsigned int)ccn_cmp(v13, v52, v47) >> 31;
    }
    else
    {
      LOBYTE(v15) = 0;
    }
    v16 = v50;
    v17 = v55;
    if ( (_BYTE)v15 )
      v16 = v55;
    if ( (_BYTE)v15 )
      v17 = v50;
    v50 = *(_QWORD *)v17;
    v18 = v45;
    *(_QWORD *)(v8 + v45) = v50;
    v52 = *(_QWORD *)v16;
    v19 = v16 + 16;
    v55 = v8 + v18 + 16;
    v20 = v54;
    *(_QWORD *)(16LL * *(_QWORD *)(v8 + v18) + v54 + 24) = (unsigned __int64)(ccn_bitlen(v52, v19) + 63) >> 6;
    ccn_set(v50, (void *)(v8 + v18 + 16), (const void *)(v17 + 16));
    ccn_set(v52, (void *)(16LL * *(_QWORD *)(v8 + v18) + v20 + 40), (const void *)v19);
    v49 = *(_QWORD *)(v8 + v18);
    v48 = 16 * v49;
    v21 = *(_QWORD *)(16 * v49 + v20 + 24);
    v22 = 16 * v49 + v20 + 40;
    ccn_mul(*(_QWORD *)(16 * v49 + v20 + 24), v51, v55, (unsigned __int64 *)v22);
    v23 = *(_QWORD *)(v8 + v18);
    v24 = v21;
    v52 = v23;
    v25 = v8;
    if ( v23 > (unsigned __int64)v24 )
    {
      v50 = v8;
      v26 = 16 * v49 + v20 + 40;
      v27 = *(_QWORD *)(v55 + 8 * v52 - 8);
      v28 = 0LL;
      if ( v27 )
      {
        v28 = 0LL;
        do
        {
          v28 += ccn_add(v24, v50 + 8 * v24 + 16, v50 + 8 * v24 + 16, v26);
          --v27;
        }
        while ( v27 );
      }
      v25 = v50;
      *(_QWORD *)(16 * v24 + v50 + 16) = v28;
      v22 = v26;
    }
    cczp_init(v25);
    v29 = ccn_bitlen(*(_QWORD *)v25, v51);
    v9 = off_69010[0];
    v11 = -5;
    if ( v29 + 1 >= (unsigned __int64)v53 )
    {
      v50 = v25;
      v30 = 2 * v49;
      *(_BYTE *)v55 &= 0xFEu;
      v31 = v54;
      v53 = v48 + v54 + 24;
      v47 = v22;
      *(_BYTE *)v22 &= 0xFEu;
      if ( v52 > (unsigned __int64)v24 )
        *(_QWORD *)(v31 + 8 * (v30 + v52) + 32) = 0LL;
      v49 = v30;
      v51 = (__int64)&v45;
      v48 = (__int64)&v45;
      v32 = (16 * (v56 + 2) + 39) & 0xFFFFFFFFFFFFFFF0LL;
      v33 = (__int64)((char *)&v45 - v32);
      v34 = v56;
      bzero((char *)&v45 + -v32 + 16, 8 * v56 + 16);
      ccn_lcm(v52, (char *)&v45 + -v32 + 16, v55, v47);
      *(_QWORD *)(v48 - v32) = v34;
      v35 = v34;
      cczp_init(v33);
      v36 = v50;
      v37 = v33;
      v38 = v54;
      cczp_mod_inv_slow(v37, v50 + 24LL * *(_QWORD *)v50 + 24, (const void *)(16LL * *(_QWORD *)v50 + v50 + 24));
      v39 = (__int64)((char *)&v45 - ((8 * v35 + 31) & 0xFFFFFFFFFFFFFFF0LL));
      ccn_set(
        v35,
        (char *)&v45 - ((8 * v35 + 31) & 0xFFFFFFFFFFFFFFF0LL),
        (const void *)(v36 + 24LL * *(_QWORD *)v36 + 24));
      bzero((void *)(v39 + 8 * v35), 0x10uLL);
      cczp_init(v38);
      cczp_modn(v38, (void *)(32LL * *(_QWORD *)v38 + v38 + 48), v35, v39);
      v40 = v53;
      cczp_init(v53);
      cczp_modn(v40, (void *)(v38 + 40LL * *(_QWORD *)v38 + 48), v35, v39);
      v41 = v52;
      *(_BYTE *)v55 |= 1u;
      v42 = (const void *)v47;
      *(_BYTE *)v42 |= 1u;
      if ( v41 > (unsigned __int64)v24 )
        *(_QWORD *)(v38 + 8 * (v41 + v49) + 32) = 0LL;
      cczp_init(v38);
      v43 = cczp_mod_inv(v38, (void *)(v38 + 48LL * *(_QWORD *)v38 + 48), v42);
      v11 = -6;
      v9 = off_69010[0];
      if ( !v43 )
      {
        cczp_init(v40);
        v11 = 0;
      }
    }
  }
  result = *(_QWORD *)v9;
  if ( *(_QWORD *)v9 == v57 )
    result = (unsigned int)v11;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000035ED9) ----------------------------------------------------
__int64 __fastcall ccrsa_make_fips186_key(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8, __int64 a9, __int64 a10, __int64 a11, __int64 a12, __int64 a13, __int64 a14, __int64 a15, __int64 a16, __int64 a17, void *a18, __int64 a19, void *a20, __int64 a21, void *a22, __int64 a23, void *a24)
{
  __int64 v24; // r14@1
  unsigned __int64 v25; // rax@1
  char *v26; // rbx@1
  char *v27; // rsi@1
  __int64 v28; // r15@1
  __int64 v29; // r12@1
  signed int v30; // er13@1
  __int64 v31; // r13@2
  __int64 v32; // rax@2
  __int64 v33; // r12@2
  __int64 v34; // rbx@2
  __int64 v35; // r13@3
  __int64 v36; // rax@3
  __int64 v37; // rbx@5
  __int64 v38; // rdi@7
  __int64 v39; // r15@8
  void *v40; // rcx@10
  __int64 v41; // rdi@10
  __int64 v42; // rdi@14
  __int64 v43; // rdi@17
  size_t v44; // rbx@18
  __int64 result; // rax@19
  unsigned __int64 v46; // [sp+10h] [bp-110h]@1
  __int64 v47; // [sp+18h] [bp-108h]@1
  __int64 v48; // [sp+20h] [bp-100h]@1
  __int64 v49; // [sp+28h] [bp-F8h]@1
  __int64 *v50; // [sp+30h] [bp-F0h]@1
  unsigned __int64 v51; // [sp+38h] [bp-E8h]@1
  void *v52; // [sp+40h] [bp-E0h]@1
  __int64 v53; // [sp+48h] [bp-D8h]@1
  __int64 v54; // [sp+50h] [bp-D0h]@1
  void *v55; // [sp+58h] [bp-C8h]@1
  char v56; // [sp+60h] [bp-C0h]@1
  char v57; // [sp+A8h] [bp-78h]@1
  __int64 v58; // [sp+F0h] [bp-30h]@1

  v52 = (void *)a6;
  v49 = a4;
  v53 = a3;
  v54 = a2;
  v47 = a1;
  v58 = *(_QWORD *)off_69010[0];
  v51 = a1 >> 1;
  v24 = (((a1 >> 1) + 63) >> 6) + 1;
  v46 = (a1 + 63) >> 6;
  v48 = 16 * v24 + 39;
  v25 = v48 & 0xFFFFFFFFFFFFFFF0LL;
  v26 = (char *)&v46 - (v48 & 0xFFFFFFFFFFFFFFF0LL);
  v27 = (char *)&v46 - (v48 & 0xFFFFFFFFFFFFFFF0LL);
  v55 = (char *)&v46 - (v48 & 0xFFFFFFFFFFFFFFF0LL);
  *(_QWORD *)a16 = v46;
  *(_QWORD *)v27 = v24;
  *(_QWORD *)v26 = v24;
  v28 = (__int64)((char *)&v46 - v25);
  v50 = (__int64 *)((char *)&v46 - v25);
  *(unsigned __int64 *)((char *)&v46 - v25) = v24;
  *(unsigned __int64 *)((char *)&v46 - v25) = v24;
  v29 = a5;
  ccrng_rsafips_test_init((__int64)&v57, a4, a5, a6, a7, a8, a9);
  ccrng_rsafips_test_init((__int64)&v56, a10, a11, a12, a13, a14, a15);
  v30 = -8;
  if ( *(_BYTE *)v53 & 1 )
  {
    v31 = ccn_bitlen(v49, v29);
    v32 = ccn_bitlen((__int64)v52, a7);
    v33 = v53;
    v52 = v26;
    v30 = ccrsa_generate_probable_prime(v51, (__int64)v26, v28, v31, v32, v54, v53, (__int64)&v57);
    v34 = (__int64)v50;
    if ( !v30 )
    {
      v35 = ccn_bitlen(a10, a11);
      v36 = ccn_bitlen(a12, a13);
      v30 = ccrsa_generate_probable_prime(v51, (__int64)v55, v34, v35, v36, v54, v33, (__int64)&v56);
      if ( !v30 )
      {
        v30 = -14;
        if ( cczp_check_delta_100bits(v24, (__int64 *)v52, (__int64)v55, v28, v34) )
        {
          v37 = (__int64)v52;
          if ( a17 && a18 )
          {
            v38 = *(_QWORD *)v52;
            *(_QWORD *)a17 = *(_QWORD *)v52;
            ccn_set(v38, a18, (const void *)(v37 + 16));
          }
          v39 = v54;
          if ( a19 && a20 )
          {
            v40 = v55;
            v41 = *(_QWORD *)v55;
            *(_QWORD *)a19 = *(_QWORD *)v55;
            ccn_set(v41, a20, (char *)v40 + 16);
          }
          v30 = ccrsa_crt_make_fips186_key(v47, a16, v39, (const void *)v33, v37, (__int64)v55);
          if ( !v30 )
          {
            if ( a21 && a22 )
            {
              v42 = *(_QWORD *)a16;
              *(_QWORD *)a21 = *(_QWORD *)a16;
              ccn_set(v42, a22, (const void *)(a16 + 16));
            }
            v30 = 0;
            if ( a23 && a24 )
            {
              v43 = v46;
              *(_QWORD *)a23 = v46;
              ccn_set(v43, a24, (const void *)(a16 + 24LL * *(_QWORD *)a16 + 24));
            }
          }
        }
      }
    }
    v44 = 16 * ((unsigned __int64)v48 >> 4);
    cc_clear(16 * ((unsigned __int64)v48 >> 4), v52);
    cc_clear(v44, v55);
  }
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v58 )
    result = (unsigned int)v30;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000036262) ----------------------------------------------------
__int64 __fastcall ccrsa_generate_probable_prime(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7, __int64 a8)
{
  __int64 v8; // r15@1
  unsigned __int64 v9; // r15@3
  unsigned __int64 v10; // r12@3
  unsigned __int64 v11; // rax@3
  char *v12; // rbx@3
  char *v13; // r14@3
  void *v14; // rdi@3
  __int64 v15; // rbx@3
  int v16; // eax@3
  signed int v17; // er14@3
  __int64 *v18; // r13@4
  __int64 *v19; // rbx@4
  __int64 v20; // r15@5
  int v21; // eax@5
  size_t v22; // r12@9
  __int64 result; // rax@9
  __int64 v24; // [sp+10h] [bp-80h]@3
  unsigned __int64 v25; // [sp+18h] [bp-78h]@1
  __int64 v26; // [sp+20h] [bp-70h]@1
  __int64 v27; // [sp+28h] [bp-68h]@1
  __int64 v28; // [sp+30h] [bp-60h]@1
  __int64 v29; // [sp+38h] [bp-58h]@3
  __int64 v30; // [sp+40h] [bp-50h]@1
  __int64 v31; // [sp+48h] [bp-48h]@1
  void *v32; // [sp+50h] [bp-40h]@3
  void *v33; // [sp+58h] [bp-38h]@3
  __int64 v34; // [sp+60h] [bp-30h]@1

  v28 = a6;
  v30 = a5;
  v31 = a4;
  v27 = a3;
  v26 = a2;
  v25 = a1;
  v34 = *(_QWORD *)off_69010[0];
  v8 = a5;
  if ( a4 > (unsigned __int64)a5 )
    v8 = a4;
  v9 = (unsigned __int64)(v8 + 63) >> 6;
  v10 = 16 * v9 + 39;
  v11 = v10 & 0xFFFFFFFFFFFFFFF0LL;
  v32 = (char *)&v24 - v11;
  v12 = (char *)&v24 - v11;
  v33 = (char *)&v24 - v11;
  *(__int64 *)((char *)&v24 - v11) = v9;
  *(__int64 *)((char *)&v24 - v11) = v9;
  v13 = (char *)&v24 + -v11 + 16;
  v9 *= 8LL;
  bzero(v13, v9);
  v14 = v12 + 16;
  v29 = (__int64)(v12 + 16);
  v15 = v31;
  bzero(v14, v9);
  v16 = ccn_random_bits(v15, (__int64)v13, a8);
  v17 = -11;
  if ( v16 )
  {
    v18 = (__int64 *)v33;
    v19 = (__int64 *)v32;
  }
  else
  {
    v20 = v15;
    v21 = ccn_random_bits(v30, v29, a8);
    v18 = (__int64 *)v33;
    v19 = (__int64 *)v32;
    if ( !v21 )
    {
      *((_QWORD *)v32 + ((unsigned __int64)(v20 - 1) >> 6) + 2) |= 1LL << ((unsigned __int8)v20 - 1);
      v18[((unsigned __int64)(v30 - 1) >> 6) + 2] |= 1LL << ((unsigned __int8)v30 - 1);
      v17 = cczp_find_next_prime(v19);
      if ( !v17 )
      {
        v17 = cczp_find_next_prime(v18);
        if ( !v17 )
          v17 = ccrsa_generate_probable_prime_from_auxilary_primes(
                  v25,
                  v26,
                  v27,
                  (__int64)v19,
                  (unsigned __int64)v18,
                  v28,
                  a7,
                  a8);
      }
    }
  }
  v22 = 16 * (v10 >> 4);
  cc_clear(v22, v19);
  cc_clear(v22, v18);
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v34 )
    result = (unsigned int)v17;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000036419) ----------------------------------------------------
bool __fastcall cczp_check_delta_100bits(__int64 a1, __int64 *a2, __int64 a3, __int64 a4, __int64 a5)
{
  unsigned __int64 v5; // rax@1
  char *v6; // r15@1
  char *v7; // rbx@1
  signed __int64 v8; // r12@1
  int v9; // eax@1
  int v10; // eax@1
  bool result; // al@1
  __int64 v12; // rcx@1
  __int64 v13; // [sp+0h] [bp-60h]@1
  __int64 v14; // [sp+8h] [bp-58h]@1
  size_t v15; // [sp+10h] [bp-50h]@1
  __int64 v16; // [sp+18h] [bp-48h]@1
  __int64 v17; // [sp+20h] [bp-40h]@1
  __int64 v18; // [sp+28h] [bp-38h]@1
  __int64 v19; // [sp+30h] [bp-30h]@1

  v16 = a5;
  v14 = a4;
  v13 = a3;
  v19 = *(_QWORD *)off_69010[0];
  v5 = (8 * a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v6 = (char *)&v13 - v5;
  v7 = (char *)&v13 - v5;
  v8 = ccn_bitlen(*a2, (__int64)(a2 + 2));
  v15 = 8 * a1;
  bzero(v7, 8 * a1);
  *(_QWORD *)&v7[8 * ((unsigned __int64)(v8 - 100) >> 6)] |= 1LL << ((unsigned __int8)v8 - 100);
  v17 = (__int64)(a2 + 2);
  v18 = v13 + 16;
  v9 = ccn_cmp(a1, (__int64)(a2 + 2), v13 + 16);
  ccn_sub(a1, (__int64)v6, *(&v17 + (((v9 + 1) >> 1) ^ 1LL)), *(&v17 + ((v9 + 1) >> 1)));
  LODWORD(v8) = ccn_cmp(a1, (__int64)v6, (__int64)v7);
  v17 = v14 + 16;
  v18 = v16 + 16;
  v10 = ccn_cmp(a1, v14 + 16, v16 + 16);
  ccn_sub(a1, (__int64)v6, *(&v17 + (((v10 + 1) >> 1) ^ 1LL)), *(&v17 + ((v10 + 1) >> 1)));
  LODWORD(v7) = ccn_cmp(a1, (__int64)v6, (__int64)v7);
  cc_clear(v15, v6);
  result = (_DWORD)v8 + (_DWORD)v7 == 2;
  v12 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000036588) ----------------------------------------------------
__int64 __fastcall ccrsa_generate_probable_prime_from_auxilary_primes(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5, __int64 a6, __int64 a7, __int64 a8)
{
  unsigned __int64 v8; // r15@1
  unsigned __int64 v9; // rcx@5
  __int64 v10; // r13@5
  unsigned __int64 v11; // rbx@5
  signed __int64 v12; // rax@5
  signed __int64 v13; // r14@5
  __int64 v14; // rdi@5
  signed __int64 v15; // rax@5
  bool v16; // cf@5
  bool v17; // zf@5
  __int64 v18; // rsi@5
  signed int v19; // er14@5
  __int64 v20; // rcx@7
  __int64 v21; // rdx@8
  __int64 v22; // r12@8
  __int64 v23; // r13@8
  __int64 v24; // rbx@8
  char *v25; // r14@10
  __int64 *v26; // rbx@10
  __int64 v27; // r12@10
  unsigned __int64 v28; // r14@11
  signed __int64 v29; // rax@13
  __int64 v30; // rbx@16
  __int64 v31; // rbx@19
  size_t v32; // rbx@27
  __int64 v34; // rbx@31
  __int64 v35; // [sp+0h] [bp-C0h]@5
  __int64 v36; // [sp+8h] [bp-B8h]@8
  size_t v37; // [sp+10h] [bp-B0h]@8
  __int64 v38; // [sp+18h] [bp-A8h]@10
  unsigned __int64 v39; // [sp+20h] [bp-A0h]@5
  unsigned __int64 v40; // [sp+28h] [bp-98h]@5
  __int64 *v41; // [sp+30h] [bp-90h]@5
  void *v42; // [sp+38h] [bp-88h]@5
  void *v43; // [sp+40h] [bp-80h]@5
  __int64 v44; // [sp+48h] [bp-78h]@1
  __int64 v45; // [sp+50h] [bp-70h]@1
  __int64 v46; // [sp+58h] [bp-68h]@1
  __int64 v47; // [sp+60h] [bp-60h]@1
  __int64 v48; // [sp+68h] [bp-58h]@1
  __int64 v49; // [sp+70h] [bp-50h]@5
  unsigned __int64 v50; // [sp+78h] [bp-48h]@1
  unsigned __int64 v51; // [sp+80h] [bp-40h]@5
  void *v52; // [sp+88h] [bp-38h]@5
  __int64 v53; // [sp+90h] [bp-30h]@1
  __int64 v54; // [sp+98h] [bp-28h]@10

  v46 = a6;
  v50 = a5;
  v48 = a4;
  v44 = a3;
  v47 = a2;
  v53 = *(_QWORD *)off_69010[0];
  v8 = *(_QWORD *)a2;
  v45 = 38LL;
  if ( a1 >= 0xAB )
  {
    v45 = 41LL;
    if ( a1 >= 0x200 )
    {
      v45 = 7LL;
      if ( a1 >= 0x400 )
        v45 = -(signed __int64)(a1 < 0x600) & 1 | 4;
    }
  }
  v49 = ccn_bitlen(v46, a7);
  v9 = (8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v40 = v9;
  v52 = (char *)&v35 - v9;
  v10 = (__int64)((char *)&v35 - v9);
  v43 = (char *)&v35 - v9;
  v39 = (16 * v8 + 39) >> 4;
  v41 = &v35;
  v11 = (16 * v8 + 39) & 0xFFFFFFFFFFFFFFF0LL;
  v42 = (char *)&v35 - v11;
  *(_QWORD *)v42 = v8;
  *(__int64 *)((char *)&v35 - v9) = a1;
  v12 = ccn_bitlen(1LL, (__int64)((char *)&v35 - v9));
  v51 = a1;
  v13 = a1 - 5 - v12;
  cczp_compute_R(v8, v10, (__int64)((char *)&v35 - v11), v48, v50);
  v14 = *(__int64 *)((char *)v41 - v11);
  v48 = (__int64)((char *)&v35 + -v11 + 16);
  v15 = ccn_bitlen(v14, v48);
  v16 = v15 < (unsigned __int64)v13;
  v17 = v15 == v13;
  v18 = v51;
  v19 = -2;
  if ( !v16 && !v17 )
    v19 = -13;
  v20 = v47;
  if ( !v16 && !v17 )
  {
LABEL_27:
    v8 *= 8LL;
    v22 = v44 + 16;
    v32 = v8;
    v23 = v20 + 16;
LABEL_28:
    cc_clear(v32, (void *)v23);
    cc_clear(v32, (void *)v22);
    goto LABEL_29;
  }
  v21 = a8;
  v22 = v44 + 16;
  v36 = v44 + 16;
  v23 = v47 + 16;
  v37 = 8 * v8;
  v50 = 5 * v51;
  v24 = 0LL;
LABEL_9:
  v19 = -4;
  if ( (unsigned int)cczp_seed_X(v44, v18, v21) )
  {
    v32 = v37;
    goto LABEL_28;
  }
  v38 = v24;
  v41 = &v54;
  v25 = (char *)&v54 - v40;
  v26 = (__int64 *)v42;
  cczp_modn((unsigned __int64)v42, (char *)&v54 - v40, v8, v22);
  cczp_sub(v26, v23, (__int64)v43, (__int64)v25);
  v27 = ccn_add(v8, v23, v22, v23);
  cc_clear(v37, v25);
  v19 = -3;
  if ( !v50 )
    goto LABEL_26;
  v28 = 0LL;
  while ( 1 )
  {
    if ( v27 || (v29 = ccn_bitlen(v8, v23), v29 > v51) )
    {
      v24 = v38 + 1;
      v19 = -2;
      v18 = v51;
      v22 = v36;
      v21 = a8;
      v20 = v47;
      if ( (unsigned __int64)(v38 + 1) >= 0x64 )
        goto LABEL_27;
      goto LABEL_9;
    }
    if ( (unsigned __int64)v49 >= 0x11 )
    {
      if ( (unsigned int)cczp_rabin_miller(v47, v45) == 1 )
      {
        *(_BYTE *)v23 &= 0xFEu;
        v30 = (__int64)v52;
        ccn_gcdn(v8, v52, v8, v23, v46, a7);
        *(_BYTE *)v23 |= 1u;
        if ( ccn_n(v8, v30) == 1 && *(_QWORD *)v52 == 1LL )
          goto LABEL_31;
      }
      goto LABEL_22;
    }
    *(_BYTE *)v23 &= 0xFEu;
    v31 = (__int64)v52;
    ccn_gcdn(v8, v52, v8, v23, v46, a7);
    *(_BYTE *)v23 |= 1u;
    if ( ccn_n(v8, v31) == 1 && *(_QWORD *)v52 == 1LL && (unsigned int)cczp_rabin_miller(v47, v45) == 1 )
      break;
LABEL_22:
    v27 = ccn_add(v8, v23, v23, v48);
    ++v28;
    if ( v28 >= v50 )
    {
      v19 = -3;
LABEL_26:
      v20 = v47;
      goto LABEL_27;
    }
  }
  do
  {
LABEL_31:
    v34 = v47;
    *(_QWORD *)v34 = (unsigned __int64)(ccn_bitlen(v8, v47 + 16) + 63) >> 6;
    cczp_init(v34);
    v8 *= 8LL;
    v19 = 0;
    v32 = v8;
LABEL_29:
    cc_clear(v32, v52);
    cc_clear(v32, v43);
    cc_clear(16 * v39, v42);
  }
  while ( *(_QWORD *)off_69010[0] != v53 );
  return (unsigned int)v19;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000369BC) ----------------------------------------------------
__int64 __fastcall cczp_compute_R(unsigned __int64 a1, __int64 a2, __int64 a3, __int64 a4, unsigned __int64 a5)
{
  unsigned __int64 v5; // rbx@1
  const void *v6; // r15@1
  signed __int64 v7; // r14@1
  signed __int64 v8; // rax@1
  signed __int64 v9; // r12@1
  signed __int64 v10; // rbx@1
  __int64 v11; // rbx@3
  unsigned __int64 v12; // rax@3
  char *v13; // r13@3
  __int64 v14; // r12@5
  __int64 v15; // r15@5
  __int64 v16; // r14@5
  unsigned __int64 *v17; // r12@5
  __int64 v18; // r15@5
  signed __int64 v19; // r14@5
  size_t v20; // r12@6
  __int64 v21; // r15@6
  __int64 v22; // r15@7
  size_t v23; // rbx@8
  __int64 v25; // [sp+0h] [bp-80h]@3
  unsigned __int64 v26; // [sp+8h] [bp-78h]@1
  __int64 v27; // [sp+10h] [bp-70h]@1
  __int64 v28; // [sp+18h] [bp-68h]@1
  const void *v29; // [sp+20h] [bp-60h]@1
  const void *v30; // [sp+28h] [bp-58h]@1
  __int64 v31; // [sp+30h] [bp-50h]@1
  unsigned __int64 v32; // [sp+38h] [bp-48h]@1
  __int64 v33; // [sp+40h] [bp-40h]@1
  void *v34; // [sp+48h] [bp-38h]@3
  __int64 v35; // [sp+50h] [bp-30h]@1

  v5 = a5;
  v26 = a5;
  v31 = a3;
  v33 = a2;
  v32 = a1;
  v35 = *(_QWORD *)off_69010[0];
  v28 = *(_QWORD *)a4;
  v6 = (const void *)(a4 + 16);
  v30 = (const void *)(a4 + 16);
  v7 = ccn_bitlen(v28, a4 + 16);
  v27 = *(_QWORD *)v5;
  v29 = (const void *)(v5 + 16);
  v8 = ccn_bitlen(v27, v5 + 16);
  v9 = v8;
  v10 = v7 + 1;
  if ( v7 + 1 <= (unsigned __int64)v8 )
    v10 = v8;
  v11 = (unsigned __int64)(v10 + 63) >> 6;
  v12 = (8 * v11 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v34 = (char *)&v25 - v12;
  v13 = (char *)&v25 - v12;
  cczp_mod_inv_slown(v26, (__int64)((char *)&v25 - v12), v28, v6);
  if ( v11 > (unsigned __int64)v27 )
    bzero((char *)v34 + 8 * v27, 8 * (v11 - v27));
  v14 = (unsigned __int64)(v9 + 63) >> 6;
  v15 = v33;
  ccn_set(v14, (void *)v33, v29);
  bzero((void *)(v15 + 8 * v14), 8 * (v11 - v14));
  ccn_sub(v11, (__int64)v34, v15, (__int64)v34);
  v16 = (unsigned __int64)(v7 + 63) >> 6;
  v17 = (unsigned __int64 *)v15;
  ccn_set(v16, v13, v30);
  bzero(&v13[8 * v16], 8 * (v11 - v16));
  ccn_add(v11, (__int64)v13, (__int64)v13, (__int64)v13);
  v18 = v31;
  ccn_mul(v11, v31 + 16, (__int64)v13, v17);
  v19 = 2 * v11;
  if ( 2 * v11 >= v32 )
  {
    cczp_init(v18);
    v22 = v33;
    ccn_mul(v11, v33, (__int64)v13, (unsigned __int64 *)v34);
    ccn_add1(2 * v11, v22, v22, 1LL);
  }
  else
  {
    v20 = 8 * (v32 - v19);
    bzero((void *)(v18 + 16 * v11 + 16), 8 * (v32 - v19));
    cczp_init(v18);
    v21 = v33;
    ccn_mul(v11, v33, (__int64)v13, (unsigned __int64 *)v34);
    ccn_add1(2 * v11, v21, v21, 1LL);
    bzero((void *)(v21 + 16 * v11), v20);
  }
  v23 = 8 * v11;
  cc_clear(v23, v34);
  cc_clear(v23, v13);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000036BE9) ----------------------------------------------------
__int64 __fastcall cczp_seed_X(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r15@1
  char *v4; // r14@1
  unsigned __int64 v5; // r13@1
  signed int v6; // ebx@2
  __int64 result; // rax@5
  __int64 v8; // [sp+0h] [bp-70h]@1
  size_t v9; // [sp+8h] [bp-68h]@1
  unsigned __int64 v10; // [sp+10h] [bp-60h]@1
  __int64 v11; // [sp+18h] [bp-58h]@1
  unsigned __int64 v12; // [sp+20h] [bp-50h]@1
  __int64 v13; // [sp+28h] [bp-48h]@1
  __int64 v14; // [sp+30h] [bp-40h]@1
  __int64 v15; // [sp+38h] [bp-38h]@1
  __int64 v16; // [sp+40h] [bp-30h]@1

  v15 = a3;
  v14 = a2;
  v13 = a1;
  v16 = *(_QWORD *)off_69010[0];
  v12 = *(_QWORD *)a1;
  v3 = a1 + 16;
  v9 = 8 * v12;
  bzero((void *)(a1 + 16), 8 * v12);
  v4 = (char *)&v8 - ((8 * v12 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  bzero((void *)(a1 + 16), 8LL * *(_QWORD *)a1);
  v11 = 1LL << ((unsigned __int8)a2 - 1);
  v10 = a2 - 256;
  v5 = 0LL;
  do
  {
    v6 = -10;
    if ( ccn_random_bits(v14, v3, v15) )
      break;
    *(_QWORD *)(v13 + 8 * ((unsigned __int64)(a2 - 1) >> 6) + 16) |= v11;
    ccn_shift_right_multi(v12, v4, (void *)v3, v10);
    v6 = 0;
    if ( (unsigned int)ccn_cmp(4LL, (__int64)v4, (__int64)sqrt_2) != -1 )
      break;
    ++v5;
    v6 = -10;
  }
  while ( v5 <= 0x63 );
  cc_clear(v9, v4);
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v16 )
    result = (unsigned int)v6;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000036D16) ----------------------------------------------------
signed __int64 __fastcall cczp_find_next_prime(__int64 *a1)
{
  __int64 v1; // r15@1
  __int64 v2; // rbx@1
  signed __int64 v3; // rax@1
  __int64 v4; // r12@1
  int v5; // ecx@8
  signed __int64 result; // rax@8

  v1 = *a1;
  v2 = (__int64)(a1 + 2);
  v3 = ccn_bitlen(*a1, (__int64)(a1 + 2));
  v4 = 38LL;
  if ( (unsigned __int64)v3 >= 0xAB )
  {
    v4 = 41LL;
    if ( (unsigned __int64)v3 >= 0x200 )
    {
      v4 = 7LL;
      if ( (unsigned __int64)v3 >= 0x400 )
        v4 = -(signed __int64)((unsigned __int64)v3 < 0x600) & 1 | 4;
    }
  }
  if ( !ccn_sub1(v1, v2, v2, 2LL) )
  {
    *(_BYTE *)v2 |= 1u;
    while ( !ccn_add1(v1, v2, v2, 2LL) )
    {
      v5 = cczp_rabin_miller((__int64)a1, v4);
      result = 0LL;
      if ( v5 )
        return result;
    }
  }
  return 4294967287LL;
}

//----- (0000000000036DC7) ----------------------------------------------------
unsigned __int64 __fastcall ccn_write_uint_size(__int64 a1, __int64 a2)
{
  return (unsigned __int64)(ccn_bitlen(a1, a2) + 7) >> 3;
}

//----- (0000000000036DDA) ----------------------------------------------------
unsigned __int64 __fastcall ccn_write_uint(__int64 a1, __int64 a2, signed __int64 a3, __int64 a4)
{
  __int64 v4; // r12@1
  signed __int64 v5; // r15@1
  __int64 v6; // r14@1
  unsigned __int64 result; // rax@1
  signed __int64 v8; // rdx@1
  signed __int64 v9; // rsi@3
  signed __int64 v10; // rcx@3
  unsigned __int64 v11; // rdi@4
  unsigned __int64 v12; // r15@4
  unsigned __int64 v13; // rcx@4
  unsigned __int64 v14; // rbx@4
  signed __int64 v15; // rbx@8
  signed __int64 v16; // r12@11
  signed __int64 v17; // r10@12
  unsigned __int64 v18; // rcx@12
  signed __int64 v19; // r8@12
  signed __int64 v20; // r9@12
  unsigned __int64 v21; // rcx@12
  signed __int64 v22; // rbx@12
  unsigned __int64 v23; // rdi@13
  signed __int64 v24; // r12@18

  v4 = a4;
  v5 = a3;
  v6 = a2;
  result = (unsigned __int64)(ccn_bitlen(a1, a2) + 7) >> 3;
  v8 = v5;
  if ( result < v5 )
    v8 = result;
  v9 = (result - v8) >> 3;
  v10 = (result - v8) & 7;
  if ( v10 )
  {
    v11 = *(_QWORD *)(v6 + 8 * v9) >> 8 * (unsigned __int8)v10;
    v12 = ~v5;
    v13 = ~result;
    v14 = ~result;
    if ( v12 > ~result )
      v14 = v12;
    v8 = (((_BYTE)v14 + (_BYTE)result + 1) & 7) - 9LL - v14;
    if ( v12 > v13 )
      v13 = v12;
    result = 8LL - (((_BYTE)v13 + (_BYTE)result + 1) & 7);
    v15 = v4 + -2LL - v13;
    do
    {
      *(_BYTE *)v15 = v11;
      v11 >>= 8;
      --v15;
      --result;
    }
    while ( result );
    ++v9;
  }
  v16 = v8 + v4;
  if ( (unsigned __int64)v8 >= 8 )
  {
    v17 = v8 - 8;
    v18 = (unsigned __int64)(v8 - 8) >> 3;
    v19 = 8 * v18;
    v20 = -8LL - 8 * v18;
    v21 = v9 + v18;
    v22 = v16;
    do
    {
      v23 = *(_QWORD *)(v6 + 8 * v9);
      result = 8LL;
      do
      {
        *(_BYTE *)(v22 + result - 9) = v23;
        v23 >>= 8;
        --result;
      }
      while ( result );
      ++v9;
      v22 -= 8LL;
      v8 -= 8LL;
    }
    while ( (unsigned __int64)v8 > 7 );
    v16 += v20;
    v9 = v21 + 1;
    v8 = v17 - v19;
  }
  if ( v8 )
  {
    result = *(_QWORD *)(v6 + 8 * v9);
    v24 = v16 - 1;
    do
    {
      *(_BYTE *)v24 = result;
      result >>= 8;
      --v24;
      --v8;
    }
    while ( v8 );
  }
  return result;
}

//----- (0000000000036F03) ----------------------------------------------------
__int64 __fastcall ccpad_xts_decrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 v6; // r13@1
  __int64 v7; // rbx@1
  __int64 v8; // r8@1
  __int64 v9; // rcx@1
  __int64 v10; // r12@1
  unsigned __int64 v11; // rdx@1
  __int64 v12; // r14@1
  __int64 v13; // r15@1
  int (__fastcall *v14)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD); // rax@1
  __int64 v15; // ST00_8@2
  const void *v16; // rax@2
  void *v17; // ST10_8@2
  __int64 v18; // r15@2
  signed __int64 v19; // rcx@2
  __int64 v20; // r12@2
  __int64 v21; // rax@2
  __int64 v22; // r13@2
  __int64 v23; // rbx@2
  signed __int64 v25; // [sp+8h] [bp-78h]@2
  __int64 v26; // [sp+28h] [bp-58h]@2
  char v27[16]; // [sp+30h] [bp-50h]@2
  char v28[16]; // [sp+40h] [bp-40h]@2
  __int64 v29; // [sp+50h] [bp-30h]@1

  v6 = a6;
  v7 = a5;
  v8 = a4;
  v9 = a3;
  v10 = v8 - (v8 & 0xF);
  v11 = (v8 - (unsigned __int64)(v8 & 0xF)) >> 4;
  v12 = v8 & 0xF;
  v13 = off_69010[0];
  v29 = *(_QWORD *)off_69010[0];
  v14 = *(int (__fastcall **)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD))(a1 + 40);
  if ( v8 & 0xF )
  {
    v26 = v9;
    v15 = v8;
    LODWORD(v16) = v14(a2, v9, v11 - 1, v7, a6);
    v17 = (void *)v16;
    v18 = (__int64)v28;
    memcpy(v28, v16, 0x10uLL);
    ccmode_xts_mult_alpha((__int64)v17);
    v19 = v7 + v10 - 16;
    v25 = v6 + v10 - 16;
    v20 = (__int64)v27;
    (*(void (__fastcall **)(__int64, __int64, signed __int64, signed __int64, _QWORD))(a1 + 40))(a2, v26, 1LL, v19, v27);
    memcpy(v17, v28, 0x10uLL);
    v21 = -v12;
    v22 = v15 + v6;
    v23 = v15 + v7;
    do
    {
      *(_BYTE *)v18 = *(_BYTE *)(v23 + v21);
      *(_BYTE *)(v22 + v21) = *(_BYTE *)v20;
      ++v18;
      ++v20;
      ++v21;
    }
    while ( v21 );
    v13 = off_69010[0];
    do
    {
      v28[v12] = v27[v12];
      ++v12;
    }
    while ( v12 != 16 );
    (*(void (__fastcall **)(__int64, __int64, signed __int64, _QWORD, signed __int64))(a1 + 40))(a2, v26, 1LL, v28, v25);
  }
  else
  {
    v14(a2, v9, v11, v7, a6);
  }
  return *(_QWORD *)v13;
}
// 69010: using guessed type __int64 off_69010[2];
// 36F03: using guessed type char var_40[16];
// 36F03: using guessed type char var_50[16];

//----- (000000000003705C) ----------------------------------------------------
__int64 __fastcall ccpad_xts_encrypt(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 v6; // r12@1
  __int64 v7; // r13@1
  __int64 v8; // rcx@1
  __int64 v9; // rbx@1
  unsigned __int64 v10; // rdx@1
  __int64 v11; // r14@1
  __int64 v12; // r15@1
  void (__fastcall *v13)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD); // rax@1
  __int64 v14; // r15@2
  __int64 v15; // rax@2
  __int64 v16; // r12@2
  __int64 v17; // rdx@2
  __int64 v19; // [sp+18h] [bp-68h]@2
  __int64 v20; // [sp+28h] [bp-58h]@2
  char v21[16]; // [sp+30h] [bp-50h]@2
  char v22[16]; // [sp+40h] [bp-40h]@2
  __int64 v23; // [sp+50h] [bp-30h]@1

  v6 = a5;
  v7 = a4;
  v8 = a3;
  v9 = v7 - (v7 & 0xF);
  v10 = (v7 - (unsigned __int64)(v7 & 0xF)) >> 4;
  v11 = v7 & 0xF;
  v12 = off_69010[0];
  v23 = *(_QWORD *)off_69010[0];
  v13 = *(void (__fastcall **)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD))(a1 + 40);
  if ( v7 & 0xF )
  {
    v20 = v8;
    v19 = a6;
    v13(a2, v8, v10 - 1, a5, a6);
    v14 = (__int64)v22;
    (*(void (__fastcall **)(__int64, __int64, signed __int64, signed __int64, _QWORD))(a1 + 40))(
      a2,
      v20,
      1LL,
      v6 + v9 - 16,
      v22);
    v15 = -v11;
    v16 = v7 + v6;
    v17 = (__int64)v21;
    do
    {
      *(_BYTE *)v17 = *(_BYTE *)(v16 + v15);
      *(_BYTE *)(v19 + v7 + v15) = *(_BYTE *)v14;
      ++v17;
      ++v14;
      ++v15;
    }
    while ( v15 );
    v12 = off_69010[0];
    do
    {
      v21[v11] = v22[v11];
      ++v11;
    }
    while ( v11 != 16 );
    (*(void (__fastcall **)(__int64, __int64, signed __int64, _QWORD, signed __int64))(a1 + 40))(
      a2,
      v20,
      1LL,
      v21,
      v9 - 16 + v19);
  }
  else
  {
    v13(a2, v8, v10, a5, a6);
  }
  return *(_QWORD *)v12;
}
// 69010: using guessed type __int64 off_69010[2];
// 3705C: using guessed type char var_40[16];
// 3705C: using guessed type char var_50[16];

//----- (0000000000037184) ----------------------------------------------------
signed __int64 __fastcall ccpbkdf2_hmac(__int64 a1, unsigned __int64 a2, const void *a3, __int64 a4, const void *a5, unsigned __int64 a6, unsigned __int64 a7, void *a8)
{
  unsigned __int64 v8; // r11@1
  __int64 v9; // rsi@1
  signed __int64 result; // rax@1
  void *v11; // r12@2
  unsigned __int64 v12; // rbx@2
  unsigned __int64 v13; // rax@2
  char *v14; // r14@2
  size_t v15; // rbx@2
  unsigned __int64 v16; // rax@2
  signed __int64 v17; // r13@2
  unsigned __int64 v18; // r15@3
  unsigned __int64 v19; // r14@4
  unsigned __int64 v20; // rcx@7
  __int64 v21; // rbx@7
  void *v22; // r14@7
  void *v23; // r15@7
  __int64 v24; // rcx@10
  size_t v25; // [sp+20h] [bp-70h]@2
  __int64 *v26; // [sp+28h] [bp-68h]@2
  __int64 v27; // [sp+30h] [bp-60h]@2
  __int64 v28; // [sp+38h] [bp-58h]@3
  void *v29; // [sp+40h] [bp-50h]@2
  const void *v30; // [sp+48h] [bp-48h]@2
  void *v31; // [sp+50h] [bp-40h]@2
  unsigned __int64 v32; // [sp+58h] [bp-38h]@2
  __int64 v33; // [sp+60h] [bp-30h]@1

  v8 = a2;
  v9 = off_69010[0];
  v33 = *(_QWORD *)off_69010[0];
  result = 0xFFFFFFFFLL;
  if ( !(a7 / *(_QWORD *)a1 >> 32) )
  {
    v27 = a4;
    v30 = a5;
    v32 = a6;
    v26 = (__int64 *)&v25;
    v11 = a8;
    v12 = (((*(_QWORD *)(a1 + 16) + 2LL * *(_QWORD *)(a1 + 8) + 19) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL;
    v31 = (char *)&v25 - v12;
    v13 = (((*(_QWORD *)(a1 + 8) + 7LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL;
    v14 = (char *)&v25 - v13;
    v29 = (char *)&v25 - v13;
    cchmac_init(a1, (__int64)((char *)&v25 - v12), v8, a3);
    memcpy(v14, (char *)&v25 + -v12 + 8, *(_QWORD *)(a1 + 8));
    v15 = *(_QWORD *)a1;
    v16 = a7 / *(_QWORD *)a1;
    v25 = a7 % *(_QWORD *)a1;
    v17 = 1LL;
    if ( v16 )
    {
      v28 = a1;
      v18 = v27;
      do
      {
        v19 = v16;
        F(v28, (__int64)v31, v29, v18, v30, v32, v17, v15, v11);
        v16 = v19;
        ++v17;
        v11 = (char *)v11 + v15;
      }
      while ( v17 <= v19 );
    }
    else
    {
      v28 = a1;
      v18 = v27;
    }
    v20 = v18;
    v21 = v28;
    v22 = v31;
    v23 = v29;
    if ( v25 )
      F(v28, (__int64)v31, v29, v20, v30, v32, v17, v25, v11);
    cc_clear(*(_QWORD *)(v21 + 16) + 2LL * *(_QWORD *)(v21 + 8) + 12, v22);
    cc_clear((*(_QWORD *)(v21 + 8) + 7LL) & 0xFFFFFFFFFFFFFFF8LL, v23);
    result = 0LL;
    v9 = off_69010[0];
  }
  v24 = *(_QWORD *)v9;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000003734C) ----------------------------------------------------
void *__fastcall F(__int64 a1, __int64 a2, const void *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, unsigned int a7, size_t a8, void *a9)
{
  unsigned __int64 v9; // r15@1
  const void *v10; // ST18_8@1
  unsigned __int64 v11; // ST10_8@1
  __int64 v12; // r14@1
  __int64 v13; // rax@1
  signed __int64 v14; // rax@1
  void *result; // rax@1
  signed __int64 v16; // r14@2
  __int64 v17; // rax@3
  size_t v18; // [sp+8h] [bp-68h]@1
  const void *v19; // [sp+30h] [bp-40h]@1
  unsigned __int64 v20; // [sp+38h] [bp-38h]@1
  unsigned int v21; // [sp+44h] [bp-2Ch]@1

  v9 = a6;
  v20 = a6;
  v10 = a5;
  v11 = a4;
  v19 = a3;
  v18 = *(_QWORD *)(a1 + 8);
  v12 = v18 + a2 + 8;
  memcpy((void *)(a2 + 8), a3, v18);
  v13 = *(_QWORD *)(a1 + 16);
  *(_QWORD *)a2 = 8 * v13;
  v14 = *(_QWORD *)(a1 + 8) + v13;
  *(_DWORD *)(a2 + v14 + 8) = 0;
  ccdigest_update(v14, v11, v10, a1, a2);
  v21 = _byteswap_ulong(a7);
  ccdigest_update(_byteswap_ulong(v21), 4uLL, &v21, a1, a2);
  cchmac_final(a1, a2, v12);
  result = memcpy(a9, (const void *)v12, a8);
  if ( v9 >= 2 )
  {
    v16 = 2LL;
    do
    {
      memcpy((void *)(a2 + 8), v19, *(_QWORD *)(a1 + 8));
      v17 = *(_QWORD *)(a1 + 16);
      *(_QWORD *)a2 = 8 * v17;
      *(_DWORD *)(a2 + *(_QWORD *)(a1 + 8) + v17 + 8) = *(_DWORD *)a1;
      cchmac_final(a1, a2, v18 + a2 + 8);
      result = (void *)a8;
      if ( a8 )
      {
        do
        {
          *((_BYTE *)result + (_QWORD)a9 - 1) ^= *((_BYTE *)result + v18 + a2 + 7);
          result = (char *)result - 1;
        }
        while ( result );
      }
      ++v16;
    }
    while ( v16 <= v20 );
  }
  return result;
}

//----- (0000000000037489) ----------------------------------------------------
__int64 __fastcall ccnistkdf_ctr_hmac_fixed(void *a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5, size_t a6, void *a7)
{
  const void *v7; // r10@1
  __int64 v8; // rbx@1
  __int64 v9; // r15@1
  unsigned __int64 v10; // rdi@1
  unsigned __int64 v11; // rtt@1
  unsigned __int64 v12; // r14@1
  size_t v13; // rdx@1
  __int64 result; // rax@1
  __int64 v15; // rax@6
  unsigned __int64 v16; // r15@6
  unsigned __int64 v17; // rax@6
  char *v18; // r13@6
  __int64 v19; // r12@6
  unsigned __int64 v20; // rbx@6
  signed __int64 v21; // r15@7
  __int64 v22; // r13@7
  __int64 v23; // r14@7
  __int64 v24; // rax@8
  signed __int64 v25; // rax@8
  __int64 v26; // r15@8
  signed __int64 v27; // rax@8
  void *v28; // r14@9
  __int64 v29; // rcx@10
  __int64 v30; // [sp+0h] [bp-90h]@6
  size_t v31; // [sp+10h] [bp-80h]@6
  size_t v32; // [sp+18h] [bp-78h]@6
  void *v33; // [sp+20h] [bp-70h]@6
  void *v34; // [sp+28h] [bp-68h]@6
  const void *v35; // [sp+30h] [bp-60h]@6
  unsigned __int64 v36; // [sp+38h] [bp-58h]@6
  unsigned __int64 v37; // [sp+40h] [bp-50h]@5
  void *v38; // [sp+48h] [bp-48h]@6
  void *v39; // [sp+50h] [bp-40h]@6
  unsigned int v40; // [sp+5Ch] [bp-34h]@8
  __int64 v41; // [sp+60h] [bp-30h]@1
  __int64 v42; // [sp+68h] [bp-28h]@6

  v7 = a3;
  v8 = (__int64)a1;
  v9 = off_69010[0];
  v41 = *(_QWORD *)off_69010[0];
  v10 = *(_QWORD *)a1;
  v11 = a6;
  v12 = (v10 * (v11 / v10) < a6) + v11 / v10;
  v13 = v10 * v12;
  result = 0xFFFFFFFFLL;
  if ( !(v12 >> 32) )
  {
    if ( a2 )
    {
      if ( v7 )
      {
        if ( a6 )
        {
          v37 = a4;
          if ( a7 )
          {
            v33 = (char *)&v30 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0LL);
            v31 = v13;
            v32 = a6;
            v15 = *(_QWORD *)(v8 + 8);
            v16 = (((*(_QWORD *)(v8 + 16) + 2 * v15 + 19) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL;
            v38 = (char *)&v42 - v16;
            v17 = (((v15 + 7) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL;
            v18 = (char *)&v42 - v17;
            v39 = (char *)&v42 - v17;
            v36 = v10;
            v35 = a5;
            cchmac_init(v8, (__int64)((char *)&v42 - v16), a2, v7);
            v34 = (char *)&v42 + -v16 + 8;
            memcpy(v18, v34, *(_QWORD *)(v8 + 8));
            v19 = v8;
            v20 = v12;
            if ( v12 )
            {
              LODWORD(v21) = 1;
              v22 = (__int64)v33;
              v23 = (__int64)v38;
              do
              {
                memcpy(v34, v39, *(_QWORD *)(v19 + 8));
                v24 = *(_QWORD *)(v19 + 16);
                *(_QWORD *)v23 = 8 * v24;
                v25 = *(_QWORD *)(v19 + 8) + v24;
                *(_DWORD *)(v23 + v25 + 8) = 0;
                v40 = _byteswap_ulong(v21);
                v26 = _byteswap_ulong(v40);
                LODWORD(v27) = ccdigest_update(v25, 4uLL, &v40, v19, v23);
                ccdigest_update(v27, v37, v35, v19, v23);
                cchmac_final(v19, v23, v22);
                v21 = v26 + 1;
                v22 += v36;
              }
              while ( v21 <= v20 );
            }
            v28 = v33;
            memcpy(a7, v33, v32);
            cc_clear(v31, v28);
            cc_clear(*(_QWORD *)(v19 + 16) + 2LL * *(_QWORD *)(v19 + 8) + 12, v38);
            cc_clear(*(_QWORD *)(v19 + 8), v39);
            result = 0LL;
            v9 = off_69010[0];
          }
        }
      }
    }
  }
  v29 = *(_QWORD *)v9;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000376A4) ----------------------------------------------------
__int64 __fastcall ccnistkdf_ctr_hmac(void *a1, unsigned __int64 a2, __int64 a3, size_t a4, const void *a5, size_t a6, const void *a7, size_t a8, const void *a9)
{
  size_t v9; // r13@1
  size_t v10; // r14@1
  unsigned __int64 v11; // rax@1
  char *v12; // rbx@1
  signed __int64 v13; // r12@4
  unsigned __int64 v14; // r15@7
  unsigned int v15; // er14@7
  __int64 result; // rax@7
  __int64 v17; // [sp+10h] [bp-50h]@1
  void *v18; // [sp+18h] [bp-48h]@1
  unsigned __int64 v19; // [sp+20h] [bp-40h]@1
  __int64 v20; // [sp+28h] [bp-38h]@1
  __int64 v21; // [sp+30h] [bp-30h]@1

  v9 = a6;
  v10 = a4;
  v20 = a3;
  v19 = a2;
  v18 = a1;
  v21 = *(_QWORD *)off_69010[0];
  v11 = (a4 + a6 + 20) & 0xFFFFFFFFFFFFFFF0LL;
  v12 = (char *)&v17 - v11;
  if ( a4 && a5 )
    memcpy((char *)&v17 - v11, a5, a4);
  v12[v10] = 0;
  v13 = v10 + 1;
  if ( v9 && a7 )
    memcpy(&v12[v13], a7, v9);
  v14 = v10 + v9 + 5;
  *(_DWORD *)(&v12[v9] + v13) = _byteswap_ulong(8 * a8);
  v15 = ccnistkdf_ctr_hmac_fixed(v18, v19, a9, v14, v12, a8, (void *)a9);
  cc_clear(v14, v12);
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v21 )
    result = v15;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000003779D) ----------------------------------------------------
_UNKNOWN *ccrc4()
{
  return &ccrc4_eay;
}

//----- (00000000000377AA) ----------------------------------------------------
__int64 __fastcall eay_RC4_set_key(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rax@1
  __int64 v4; // r9@3
  __int64 v5; // r11@3
  __int64 v6; // rcx@3
  int v7; // er10@4
  int v8; // ecx@4
  __int64 v9; // r11@4
  int v10; // er10@6
  int v11; // ecx@6
  __int64 v12; // r11@6
  int v13; // er10@8
  int v14; // ecx@8
  __int64 v15; // r11@8
  __int64 result; // rax@10

  v3 = 0LL;
  do
  {
    *(_DWORD *)(a1 + 4 * v3 + 8) = v3;
    ++v3;
  }
  while ( v3 != 256 );
  *(_QWORD *)a1 = 0LL;
  v4 = 0LL;
  LODWORD(v5) = 0;
  LODWORD(v6) = 0;
  do
  {
    v7 = *(_DWORD *)(a1 + 4 * v4 + 8);
    v8 = *(_BYTE *)(a3 + (unsigned int)v5) + v7 + v6;
    v9 = (unsigned int)(v5 + 1);
    if ( v9 == a2 )
      LODWORD(v9) = 0;
    *(_DWORD *)(a1 + 4 * v4 + 8) = *(_DWORD *)(a1 + 4LL * (unsigned __int8)v8 + 8);
    *(_DWORD *)(a1 + 4LL * (unsigned __int8)v8 + 8) = v7;
    v10 = *(_DWORD *)(a1 + 4 * v4 + 12);
    v11 = *(_BYTE *)(a3 + (unsigned int)v9) + v10 + v8;
    v12 = (unsigned int)(v9 + 1);
    if ( v12 == a2 )
      LODWORD(v12) = 0;
    *(_DWORD *)(a1 + 4 * v4 + 12) = *(_DWORD *)(a1 + 4LL * (unsigned __int8)v11 + 8);
    *(_DWORD *)(a1 + 4LL * (unsigned __int8)v11 + 8) = v10;
    v13 = *(_DWORD *)(a1 + 4 * v4 + 16);
    v14 = *(_BYTE *)(a3 + (unsigned int)v12) + v13 + v11;
    v15 = (unsigned int)(v12 + 1);
    if ( v15 == a2 )
      LODWORD(v15) = 0;
    *(_DWORD *)(a1 + 4 * v4 + 16) = *(_DWORD *)(a1 + 4LL * (unsigned __int8)v14 + 8);
    *(_DWORD *)(a1 + 4LL * (unsigned __int8)v14 + 8) = v13;
    result = *(_DWORD *)(a1 + 4 * v4 + 20);
    v6 = (unsigned __int8)(*(_BYTE *)(a3 + (unsigned int)v15) + result + v14);
    v5 = (unsigned int)(v15 + 1);
    if ( v5 == a2 )
      LODWORD(v5) = 0;
    *(_DWORD *)(a1 + 4 * v4 + 20) = *(_DWORD *)(a1 + 4 * v6 + 8);
    *(_DWORD *)(a1 + 4 * v6 + 8) = result;
    v4 += 4LL;
  }
  while ( (unsigned int)v4 < 0x100 );
  return result;
}

//----- (0000000000037894) ----------------------------------------------------
__int64 __fastcall eay_RC4(__int64 a1, unsigned __int64 a2, __int64 a3, signed __int64 a4)
{
  __int64 v4; // r8@1
  __int64 result; // rax@1
  unsigned __int64 v6; // r11@1
  signed __int64 v7; // r10@2
  signed __int64 v8; // r9@2
  signed __int64 v9; // r10@2
  int v10; // er11@2
  __int64 v11; // r14@2
  __int64 v12; // r15@3
  int v13; // er13@3
  unsigned __int8 v14; // al@3
  int v15; // ebx@3
  __int64 v16; // r15@3
  int v17; // er13@3
  unsigned __int8 v18; // al@3
  int v19; // ebx@3
  __int64 v20; // r15@3
  int v21; // er13@3
  unsigned __int8 v22; // al@3
  int v23; // ebx@3
  __int64 v24; // r15@3
  int v25; // er13@3
  unsigned __int8 v26; // al@3
  int v27; // ebx@3
  __int64 v28; // r15@3
  int v29; // er13@3
  unsigned __int8 v30; // al@3
  int v31; // ebx@3
  __int64 v32; // r15@3
  int v33; // er13@3
  unsigned __int8 v34; // al@3
  int v35; // ebx@3
  __int64 v36; // r15@3
  int v37; // er13@3
  unsigned __int8 v38; // al@3
  int v39; // ebx@3
  int v40; // ebx@3
  int v41; // esi@7
  __int64 v42; // r15@7
  __int64 v43; // rbx@8
  int v44; // er11@8
  int v45; // er10@8
  int v46; // er14@8
  int v47; // er11@9
  int v48; // ecx@9
  int v49; // er11@10
  int v50; // ecx@10
  int v51; // er11@11
  int v52; // ecx@11
  int v53; // er11@12
  int v54; // ecx@12
  int v55; // er11@13
  int v56; // ecx@13
  int v57; // ecx@14

  LODWORD(v4) = *(_DWORD *)a1;
  result = *(_DWORD *)(a1 + 4);
  v6 = a2 >> 3;
  if ( (unsigned int)(a2 >> 3) )
  {
    v7 = (8 * v6 + 34359738360LL) & 0x7FFFFFFF8LL;
    v8 = a4 + v7 + 8;
    v9 = v7 + 8;
    v10 = -(signed int)v6;
    v11 = a3;
    do
    {
      v12 = (unsigned __int8)(v4 + 1);
      v13 = *(_DWORD *)(a1 + 4 * v12 + 8);
      v14 = v13 + result;
      v15 = *(_DWORD *)(a1 + 4LL * v14 + 8);
      *(_DWORD *)(a1 + 4 * v12 + 8) = v15;
      *(_DWORD *)(a1 + 4LL * v14 + 8) = v13;
      *(_BYTE *)a4 = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v13 + v15) + 8) ^ *(_BYTE *)v11;
      v16 = (unsigned __int8)(v4 + 2);
      v17 = *(_DWORD *)(a1 + 4 * v16 + 8);
      v18 = v17 + v14;
      v19 = *(_DWORD *)(a1 + 4LL * v18 + 8);
      *(_DWORD *)(a1 + 4 * v16 + 8) = v19;
      *(_DWORD *)(a1 + 4LL * v18 + 8) = v17;
      *(_BYTE *)(a4 + 1) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v17 + v19) + 8) ^ *(_BYTE *)(v11 + 1);
      v20 = (unsigned __int8)(v4 + 3);
      v21 = *(_DWORD *)(a1 + 4 * v20 + 8);
      v22 = v21 + v18;
      v23 = *(_DWORD *)(a1 + 4LL * v22 + 8);
      *(_DWORD *)(a1 + 4 * v20 + 8) = v23;
      *(_DWORD *)(a1 + 4LL * v22 + 8) = v21;
      *(_BYTE *)(a4 + 2) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v21 + v23) + 8) ^ *(_BYTE *)(v11 + 2);
      v24 = (unsigned __int8)(v4 + 4);
      v25 = *(_DWORD *)(a1 + 4 * v24 + 8);
      v26 = v25 + v22;
      v27 = *(_DWORD *)(a1 + 4LL * v26 + 8);
      *(_DWORD *)(a1 + 4 * v24 + 8) = v27;
      *(_DWORD *)(a1 + 4LL * v26 + 8) = v25;
      *(_BYTE *)(a4 + 3) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v25 + v27) + 8) ^ *(_BYTE *)(v11 + 3);
      v28 = (unsigned __int8)(v4 + 5);
      v29 = *(_DWORD *)(a1 + 4 * v28 + 8);
      v30 = v29 + v26;
      v31 = *(_DWORD *)(a1 + 4LL * v30 + 8);
      *(_DWORD *)(a1 + 4 * v28 + 8) = v31;
      *(_DWORD *)(a1 + 4LL * v30 + 8) = v29;
      *(_BYTE *)(a4 + 4) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v29 + v31) + 8) ^ *(_BYTE *)(v11 + 4);
      v32 = (unsigned __int8)(v4 + 6);
      v33 = *(_DWORD *)(a1 + 4 * v32 + 8);
      v34 = v33 + v30;
      v35 = *(_DWORD *)(a1 + 4LL * v34 + 8);
      *(_DWORD *)(a1 + 4 * v32 + 8) = v35;
      *(_DWORD *)(a1 + 4LL * v34 + 8) = v33;
      *(_BYTE *)(a4 + 5) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v33 + v35) + 8) ^ *(_BYTE *)(v11 + 5);
      v36 = (unsigned __int8)(v4 + 7);
      v37 = *(_DWORD *)(a1 + 4 * v36 + 8);
      v38 = v37 + v34;
      v39 = *(_DWORD *)(a1 + 4LL * v38 + 8);
      *(_DWORD *)(a1 + 4 * v36 + 8) = v39;
      *(_DWORD *)(a1 + 4LL * v38 + 8) = v37;
      *(_BYTE *)(a4 + 6) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v37 + v39) + 8) ^ *(_BYTE *)(v11 + 6);
      v4 = (unsigned __int8)(v4 + 8);
      LODWORD(v36) = *(_DWORD *)(a1 + 4 * v4 + 8);
      result = (unsigned __int8)(v36 + v38);
      v40 = *(_DWORD *)(a1 + 4 * result + 8);
      *(_DWORD *)(a1 + 4 * v4 + 8) = v40;
      *(_DWORD *)(a1 + 4 * result + 8) = v36;
      *(_BYTE *)(a4 + 7) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v36 + v40) + 8) ^ *(_BYTE *)(v11 + 7);
      v11 += 8LL;
      a4 += 8LL;
      ++v10;
    }
    while ( v10 );
    a3 += v9;
  }
  else
  {
    v8 = a4;
  }
  if ( a2 & 7 )
  {
    v41 = a2 & 7;
    v42 = 0LL;
    do
    {
      v43 = (unsigned __int8)(v4 + 1);
      v44 = *(_DWORD *)(a1 + 4 * v43 + 8);
      result = (unsigned int)(v44 + result);
      v45 = (unsigned __int8)result;
      v46 = *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8);
      *(_DWORD *)(a1 + 4 * v43 + 8) = v46;
      *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8) = v44;
      *(_BYTE *)(v8 + v42) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v44 + v46) + 8) ^ *(_BYTE *)(a3 + v42);
      if ( v41 - 1 == (_DWORD)v42 )
        break;
      v43 = (unsigned __int8)(v4 + 2);
      v47 = *(_DWORD *)(a1 + 4 * v43 + 8);
      result = (unsigned int)(v47 + result);
      v45 = (unsigned __int8)result;
      v48 = *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8);
      *(_DWORD *)(a1 + 4 * v43 + 8) = v48;
      *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8) = v47;
      *(_BYTE *)(v8 + v42 + 1) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v47 + v48) + 8) ^ *(_BYTE *)(a3 + v42 + 1);
      if ( v41 - 2 == (_DWORD)v42 )
        break;
      v43 = (unsigned __int8)(v4 + 3);
      v49 = *(_DWORD *)(a1 + 4 * v43 + 8);
      result = (unsigned int)(v49 + result);
      v45 = (unsigned __int8)result;
      v50 = *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8);
      *(_DWORD *)(a1 + 4 * v43 + 8) = v50;
      *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8) = v49;
      *(_BYTE *)(v8 + v42 + 2) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v49 + v50) + 8) ^ *(_BYTE *)(a3 + v42 + 2);
      if ( v41 - 3 == (_DWORD)v42 )
        break;
      v43 = (unsigned __int8)(v4 + 4);
      v51 = *(_DWORD *)(a1 + 4 * v43 + 8);
      result = (unsigned int)(v51 + result);
      v45 = (unsigned __int8)result;
      v52 = *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8);
      *(_DWORD *)(a1 + 4 * v43 + 8) = v52;
      *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8) = v51;
      *(_BYTE *)(v8 + v42 + 3) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v51 + v52) + 8) ^ *(_BYTE *)(a3 + v42 + 3);
      if ( v41 - 4 == (_DWORD)v42 )
        break;
      v43 = (unsigned __int8)(v4 + 5);
      v53 = *(_DWORD *)(a1 + 4 * v43 + 8);
      result = (unsigned int)(v53 + result);
      v45 = (unsigned __int8)result;
      v54 = *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8);
      *(_DWORD *)(a1 + 4 * v43 + 8) = v54;
      *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8) = v53;
      *(_BYTE *)(v8 + v42 + 4) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v53 + v54) + 8) ^ *(_BYTE *)(a3 + v42 + 4);
      if ( v41 - 5 == (_DWORD)v42 )
        break;
      v43 = (unsigned __int8)(v4 + 6);
      v55 = *(_DWORD *)(a1 + 4 * v43 + 8);
      result = (unsigned int)(v55 + result);
      v45 = (unsigned __int8)result;
      v56 = *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8);
      *(_DWORD *)(a1 + 4 * v43 + 8) = v56;
      *(_DWORD *)(a1 + 4LL * (unsigned __int8)result + 8) = v55;
      *(_BYTE *)(v8 + v42 + 5) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v55 + v56) + 8) ^ *(_BYTE *)(a3 + v42 + 5);
      if ( v41 - 6 == (_DWORD)v42 )
        break;
      v4 = (unsigned __int8)(v4 + 7);
      v57 = *(_DWORD *)(a1 + 4 * v4 + 8);
      result = (unsigned __int8)(v57 + result);
      LODWORD(v43) = *(_DWORD *)(a1 + 4 * result + 8);
      *(_DWORD *)(a1 + 4 * v4 + 8) = v43;
      *(_DWORD *)(a1 + 4 * result + 8) = v57;
      *(_BYTE *)(v8 + v42 + 6) = *(_BYTE *)(a1 + 4LL * (unsigned __int8)(v57 + v43) + 8) ^ *(_BYTE *)(a3 + v42 + 6);
      v42 += 7LL;
      v45 = result;
      LODWORD(v43) = v4;
    }
    while ( v41 != (_DWORD)v42 );
  }
  else
  {
    v45 = result;
    LODWORD(v43) = v4;
  }
  *(_DWORD *)a1 = v43;
  *(_DWORD *)(a1 + 4) = v45;
  return result;
}

//----- (0000000000037CD5) ----------------------------------------------------
__int64 __fastcall ccrng_system_init(__int64 a1)
{
  *(_QWORD *)a1 = get_kernel_entropy;
  return 0LL;
}

//----- (0000000000037CE7) ----------------------------------------------------
__int64 __fastcall get_kernel_entropy(__int64 a1, u_int a2, void *a3)
{
  read_random(a3, a2);
  return 0LL;
}

//----- (0000000000037CF7) ----------------------------------------------------
void ccrng_system_done()
{
  ;
}

//----- (0000000000037CFD) ----------------------------------------------------
__int64 __fastcall ccrng_rsafips_test_init(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, __int64 a7)
{
  *(_QWORD *)a1 = ccrng_rsafips_test_generate;
  *(_QWORD *)(a1 + 16) = 0LL;
  *(_QWORD *)(a1 + 24) = a2;
  *(_QWORD *)(a1 + 32) = a3;
  *(_QWORD *)(a1 + 40) = a4;
  *(_QWORD *)(a1 + 48) = a5;
  *(_QWORD *)(a1 + 56) = a6;
  *(_QWORD *)(a1 + 64) = a7;
  return 0LL;
}

//----- (0000000000037D33) ----------------------------------------------------
__int64 __fastcall ccrng_rsafips_test_generate(__int64 a1, unsigned __int64 a2, void *a3)
{
  __int64 v3; // rax@1
  void *v4; // r14@3
  signed __int64 v5; // rax@3
  signed __int64 v6; // rcx@3
  __int64 v7; // r12@7
  signed __int64 v8; // rax@7
  unsigned __int64 v9; // rbx@7
  signed int v11; // [sp+0h] [bp-2Ch]@3

  v3 = *(_QWORD *)(a1 + 16);
  if ( v3 == 2 )
  {
    v4 = a3;
    v11 = -1;
    v5 = a1 + 64;
    v6 = a1 + 56;
  }
  else if ( v3 == 1 )
  {
    v4 = a3;
    v11 = -1;
    v5 = a1 + 48;
    v6 = a1 + 40;
  }
  else
  {
    if ( v3 )
    {
      v11 = -1;
      return (unsigned int)v11;
    }
    v4 = a3;
    v11 = -1;
    v5 = a1 + 32;
    v6 = a1 + 24;
  }
  v7 = *(_QWORD *)v5;
  v8 = ccn_bitlen(*(_QWORD *)v6, *(_QWORD *)v5);
  v9 = (unsigned __int64)(v8 + 7) >> 3;
  if ( v9 <= a2 )
  {
    memcpy(v4, (const void *)v7, (unsigned __int64)(v8 + 7) >> 3);
    v11 = 0;
    memset((char *)v4 + v9, 0, a2 - v9);
    ++*(_QWORD *)(a1 + 16);
  }
  return (unsigned int)v11;
}

//----- (0000000000037DFB) ----------------------------------------------------
__int64 (*ccsha1_di())[4]
{
  __int64 v0; // rax@1
  bool v1; // zf@1
  __int64 (*result)[4]; // rax@1

  LODWORD(v0) = cpuid_features();
  v1 = (v0 & 0x20000000000uLL) >> 41 == 0;
  result = (__int64 (*)[4])ccsha1_vng_intel_NOSupplementalSSE3_di;
  if ( !v1 )
    result = off_69050[0];
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 69050: using guessed type __int64 (*off_69050[3])[4];
// 69B28: using guessed type __int64 ccsha1_vng_intel_NOSupplementalSSE3_di[4];

//----- (0000000000037E26) ----------------------------------------------------
__int64 __fastcall sha1_compress(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r8@1
  __int64 v4; // rax@3
  int v5; // er13@5
  int v6; // er12@5
  int v7; // er9@5
  int v8; // er10@5
  int v9; // er11@5
  __int64 v10; // rax@5
  int v11; // ebx@6
  __int64 v12; // rcx@7
  int v13; // eax@8
  int v14; // er11@8
  int v15; // er12@8
  int v16; // eax@8
  int v17; // er10@8
  int v18; // er13@8
  int v19; // eax@8
  int v20; // er9@8
  int v21; // eax@8
  int v22; // eax@8
  int v23; // eax@8
  signed __int64 v24; // r14@9
  int v25; // eax@10
  int v26; // er11@10
  int v27; // er12@10
  int v28; // eax@10
  int v29; // er10@10
  int v30; // er13@10
  int v31; // eax@10
  int v32; // er9@10
  int v33; // eax@10
  int v34; // eax@10
  int v35; // ebx@10
  signed __int64 v36; // r15@10
  int v37; // eax@11
  int v38; // er11@11
  int v39; // er12@11
  int v40; // eax@11
  int v41; // er10@11
  int v42; // er13@11
  int v43; // eax@11
  int v44; // er9@11
  int v45; // eax@11
  int v46; // eax@11
  signed __int64 v47; // rcx@11
  int v48; // eax@12
  int v49; // er11@12
  int v50; // er12@12
  int v51; // eax@12
  int v52; // er10@12
  int v53; // er13@12
  int v54; // eax@12
  int v55; // er9@12
  int v56; // eax@12
  int v57; // eax@12
  int v59; // [sp+0h] [bp-170h]@4
  int v60; // [sp+4h] [bp-16Ch]@8
  int v61; // [sp+8h] [bp-168h]@6
  int v62; // [sp+Ch] [bp-164h]@8
  int v63[4]; // [sp+10h] [bp-160h]@8
  int v64[5]; // [sp+20h] [bp-150h]@6
  int v65[3]; // [sp+34h] [bp-13Ch]@6
  int v66[64]; // [sp+40h] [bp-130h]@6
  __int64 v67; // [sp+140h] [bp-30h]@1

  v3 = off_69010[0];
  v67 = *(_QWORD *)off_69010[0];
  while ( a2 )
  {
    v4 = 0LL;
    do
    {
      *(int *)((char *)&v59 + v4 * 4) = _byteswap_ulong(*(_DWORD *)(a3 + v4 * 4));
      ++v4;
    }
    while ( v4 != 16 );
    v5 = *(_DWORD *)a1;
    v6 = *(_DWORD *)(a1 + 4);
    v7 = *(_DWORD *)(a1 + 8);
    v8 = *(_DWORD *)(a1 + 12);
    v9 = *(_DWORD *)(a1 + 16);
    v10 = 0LL;
    do
    {
      v11 = __ROL4__(*(&v59 + v10) ^ *(&v61 + v10) ^ v65[v10] ^ v64[v10], 1);
      v66[v10++] = v11;
    }
    while ( v10 != 64 );
    v12 = 0LL;
    do
    {
      v13 = __ROL4__(v5, 5);
      v14 = *(&v59 + v12) + v13 + v9 + (v8 ^ v6 & (v8 ^ v7)) + 1518500249;
      v15 = __ROL4__(v6, 30);
      v16 = __ROL4__(v14, 5);
      v17 = (v7 ^ v5 & (v7 ^ v15)) + *(&v60 + v12) + v8 + v16 + 1518500249;
      v18 = __ROL4__(v5, 30);
      v19 = __ROL4__(v17, 5);
      v20 = (v15 ^ v14 & (v15 ^ v18)) + *(&v61 + v12) + v7 + v19 + 1518500249;
      v9 = __ROL4__(v14, 30);
      v21 = __ROL4__(v20, 5);
      v6 = (v18 ^ v17 & (v18 ^ v9)) + *(&v62 + v12) + v15 + v21 + 1518500249;
      v8 = __ROL4__(v17, 30);
      v22 = __ROL4__(v6, 5);
      v23 = v63[v12] + v18 + v22;
      v12 += 5LL;
      v5 = (v9 ^ v20 & (v9 ^ v8)) + v23 + 1518500249;
      v7 = __ROL4__(v20, 30);
    }
    while ( (unsigned int)v12 < 0x14 );
    --a2;
    v24 = 20LL;
    do
    {
      v25 = __ROL4__(v5, 5);
      v26 = *(&v59 + v24) + v25 + v9 + (v6 ^ v8 ^ v7) + 1859775393;
      v27 = __ROL4__(v6, 30);
      v28 = __ROL4__(v26, 5);
      v29 = *(&v60 + v24) + v28 + v8 + (v27 ^ v7 ^ v5) + 1859775393;
      v30 = __ROL4__(v5, 30);
      v31 = __ROL4__(v29, 5);
      v32 = *(&v61 + v24) + v31 + v7 + (v30 ^ v26 ^ v27) + 1859775393;
      v9 = __ROL4__(v26, 30);
      v33 = __ROL4__(v32, 5);
      v6 = *(&v62 + v24) + v33 + v27 + (v9 ^ v30 ^ v29) + 1859775393;
      v8 = __ROL4__(v29, 30);
      v34 = __ROL4__(v6, 5);
      v35 = v63[v24];
      v24 += 5LL;
      v5 = v35 + v34 + v30 + (v8 ^ v32 ^ v9) + 1859775393;
      v7 = __ROL4__(v32, 30);
      v36 = 40LL;
    }
    while ( (unsigned int)v24 < 0x28 );
    do
    {
      v37 = __ROL4__(v5, 5);
      v38 = *(&v59 + v36) + v37 + v9 + (v7 & v6 | v8 & (v7 | v6)) - 1894007588;
      v39 = __ROL4__(v6, 30);
      v40 = __ROL4__(v38, 5);
      v41 = (v5 & v39 | v7 & (v5 | v39)) + *(&v60 + v36) + v8 + v40 - 1894007588;
      v42 = __ROL4__(v5, 30);
      v43 = __ROL4__(v41, 5);
      v44 = (v38 & v42 | v39 & (v38 | v42)) + *(&v61 + v36) + v7 + v43 - 1894007588;
      v9 = __ROL4__(v38, 30);
      v45 = __ROL4__(v44, 5);
      v6 = (v41 & v9 | v42 & (v41 | v9)) + *(&v62 + v36) + v39 + v45 - 1894007588;
      v8 = __ROL4__(v41, 30);
      v46 = __ROL4__(v6, 5);
      v5 = (v44 & v8 | v9 & (v44 | v8)) + v63[v36] + v42 + v46 - 1894007588;
      v7 = __ROL4__(v44, 30);
      v36 += 5LL;
      v47 = 60LL;
    }
    while ( (unsigned int)v36 < 0x3C );
    do
    {
      v48 = __ROL4__(v5, 5);
      v49 = *(&v59 + v47) + v48 + v9 + (v6 ^ v8 ^ v7) - 899497514;
      v50 = __ROL4__(v6, 30);
      v51 = __ROL4__(v49, 5);
      v52 = *(&v60 + v47) + v51 + v8 + (v50 ^ v7 ^ v5) - 899497514;
      v53 = __ROL4__(v5, 30);
      v54 = __ROL4__(v52, 5);
      v55 = *(&v61 + v47) + v54 + v7 + (v53 ^ v49 ^ v50) - 899497514;
      v9 = __ROL4__(v49, 30);
      v56 = __ROL4__(v55, 5);
      v6 = *(&v62 + v47) + v56 + v50 + (v9 ^ v53 ^ v52) - 899497514;
      v8 = __ROL4__(v52, 30);
      v57 = __ROL4__(v6, 5);
      v5 = v63[v47] + v57 + v53 + (v8 ^ v55 ^ v9) - 899497514;
      v7 = __ROL4__(v55, 30);
      v47 += 5LL;
    }
    while ( (unsigned int)v47 < 0x50 );
    *(_DWORD *)a1 += v5;
    *(_DWORD *)(a1 + 4) += v6;
    *(_DWORD *)(a1 + 8) += v7;
    *(_DWORD *)(a1 + 12) += v8;
    *(_DWORD *)(a1 + 16) += v9;
    a3 += 64LL;
  }
  return *(_QWORD *)v3;
}
// 69010: using guessed type __int64 off_69010[2];
// 37E26: using guessed type int var_150[5];
// 37E26: using guessed type int var_13C[3];
// 37E26: using guessed type int var_130[64];
// 37E26: using guessed type int var_160[4];

//----- (0000000000038281) ----------------------------------------------------
__int64 __fastcall ccrsa_verify_pkcs1v15(__int64 *a1, __int64 a2, size_t a3, const void *a4, unsigned __int64 a5, unsigned __int64 a6, __int64 a7)
{
  unsigned __int64 v7; // r13@1
  __int64 v8; // rbx@1
  __int64 v9; // r15@1
  unsigned __int64 v10; // rax@1
  signed int v11; // ecx@1
  char *v12; // r12@2
  unsigned __int64 v13; // rax@2
  size_t v14; // rbx@2
  __int64 result; // rax@7
  unsigned __int64 v16; // [sp+0h] [bp-50h]@1
  size_t v17; // [sp+8h] [bp-48h]@1
  __int64 v18; // [sp+10h] [bp-40h]@1
  const void *v19; // [sp+18h] [bp-38h]@1
  __int64 v20; // [sp+20h] [bp-30h]@1
  __int64 v21; // [sp+28h] [bp-28h]@2

  v16 = a6;
  v7 = a5;
  v19 = a4;
  v17 = a3;
  v18 = a2;
  v8 = off_69010[0];
  v20 = *(_QWORD *)off_69010[0];
  v9 = *a1;
  v10 = ccn_write_uint_size(*a1, (__int64)(a1 + 2));
  *(_BYTE *)a7 = 0;
  v11 = -2;
  if ( v10 == v7 )
  {
    ccn_read_uint(v9, (__int64)((char *)&v16 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL)), v7, v16);
    ccrsa_pub_crypt(
      a1,
      (__int64)((char *)&v16 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL)),
      (__int64)((char *)&v16 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL)));
    v12 = (char *)&v21 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0LL);
    v13 = ccn_write_uint_size(v9, (__int64)((char *)&v16 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL)));
    v14 = v7 - v13;
    if ( v7 <= v13 )
      v14 = 0LL;
    bzero(v12, v14);
    ccn_write_uint(v9, (__int64)((char *)&v16 - ((8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL)), v7 - v14, (__int64)&v12[v14]);
    if ( !(unsigned int)ccrsa_emsa_pkcs1v15_verify(v7, (__int64)v12, v17, v19, v18) )
      *(_BYTE *)a7 = 1;
    v8 = off_69010[0];
    v11 = 0;
  }
  result = *(_QWORD *)v8;
  if ( *(_QWORD *)v8 == v20 )
    result = (unsigned int)v11;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000383A5) ----------------------------------------------------
__int64 __fastcall ccmac_generate_subkeys(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // r15@1
  __int64 v6; // rax@1
  char v7; // cl@1
  signed __int64 v8; // rax@1
  unsigned __int8 v9; // bl@2
  signed __int64 v10; // rax@3
  char v11; // cl@4
  unsigned __int8 v12; // bl@7
  signed __int64 v13; // rax@8
  char v15[16]; // [sp+0h] [bp-40h]@1
  __int64 v16; // [sp+10h] [bp-30h]@1

  v4 = a4;
  v5 = a3;
  v16 = *(_QWORD *)off_69010[0];
  bzero(v15, 0x10uLL);
  ccecb_one_shot(v6, (__int64)v15, (__int64)v15, a1, a2);
  v7 = 0;
  v8 = 15LL;
  do
  {
    v9 = v15[v8];
    *(_BYTE *)(v5 + v8) = v7 | 2 * v15[v8];
    --v8;
    v7 = v9 >> 7;
  }
  while ( v8 != -1 );
  v10 = 15LL;
  if ( v15[0] < 0 )
  {
    do
    {
      *(_BYTE *)(v5 + v10) ^= *((_BYTE *)&sl_test_xor_RB + v10);
      --v10;
    }
    while ( v10 != -1 );
    v11 = 0;
    v10 = 15LL;
  }
  else
  {
    v11 = 0;
  }
  do
  {
    v12 = *(_BYTE *)(v5 + v10);
    *(_BYTE *)(v4 + v10) = v11 | 2 * *(_BYTE *)(v5 + v10);
    --v10;
    v11 = v12 >> 7;
  }
  while ( v10 != -1 );
  v13 = 15LL;
  if ( *(_BYTE *)v5 < 0 )
  {
    do
    {
      *(_BYTE *)(v4 + v13) ^= *((_BYTE *)&sl_test_xor_RB + v13);
      --v13;
    }
    while ( v13 != -1 );
  }
  bzero(v15, 0x10uLL);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];
// 6B570: using guessed type __int64 sl_test_xor_RB;
// 383A5: using guessed type char var_40[16];

//----- (00000000000384B2) ----------------------------------------------------
__int64 __usercall ccecb_one_shot@<rax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rcx>, __int64 a4@<rdi>, __int64 a5@<rsi>)
{
  __int64 v5; // r14@1
  __int64 v6; // r15@1
  __int64 v7; // r13@1
  char *v8; // rbx@1
  __int64 v10; // [sp+0h] [bp-30h]@1

  v10 = a1;
  v5 = a3;
  v6 = a2;
  v7 = off_69010[0];
  v10 = *(_QWORD *)off_69010[0];
  v8 = (char *)&v10 - ((*(_QWORD *)a4 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  (*(void (__fastcall **)(__int64, char *, signed __int64, __int64))(a4 + 16))(a4, v8, 16LL, a5);
  (*(void (__fastcall **)(char *, signed __int64, __int64, __int64))(a4 + 24))(v8, 1LL, v6, v5);
  cc_clear(*(_QWORD *)a4, v8);
  return *(_QWORD *)v7;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000003853D) ----------------------------------------------------
__int64 *ccaes_gcm_encrypt_mode()
{
  __int64 (*v0)[2]; // rax@1

  v0 = ccaes_ecb_encrypt_mode();
  gcm_aes_encrypt = (((*v0)[0] + 7) & 0xFFFFFFFFFFFFFFF8LL) + 5 * (((*v0)[1] + 7) & 0xFFFFFFFFFFFFFFF8LL) + 384;
  qword_6BC30 = 1LL;
  qword_6BC38 = (__int64)ccmode_gcm_init;
  qword_6BC40 = (__int64)ccmode_gcm_set_iv;
  qword_6BC48 = (__int64)ccmode_gcm_gmac;
  qword_6BC50 = (__int64)ccmode_gcm_encrypt;
  qword_6BC58 = (__int64)ccmode_gcm_finalize;
  qword_6BC60 = (__int64)ccmode_gcm_reset;
  qword_6BC68 = (__int64)v0;
  return &gcm_aes_encrypt;
}
// 23252: using guessed type __int64 __fastcall ccmode_gcm_encrypt();
// 6BC28: using guessed type __int64 gcm_aes_encrypt;
// 6BC30: using guessed type __int64 qword_6BC30;
// 6BC38: using guessed type __int64 qword_6BC38;
// 6BC40: using guessed type __int64 qword_6BC40;
// 6BC48: using guessed type __int64 qword_6BC48;
// 6BC50: using guessed type __int64 qword_6BC50;
// 6BC58: using guessed type __int64 qword_6BC58;
// 6BC60: using guessed type __int64 qword_6BC60;
// 6BC68: using guessed type __int64 qword_6BC68;

//----- (00000000000385E2) ----------------------------------------------------
__int64 __fastcall ccmode_ccm_macdata(__int64 a1, __int64 a2, int a3, unsigned __int64 a4, __int64 a5)
{
  unsigned __int64 v5; // r12@1
  __int64 v6; // r14@1
  unsigned int v7; // edx@2
  unsigned __int64 v8; // r15@5
  unsigned __int64 v9; // rax@6
  __int64 v10; // rcx@8
  unsigned __int64 v11; // rdx@9
  signed __int64 v12; // rdx@11
  __int64 result; // rax@18
  __int64 v14; // [sp+8h] [bp-58h]@5
  unsigned __int64 v15; // [sp+28h] [bp-38h]@5
  __int64 v16; // [sp+30h] [bp-30h]@1
  signed __int64 v17; // [sp+30h] [bp-30h]@5

  v16 = a5;
  v5 = a4;
  v6 = *(_QWORD *)(*(_QWORD *)a1 + 8LL);
  if ( a3 )
  {
    (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(*(_QWORD *)a1 + 24LL))(
      a1 + 8,
      1LL,
      a2 + 16,
      a2 + 16);
    *(_DWORD *)(a2 + 72) = 0;
    v7 = 0;
  }
  else
  {
    v7 = *(_DWORD *)(a2 + 72);
  }
  if ( v5 )
  {
    v15 = ~v5;
    v17 = v16 - 2;
    v14 = a2;
    v8 = 0LL;
    do
    {
      v9 = v5 - v8;
      if ( v5 - v8 >= (unsigned int)v6 - v7 )
        v9 = (unsigned int)v6 - v7;
      v10 = v7;
      if ( v9 )
      {
        v11 = ~(unsigned __int64)((unsigned int)v6 - v7);
        if ( v15 + v8 > v11 )
          v11 = v15 + v8;
        v12 = -(signed __int64)v11;
        do
        {
          *(_BYTE *)(a2 + 14 + v10 + v12) ^= *(_BYTE *)(v17 + v8 + v12);
          --v12;
        }
        while ( v12 != 1 );
      }
      v8 += v9;
      v7 = ((signed int)v10 + (signed int)v9) % (unsigned int)v6;
      if ( !v7 )
      {
        (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(*(_QWORD *)a1 + 24LL))(
          a1 + 8,
          1LL,
          a2 + 16,
          a2 + 16);
        v7 = 0;
      }
    }
    while ( v8 < v5 );
  }
  else
  {
    v14 = a2;
  }
  result = v14;
  *(_DWORD *)(v14 + 72) = v7;
  return result;
}

//----- (00000000000386FF) ----------------------------------------------------
__int64 __fastcall ccmode_ccm_cbcmac(__int64 a1, __int64 a2, unsigned __int64 a3, __int64 a4)
{
  __int64 result; // rax@1

  result = a4;
  if ( *(_DWORD *)(a2 + 64) == 1 )
    result = ccmode_ccm_macdata(a1, a2, 0, a3, a4);
  return result;
}

//----- (000000000003871C) ----------------------------------------------------
__int64 __fastcall ccrsa_encrypt_eme_pkcs1v15(__int64 *a1, __int64 a2, __int64 a3, void *a4, size_t a5, const void *a6)
{
  __int64 v6; // r15@1
  __int64 v7; // rbx@1
  __int64 v8; // rax@1
  __int64 v9; // r12@1
  signed int v10; // er14@1
  __int64 v11; // r14@2
  int v12; // eax@2
  __int64 v13; // rcx@2
  __int64 v14; // r13@3
  unsigned __int64 v15; // rax@3
  size_t v16; // r15@3
  void *v17; // rbx@5
  __int64 result; // rax@6
  __int64 v19; // [sp+0h] [bp-60h]@1
  __int64 v20; // [sp+8h] [bp-58h]@3
  void *v21; // [sp+10h] [bp-50h]@1
  __int64 v22; // [sp+18h] [bp-48h]@1
  size_t v23; // [sp+20h] [bp-40h]@1
  const void *v24; // [sp+28h] [bp-38h]@1
  __int64 v25; // [sp+30h] [bp-30h]@1

  v24 = a6;
  v23 = a5;
  v21 = a4;
  v6 = a3;
  v22 = a2;
  v25 = *(_QWORD *)off_69010[0];
  v7 = *a1;
  v8 = ccn_write_uint_size(*a1, (__int64)(a1 + 2));
  v9 = (__int64)((char *)&v19 - ((8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v10 = -2;
  if ( *(_QWORD *)v6 >= (unsigned __int64)v8 )
  {
    *(_QWORD *)v6 = v8;
    v11 = v8;
    v12 = ccrsa_eme_pkcs1v15_encode(v22, v8, (char *)&v19 - ((8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL), v23, v24);
    v13 = v11;
    v10 = v12;
    if ( !v12 )
    {
      v14 = v13;
      ccrsa_pub_crypt(
        a1,
        (__int64)((char *)&v19 - ((8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL)),
        (__int64)((char *)&v19 - ((8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL)));
      v20 = v7;
      v15 = ccn_write_uint_size(v7, (__int64)((char *)&v19 - ((8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL)));
      v10 = 0;
      v16 = v14 - v15;
      if ( v14 <= v15 )
        v16 = 0LL;
      v17 = v21;
      bzero(v21, v16);
      ccn_write_uint(v20, v9, v14 - v16, (__int64)((char *)v17 + v16));
    }
  }
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v25 )
    result = (unsigned int)v10;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000038824) ----------------------------------------------------
int __fastcall ccec_generate_key_fips(__int64 *a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r14@1
  int result; // eax@1
  bool v5; // zf@2

  v3 = a3;
  result = ccec_generate_key_internal_fips(a1, a2, a3);
  if ( !result )
  {
    v5 = ccec_pairwise_consistency_check(v3, a2) == 0;
    result = -13;
    if ( !v5 )
      result = 0;
  }
  return result;
}

//----- (0000000000038856) ----------------------------------------------------
__int64 __usercall ccmode_gcm_gf_mult@<rax>(__m128i *a1@<rdx>, const __m128i *a2@<rdi>, const __m128i *a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>)
{
  __m128i *v7; // r14@1
  __int64 v8; // r12@1
  __int64 v9; // rax@1
  __int64 v10; // rax@2
  __m128i v11; // xmm4@2
  unsigned __int64 v12; // rax@4
  unsigned __int64 v15; // rax@6
  __int64 v16; // rcx@6
  unsigned __int64 v17; // rdx@7
  signed __int64 v18; // rax@8
  __int64 v19; // r8@9
  __int64 v20; // r9@9
  __int64 v21; // rsi@9
  __int64 v22; // rbx@9
  __int64 v23; // rdi@9
  __int64 v24; // rdx@9
  __int64 v25; // rcx@9
  __int64 v26; // rbx@9
  signed int v27; // er8@10
  __int64 v28; // r9@10
  __int64 v29; // rsi@11
  char *v30; // rdi@12
  __int64 v31; // rbx@12
  __int64 v32; // rcx@15
  __int64 v33; // rsi@15
  __int64 v34; // rdx@16
  bool v35; // zf@17
  bool v36; // sf@17
  char v40; // bl@19
  signed __int64 v41; // rcx@19
  __int64 v42; // rax@19
  __int64 v43; // rsi@20
  char v44; // dl@20
  char v46; // [sp+0h] [bp-210h]@20
  char v47[15]; // [sp+1h] [bp-20Fh]@20
  char v48[16]; // [sp+10h] [bp-200h]@18
  char v49; // [sp+20h] [bp-1F0h]@19
  __int64 v50[2]; // [sp+30h] [bp-1E0h]@5
  __int64 v51[4]; // [sp+40h] [bp-1D0h]@10
  char v52[24]; // [sp+60h] [bp-1B0h]@4
  unsigned __int64 v53; // [sp+78h] [bp-198h]@6
  __int64 v54[2]; // [sp+80h] [bp-190h]@7
  unsigned __int64 v55; // [sp+90h] [bp-180h]@6
  __int64 v56[2]; // [sp+98h] [bp-178h]@7
  __int64 v57[3]; // [sp+A8h] [bp-168h]@9
  unsigned __int64 v58; // [sp+C0h] [bp-150h]@6
  __int64 v59[2]; // [sp+C8h] [bp-148h]@7
  __int64 v60[3]; // [sp+D8h] [bp-138h]@9
  __int64 v61[3]; // [sp+F0h] [bp-120h]@9
  __int64 v62[3]; // [sp+108h] [bp-108h]@9
  unsigned __int64 v63; // [sp+120h] [bp-F0h]@4
  __int64 v64[2]; // [sp+128h] [bp-E8h]@7
  __int64 v65[3]; // [sp+138h] [bp-D8h]@9
  __int64 v66[3]; // [sp+150h] [bp-C0h]@9
  __int64 v67[3]; // [sp+168h] [bp-A8h]@9
  __int64 v68[3]; // [sp+180h] [bp-90h]@9
  __int64 v69[3]; // [sp+198h] [bp-78h]@9
  __int64 v70[3]; // [sp+1B0h] [bp-60h]@9
  __int64 v71[3]; // [sp+1C8h] [bp-48h]@9
  __int64 v72; // [sp+1E0h] [bp-30h]@9
  __int64 v73; // [sp+1E8h] [bp-28h]@1

  v7 = a1;
  v8 = off_69010[0];
  v73 = *(_QWORD *)off_69010[0];
  LODWORD(v9) = cpuid_features();
  if ( _bittest((const unsigned __int64 *)&v9, 0x39u)
    && (LODWORD(v10) = cpuid_features(), _bittest((const unsigned __int64 *)&v10, 0x29u)) )
  {
    gcm_gmult(v7, a3, a2, a4, a5, a6, a7, v11);
  }
  else
  {
    bzero(v52, 0x18uLL);
    bzero(&v63, 0x18uLL);
    v12 = 0LL;
    do
    {
      _RCX = a2->m128i_i64[v12];
      __asm { bswap   rcx }
      *(unsigned __int64 *)((char *)&v63 + v12 * 8) = _RCX;
      v50[v12] = ((unsigned __int64)BYTE7(a3->m128i_i64[v12]) << 56) | ((unsigned __int64)BYTE6(a3->m128i_i64[v12]) << 48) | ((unsigned __int64)BYTE5(a3->m128i_i64[v12]) << 40) | ((unsigned __int64)BYTE4(a3->m128i_i64[v12]) << 32) | ((unsigned __int64)BYTE3(a3->m128i_i64[v12]) << 24) | ((unsigned __int64)BYTE2(a3->m128i_i64[v12]) << 16) | ((unsigned __int64)BYTE1(a3->m128i_i64[v12]) << 8) | LOBYTE(a3->m128i_i64[v12]);
      ++v12;
    }
    while ( v12 != 2 );
    v15 = v63;
    v58 = v63 >> 1;
    v55 = v63 >> 2;
    v53 = v63 >> 3;
    v16 = 0LL;
    do
    {
      v17 = v64[v16];
      v59[v16] = (v15 << 63) | (v17 >> 1);
      v56[v16] = (v15 << 62) | (v17 >> 2);
      v54[v16++] = (v15 << 61) | (v17 >> 3);
      v15 = v17;
    }
    while ( v16 != 2 );
    v18 = -3LL;
    do
    {
      v19 = v65[v18];
      v20 = v60[v18];
      v21 = v19 ^ v60[v18];
      v69[v18] = v21;
      v22 = v57[v18];
      v67[v18] = v19 ^ v57[v18];
      v23 = v20 ^ v22;
      v62[v18] = v20 ^ v22;
      v24 = *(&v55 + v18);
      v66[v18] = v19 ^ *(&v55 + v18);
      v61[v18] = v24 ^ v20;
      v25 = v24 ^ v22;
      *(&v58 + v18) = v24 ^ v22;
      v26 = v21 ^ v22;
      v71[v18] = v26;
      v70[v18] = v24 ^ v21;
      v68[v18] = v19 ^ v25;
      *(&v63 + v18) = v24 ^ v23;
      *(&v72 + v18++) = v24 ^ v26;
    }
    while ( v18 );
    bzero(v51, 0x20uLL);
    v27 = 15;
    v28 = 0LL;
    do
    {
      v29 = 0LL;
      do
      {
        v30 = &v52[24 * (((unsigned __int64)v50[v29] >> (4 * v27 ^ 4u)) & 0xF)];
        v31 = 0LL;
        do
        {
          *(&v51[v31] + v29) ^= *(_QWORD *)&v30[v31 * 8];
          ++v31;
        }
        while ( v31 != 3 );
        ++v29;
      }
      while ( v29 != 2 );
      v32 = 0LL;
      v33 = 0LL;
      if ( !v27 )
        break;
      do
      {
        v34 = v51[v32] << 60;
        v51[v32] = v33 | ((unsigned __int64)v51[v32] >> 4);
        ++v32;
        v33 = v34;
      }
      while ( v32 != 4 );
      v35 = v27 == 0;
      v36 = v27-- < 0;
    }
    while ( !v36 && !v35 );
    do
    {
      _RAX = v51[v28];
      __asm { bswap   rax }
      *(_QWORD *)&v48[v28 * 8] = _RAX;
      __asm { bswap   rax }
      ++v28;
    }
    while ( v28 != 4 );
    v40 = v49;
    v41 = 31LL;
    v42 = 0LL;
    do
    {
      v43 = (unsigned __int8)v48[v41];
      v44 = *((_BYTE *)gcm_shift_table + 2 * v43) ^ *(&v46 + v41);
      *(&v46 + v41) = v44;
      v47[v41--] = *((_BYTE *)gcm_shift_table + 2 * v43 + 1) ^ v40;
      v40 = v44;
    }
    while ( (signed int)v41 > 15 );
    do
    {
      *((_BYTE *)v7->m128i_i64 + v42) = v48[v42];
      ++v42;
    }
    while ( v42 != 16 );
  }
  return *(_QWORD *)v8;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 66C90: using guessed type __int64 gcm_shift_table[64];
// 69010: using guessed type __int64 off_69010[2];
// 38856: using guessed type char var_1B0[24];
// 38856: using guessed type __int64 var_1E0[2];
// 38856: using guessed type __int64 var_E8[2];
// 38856: using guessed type __int64 var_148[2];
// 38856: using guessed type __int64 var_178[2];
// 38856: using guessed type __int64 var_190[2];
// 38856: using guessed type __int64 var_D8[3];
// 38856: using guessed type __int64 var_138[3];
// 38856: using guessed type __int64 var_78[3];
// 38856: using guessed type __int64 var_168[3];
// 38856: using guessed type __int64 var_A8[3];
// 38856: using guessed type __int64 var_108[3];
// 38856: using guessed type __int64 var_C0[3];
// 38856: using guessed type __int64 var_120[3];
// 38856: using guessed type __int64 var_48[3];
// 38856: using guessed type __int64 var_60[3];
// 38856: using guessed type __int64 var_90[3];
// 38856: using guessed type __int64 var_1D0[4];
// 38856: using guessed type char var_200[16];
// 38856: using guessed type char var_20F[15];

//----- (0000000000038BB2) ----------------------------------------------------
int __fastcall ccec_compact_generate_key(__int64 *a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r15@1
  __int64 v4; // rbx@1
  int result; // eax@1
  __int64 v6; // r12@2
  char *v7; // r13@2
  __int64 v8; // rbx@2
  __int64 v9; // rbx@3
  __int64 v10; // rcx@5
  __int64 v11; // [sp+0h] [bp-50h]@2
  __int64 v12; // [sp+8h] [bp-48h]@3
  __int64 *v13; // [sp+10h] [bp-40h]@2
  __int64 v14; // [sp+18h] [bp-38h]@2
  __int64 v15; // [sp+20h] [bp-30h]@1

  v3 = a3;
  v4 = off_69010[0];
  v15 = *(_QWORD *)off_69010[0];
  result = ccec_generate_key_internal_fips(a1, a2, a3);
  if ( !result )
  {
    v14 = a2;
    v13 = &v11;
    v6 = *a1;
    v7 = (char *)&v11 - ((8 * *a1 + 15) & 0xFFFFFFFFFFFFFFF0LL);
    ccn_sub(*a1, (__int64)v7, (__int64)(a1 + 2), v3 + 8LL * **(_QWORD **)v3 + 16);
    v8 = v3 + 8LL * **(_QWORD **)v3 + 16;
    if ( (signed int)ccn_cmp(v6, (__int64)v7, v8) < 0 )
    {
      v12 = v3 + 16;
      ccn_set(v6, (void *)v8, v7);
      v9 = v12;
      ccn_sub(v6, (__int64)v7, (__int64)&a1[4 * v6 + 4], v12 + 24LL * **(_QWORD **)v3);
      ccn_set(v6, (void *)(v9 + 24LL * **(_QWORD **)v3), v7);
    }
    result = (unsigned __int8)~ccec_pairwise_consistency_check(v3, v14) << 31 >> 31;
    v4 = off_69010[0];
  }
  v10 = *(_QWORD *)v4;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000038CE0) ----------------------------------------------------
__int64 __fastcall ccsha1_vng_intel_compress_SupplementalSSE3(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r9@1
  __int64 v4; // r10@1
  __int64 v5; // r11@1
  __m128i v6; // xmm10@1
  int v7; // ecx@1
  int v8; // esi@1
  int v9; // edi@1
  int v10; // ebp@1
  int v11; // edx@1
  __m128i v12; // xmm2@1
  __m128i v13; // xmm9@1
  __m128i v14; // xmm8@1
  __m128i v15; // xmm7@1
  int v16; // eax@2
  int v17; // esi@2
  int v18; // ebx@2
  __m128i v19; // xmm0@2
  __m128i v20; // xmm1@2
  __int32 v21; // ebx@2
  __int32 v22; // edx@2
  int v23; // ebp@2
  int v24; // ecx@2
  int v25; // eax@2
  int v26; // esi@2
  int v27; // ebx@2
  int v28; // edi@2
  __m128i v29; // xmm6@2
  __m128i v30; // ST00_16@2
  int v31; // ebx@2
  int v32; // edi@2
  int v33; // esi@2
  int v34; // eax@2
  int v35; // ebx@2
  __m128i v36; // xmm0@2
  __m128i v37; // xmm1@2
  __int32 v38; // ebx@2
  __int32 v39; // ecx@2
  int v40; // edx@2
  int v41; // eax@2
  int v42; // edi@2
  int v43; // ebx@2
  int v44; // ebp@2
  __m128i v45; // xmm5@2
  __m128i v46; // ST10_16@2
  int v47; // ebx@2
  int v48; // ebp@2
  int v49; // edi@2
  int v50; // eax@2
  int v51; // ebx@2
  __m128i v52; // xmm0@2
  __m128i v53; // xmm1@2
  __int32 v54; // ebx@2
  __int32 v55; // esi@2
  int v56; // ecx@2
  int v57; // eax@2
  int v58; // ebp@2
  int v59; // ebx@2
  int v60; // edx@2
  __m128i v61; // xmm4@2
  __m128i v62; // ST20_16@2
  int v63; // ebx@2
  int v64; // edx@2
  int v65; // ebp@2
  int v66; // eax@2
  int v67; // ebx@2
  __m128i v68; // xmm0@2
  __m128i v69; // xmm1@2
  __int32 v70; // ebx@2
  __int32 v71; // edi@2
  int v72; // esi@2
  int v73; // eax@2
  int v74; // edx@2
  int v75; // ebx@2
  int v76; // ecx@2
  __m128i v77; // xmm3@2
  __m128i v78; // ST30_16@2
  int v79; // ebx@2
  int v80; // ecx@2
  int v81; // edx@2
  int v82; // eax@2
  int v83; // ebx@2
  __m128i v84; // xmm0@2
  __int32 v85; // ebx@2
  __int32 v86; // ebp@2
  int v87; // edi@2
  int v88; // eax@2
  int v89; // ecx@2
  int v90; // ebx@2
  int v91; // esi@2
  __m128i v92; // xmm2@2
  __m128i v93; // ST00_16@2
  int v94; // ebx@2
  int v95; // esi@2
  int v96; // ecx@2
  int v97; // eax@2
  int v98; // ebx@2
  __m128i v99; // xmm0@2
  __int32 v100; // ebx@2
  __int32 v101; // edx@2
  int v102; // ebp@2
  int v103; // eax@2
  int v104; // esi@2
  int v105; // ebx@2
  int v106; // edi@2
  __m128i v107; // xmm9@2
  __m128i v108; // ST10_16@2
  int v109; // ebx@2
  int v110; // edi@2
  int v111; // esi@2
  int v112; // eax@2
  int v113; // ebx@2
  __m128i v114; // xmm0@2
  __int32 v115; // ebx@2
  __int32 v116; // ecx@2
  int v117; // edx@2
  int v118; // eax@2
  int v119; // edi@2
  int v120; // ebx@2
  int v121; // ebp@2
  __m128i v122; // xmm8@2
  __m128i v123; // ST20_16@2
  int v124; // ebx@2
  int v125; // ebp@2
  int v126; // edi@2
  int v127; // eax@2
  int v128; // ebx@2
  __m128i v129; // xmm0@2
  __int32 v130; // ebx@2
  __int32 v131; // esi@2
  int v132; // ecx@2
  int v133; // eax@2
  int v134; // ebp@2
  int v135; // ebx@2
  int v136; // edx@2
  __m128i v137; // xmm7@2
  __m128i v138; // ST30_16@2
  int v139; // ebx@2
  int v140; // edx@2
  int v141; // ebp@2
  int v142; // eax@2
  int v143; // ebx@2
  __m128i v144; // xmm0@2
  __int32 v145; // ebx@2
  __int32 v146; // edi@2
  int v147; // esi@2
  int v148; // eax@2
  int v149; // edx@2
  int v150; // ebx@2
  int v151; // ecx@2
  __m128i v152; // xmm6@2
  __m128i v153; // ST00_16@2
  int v154; // ebx@2
  int v155; // ecx@2
  int v156; // edx@2
  int v157; // eax@2
  int v158; // ebx@2
  __m128i v159; // xmm0@2
  __int32 v160; // ebx@2
  __int32 v161; // ebp@2
  int v162; // edi@2
  int v163; // eax@2
  int v164; // ecx@2
  int v165; // ebx@2
  int v166; // esi@2
  __m128i v167; // xmm5@2
  __m128i v168; // ST10_16@2
  int v169; // ebx@2
  int v170; // esi@2
  int v171; // ecx@2
  int v172; // eax@2
  int v173; // ebx@2
  __m128i v174; // xmm0@2
  __int32 v175; // ebx@2
  __int32 v176; // edx@2
  int v177; // ebp@2
  int v178; // eax@2
  int v179; // esi@2
  int v180; // ebx@2
  int v181; // edi@2
  __m128i v182; // xmm4@2
  __m128i v183; // ST20_16@2
  int v184; // ebx@2
  int v185; // edi@2
  int v186; // esi@2
  int v187; // eax@2
  int v188; // ebx@2
  __m128i v189; // xmm0@2
  __int32 v190; // ebx@2
  __int32 v191; // ecx@2
  int v192; // edx@2
  int v193; // eax@2
  int v194; // edi@2
  int v195; // ebx@2
  int v196; // ebp@2
  __m128i v197; // xmm3@2
  __m128i v198; // ST30_16@2
  int v199; // ebx@2
  int v200; // ebp@2
  int v201; // edi@2
  int v202; // eax@2
  int v203; // ebx@2
  __m128i v204; // xmm0@2
  __int32 v205; // ebx@2
  __int32 v206; // esi@2
  int v207; // ecx@2
  int v208; // eax@2
  int v209; // ebp@2
  int v210; // ebx@2
  int v211; // edx@2
  __m128i v212; // xmm2@2
  int v213; // ebx@2
  int v214; // edx@2
  int v215; // ebp@2
  int v216; // eax@2
  int v217; // ebx@2
  __m128i v218; // xmm0@2
  __int32 v219; // ebx@2
  __int32 v220; // edi@2
  int v221; // esi@2
  int v222; // eax@2
  int v223; // edx@2
  int v224; // ebx@2
  int v225; // ecx@2
  __m128i v226; // xmm9@2
  int v227; // ebx@2
  int v228; // ecx@2
  int v229; // edx@2
  int v230; // eax@2
  int v231; // ebx@2
  __m128i v232; // xmm0@2
  __int32 v233; // ebx@2
  __int32 v234; // ebp@2
  int v235; // edi@2
  __m128i v236; // xmm0@2
  int v237; // eax@2
  int v238; // ecx@2
  int v239; // ebx@2
  int v240; // esi@2
  int v241; // ebx@2
  int v242; // esi@2
  int v243; // ecx@2
  int v244; // eax@2
  int v245; // ebx@2
  __m128i v246; // xmm0@2
  __int32 v247; // ebx@2
  __int32 v248; // edx@2
  int v249; // ebp@2
  int v250; // ecx@2
  int v251; // eax@2
  int v252; // edx@2
  int v253; // esi@2
  int v254; // ebx@2
  int v255; // edi@2
  int v256; // ebx@2
  int v257; // edi@2
  int v258; // esi@2
  int v259; // ebp@2
  bool v260; // cf@2
  int v261; // eax@3
  int v262; // edi@3
  int v263; // ebx@3
  __int32 v264; // ebx@3
  __int32 v265; // ecx@3
  int v266; // edx@3
  int v267; // esi@3
  int v268; // eax@3
  int v269; // edi@3
  int v270; // ebx@3
  int v271; // ebp@3
  int v272; // ebx@3
  int v273; // ebp@3
  int v274; // edi@3
  int v275; // eax@3
  int v276; // ebx@3
  __int32 v277; // ebx@3
  __int32 v278; // esi@3
  int v279; // ecx@3
  int v280; // eax@3
  int v281; // ebp@3
  int v282; // ebx@3
  int v283; // edx@3
  int v284; // ebx@3
  int v285; // edx@3
  int v286; // ebp@3
  int v287; // eax@3
  int v288; // ebx@3
  __int32 v289; // ebx@3
  __int32 v290; // edi@3
  int v291; // esi@3
  int v292; // eax@3
  int v293; // edx@3
  int v294; // ebx@3
  int v295; // ecx@3
  int v296; // ebx@3
  int v297; // ecx@3
  int v298; // edx@3
  int v299; // eax@3
  int v300; // ebx@3
  __int32 v301; // ebx@3
  __int32 v302; // ebp@3
  int v303; // edi@3
  int v304; // eax@3
  int v305; // ecx@3
  int v306; // ebx@3
  int v307; // esi@3
  int v308; // ebx@3
  int v309; // esi@3
  int v310; // eax@3
  int v311; // eax@4
  int v312; // edi@4
  int v313; // ebx@4
  __int32 v314; // ebx@4
  __int32 v315; // ecx@4
  int v316; // edx@4
  int v317; // esi@4
  int v318; // eax@4
  int v319; // ebx@4
  int v320; // ebx@4
  int v321; // ebp@4
  int v322; // edi@4
  int v323; // eax@4
  int v324; // ebx@4
  __int32 v325; // ebx@4
  __int32 v326; // esi@4
  int v327; // ecx@4
  int v328; // eax@4
  int v329; // ebx@4
  int v330; // ebx@4
  int v331; // edx@4
  int v332; // ebp@4
  int v333; // eax@4
  int v334; // ebx@4
  __int32 v335; // ebx@4
  __int32 v336; // edi@4
  int v337; // esi@4
  int v338; // eax@4
  int v339; // ebx@4
  int v340; // ebx@4
  int v341; // ecx@4
  int v342; // edx@4
  int v343; // eax@4
  int v344; // ebx@4
  __int32 v345; // ebx@4
  __int32 v346; // ebp@4
  int v347; // edi@4
  int v348; // eax@4
  int v349; // ebx@4
  int v350; // ebx@4
  int v351; // esi@4
  __int64 result; // rax@4
  __m128i v353; // [sp+0h] [bp-108h]@1
  __m128i v354; // [sp+0h] [bp-108h]@2
  __m128i v355; // [sp+10h] [bp-F8h]@1
  __m128i v356; // [sp+10h] [bp-F8h]@2
  __m128i v357; // [sp+20h] [bp-E8h]@1
  __m128i v358; // [sp+20h] [bp-E8h]@2
  __m128i v359; // [sp+30h] [bp-D8h]@1
  __m128i v360; // [sp+30h] [bp-D8h]@2

  v3 = a1;
  v4 = a3;
  v5 = a2;
  v6 = *(__m128i *)&K_XMM_AR[8];
  v7 = *(_DWORD *)a1;
  v8 = *(_DWORD *)(a1 + 4);
  v9 = *(_DWORD *)(a1 + 8);
  v10 = *(_DWORD *)(v3 + 12);
  v11 = *(_DWORD *)(v3 + 16);
  v12 = _mm_shuffle_epi8(*(__m128i *)v4, v6);
  v353 = _mm_add_epi32(v12, *(__m128i *)K_XMM_AR);
  v13 = _mm_shuffle_epi8(*(__m128i *)(v4 + 16), v6);
  v355 = _mm_add_epi32(v13, *(__m128i *)K_XMM_AR);
  v14 = _mm_shuffle_epi8(*(__m128i *)(v4 + 32), v6);
  v357 = _mm_add_epi32(v14, *(__m128i *)K_XMM_AR);
  v15 = _mm_shuffle_epi8(*(__m128i *)(v4 + 48), v6);
  v359 = _mm_add_epi32(v15, *(__m128i *)K_XMM_AR);
  while ( 1 )
  {
    v16 = v10 ^ v8 & (v10 ^ v9);
    v17 = __ROL4__(v8, 30);
    v18 = __ROL4__(v7, 5);
    v19 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v13, v12, 8), v14), _mm_xor_si128(_mm_srli_si128(v15, 4), v12));
    v20 = _mm_slli_si128(v19, 12);
    v21 = v16 + v353.m128i_i32[0] + v11 + v18;
    v22 = v21;
    v21 = __ROL4__(v21, 5);
    v23 = (v9 ^ v7 & (v9 ^ v17)) + v21 + v353.m128i_i32[1] + v10;
    v24 = __ROL4__(v7, 30);
    v25 = v17 ^ v22 & (v17 ^ v24);
    v22 = __ROL4__(v22, 30);
    v26 = v353.m128i_i32[3] + v17;
    v27 = __ROL4__(v23, 5);
    v28 = v25 + v353.m128i_i32[2] + v9;
    v29 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_sll_epi32(v19, 1u), _mm_srl_epi32(v19, 0x1Fu)), _mm_sll_epi32(v20, 2u)),
            _mm_srl_epi32(v20, 0x1Eu));
    v30 = _mm_add_epi32(v29, *(__m128i *)K_XMM_AR);
    v31 = v28 + v27;
    v32 = v31;
    v31 = __ROL4__(v31, 5);
    v33 = (v24 ^ v23 & (v24 ^ v22)) + v31 + v26;
    v23 = __ROL4__(v23, 30);
    v34 = v22 ^ v32 & (v22 ^ v23);
    v32 = __ROL4__(v32, 30);
    v35 = __ROL4__(v33, 5);
    v36 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v14, v13, 8), v15), _mm_xor_si128(_mm_srli_si128(v29, 4), v13));
    v37 = _mm_slli_si128(v36, 12);
    v38 = v34 + v355.m128i_i32[0] + v24 + v35;
    v39 = v38;
    v38 = __ROL4__(v38, 5);
    v40 = (v23 ^ v33 & (v23 ^ v32)) + v38 + v355.m128i_i32[1] + v22;
    v33 = __ROL4__(v33, 30);
    v41 = v32 ^ v39 & (v32 ^ v33);
    v39 = __ROL4__(v39, 30);
    v42 = v355.m128i_i32[3] + v32;
    v43 = __ROL4__(v40, 5);
    v44 = v41 + v355.m128i_i32[2] + v23;
    v45 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_sll_epi32(v36, 1u), _mm_srl_epi32(v36, 0x1Fu)), _mm_sll_epi32(v37, 2u)),
            _mm_srl_epi32(v37, 0x1Eu));
    v46 = _mm_add_epi32(v45, *(__m128i *)&K_XMM_AR[2]);
    v47 = v44 + v43;
    v48 = v47;
    v47 = __ROL4__(v47, 5);
    v49 = (v33 ^ v40 & (v33 ^ v39)) + v47 + v42;
    v40 = __ROL4__(v40, 30);
    v50 = v39 ^ v48 & (v39 ^ v40);
    v48 = __ROL4__(v48, 30);
    v51 = __ROL4__(v49, 5);
    v52 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v15, v14, 8), v29), _mm_xor_si128(_mm_srli_si128(v45, 4), v14));
    v53 = _mm_slli_si128(v52, 12);
    v54 = v50 + v357.m128i_i32[0] + v33 + v51;
    v55 = v54;
    v54 = __ROL4__(v54, 5);
    v56 = (v40 ^ v49 & (v40 ^ v48)) + v54 + v357.m128i_i32[1] + v39;
    v49 = __ROL4__(v49, 30);
    v57 = v48 ^ v55 & (v48 ^ v49);
    v55 = __ROL4__(v55, 30);
    v58 = v357.m128i_i32[3] + v48;
    v59 = __ROL4__(v56, 5);
    v60 = v57 + v357.m128i_i32[2] + v40;
    v61 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_sll_epi32(v52, 1u), _mm_srl_epi32(v52, 0x1Fu)), _mm_sll_epi32(v53, 2u)),
            _mm_srl_epi32(v53, 0x1Eu));
    v62 = _mm_add_epi32(v61, *(__m128i *)&K_XMM_AR[2]);
    v63 = v60 + v59;
    v64 = v63;
    v63 = __ROL4__(v63, 5);
    v65 = (v49 ^ v56 & (v49 ^ v55)) + v63 + v58;
    v56 = __ROL4__(v56, 30);
    v66 = v55 ^ v64 & (v55 ^ v56);
    v64 = __ROL4__(v64, 30);
    v67 = __ROL4__(v65, 5);
    v68 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v29, v15, 8), v45), _mm_xor_si128(_mm_srli_si128(v61, 4), v15));
    v69 = _mm_slli_si128(v68, 12);
    v70 = v66 + v359.m128i_i32[0] + v49 + v67;
    v71 = v70;
    v70 = __ROL4__(v70, 5);
    v72 = (v56 ^ v65 & (v56 ^ v64)) + v70 + v359.m128i_i32[1] + v55;
    v65 = __ROL4__(v65, 30);
    v73 = v64 ^ v71 & (v64 ^ v65);
    v71 = __ROL4__(v71, 30);
    v74 = v359.m128i_i32[3] + v64;
    v75 = __ROL4__(v72, 5);
    v76 = v73 + v359.m128i_i32[2] + v56;
    v77 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_sll_epi32(v68, 1u), _mm_srl_epi32(v68, 0x1Fu)), _mm_sll_epi32(v69, 2u)),
            _mm_srl_epi32(v69, 0x1Eu));
    v78 = _mm_add_epi32(v77, *(__m128i *)&K_XMM_AR[2]);
    v79 = v76 + v75;
    v80 = v79;
    v79 = __ROL4__(v79, 5);
    v81 = (v65 ^ v72 & (v65 ^ v71)) + v79 + v74;
    v72 = __ROL4__(v72, 30);
    v82 = v71 ^ v80 & (v71 ^ v72);
    v80 = __ROL4__(v80, 30);
    v83 = __ROL4__(v81, 5);
    v84 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v77, v61, 8), v29), _mm_xor_si128(v12, v13));
    v85 = v82 + v30.m128i_i32[0] + v65 + v83;
    v86 = v85;
    v85 = __ROL4__(v85, 5);
    v87 = (v72 ^ v81 & (v72 ^ v80)) + v85 + v30.m128i_i32[1] + v71;
    v81 = __ROL4__(v81, 30);
    v88 = v80 ^ v86 & (v80 ^ v81);
    v86 = __ROL4__(v86, 30);
    v89 = v30.m128i_i32[3] + v80;
    v90 = __ROL4__(v87, 5);
    v91 = v88 + v30.m128i_i32[2] + v72;
    v92 = _mm_or_si128(_mm_sll_epi32(v84, 2u), _mm_srl_epi32(v84, 0x1Eu));
    v93 = _mm_add_epi32(v92, *(__m128i *)&K_XMM_AR[2]);
    v94 = v91 + v90;
    v95 = v94;
    v94 = __ROL4__(v94, 5);
    v96 = (v81 ^ v87 & (v81 ^ v86)) + v94 + v89;
    v87 = __ROL4__(v87, 30);
    v97 = v95 ^ v87 ^ v86;
    v95 = __ROL4__(v95, 30);
    v98 = __ROL4__(v96, 5);
    v99 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v92, v77, 8), v45), _mm_xor_si128(v13, v14));
    v100 = v97 + v46.m128i_i32[0] + v81 + v98;
    v101 = v100;
    v100 = __ROL4__(v100, 5);
    v102 = (v96 ^ v95 ^ v87) + v100 + v46.m128i_i32[1] + v86;
    v96 = __ROL4__(v96, 30);
    v103 = v101 ^ v96 ^ v95;
    v101 = __ROL4__(v101, 30);
    v104 = v46.m128i_i32[3] + v95;
    v105 = __ROL4__(v102, 5);
    v106 = v103 + v46.m128i_i32[2] + v87;
    v107 = _mm_or_si128(_mm_sll_epi32(v99, 2u), _mm_srl_epi32(v99, 0x1Eu));
    v108 = _mm_add_epi32(v107, *(__m128i *)&K_XMM_AR[2]);
    v109 = v106 + v105;
    v110 = v109;
    v109 = __ROL4__(v109, 5);
    v111 = (v102 ^ v101 ^ v96) + v109 + v104;
    v102 = __ROL4__(v102, 30);
    v112 = v110 ^ v102 ^ v101;
    v110 = __ROL4__(v110, 30);
    v113 = __ROL4__(v111, 5);
    v114 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v107, v92, 8), v61), _mm_xor_si128(v14, v15));
    v115 = v112 + v62.m128i_i32[0] + v96 + v113;
    v116 = v115;
    v115 = __ROL4__(v115, 5);
    v117 = (v111 ^ v110 ^ v102) + v115 + v62.m128i_i32[1] + v101;
    v111 = __ROL4__(v111, 30);
    v118 = v116 ^ v111 ^ v110;
    v116 = __ROL4__(v116, 30);
    v119 = v62.m128i_i32[3] + v110;
    v120 = __ROL4__(v117, 5);
    v121 = v118 + v62.m128i_i32[2] + v102;
    v122 = _mm_or_si128(_mm_sll_epi32(v114, 2u), _mm_srl_epi32(v114, 0x1Eu));
    v123 = _mm_add_epi32(v122, *(__m128i *)&K_XMM_AR[4]);
    v124 = v121 + v120;
    v125 = v124;
    v124 = __ROL4__(v124, 5);
    v126 = (v117 ^ v116 ^ v111) + v124 + v119;
    v117 = __ROL4__(v117, 30);
    v127 = v125 ^ v117 ^ v116;
    v125 = __ROL4__(v125, 30);
    v128 = __ROL4__(v126, 5);
    v129 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v122, v107, 8), v77), _mm_xor_si128(v15, v29));
    v130 = v127 + v78.m128i_i32[0] + v111 + v128;
    v131 = v130;
    v130 = __ROL4__(v130, 5);
    v132 = (v126 ^ v125 ^ v117) + v130 + v78.m128i_i32[1] + v116;
    v126 = __ROL4__(v126, 30);
    v133 = v131 ^ v126 ^ v125;
    v131 = __ROL4__(v131, 30);
    v134 = v78.m128i_i32[3] + v125;
    v135 = __ROL4__(v132, 5);
    v136 = v133 + v78.m128i_i32[2] + v117;
    v137 = _mm_or_si128(_mm_sll_epi32(v129, 2u), _mm_srl_epi32(v129, 0x1Eu));
    v138 = _mm_add_epi32(v137, *(__m128i *)&K_XMM_AR[4]);
    v139 = v136 + v135;
    v140 = v139;
    v139 = __ROL4__(v139, 5);
    v141 = (v132 ^ v131 ^ v126) + v139 + v134;
    v132 = __ROL4__(v132, 30);
    v142 = v140 ^ v132 ^ v131;
    v140 = __ROL4__(v140, 30);
    v143 = __ROL4__(v141, 5);
    v144 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v137, v122, 8), v92), _mm_xor_si128(v29, v45));
    v145 = v142 + v93.m128i_i32[0] + v126 + v143;
    v146 = v145;
    v145 = __ROL4__(v145, 5);
    v147 = (v141 ^ v140 ^ v132) + v145 + v93.m128i_i32[1] + v131;
    v141 = __ROL4__(v141, 30);
    v148 = v146 ^ v141 ^ v140;
    v146 = __ROL4__(v146, 30);
    v149 = v93.m128i_i32[3] + v140;
    v150 = __ROL4__(v147, 5);
    v151 = v148 + v93.m128i_i32[2] + v132;
    v152 = _mm_or_si128(_mm_sll_epi32(v144, 2u), _mm_srl_epi32(v144, 0x1Eu));
    v153 = _mm_add_epi32(v152, *(__m128i *)&K_XMM_AR[4]);
    v154 = v151 + v150;
    v155 = v154;
    v154 = __ROL4__(v154, 5);
    v156 = (v147 ^ v146 ^ v141) + v154 + v149;
    v147 = __ROL4__(v147, 30);
    v157 = v155 ^ v147 ^ v146;
    v155 = __ROL4__(v155, 30);
    v158 = __ROL4__(v156, 5);
    v159 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v152, v137, 8), v107), _mm_xor_si128(v45, v61));
    v160 = v157 + v108.m128i_i32[0] + v141 + v158;
    v161 = v160;
    v160 = __ROL4__(v160, 5);
    v162 = (v156 ^ v155 ^ v147) + v160 + v108.m128i_i32[1] + v146;
    v156 = __ROL4__(v156, 30);
    v163 = v161 ^ v156 ^ v155;
    v161 = __ROL4__(v161, 30);
    v164 = v108.m128i_i32[3] + v155;
    v165 = __ROL4__(v162, 5);
    v166 = v163 + v108.m128i_i32[2] + v147;
    v167 = _mm_or_si128(_mm_sll_epi32(v159, 2u), _mm_srl_epi32(v159, 0x1Eu));
    v168 = _mm_add_epi32(v167, *(__m128i *)&K_XMM_AR[4]);
    v169 = v166 + v165;
    v170 = v169;
    v169 = __ROL4__(v169, 5);
    v171 = (v162 ^ v161 ^ v156) + v169 + v164;
    v162 = __ROL4__(v162, 30);
    v172 = v162 & v170 | v161 & (v170 | v162);
    v170 = __ROL4__(v170, 30);
    v173 = __ROL4__(v171, 5);
    v174 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v167, v152, 8), v122), _mm_xor_si128(v61, v77));
    v175 = v172 + v123.m128i_i32[0] + v156 + v173;
    v176 = v175;
    v175 = __ROL4__(v175, 5);
    v177 = (v170 & v171 | v162 & (v171 | v170)) + v175 + v123.m128i_i32[1] + v161;
    v171 = __ROL4__(v171, 30);
    v178 = v171 & v176 | v170 & (v176 | v171);
    v176 = __ROL4__(v176, 30);
    v179 = v123.m128i_i32[3] + v170;
    v180 = __ROL4__(v177, 5);
    v181 = v178 + v123.m128i_i32[2] + v162;
    v182 = _mm_or_si128(_mm_sll_epi32(v174, 2u), _mm_srl_epi32(v174, 0x1Eu));
    v183 = _mm_add_epi32(v182, *(__m128i *)&K_XMM_AR[4]);
    v184 = v181 + v180;
    v185 = v184;
    v184 = __ROL4__(v184, 5);
    v186 = (v176 & v177 | v171 & (v177 | v176)) + v184 + v179;
    v177 = __ROL4__(v177, 30);
    v187 = v177 & v185 | v176 & (v185 | v177);
    v185 = __ROL4__(v185, 30);
    v188 = __ROL4__(v186, 5);
    v189 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v182, v167, 8), v137), _mm_xor_si128(v77, v92));
    v190 = v187 + v138.m128i_i32[0] + v171 + v188;
    v191 = v190;
    v190 = __ROL4__(v190, 5);
    v192 = (v185 & v186 | v177 & (v186 | v185)) + v190 + v138.m128i_i32[1] + v176;
    v186 = __ROL4__(v186, 30);
    v193 = v186 & v191 | v185 & (v191 | v186);
    v191 = __ROL4__(v191, 30);
    v194 = v138.m128i_i32[3] + v185;
    v195 = __ROL4__(v192, 5);
    v196 = v193 + v138.m128i_i32[2] + v177;
    v197 = _mm_or_si128(_mm_sll_epi32(v189, 2u), _mm_srl_epi32(v189, 0x1Eu));
    v198 = _mm_add_epi32(v197, *(__m128i *)&K_XMM_AR[6]);
    v199 = v196 + v195;
    v200 = v199;
    v199 = __ROL4__(v199, 5);
    v201 = (v191 & v192 | v186 & (v192 | v191)) + v199 + v194;
    v192 = __ROL4__(v192, 30);
    v202 = v192 & v200 | v191 & (v200 | v192);
    v200 = __ROL4__(v200, 30);
    v203 = __ROL4__(v201, 5);
    v204 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v197, v182, 8), v152), _mm_xor_si128(v92, v107));
    v205 = v202 + v153.m128i_i32[0] + v186 + v203;
    v206 = v205;
    v205 = __ROL4__(v205, 5);
    v207 = (v200 & v201 | v192 & (v201 | v200)) + v205 + v153.m128i_i32[1] + v191;
    v201 = __ROL4__(v201, 30);
    v208 = v201 & v206 | v200 & (v206 | v201);
    v206 = __ROL4__(v206, 30);
    v209 = v153.m128i_i32[3] + v200;
    v210 = __ROL4__(v207, 5);
    v211 = v208 + v153.m128i_i32[2] + v192;
    v212 = _mm_or_si128(_mm_sll_epi32(v204, 2u), _mm_srl_epi32(v204, 0x1Eu));
    v354 = _mm_add_epi32(v212, *(__m128i *)&K_XMM_AR[6]);
    v213 = v211 + v210;
    v214 = v213;
    v213 = __ROL4__(v213, 5);
    v215 = (v206 & v207 | v201 & (v207 | v206)) + v213 + v209;
    v207 = __ROL4__(v207, 30);
    v216 = v207 & v214 | v206 & (v214 | v207);
    v214 = __ROL4__(v214, 30);
    v217 = __ROL4__(v215, 5);
    v218 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v212, v197, 8), v167), _mm_xor_si128(v107, v122));
    v219 = v216 + v168.m128i_i32[0] + v201 + v217;
    v220 = v219;
    v219 = __ROL4__(v219, 5);
    v221 = (v214 & v215 | v207 & (v215 | v214)) + v219 + v168.m128i_i32[1] + v206;
    v215 = __ROL4__(v215, 30);
    v222 = v215 & v220 | v214 & (v220 | v215);
    v220 = __ROL4__(v220, 30);
    v223 = v168.m128i_i32[3] + v214;
    v224 = __ROL4__(v221, 5);
    v225 = v222 + v168.m128i_i32[2] + v207;
    v226 = _mm_or_si128(_mm_sll_epi32(v218, 2u), _mm_srl_epi32(v218, 0x1Eu));
    v356 = _mm_add_epi32(v226, *(__m128i *)&K_XMM_AR[6]);
    v227 = v225 + v224;
    v228 = v227;
    v227 = __ROL4__(v227, 5);
    v229 = (v220 & v221 | v215 & (v221 | v220)) + v227 + v223;
    v221 = __ROL4__(v221, 30);
    v230 = v221 & v228 | v220 & (v228 | v221);
    v228 = __ROL4__(v228, 30);
    v231 = __ROL4__(v229, 5);
    v232 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v226, v212, 8), v182), _mm_xor_si128(v122, v137));
    v233 = v230 + v183.m128i_i32[0] + v215 + v231;
    v234 = v233;
    v233 = __ROL4__(v233, 5);
    v235 = (v228 & v229 | v221 & (v229 | v228)) + v233 + v183.m128i_i32[1] + v220;
    v229 = __ROL4__(v229, 30);
    v236 = _mm_or_si128(_mm_sll_epi32(v232, 2u), _mm_srl_epi32(v232, 0x1Eu));
    v237 = v229 & v234 | v228 & (v234 | v229);
    v234 = __ROL4__(v234, 30);
    v238 = v183.m128i_i32[3] + v228;
    v239 = __ROL4__(v235, 5);
    v240 = v237 + v183.m128i_i32[2] + v221;
    v358 = _mm_add_epi32(v236, *(__m128i *)&K_XMM_AR[6]);
    v241 = v240 + v239;
    v242 = v241;
    v241 = __ROL4__(v241, 5);
    v243 = (v234 & v235 | v229 & (v235 | v234)) + v241 + v238;
    v235 = __ROL4__(v235, 30);
    v244 = v242 ^ v235 ^ v234;
    v242 = __ROL4__(v242, 30);
    v245 = __ROL4__(v243, 5);
    v246 = _mm_xor_si128(_mm_xor_si128(_mm_alignr_epi8(v236, v226, 8), v197), _mm_xor_si128(v137, v152));
    v247 = v244 + v198.m128i_i32[0] + v229 + v245;
    v248 = v247;
    v247 = __ROL4__(v247, 5);
    v249 = (v243 ^ v242 ^ v235) + v247 + v198.m128i_i32[1] + v234;
    v250 = __ROL4__(v243, 30);
    v251 = v248 ^ v250 ^ v242;
    v252 = __ROL4__(v248, 30);
    v253 = v198.m128i_i32[3] + v242;
    v254 = __ROL4__(v249, 5);
    v255 = v251 + v198.m128i_i32[2] + v235;
    v360 = _mm_add_epi32(_mm_or_si128(_mm_sll_epi32(v246, 2u), _mm_srl_epi32(v246, 0x1Eu)), *(__m128i *)&K_XMM_AR[6]);
    v256 = v255 + v254;
    v257 = v256;
    v256 = __ROL4__(v256, 5);
    v258 = (v249 ^ v252 ^ v250) + v256 + v253;
    v259 = __ROL4__(v249, 30);
    v4 += 64LL;
    v260 = (unsigned __int64)v5-- < 1;
    if ( v260 || v5 == 0 )
      break;
    v261 = v257 ^ v259 ^ v252;
    v262 = __ROL4__(v257, 30);
    v263 = __ROL4__(v258, 5);
    v12 = _mm_shuffle_epi8(*(__m128i *)v4, v6);
    v264 = v261 + v354.m128i_i32[0] + v250 + v263;
    v265 = v264;
    v264 = __ROL4__(v264, 5);
    v266 = (v258 ^ v262 ^ v259) + v264 + v354.m128i_i32[1] + v252;
    v267 = __ROL4__(v258, 30);
    v268 = v265 ^ v267 ^ v262;
    v265 = __ROL4__(v265, 30);
    v269 = v354.m128i_i32[3] + v262;
    v270 = __ROL4__(v266, 5);
    v271 = v268 + v354.m128i_i32[2] + v259;
    v353 = _mm_add_epi32(v12, *(__m128i *)K_XMM_AR);
    v272 = v271 + v270;
    v273 = v272;
    v272 = __ROL4__(v272, 5);
    v274 = (v266 ^ v265 ^ v267) + v272 + v269;
    v266 = __ROL4__(v266, 30);
    v275 = v273 ^ v266 ^ v265;
    v273 = __ROL4__(v273, 30);
    v276 = __ROL4__(v274, 5);
    v13 = _mm_shuffle_epi8(*(__m128i *)(v4 + 16), v6);
    v277 = v275 + v356.m128i_i32[0] + v267 + v276;
    v278 = v277;
    v277 = __ROL4__(v277, 5);
    v279 = (v274 ^ v273 ^ v266) + v277 + v356.m128i_i32[1] + v265;
    v274 = __ROL4__(v274, 30);
    v280 = v278 ^ v274 ^ v273;
    v278 = __ROL4__(v278, 30);
    v281 = v356.m128i_i32[3] + v273;
    v282 = __ROL4__(v279, 5);
    v283 = v280 + v356.m128i_i32[2] + v266;
    v355 = _mm_add_epi32(v13, *(__m128i *)K_XMM_AR);
    v284 = v283 + v282;
    v285 = v284;
    v284 = __ROL4__(v284, 5);
    v286 = (v279 ^ v278 ^ v274) + v284 + v281;
    v279 = __ROL4__(v279, 30);
    v287 = v285 ^ v279 ^ v278;
    v285 = __ROL4__(v285, 30);
    v288 = __ROL4__(v286, 5);
    v14 = _mm_shuffle_epi8(*(__m128i *)(v4 + 32), v6);
    v289 = v287 + v358.m128i_i32[0] + v274 + v288;
    v290 = v289;
    v289 = __ROL4__(v289, 5);
    v291 = (v286 ^ v285 ^ v279) + v289 + v358.m128i_i32[1] + v278;
    v286 = __ROL4__(v286, 30);
    v292 = v290 ^ v286 ^ v285;
    v290 = __ROL4__(v290, 30);
    v293 = v358.m128i_i32[3] + v285;
    v294 = __ROL4__(v291, 5);
    v295 = v292 + v358.m128i_i32[2] + v279;
    v357 = _mm_add_epi32(v14, *(__m128i *)K_XMM_AR);
    v296 = v295 + v294;
    v297 = v296;
    v296 = __ROL4__(v296, 5);
    v298 = (v291 ^ v290 ^ v286) + v296 + v293;
    v291 = __ROL4__(v291, 30);
    v299 = v297 ^ v291 ^ v290;
    v297 = __ROL4__(v297, 30);
    v300 = __ROL4__(v298, 5);
    v15 = _mm_shuffle_epi8(*(__m128i *)(v4 + 48), v6);
    v301 = v299 + v360.m128i_i32[0] + v286 + v300;
    v302 = v301;
    v301 = __ROL4__(v301, 5);
    v303 = (v298 ^ v297 ^ v291) + v301 + v360.m128i_i32[1] + v290;
    v298 = __ROL4__(v298, 30);
    v304 = v302 ^ v298 ^ v297;
    v302 = __ROL4__(v302, 30);
    v305 = v360.m128i_i32[3] + v297;
    v306 = __ROL4__(v303, 5);
    v307 = v304 + v360.m128i_i32[2] + v291;
    v359 = _mm_add_epi32(v15, *(__m128i *)K_XMM_AR);
    v308 = v307 + v306;
    v309 = v308;
    v308 = __ROL4__(v308, 5);
    v310 = v303 ^ v302 ^ v298;
    v303 = __ROL4__(v303, 30);
    v7 = *(_DWORD *)v3 + v310 + v308 + v305;
    *(_DWORD *)v3 = v7;
    v8 = *(_DWORD *)(v3 + 4) + v309;
    *(_DWORD *)(v3 + 4) = v8;
    v9 = *(_DWORD *)(v3 + 8) + v303;
    *(_DWORD *)(v3 + 8) = v9;
    v10 = *(_DWORD *)(v3 + 12) + v302;
    *(_DWORD *)(v3 + 12) = v10;
    v11 = *(_DWORD *)(v3 + 16) + v298;
    *(_DWORD *)(v3 + 16) = v11;
  }
  v311 = v257 ^ v259 ^ v252;
  v312 = __ROL4__(v257, 30);
  v313 = __ROL4__(v258, 5);
  v314 = v311 + v354.m128i_i32[0] + v250 + v313;
  v315 = v314;
  v314 = __ROL4__(v314, 5);
  v316 = (v258 ^ v312 ^ v259) + v314 + v354.m128i_i32[1] + v252;
  v317 = __ROL4__(v258, 30);
  v318 = v315 ^ v317 ^ v312;
  v315 = __ROL4__(v315, 30);
  v319 = __ROL4__(v316, 5);
  v320 = v318 + v354.m128i_i32[2] + v259 + v319;
  v321 = v320;
  v320 = __ROL4__(v320, 5);
  v322 = (v316 ^ v315 ^ v317) + v320 + v354.m128i_i32[3] + v312;
  v316 = __ROL4__(v316, 30);
  v323 = v321 ^ v316 ^ v315;
  v321 = __ROL4__(v321, 30);
  v324 = __ROL4__(v322, 5);
  v325 = v323 + v356.m128i_i32[0] + v317 + v324;
  v326 = v325;
  v325 = __ROL4__(v325, 5);
  v327 = (v322 ^ v321 ^ v316) + v325 + v356.m128i_i32[1] + v315;
  v322 = __ROL4__(v322, 30);
  v328 = v326 ^ v322 ^ v321;
  v326 = __ROL4__(v326, 30);
  v329 = __ROL4__(v327, 5);
  v330 = v328 + v356.m128i_i32[2] + v316 + v329;
  v331 = v330;
  v330 = __ROL4__(v330, 5);
  v332 = (v327 ^ v326 ^ v322) + v330 + v356.m128i_i32[3] + v321;
  v327 = __ROL4__(v327, 30);
  v333 = v331 ^ v327 ^ v326;
  v331 = __ROL4__(v331, 30);
  v334 = __ROL4__(v332, 5);
  v335 = v333 + v358.m128i_i32[0] + v322 + v334;
  v336 = v335;
  v335 = __ROL4__(v335, 5);
  v337 = (v332 ^ v331 ^ v327) + v335 + v358.m128i_i32[1] + v326;
  v332 = __ROL4__(v332, 30);
  v338 = v336 ^ v332 ^ v331;
  v336 = __ROL4__(v336, 30);
  v339 = __ROL4__(v337, 5);
  v340 = v338 + v358.m128i_i32[2] + v327 + v339;
  v341 = v340;
  v340 = __ROL4__(v340, 5);
  v342 = (v337 ^ v336 ^ v332) + v340 + v358.m128i_i32[3] + v331;
  v337 = __ROL4__(v337, 30);
  v343 = v341 ^ v337 ^ v336;
  v341 = __ROL4__(v341, 30);
  v344 = __ROL4__(v342, 5);
  v345 = v343 + v360.m128i_i32[0] + v332 + v344;
  v346 = v345;
  v345 = __ROL4__(v345, 5);
  v347 = (v342 ^ v341 ^ v337) + v345 + v360.m128i_i32[1] + v336;
  v342 = __ROL4__(v342, 30);
  v348 = v346 ^ v342 ^ v341;
  v346 = __ROL4__(v346, 30);
  v349 = __ROL4__(v347, 5);
  v350 = v348 + v360.m128i_i32[2] + v337 + v349;
  v351 = v350;
  v350 = __ROL4__(v350, 5);
  result = v347 ^ v346 ^ (unsigned int)v342;
  v347 = __ROL4__(v347, 30);
  *(_DWORD *)v3 += result + v350 + v360.m128i_i32[3] + v341;
  *(_DWORD *)(v3 + 4) += v351;
  *(_DWORD *)(v3 + 8) += v347;
  *(_DWORD *)(v3 + 12) += v346;
  *(_DWORD *)(v3 + 16) += v342;
  return result;
}
// 66E90: using guessed type __int64 K_XMM_AR[10];

//----- (0000000000039D2F) ----------------------------------------------------
// local variable allocation has failed, the output may be wrong!
__int64 __fastcall SHA1Transform_nossse3(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r9@1
  __int64 v4; // r10@1
  __int64 v5; // r11@1
  int v6; // ecx@1
  int v7; // esi@1
  int v8; // edi@1
  int v9; // ebp@1
  int v10; // edx@1
  __m128i v11; // STE0_16@1 OVERLAPPED
  __m128i v12; // xmm2@1
  __m128i v13; // xmm9@1
  __m128i v14; // xmm8@1
  __m128i v15; // xmm7@1
  int v16; // eax@2
  int v17; // esi@2
  int v18; // ebx@2
  __m128i v19; // xmm0@2
  __m128i v20; // xmm1@2
  __int32 v21; // ebx@2
  __int32 v22; // edx@2
  int v23; // ebp@2
  int v24; // ecx@2
  int v25; // eax@2
  int v26; // esi@2
  int v27; // ebx@2
  int v28; // edi@2
  __m128i v29; // xmm6@2
  __m128i v30; // ST00_16@2
  int v31; // ebx@2
  int v32; // edi@2
  int v33; // esi@2
  int v34; // eax@2
  int v35; // ebx@2
  __m128i v36; // xmm0@2
  __m128i v37; // xmm1@2
  __int32 v38; // ebx@2
  __int32 v39; // ecx@2
  int v40; // edx@2
  int v41; // eax@2
  int v42; // edi@2
  int v43; // ebx@2
  int v44; // ebp@2
  __m128i v45; // xmm5@2
  __m128i v46; // ST10_16@2
  int v47; // ebx@2
  int v48; // ebp@2
  int v49; // edi@2
  int v50; // eax@2
  int v51; // ebx@2
  __m128i v52; // xmm0@2
  __m128i v53; // xmm1@2
  __int32 v54; // ebx@2
  __int32 v55; // esi@2
  int v56; // ecx@2
  int v57; // eax@2
  int v58; // ebp@2
  int v59; // ebx@2
  int v60; // edx@2
  __m128i v61; // xmm4@2
  __m128i v62; // ST20_16@2
  int v63; // ebx@2
  int v64; // edx@2
  int v65; // ebp@2
  int v66; // eax@2
  int v67; // ebx@2
  __m128i v68; // xmm0@2
  __m128i v69; // xmm1@2
  __int32 v70; // ebx@2
  __int32 v71; // edi@2
  int v72; // esi@2
  int v73; // eax@2
  int v74; // edx@2
  int v75; // ebx@2
  int v76; // ecx@2
  __m128i v77; // xmm3@2
  __m128i v78; // ST30_16@2
  int v79; // ebx@2
  int v80; // ecx@2
  int v81; // edx@2
  int v82; // eax@2
  int v83; // ebx@2
  __m128i v84; // xmm0@2
  __int32 v85; // ebx@2
  __int32 v86; // ebp@2
  int v87; // edi@2
  int v88; // eax@2
  int v89; // ecx@2
  int v90; // ebx@2
  int v91; // esi@2
  __m128i v92; // xmm2@2
  __m128i v93; // ST00_16@2
  int v94; // ebx@2
  int v95; // esi@2
  int v96; // ecx@2
  int v97; // eax@2
  int v98; // ebx@2
  __m128i v99; // xmm0@2
  __int32 v100; // ebx@2
  __int32 v101; // edx@2
  int v102; // ebp@2
  int v103; // eax@2
  int v104; // esi@2
  int v105; // ebx@2
  int v106; // edi@2
  __m128i v107; // xmm9@2
  __m128i v108; // ST10_16@2
  int v109; // ebx@2
  int v110; // edi@2
  int v111; // esi@2
  int v112; // eax@2
  int v113; // ebx@2
  __m128i v114; // xmm0@2
  __int32 v115; // ebx@2
  __int32 v116; // ecx@2
  int v117; // edx@2
  int v118; // eax@2
  int v119; // edi@2
  int v120; // ebx@2
  int v121; // ebp@2
  __m128i v122; // xmm8@2
  __m128i v123; // ST20_16@2
  int v124; // ebx@2
  int v125; // ebp@2
  int v126; // edi@2
  int v127; // eax@2
  int v128; // ebx@2
  __m128i v129; // xmm0@2
  __int32 v130; // ebx@2
  __int32 v131; // esi@2
  int v132; // ecx@2
  int v133; // eax@2
  int v134; // ebp@2
  int v135; // ebx@2
  int v136; // edx@2
  __m128i v137; // xmm7@2
  __m128i v138; // ST30_16@2
  int v139; // ebx@2
  int v140; // edx@2
  int v141; // ebp@2
  int v142; // eax@2
  int v143; // ebx@2
  __m128i v144; // xmm0@2
  __int32 v145; // ebx@2
  __int32 v146; // edi@2
  int v147; // esi@2
  int v148; // eax@2
  int v149; // edx@2
  int v150; // ebx@2
  int v151; // ecx@2
  __m128i v152; // xmm6@2
  __m128i v153; // ST00_16@2
  int v154; // ebx@2
  int v155; // ecx@2
  int v156; // edx@2
  int v157; // eax@2
  int v158; // ebx@2
  __m128i v159; // xmm0@2
  __int32 v160; // ebx@2
  __int32 v161; // ebp@2
  int v162; // edi@2
  int v163; // eax@2
  int v164; // ecx@2
  int v165; // ebx@2
  int v166; // esi@2
  __m128i v167; // xmm5@2
  __m128i v168; // ST10_16@2
  int v169; // ebx@2
  int v170; // esi@2
  int v171; // ecx@2
  int v172; // eax@2
  int v173; // ebx@2
  __m128i v174; // xmm0@2
  __int32 v175; // ebx@2
  __int32 v176; // edx@2
  int v177; // ebp@2
  int v178; // eax@2
  int v179; // esi@2
  int v180; // ebx@2
  int v181; // edi@2
  __m128i v182; // xmm4@2
  __m128i v183; // ST20_16@2
  int v184; // ebx@2
  int v185; // edi@2
  int v186; // esi@2
  int v187; // eax@2
  int v188; // ebx@2
  __m128i v189; // xmm0@2
  __int32 v190; // ebx@2
  __int32 v191; // ecx@2
  int v192; // edx@2
  int v193; // eax@2
  int v194; // edi@2
  int v195; // ebx@2
  int v196; // ebp@2
  __m128i v197; // xmm3@2
  __m128i v198; // ST30_16@2
  int v199; // ebx@2
  int v200; // ebp@2
  int v201; // edi@2
  int v202; // eax@2
  int v203; // ebx@2
  __m128i v204; // xmm0@2
  __int32 v205; // ebx@2
  __int32 v206; // esi@2
  int v207; // ecx@2
  int v208; // eax@2
  int v209; // ebp@2
  int v210; // ebx@2
  int v211; // edx@2
  __m128i v212; // xmm2@2
  int v213; // ebx@2
  int v214; // edx@2
  int v215; // ebp@2
  int v216; // eax@2
  int v217; // ebx@2
  __m128i v218; // xmm0@2
  __int32 v219; // ebx@2
  __int32 v220; // edi@2
  int v221; // esi@2
  int v222; // eax@2
  int v223; // edx@2
  int v224; // ebx@2
  int v225; // ecx@2
  __m128i v226; // xmm9@2
  int v227; // ebx@2
  int v228; // ecx@2
  int v229; // edx@2
  int v230; // eax@2
  int v231; // ebx@2
  __m128i v232; // xmm0@2
  __int32 v233; // ebx@2
  __int32 v234; // ebp@2
  int v235; // edi@2
  __m128i v236; // xmm0@2
  int v237; // eax@2
  int v238; // ecx@2
  int v239; // ebx@2
  int v240; // esi@2
  int v241; // ebx@2
  int v242; // esi@2
  int v243; // ecx@2
  int v244; // eax@2
  int v245; // ebx@2
  __m128i v246; // xmm0@2
  __int32 v247; // ebx@2
  __int32 v248; // edx@2
  int v249; // ebp@2
  int v250; // ecx@2
  int v251; // eax@2
  int v252; // edx@2
  int v253; // esi@2
  int v254; // ebx@2
  int v255; // edi@2
  int v256; // ebx@2
  int v257; // edi@2
  int v258; // esi@2
  int v259; // ebp@2
  bool v260; // cf@2
  __m128i v261; // STE0_16@3 OVERLAPPED
  int v262; // eax@3
  int v263; // edi@3
  int v264; // ebx@3
  __int32 v265; // ebx@3
  __int32 v266; // ecx@3
  int v267; // edx@3
  int v268; // esi@3
  int v269; // eax@3
  int v270; // edi@3
  int v271; // ebx@3
  int v272; // ebp@3
  int v273; // ebx@3
  int v274; // ebp@3
  int v275; // edi@3
  int v276; // eax@3
  int v277; // ebx@3
  __int32 v278; // ebx@3
  __int32 v279; // esi@3
  int v280; // ecx@3
  int v281; // eax@3
  int v282; // ebp@3
  int v283; // ebx@3
  int v284; // edx@3
  int v285; // ebx@3
  int v286; // edx@3
  int v287; // ebp@3
  int v288; // eax@3
  int v289; // ebx@3
  __int32 v290; // ebx@3
  __int32 v291; // edi@3
  int v292; // esi@3
  int v293; // eax@3
  int v294; // edx@3
  int v295; // ebx@3
  int v296; // ecx@3
  int v297; // ebx@3
  int v298; // ecx@3
  int v299; // edx@3
  int v300; // eax@3
  int v301; // ebx@3
  __int32 v302; // ebx@3
  __int32 v303; // ebp@3
  int v304; // edi@3
  int v305; // eax@3
  int v306; // ecx@3
  int v307; // ebx@3
  int v308; // esi@3
  int v309; // ebx@3
  int v310; // esi@3
  int v311; // eax@3
  int v312; // eax@4
  int v313; // edi@4
  int v314; // ebx@4
  __int32 v315; // ebx@4
  __int32 v316; // ecx@4
  int v317; // edx@4
  int v318; // esi@4
  int v319; // eax@4
  int v320; // ebx@4
  int v321; // ebx@4
  int v322; // ebp@4
  int v323; // edi@4
  int v324; // eax@4
  int v325; // ebx@4
  __int32 v326; // ebx@4
  __int32 v327; // esi@4
  int v328; // ecx@4
  int v329; // eax@4
  int v330; // ebx@4
  int v331; // ebx@4
  int v332; // edx@4
  int v333; // ebp@4
  int v334; // eax@4
  int v335; // ebx@4
  __int32 v336; // ebx@4
  __int32 v337; // edi@4
  int v338; // esi@4
  int v339; // eax@4
  int v340; // ebx@4
  int v341; // ebx@4
  int v342; // ecx@4
  int v343; // edx@4
  int v344; // eax@4
  int v345; // ebx@4
  __int32 v346; // ebx@4
  __int32 v347; // ebp@4
  int v348; // edi@4
  int v349; // eax@4
  int v350; // ebx@4
  int v351; // ebx@4
  int v352; // esi@4
  __int64 result; // rax@4
  __m128i v354; // [sp+0h] [bp-108h]@1
  __m128i v355; // [sp+0h] [bp-108h]@2
  __m128i v356; // [sp+10h] [bp-F8h]@1
  __m128i v357; // [sp+10h] [bp-F8h]@2
  __m128i v358; // [sp+20h] [bp-E8h]@1
  __m128i v359; // [sp+20h] [bp-E8h]@2
  __m128i v360; // [sp+30h] [bp-D8h]@1
  __m128i v361; // [sp+30h] [bp-D8h]@2

  v3 = a1;
  v4 = a3;
  v5 = a2;
  v6 = *(_DWORD *)a1;
  v7 = *(_DWORD *)(a1 + 4);
  v8 = *(_DWORD *)(a1 + 8);
  v9 = *(_DWORD *)(v3 + 12);
  v10 = *(_DWORD *)(v3 + 16);
  v11.m128i_i64[0] = _byteswap_uint64(__PAIR__(*(_DWORD *)v4, *(_DWORD *)(v4 + 4)));
  v11.m128i_i64[1] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 8), *(_DWORD *)(v4 + 12)));
  v12 = v11;
  v354 = _mm_add_epi32(v11, *(__m128i *)K_XMM_AR);
  v11.m128i_i64[0] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 16), *(_DWORD *)(v4 + 20)));
  v11.m128i_i64[1] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 24), *(_DWORD *)(v4 + 28)));
  v13 = v11;
  v356 = _mm_add_epi32(v11, *(__m128i *)K_XMM_AR);
  v11.m128i_i64[0] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 32), *(_DWORD *)(v4 + 36)));
  v11.m128i_i64[1] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 40), *(_DWORD *)(v4 + 44)));
  v14 = v11;
  v358 = _mm_add_epi32(v11, *(__m128i *)K_XMM_AR);
  v11.m128i_i64[0] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 48), *(_DWORD *)(v4 + 52)));
  v11.m128i_i64[1] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 56), *(_DWORD *)(v4 + 60)));
  v15 = v11;
  v360 = _mm_add_epi32(v11, *(__m128i *)K_XMM_AR);
  while ( 1 )
  {
    v16 = v9 ^ v7 & (v9 ^ v8);
    v17 = __ROL4__(v7, 30);
    v18 = __ROL4__(v6, 5);
    v19 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v13, 8), _mm_srli_si128(v12, 8)), v14),
            _mm_xor_si128(_mm_srli_si128(v15, 4), v12));
    v20 = _mm_slli_si128(v19, 12);
    v21 = v16 + v354.m128i_i32[0] + v10 + v18;
    v22 = v21;
    v21 = __ROL4__(v21, 5);
    v23 = (v8 ^ v6 & (v8 ^ v17)) + v21 + v354.m128i_i32[1] + v9;
    v24 = __ROL4__(v6, 30);
    v25 = v17 ^ v22 & (v17 ^ v24);
    v22 = __ROL4__(v22, 30);
    v26 = v354.m128i_i32[3] + v17;
    v27 = __ROL4__(v23, 5);
    v28 = v25 + v354.m128i_i32[2] + v8;
    v29 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_sll_epi32(v19, 1u), _mm_srl_epi32(v19, 0x1Fu)), _mm_sll_epi32(v20, 2u)),
            _mm_srl_epi32(v20, 0x1Eu));
    v30 = _mm_add_epi32(v29, *(__m128i *)K_XMM_AR);
    v31 = v28 + v27;
    v32 = v31;
    v31 = __ROL4__(v31, 5);
    v33 = (v24 ^ v23 & (v24 ^ v22)) + v31 + v26;
    v23 = __ROL4__(v23, 30);
    v34 = v22 ^ v32 & (v22 ^ v23);
    v32 = __ROL4__(v32, 30);
    v35 = __ROL4__(v33, 5);
    v36 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v14, 8), _mm_srli_si128(v13, 8)), v15),
            _mm_xor_si128(_mm_srli_si128(v29, 4), v13));
    v37 = _mm_slli_si128(v36, 12);
    v38 = v34 + v356.m128i_i32[0] + v24 + v35;
    v39 = v38;
    v38 = __ROL4__(v38, 5);
    v40 = (v23 ^ v33 & (v23 ^ v32)) + v38 + v356.m128i_i32[1] + v22;
    v33 = __ROL4__(v33, 30);
    v41 = v32 ^ v39 & (v32 ^ v33);
    v39 = __ROL4__(v39, 30);
    v42 = v356.m128i_i32[3] + v32;
    v43 = __ROL4__(v40, 5);
    v44 = v41 + v356.m128i_i32[2] + v23;
    v45 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_sll_epi32(v36, 1u), _mm_srl_epi32(v36, 0x1Fu)), _mm_sll_epi32(v37, 2u)),
            _mm_srl_epi32(v37, 0x1Eu));
    v46 = _mm_add_epi32(v45, *(__m128i *)&K_XMM_AR[2]);
    v47 = v44 + v43;
    v48 = v47;
    v47 = __ROL4__(v47, 5);
    v49 = (v33 ^ v40 & (v33 ^ v39)) + v47 + v42;
    v40 = __ROL4__(v40, 30);
    v50 = v39 ^ v48 & (v39 ^ v40);
    v48 = __ROL4__(v48, 30);
    v51 = __ROL4__(v49, 5);
    v52 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v15, 8), _mm_srli_si128(v14, 8)), v29),
            _mm_xor_si128(_mm_srli_si128(v45, 4), v14));
    v53 = _mm_slli_si128(v52, 12);
    v54 = v50 + v358.m128i_i32[0] + v33 + v51;
    v55 = v54;
    v54 = __ROL4__(v54, 5);
    v56 = (v40 ^ v49 & (v40 ^ v48)) + v54 + v358.m128i_i32[1] + v39;
    v49 = __ROL4__(v49, 30);
    v57 = v48 ^ v55 & (v48 ^ v49);
    v55 = __ROL4__(v55, 30);
    v58 = v358.m128i_i32[3] + v48;
    v59 = __ROL4__(v56, 5);
    v60 = v57 + v358.m128i_i32[2] + v40;
    v61 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_sll_epi32(v52, 1u), _mm_srl_epi32(v52, 0x1Fu)), _mm_sll_epi32(v53, 2u)),
            _mm_srl_epi32(v53, 0x1Eu));
    v62 = _mm_add_epi32(v61, *(__m128i *)&K_XMM_AR[2]);
    v63 = v60 + v59;
    v64 = v63;
    v63 = __ROL4__(v63, 5);
    v65 = (v49 ^ v56 & (v49 ^ v55)) + v63 + v58;
    v56 = __ROL4__(v56, 30);
    v66 = v55 ^ v64 & (v55 ^ v56);
    v64 = __ROL4__(v64, 30);
    v67 = __ROL4__(v65, 5);
    v68 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v29, 8), _mm_srli_si128(v15, 8)), v45),
            _mm_xor_si128(_mm_srli_si128(v61, 4), v15));
    v69 = _mm_slli_si128(v68, 12);
    v70 = v66 + v360.m128i_i32[0] + v49 + v67;
    v71 = v70;
    v70 = __ROL4__(v70, 5);
    v72 = (v56 ^ v65 & (v56 ^ v64)) + v70 + v360.m128i_i32[1] + v55;
    v65 = __ROL4__(v65, 30);
    v73 = v64 ^ v71 & (v64 ^ v65);
    v71 = __ROL4__(v71, 30);
    v74 = v360.m128i_i32[3] + v64;
    v75 = __ROL4__(v72, 5);
    v76 = v73 + v360.m128i_i32[2] + v56;
    v77 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_sll_epi32(v68, 1u), _mm_srl_epi32(v68, 0x1Fu)), _mm_sll_epi32(v69, 2u)),
            _mm_srl_epi32(v69, 0x1Eu));
    v78 = _mm_add_epi32(v77, *(__m128i *)&K_XMM_AR[2]);
    v79 = v76 + v75;
    v80 = v79;
    v79 = __ROL4__(v79, 5);
    v81 = (v65 ^ v72 & (v65 ^ v71)) + v79 + v74;
    v72 = __ROL4__(v72, 30);
    v82 = v71 ^ v80 & (v71 ^ v72);
    v80 = __ROL4__(v80, 30);
    v83 = __ROL4__(v81, 5);
    v84 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v77, 8), _mm_srli_si128(v61, 8)), v29),
            _mm_xor_si128(v12, v13));
    v85 = v82 + v30.m128i_i32[0] + v65 + v83;
    v86 = v85;
    v85 = __ROL4__(v85, 5);
    v87 = (v72 ^ v81 & (v72 ^ v80)) + v85 + v30.m128i_i32[1] + v71;
    v81 = __ROL4__(v81, 30);
    v88 = v80 ^ v86 & (v80 ^ v81);
    v86 = __ROL4__(v86, 30);
    v89 = v30.m128i_i32[3] + v80;
    v90 = __ROL4__(v87, 5);
    v91 = v88 + v30.m128i_i32[2] + v72;
    v92 = _mm_or_si128(_mm_sll_epi32(v84, 2u), _mm_srl_epi32(v84, 0x1Eu));
    v93 = _mm_add_epi32(v92, *(__m128i *)&K_XMM_AR[2]);
    v94 = v91 + v90;
    v95 = v94;
    v94 = __ROL4__(v94, 5);
    v96 = (v81 ^ v87 & (v81 ^ v86)) + v94 + v89;
    v87 = __ROL4__(v87, 30);
    v97 = v95 ^ v87 ^ v86;
    v95 = __ROL4__(v95, 30);
    v98 = __ROL4__(v96, 5);
    v99 = _mm_xor_si128(
            _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v92, 8), _mm_srli_si128(v77, 8)), v45),
            _mm_xor_si128(v13, v14));
    v100 = v97 + v46.m128i_i32[0] + v81 + v98;
    v101 = v100;
    v100 = __ROL4__(v100, 5);
    v102 = (v96 ^ v95 ^ v87) + v100 + v46.m128i_i32[1] + v86;
    v96 = __ROL4__(v96, 30);
    v103 = v101 ^ v96 ^ v95;
    v101 = __ROL4__(v101, 30);
    v104 = v46.m128i_i32[3] + v95;
    v105 = __ROL4__(v102, 5);
    v106 = v103 + v46.m128i_i32[2] + v87;
    v107 = _mm_or_si128(_mm_sll_epi32(v99, 2u), _mm_srl_epi32(v99, 0x1Eu));
    v108 = _mm_add_epi32(v107, *(__m128i *)&K_XMM_AR[2]);
    v109 = v106 + v105;
    v110 = v109;
    v109 = __ROL4__(v109, 5);
    v111 = (v102 ^ v101 ^ v96) + v109 + v104;
    v102 = __ROL4__(v102, 30);
    v112 = v110 ^ v102 ^ v101;
    v110 = __ROL4__(v110, 30);
    v113 = __ROL4__(v111, 5);
    v114 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v107, 8), _mm_srli_si128(v92, 8)), v61),
             _mm_xor_si128(v14, v15));
    v115 = v112 + v62.m128i_i32[0] + v96 + v113;
    v116 = v115;
    v115 = __ROL4__(v115, 5);
    v117 = (v111 ^ v110 ^ v102) + v115 + v62.m128i_i32[1] + v101;
    v111 = __ROL4__(v111, 30);
    v118 = v116 ^ v111 ^ v110;
    v116 = __ROL4__(v116, 30);
    v119 = v62.m128i_i32[3] + v110;
    v120 = __ROL4__(v117, 5);
    v121 = v118 + v62.m128i_i32[2] + v102;
    v122 = _mm_or_si128(_mm_sll_epi32(v114, 2u), _mm_srl_epi32(v114, 0x1Eu));
    v123 = _mm_add_epi32(v122, *(__m128i *)&K_XMM_AR[4]);
    v124 = v121 + v120;
    v125 = v124;
    v124 = __ROL4__(v124, 5);
    v126 = (v117 ^ v116 ^ v111) + v124 + v119;
    v117 = __ROL4__(v117, 30);
    v127 = v125 ^ v117 ^ v116;
    v125 = __ROL4__(v125, 30);
    v128 = __ROL4__(v126, 5);
    v129 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v122, 8), _mm_srli_si128(v107, 8)), v77),
             _mm_xor_si128(v15, v29));
    v130 = v127 + v78.m128i_i32[0] + v111 + v128;
    v131 = v130;
    v130 = __ROL4__(v130, 5);
    v132 = (v126 ^ v125 ^ v117) + v130 + v78.m128i_i32[1] + v116;
    v126 = __ROL4__(v126, 30);
    v133 = v131 ^ v126 ^ v125;
    v131 = __ROL4__(v131, 30);
    v134 = v78.m128i_i32[3] + v125;
    v135 = __ROL4__(v132, 5);
    v136 = v133 + v78.m128i_i32[2] + v117;
    v137 = _mm_or_si128(_mm_sll_epi32(v129, 2u), _mm_srl_epi32(v129, 0x1Eu));
    v138 = _mm_add_epi32(v137, *(__m128i *)&K_XMM_AR[4]);
    v139 = v136 + v135;
    v140 = v139;
    v139 = __ROL4__(v139, 5);
    v141 = (v132 ^ v131 ^ v126) + v139 + v134;
    v132 = __ROL4__(v132, 30);
    v142 = v140 ^ v132 ^ v131;
    v140 = __ROL4__(v140, 30);
    v143 = __ROL4__(v141, 5);
    v144 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v137, 8), _mm_srli_si128(v122, 8)), v92),
             _mm_xor_si128(v29, v45));
    v145 = v142 + v93.m128i_i32[0] + v126 + v143;
    v146 = v145;
    v145 = __ROL4__(v145, 5);
    v147 = (v141 ^ v140 ^ v132) + v145 + v93.m128i_i32[1] + v131;
    v141 = __ROL4__(v141, 30);
    v148 = v146 ^ v141 ^ v140;
    v146 = __ROL4__(v146, 30);
    v149 = v93.m128i_i32[3] + v140;
    v150 = __ROL4__(v147, 5);
    v151 = v148 + v93.m128i_i32[2] + v132;
    v152 = _mm_or_si128(_mm_sll_epi32(v144, 2u), _mm_srl_epi32(v144, 0x1Eu));
    v153 = _mm_add_epi32(v152, *(__m128i *)&K_XMM_AR[4]);
    v154 = v151 + v150;
    v155 = v154;
    v154 = __ROL4__(v154, 5);
    v156 = (v147 ^ v146 ^ v141) + v154 + v149;
    v147 = __ROL4__(v147, 30);
    v157 = v155 ^ v147 ^ v146;
    v155 = __ROL4__(v155, 30);
    v158 = __ROL4__(v156, 5);
    v159 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v152, 8), _mm_srli_si128(v137, 8)), v107),
             _mm_xor_si128(v45, v61));
    v160 = v157 + v108.m128i_i32[0] + v141 + v158;
    v161 = v160;
    v160 = __ROL4__(v160, 5);
    v162 = (v156 ^ v155 ^ v147) + v160 + v108.m128i_i32[1] + v146;
    v156 = __ROL4__(v156, 30);
    v163 = v161 ^ v156 ^ v155;
    v161 = __ROL4__(v161, 30);
    v164 = v108.m128i_i32[3] + v155;
    v165 = __ROL4__(v162, 5);
    v166 = v163 + v108.m128i_i32[2] + v147;
    v167 = _mm_or_si128(_mm_sll_epi32(v159, 2u), _mm_srl_epi32(v159, 0x1Eu));
    v168 = _mm_add_epi32(v167, *(__m128i *)&K_XMM_AR[4]);
    v169 = v166 + v165;
    v170 = v169;
    v169 = __ROL4__(v169, 5);
    v171 = (v162 ^ v161 ^ v156) + v169 + v164;
    v162 = __ROL4__(v162, 30);
    v172 = v162 & v170 | v161 & (v170 | v162);
    v170 = __ROL4__(v170, 30);
    v173 = __ROL4__(v171, 5);
    v174 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v167, 8), _mm_srli_si128(v152, 8)), v122),
             _mm_xor_si128(v61, v77));
    v175 = v172 + v123.m128i_i32[0] + v156 + v173;
    v176 = v175;
    v175 = __ROL4__(v175, 5);
    v177 = (v170 & v171 | v162 & (v171 | v170)) + v175 + v123.m128i_i32[1] + v161;
    v171 = __ROL4__(v171, 30);
    v178 = v171 & v176 | v170 & (v176 | v171);
    v176 = __ROL4__(v176, 30);
    v179 = v123.m128i_i32[3] + v170;
    v180 = __ROL4__(v177, 5);
    v181 = v178 + v123.m128i_i32[2] + v162;
    v182 = _mm_or_si128(_mm_sll_epi32(v174, 2u), _mm_srl_epi32(v174, 0x1Eu));
    v183 = _mm_add_epi32(v182, *(__m128i *)&K_XMM_AR[4]);
    v184 = v181 + v180;
    v185 = v184;
    v184 = __ROL4__(v184, 5);
    v186 = (v176 & v177 | v171 & (v177 | v176)) + v184 + v179;
    v177 = __ROL4__(v177, 30);
    v187 = v177 & v185 | v176 & (v185 | v177);
    v185 = __ROL4__(v185, 30);
    v188 = __ROL4__(v186, 5);
    v189 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v182, 8), _mm_srli_si128(v167, 8)), v137),
             _mm_xor_si128(v77, v92));
    v190 = v187 + v138.m128i_i32[0] + v171 + v188;
    v191 = v190;
    v190 = __ROL4__(v190, 5);
    v192 = (v185 & v186 | v177 & (v186 | v185)) + v190 + v138.m128i_i32[1] + v176;
    v186 = __ROL4__(v186, 30);
    v193 = v186 & v191 | v185 & (v191 | v186);
    v191 = __ROL4__(v191, 30);
    v194 = v138.m128i_i32[3] + v185;
    v195 = __ROL4__(v192, 5);
    v196 = v193 + v138.m128i_i32[2] + v177;
    v197 = _mm_or_si128(_mm_sll_epi32(v189, 2u), _mm_srl_epi32(v189, 0x1Eu));
    v198 = _mm_add_epi32(v197, *(__m128i *)&K_XMM_AR[6]);
    v199 = v196 + v195;
    v200 = v199;
    v199 = __ROL4__(v199, 5);
    v201 = (v191 & v192 | v186 & (v192 | v191)) + v199 + v194;
    v192 = __ROL4__(v192, 30);
    v202 = v192 & v200 | v191 & (v200 | v192);
    v200 = __ROL4__(v200, 30);
    v203 = __ROL4__(v201, 5);
    v204 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v197, 8), _mm_srli_si128(v182, 8)), v152),
             _mm_xor_si128(v92, v107));
    v205 = v202 + v153.m128i_i32[0] + v186 + v203;
    v206 = v205;
    v205 = __ROL4__(v205, 5);
    v207 = (v200 & v201 | v192 & (v201 | v200)) + v205 + v153.m128i_i32[1] + v191;
    v201 = __ROL4__(v201, 30);
    v208 = v201 & v206 | v200 & (v206 | v201);
    v206 = __ROL4__(v206, 30);
    v209 = v153.m128i_i32[3] + v200;
    v210 = __ROL4__(v207, 5);
    v211 = v208 + v153.m128i_i32[2] + v192;
    v212 = _mm_or_si128(_mm_sll_epi32(v204, 2u), _mm_srl_epi32(v204, 0x1Eu));
    v355 = _mm_add_epi32(v212, *(__m128i *)&K_XMM_AR[6]);
    v213 = v211 + v210;
    v214 = v213;
    v213 = __ROL4__(v213, 5);
    v215 = (v206 & v207 | v201 & (v207 | v206)) + v213 + v209;
    v207 = __ROL4__(v207, 30);
    v216 = v207 & v214 | v206 & (v214 | v207);
    v214 = __ROL4__(v214, 30);
    v217 = __ROL4__(v215, 5);
    v218 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v212, 8), _mm_srli_si128(v197, 8)), v167),
             _mm_xor_si128(v107, v122));
    v219 = v216 + v168.m128i_i32[0] + v201 + v217;
    v220 = v219;
    v219 = __ROL4__(v219, 5);
    v221 = (v214 & v215 | v207 & (v215 | v214)) + v219 + v168.m128i_i32[1] + v206;
    v215 = __ROL4__(v215, 30);
    v222 = v215 & v220 | v214 & (v220 | v215);
    v220 = __ROL4__(v220, 30);
    v223 = v168.m128i_i32[3] + v214;
    v224 = __ROL4__(v221, 5);
    v225 = v222 + v168.m128i_i32[2] + v207;
    v226 = _mm_or_si128(_mm_sll_epi32(v218, 2u), _mm_srl_epi32(v218, 0x1Eu));
    v357 = _mm_add_epi32(v226, *(__m128i *)&K_XMM_AR[6]);
    v227 = v225 + v224;
    v228 = v227;
    v227 = __ROL4__(v227, 5);
    v229 = (v220 & v221 | v215 & (v221 | v220)) + v227 + v223;
    v221 = __ROL4__(v221, 30);
    v230 = v221 & v228 | v220 & (v228 | v221);
    v228 = __ROL4__(v228, 30);
    v231 = __ROL4__(v229, 5);
    v232 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v226, 8), _mm_srli_si128(v212, 8)), v182),
             _mm_xor_si128(v122, v137));
    v233 = v230 + v183.m128i_i32[0] + v215 + v231;
    v234 = v233;
    v233 = __ROL4__(v233, 5);
    v235 = (v228 & v229 | v221 & (v229 | v228)) + v233 + v183.m128i_i32[1] + v220;
    v229 = __ROL4__(v229, 30);
    v236 = _mm_or_si128(_mm_sll_epi32(v232, 2u), _mm_srl_epi32(v232, 0x1Eu));
    v237 = v229 & v234 | v228 & (v234 | v229);
    v234 = __ROL4__(v234, 30);
    v238 = v183.m128i_i32[3] + v228;
    v239 = __ROL4__(v235, 5);
    v240 = v237 + v183.m128i_i32[2] + v221;
    v359 = _mm_add_epi32(v236, *(__m128i *)&K_XMM_AR[6]);
    v241 = v240 + v239;
    v242 = v241;
    v241 = __ROL4__(v241, 5);
    v243 = (v234 & v235 | v229 & (v235 | v234)) + v241 + v238;
    v235 = __ROL4__(v235, 30);
    v244 = v242 ^ v235 ^ v234;
    v242 = __ROL4__(v242, 30);
    v245 = __ROL4__(v243, 5);
    v246 = _mm_xor_si128(
             _mm_xor_si128(_mm_or_si128(_mm_slli_si128(v236, 8), _mm_srli_si128(v226, 8)), v197),
             _mm_xor_si128(v137, v152));
    v247 = v244 + v198.m128i_i32[0] + v229 + v245;
    v248 = v247;
    v247 = __ROL4__(v247, 5);
    v249 = (v243 ^ v242 ^ v235) + v247 + v198.m128i_i32[1] + v234;
    v250 = __ROL4__(v243, 30);
    v251 = v248 ^ v250 ^ v242;
    v252 = __ROL4__(v248, 30);
    v253 = v198.m128i_i32[3] + v242;
    v254 = __ROL4__(v249, 5);
    v255 = v251 + v198.m128i_i32[2] + v235;
    v361 = _mm_add_epi32(_mm_or_si128(_mm_sll_epi32(v246, 2u), _mm_srl_epi32(v246, 0x1Eu)), *(__m128i *)&K_XMM_AR[6]);
    v256 = v255 + v254;
    v257 = v256;
    v256 = __ROL4__(v256, 5);
    v258 = (v249 ^ v252 ^ v250) + v256 + v253;
    v259 = __ROL4__(v249, 30);
    v4 += 64LL;
    v260 = (unsigned __int64)v5-- < 1;
    if ( v260 || v5 == 0 )
      break;
    v261.m128i_i64[0] = _byteswap_uint64(__PAIR__(*(_DWORD *)v4, *(_DWORD *)(v4 + 4)));
    v261.m128i_i64[1] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 8), *(_DWORD *)(v4 + 12)));
    v262 = v257 ^ v259 ^ v252;
    v263 = __ROL4__(v257, 30);
    v264 = __ROL4__(v258, 5);
    v12 = v261;
    v265 = v262 + v355.m128i_i32[0] + v250 + v264;
    v266 = v265;
    v265 = __ROL4__(v265, 5);
    v267 = (v258 ^ v263 ^ v259) + v265 + v355.m128i_i32[1] + v252;
    v268 = __ROL4__(v258, 30);
    v269 = v266 ^ v268 ^ v263;
    v266 = __ROL4__(v266, 30);
    v270 = v355.m128i_i32[3] + v263;
    v271 = __ROL4__(v267, 5);
    v272 = v269 + v355.m128i_i32[2] + v259;
    v354 = _mm_add_epi32(v261, *(__m128i *)K_XMM_AR);
    v273 = v272 + v271;
    v274 = v273;
    v273 = __ROL4__(v273, 5);
    v275 = (v267 ^ v266 ^ v268) + v273 + v270;
    v267 = __ROL4__(v267, 30);
    v261.m128i_i64[0] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 16), *(_DWORD *)(v4 + 20)));
    v261.m128i_i64[1] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 24), *(_DWORD *)(v4 + 28)));
    v276 = v274 ^ v267 ^ v266;
    v274 = __ROL4__(v274, 30);
    v277 = __ROL4__(v275, 5);
    v13 = v261;
    v278 = v276 + v357.m128i_i32[0] + v268 + v277;
    v279 = v278;
    v278 = __ROL4__(v278, 5);
    v280 = (v275 ^ v274 ^ v267) + v278 + v357.m128i_i32[1] + v266;
    v275 = __ROL4__(v275, 30);
    v281 = v279 ^ v275 ^ v274;
    v279 = __ROL4__(v279, 30);
    v282 = v357.m128i_i32[3] + v274;
    v283 = __ROL4__(v280, 5);
    v284 = v281 + v357.m128i_i32[2] + v267;
    v356 = _mm_add_epi32(v261, *(__m128i *)K_XMM_AR);
    v285 = v284 + v283;
    v286 = v285;
    v285 = __ROL4__(v285, 5);
    v287 = (v280 ^ v279 ^ v275) + v285 + v282;
    v280 = __ROL4__(v280, 30);
    v261.m128i_i64[0] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 32), *(_DWORD *)(v4 + 36)));
    v261.m128i_i64[1] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 40), *(_DWORD *)(v4 + 44)));
    v288 = v286 ^ v280 ^ v279;
    v286 = __ROL4__(v286, 30);
    v289 = __ROL4__(v287, 5);
    v14 = v261;
    v290 = v288 + v359.m128i_i32[0] + v275 + v289;
    v291 = v290;
    v290 = __ROL4__(v290, 5);
    v292 = (v287 ^ v286 ^ v280) + v290 + v359.m128i_i32[1] + v279;
    v287 = __ROL4__(v287, 30);
    v293 = v291 ^ v287 ^ v286;
    v291 = __ROL4__(v291, 30);
    v294 = v359.m128i_i32[3] + v286;
    v295 = __ROL4__(v292, 5);
    v296 = v293 + v359.m128i_i32[2] + v280;
    v358 = _mm_add_epi32(v261, *(__m128i *)K_XMM_AR);
    v297 = v296 + v295;
    v298 = v297;
    v297 = __ROL4__(v297, 5);
    v299 = (v292 ^ v291 ^ v287) + v297 + v294;
    v292 = __ROL4__(v292, 30);
    v261.m128i_i64[0] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 48), *(_DWORD *)(v4 + 52)));
    v261.m128i_i64[1] = _byteswap_uint64(__PAIR__(*(_DWORD *)(v4 + 56), *(_DWORD *)(v4 + 60)));
    v300 = v298 ^ v292 ^ v291;
    v298 = __ROL4__(v298, 30);
    v301 = __ROL4__(v299, 5);
    v15 = v261;
    v302 = v300 + v361.m128i_i32[0] + v287 + v301;
    v303 = v302;
    v302 = __ROL4__(v302, 5);
    v304 = (v299 ^ v298 ^ v292) + v302 + v361.m128i_i32[1] + v291;
    v299 = __ROL4__(v299, 30);
    v305 = v303 ^ v299 ^ v298;
    v303 = __ROL4__(v303, 30);
    v306 = v361.m128i_i32[3] + v298;
    v307 = __ROL4__(v304, 5);
    v308 = v305 + v361.m128i_i32[2] + v292;
    v360 = _mm_add_epi32(v261, *(__m128i *)K_XMM_AR);
    v309 = v308 + v307;
    v310 = v309;
    v309 = __ROL4__(v309, 5);
    v311 = v304 ^ v303 ^ v299;
    v304 = __ROL4__(v304, 30);
    v6 = *(_DWORD *)v3 + v311 + v309 + v306;
    *(_DWORD *)v3 = v6;
    v7 = *(_DWORD *)(v3 + 4) + v310;
    *(_DWORD *)(v3 + 4) = v7;
    v8 = *(_DWORD *)(v3 + 8) + v304;
    *(_DWORD *)(v3 + 8) = v8;
    v9 = *(_DWORD *)(v3 + 12) + v303;
    *(_DWORD *)(v3 + 12) = v9;
    v10 = *(_DWORD *)(v3 + 16) + v299;
    *(_DWORD *)(v3 + 16) = v10;
  }
  v312 = v257 ^ v259 ^ v252;
  v313 = __ROL4__(v257, 30);
  v314 = __ROL4__(v258, 5);
  v315 = v312 + v355.m128i_i32[0] + v250 + v314;
  v316 = v315;
  v315 = __ROL4__(v315, 5);
  v317 = (v258 ^ v313 ^ v259) + v315 + v355.m128i_i32[1] + v252;
  v318 = __ROL4__(v258, 30);
  v319 = v316 ^ v318 ^ v313;
  v316 = __ROL4__(v316, 30);
  v320 = __ROL4__(v317, 5);
  v321 = v319 + v355.m128i_i32[2] + v259 + v320;
  v322 = v321;
  v321 = __ROL4__(v321, 5);
  v323 = (v317 ^ v316 ^ v318) + v321 + v355.m128i_i32[3] + v313;
  v317 = __ROL4__(v317, 30);
  v324 = v322 ^ v317 ^ v316;
  v322 = __ROL4__(v322, 30);
  v325 = __ROL4__(v323, 5);
  v326 = v324 + v357.m128i_i32[0] + v318 + v325;
  v327 = v326;
  v326 = __ROL4__(v326, 5);
  v328 = (v323 ^ v322 ^ v317) + v326 + v357.m128i_i32[1] + v316;
  v323 = __ROL4__(v323, 30);
  v329 = v327 ^ v323 ^ v322;
  v327 = __ROL4__(v327, 30);
  v330 = __ROL4__(v328, 5);
  v331 = v329 + v357.m128i_i32[2] + v317 + v330;
  v332 = v331;
  v331 = __ROL4__(v331, 5);
  v333 = (v328 ^ v327 ^ v323) + v331 + v357.m128i_i32[3] + v322;
  v328 = __ROL4__(v328, 30);
  v334 = v332 ^ v328 ^ v327;
  v332 = __ROL4__(v332, 30);
  v335 = __ROL4__(v333, 5);
  v336 = v334 + v359.m128i_i32[0] + v323 + v335;
  v337 = v336;
  v336 = __ROL4__(v336, 5);
  v338 = (v333 ^ v332 ^ v328) + v336 + v359.m128i_i32[1] + v327;
  v333 = __ROL4__(v333, 30);
  v339 = v337 ^ v333 ^ v332;
  v337 = __ROL4__(v337, 30);
  v340 = __ROL4__(v338, 5);
  v341 = v339 + v359.m128i_i32[2] + v328 + v340;
  v342 = v341;
  v341 = __ROL4__(v341, 5);
  v343 = (v338 ^ v337 ^ v333) + v341 + v359.m128i_i32[3] + v332;
  v338 = __ROL4__(v338, 30);
  v344 = v342 ^ v338 ^ v337;
  v342 = __ROL4__(v342, 30);
  v345 = __ROL4__(v343, 5);
  v346 = v344 + v361.m128i_i32[0] + v333 + v345;
  v347 = v346;
  v346 = __ROL4__(v346, 5);
  v348 = (v343 ^ v342 ^ v338) + v346 + v361.m128i_i32[1] + v337;
  v343 = __ROL4__(v343, 30);
  v349 = v347 ^ v343 ^ v342;
  v347 = __ROL4__(v347, 30);
  v350 = __ROL4__(v348, 5);
  v351 = v349 + v361.m128i_i32[2] + v338 + v350;
  v352 = v351;
  v351 = __ROL4__(v351, 5);
  result = v348 ^ v347 ^ (unsigned int)v343;
  v348 = __ROL4__(v348, 30);
  *(_DWORD *)v3 += result + v351 + v361.m128i_i32[3] + v342;
  *(_DWORD *)(v3 + 4) += v352;
  *(_DWORD *)(v3 + 8) += v348;
  *(_DWORD *)(v3 + 12) += v347;
  *(_DWORD *)(v3 + 16) += v343;
  return result;
}
// 39D2F: failed to expand linear variable var28
// 39D2F: failed to expand linear variable var28a
// 66E90: using guessed type __int64 K_XMM_AR[10];

//----- (000000000003AFA6) ----------------------------------------------------
__int64 __fastcall ccsha256_ltc_compress(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r8@1
  __int64 v4; // rcx@2
  __int64 v5; // rax@3
  signed __int64 v6; // rcx@4
  int v7; // er9@5
  int v8; // er10@5
  unsigned int v9; // eax@5
  int v10; // er9@5
  int v11; // er10@5
  int v12; // eax@6
  int v13; // ecx@6
  int v14; // ecx@6
  int v15; // eax@6
  int v16; // eax@6
  int v17; // ecx@6
  int v18; // eax@6
  int v19; // er9@6
  int v20; // er11@6
  int v21; // er10@6
  int v22; // eax@6
  int v23; // ecx@6
  int v24; // er11@6
  int v25; // ecx@6
  int v26; // ebx@6
  int v27; // er9@6
  int v28; // eax@6
  int v29; // ebx@6
  int v30; // ebx@6
  int v31; // eax@6
  int v32; // er11@6
  int v33; // ecx@6
  int v34; // ebx@6
  int v35; // er9@6
  int v36; // er10@6
  int v37; // eax@6
  int v38; // ebx@6
  int v39; // er11@6
  int v40; // ecx@6
  int v41; // ebx@6
  int v42; // er9@6
  int v43; // eax@6
  int v44; // ebx@6
  int v45; // ebx@6
  int v46; // eax@6
  int v47; // er11@6
  int v48; // ecx@6
  int v49; // ebx@6
  int v50; // er9@6
  int v51; // er10@6
  int v52; // eax@6
  int v53; // ebx@6
  int v54; // er11@6
  int v55; // ecx@6
  int v56; // ebx@6
  int v57; // er9@6
  int v58; // er10@6
  int v59; // eax@6
  int v60; // ebx@6
  int v61; // er11@6
  int v62; // ecx@6
  int v63; // ebx@6
  int v64; // er9@6
  int v65; // er10@6
  int v66; // eax@6
  int v67; // ebx@6
  int v68; // er11@6
  int v69; // ecx@6
  int v70; // ebx@6
  int v71; // er9@6
  int v72; // er10@6
  int v73; // eax@6
  int v74; // ebx@6
  int v75; // er11@6
  int v76; // ecx@6
  int v77; // ebx@6
  int v78; // er9@6
  int v79; // er10@6
  int v80; // eax@6
  int v81; // ebx@6
  int v82; // er11@6
  int v83; // ecx@6
  int v84; // ebx@6
  int v85; // er9@6
  int v86; // eax@6
  int v87; // ebx@6
  int v88; // ebx@6
  int v89; // eax@6
  int v90; // er11@6
  int v91; // ecx@6
  int v92; // ebx@6
  int v93; // er9@6
  int v94; // er10@6
  int v95; // eax@6
  int v96; // ebx@6
  int v97; // er11@6
  int v98; // ecx@6
  int v99; // ebx@6
  int v100; // er9@6
  int v101; // eax@6
  int v102; // ebx@6
  int v103; // ebx@6
  int v104; // eax@6
  int v105; // er11@6
  int v106; // ecx@6
  int v107; // ebx@6
  int v108; // er9@6
  int v109; // er10@6
  int v110; // eax@6
  int v111; // ebx@6
  int v112; // er11@6
  int v113; // ecx@6
  int v114; // ebx@6
  int v115; // er9@6
  int v116; // er10@6
  int v117; // eax@6
  int v118; // ebx@6
  int v119; // er11@6
  int v120; // ecx@6
  int v121; // ebx@6
  int v122; // er9@6
  int v123; // er10@6
  int v124; // eax@6
  int v125; // ebx@6
  int v126; // er11@6
  int v127; // ecx@6
  int v128; // ebx@6
  int v129; // er9@6
  int v130; // er10@6
  int v131; // eax@6
  int v132; // ebx@6
  int v133; // er11@6
  int v134; // ecx@6
  int v135; // ebx@6
  int v136; // er9@6
  int v137; // er10@6
  int v138; // eax@6
  int v139; // ebx@6
  int v140; // er11@6
  int v141; // ecx@6
  int v142; // ebx@6
  int v143; // er9@6
  int v144; // eax@6
  int v145; // ebx@6
  int v146; // ebx@6
  int v147; // eax@6
  int v148; // er11@6
  int v149; // ecx@6
  int v150; // ebx@6
  int v151; // er9@6
  int v152; // er10@6
  int v153; // eax@6
  int v154; // ebx@6
  int v155; // er11@6
  int v156; // ecx@6
  int v157; // ebx@6
  int v158; // er9@6
  int v159; // eax@6
  int v160; // ebx@6
  int v161; // ebx@6
  int v162; // eax@6
  int v163; // er11@6
  int v164; // ecx@6
  int v165; // ebx@6
  int v166; // er9@6
  int v167; // er10@6
  int v168; // eax@6
  int v169; // ebx@6
  int v170; // er11@6
  int v171; // ecx@6
  int v172; // ebx@6
  int v173; // er9@6
  int v174; // er10@6
  int v175; // eax@6
  int v176; // ebx@6
  int v177; // er11@6
  int v178; // ecx@6
  int v179; // ebx@6
  int v180; // er9@6
  int v181; // er10@6
  int v182; // eax@6
  int v183; // ebx@6
  int v184; // er11@6
  int v185; // ecx@6
  int v186; // ebx@6
  int v187; // er9@6
  int v188; // er10@6
  int v189; // eax@6
  int v190; // ebx@6
  int v191; // er11@6
  int v192; // ecx@6
  int v193; // ebx@6
  int v194; // er9@6
  int v195; // er10@6
  int v196; // eax@6
  int v197; // ebx@6
  int v198; // er11@6
  int v199; // ecx@6
  int v200; // ebx@6
  int v201; // er9@6
  int v202; // eax@6
  int v203; // ebx@6
  int v204; // ebx@6
  int v205; // eax@6
  int v206; // er11@6
  int v207; // ecx@6
  int v208; // ebx@6
  int v209; // er9@6
  int v210; // er10@6
  int v211; // eax@6
  int v212; // ebx@6
  int v213; // er11@6
  int v214; // ecx@6
  int v215; // ebx@6
  int v216; // er9@6
  int v217; // eax@6
  int v218; // ebx@6
  int v219; // ebx@6
  int v220; // eax@6
  int v221; // er11@6
  int v222; // ecx@6
  int v223; // ebx@6
  int v224; // er9@6
  int v225; // er10@6
  int v226; // eax@6
  int v227; // ebx@6
  int v228; // er11@6
  int v229; // ecx@6
  int v230; // ebx@6
  int v231; // er9@6
  int v232; // er10@6
  int v233; // eax@6
  int v234; // ebx@6
  int v235; // er11@6
  int v236; // ecx@6
  int v237; // ebx@6
  int v238; // er9@6
  int v239; // er10@6
  int v240; // eax@6
  int v241; // ebx@6
  int v242; // er11@6
  int v243; // ecx@6
  int v244; // ebx@6
  int v245; // er9@6
  int v246; // er10@6
  int v247; // eax@6
  int v248; // ebx@6
  int v249; // er11@6
  int v250; // ecx@6
  int v251; // ebx@6
  int v252; // er9@6
  int v253; // er10@6
  int v254; // eax@6
  int v255; // ebx@6
  int v256; // er11@6
  int v257; // ecx@6
  int v258; // ebx@6
  int v259; // er9@6
  int v260; // eax@6
  int v261; // ebx@6
  int v262; // ebx@6
  int v263; // eax@6
  int v264; // er11@6
  int v265; // ecx@6
  int v266; // ebx@6
  int v267; // er9@6
  int v268; // er10@6
  int v269; // eax@6
  int v270; // ebx@6
  int v271; // er11@6
  int v272; // ecx@6
  int v273; // ebx@6
  int v274; // er9@6
  int v275; // eax@6
  int v276; // ebx@6
  int v277; // ebx@6
  int v278; // eax@6
  int v279; // er11@6
  int v280; // ecx@6
  int v281; // ebx@6
  int v282; // er9@6
  int v283; // er10@6
  int v284; // eax@6
  int v285; // ebx@6
  int v286; // er11@6
  int v287; // ecx@6
  int v288; // ebx@6
  int v289; // er9@6
  int v290; // er10@6
  int v291; // eax@6
  int v292; // ebx@6
  int v293; // er11@6
  int v294; // ecx@6
  int v295; // ebx@6
  int v296; // er9@6
  int v297; // er10@6
  int v298; // eax@6
  int v299; // ebx@6
  int v300; // er11@6
  int v301; // ecx@6
  int v302; // ebx@6
  int v303; // er9@6
  int v304; // er10@6
  int v305; // eax@6
  int v306; // ebx@6
  int v307; // er11@6
  int v308; // ecx@6
  int v309; // ebx@6
  int v310; // er9@6
  int v311; // er10@6
  int v312; // eax@6
  int v313; // ebx@6
  int v314; // er11@6
  int v315; // ecx@6
  int v316; // ebx@6
  int v317; // er9@6
  int v318; // eax@6
  int v319; // ebx@6
  int v320; // ebx@6
  int v321; // eax@6
  int v322; // er11@6
  int v323; // ecx@6
  int v324; // ebx@6
  int v325; // er9@6
  int v326; // er10@6
  int v327; // eax@6
  int v328; // ebx@6
  int v329; // er11@6
  int v330; // ecx@6
  int v331; // ebx@6
  int v332; // er9@6
  int v333; // eax@6
  int v334; // ebx@6
  int v335; // ebx@6
  int v336; // eax@6
  int v337; // er11@6
  int v338; // ecx@6
  int v339; // ebx@6
  int v340; // er9@6
  int v341; // er10@6
  int v342; // eax@6
  int v343; // ebx@6
  int v344; // er11@6
  int v345; // ecx@6
  int v346; // ebx@6
  int v347; // er9@6
  int v348; // er10@6
  int v349; // eax@6
  int v350; // ebx@6
  int v351; // er11@6
  int v352; // ecx@6
  int v353; // ebx@6
  int v354; // er9@6
  int v355; // er10@6
  int v356; // eax@6
  int v357; // ebx@6
  int v358; // er11@6
  int v359; // ecx@6
  int v360; // ebx@6
  int v361; // er9@6
  int v362; // er10@6
  int v363; // eax@6
  int v364; // ebx@6
  int v365; // er11@6
  int v366; // ecx@6
  int v367; // ebx@6
  int v368; // er9@6
  int v369; // er10@6
  int v370; // eax@6
  int v371; // ebx@6
  int v372; // er11@6
  int v373; // ecx@6
  int v374; // ebx@6
  int v375; // er9@6
  int v376; // eax@6
  int v377; // ebx@6
  int v378; // ebx@6
  int v379; // eax@6
  int v380; // er11@6
  int v381; // ecx@6
  int v382; // ebx@6
  int v383; // er9@6
  int v384; // er10@6
  int v385; // eax@6
  int v386; // ebx@6
  int v387; // er11@6
  int v388; // ecx@6
  int v389; // ebx@6
  int v390; // er9@6
  int v391; // eax@6
  int v392; // ebx@6
  int v393; // ebx@6
  int v394; // eax@6
  int v395; // er11@6
  int v396; // ecx@6
  int v397; // ebx@6
  int v398; // er9@6
  int v399; // er10@6
  int v400; // eax@6
  int v401; // ebx@6
  int v402; // er11@6
  int v403; // ecx@6
  int v404; // ebx@6
  int v405; // er9@6
  int v406; // er10@6
  int v407; // eax@6
  int v408; // ebx@6
  int v409; // er11@6
  int v410; // ecx@6
  int v411; // ebx@6
  int v412; // er9@6
  int v413; // er10@6
  int v414; // eax@6
  int v415; // ebx@6
  int v416; // er11@6
  int v417; // ecx@6
  int v418; // ebx@6
  int v419; // er9@6
  int v420; // er10@6
  int v421; // eax@6
  int v422; // ebx@6
  int v423; // er11@6
  int v424; // ecx@6
  int v425; // ebx@6
  int v426; // er9@6
  int v427; // er10@6
  int v428; // eax@6
  int v429; // ebx@6
  int v430; // er11@6
  int v431; // ecx@6
  int v432; // ebx@6
  int v433; // er9@6
  int v434; // eax@6
  int v435; // ebx@6
  int v436; // ebx@6
  int v437; // eax@6
  int v438; // er11@6
  int v439; // ecx@6
  int v440; // ebx@6
  int v441; // er9@6
  int v442; // er10@6
  int v443; // eax@6
  int v444; // ebx@6
  int v445; // er11@6
  int v446; // ecx@6
  int v447; // ebx@6
  int v448; // er9@6
  int v449; // eax@6
  int v450; // ebx@6
  int v451; // ebx@6
  int v452; // eax@6
  int v453; // er11@6
  int v454; // ecx@6
  int v455; // ebx@6
  int v456; // er9@6
  int v457; // er10@6
  int v458; // eax@6
  int v459; // ebx@6
  int v460; // er11@6
  int v461; // ecx@6
  int v462; // ebx@6
  int v463; // er9@6
  int v464; // er10@6
  int v465; // eax@6
  int v466; // ebx@6
  int v467; // er11@6
  int v468; // ecx@6
  int v469; // ebx@6
  int v470; // er9@6
  int v471; // er10@6
  int v472; // eax@6
  int v473; // ebx@6
  int v474; // er11@6
  int v475; // ecx@6
  int v476; // ebx@6
  int v477; // er9@6
  int v478; // er10@6
  int v479; // ebx@6
  int v480; // eax@6
  int v481; // eax@6
  __int64 j; // rcx@6
  int v484; // [sp+0h] [bp-170h]@5
  int v485[8]; // [sp+4h] [bp-16Ch]@5
  int v486[5]; // [sp+24h] [bp-14Ch]@5
  int v487[2]; // [sp+38h] [bp-138h]@5
  int v488; // [sp+40h] [bp-130h]@4
  int v489; // [sp+44h] [bp-12Ch]@6
  int v490; // [sp+48h] [bp-128h]@6
  int v491; // [sp+4Ch] [bp-124h]@6
  int v492; // [sp+50h] [bp-120h]@6
  int v493; // [sp+54h] [bp-11Ch]@6
  int v494; // [sp+58h] [bp-118h]@6
  int v495; // [sp+5Ch] [bp-114h]@6
  int v496; // [sp+60h] [bp-110h]@6
  int v497; // [sp+64h] [bp-10Ch]@6
  int v498; // [sp+68h] [bp-108h]@6
  int v499; // [sp+6Ch] [bp-104h]@6
  int v500; // [sp+70h] [bp-100h]@6
  int v501; // [sp+74h] [bp-FCh]@6
  int v502; // [sp+78h] [bp-F8h]@6
  int v503; // [sp+7Ch] [bp-F4h]@6
  int v504; // [sp+80h] [bp-F0h]@6
  int v505; // [sp+84h] [bp-ECh]@6
  int v506; // [sp+88h] [bp-E8h]@6
  int v507; // [sp+8Ch] [bp-E4h]@6
  int v508; // [sp+90h] [bp-E0h]@6
  int v509; // [sp+94h] [bp-DCh]@6
  int v510; // [sp+98h] [bp-D8h]@6
  int v511; // [sp+9Ch] [bp-D4h]@6
  int v512; // [sp+A0h] [bp-D0h]@6
  int v513; // [sp+A4h] [bp-CCh]@6
  int v514; // [sp+A8h] [bp-C8h]@6
  int v515; // [sp+ACh] [bp-C4h]@6
  int v516; // [sp+B0h] [bp-C0h]@6
  int v517; // [sp+B4h] [bp-BCh]@6
  int v518; // [sp+B8h] [bp-B8h]@6
  int v519; // [sp+BCh] [bp-B4h]@6
  int v520; // [sp+C0h] [bp-B0h]@6
  int v521; // [sp+C4h] [bp-ACh]@6
  int v522; // [sp+C8h] [bp-A8h]@6
  int v523; // [sp+CCh] [bp-A4h]@6
  int v524; // [sp+D0h] [bp-A0h]@6
  int v525; // [sp+D4h] [bp-9Ch]@6
  int v526; // [sp+D8h] [bp-98h]@6
  int v527; // [sp+DCh] [bp-94h]@6
  int v528; // [sp+E0h] [bp-90h]@6
  int v529; // [sp+E4h] [bp-8Ch]@6
  int v530; // [sp+E8h] [bp-88h]@6
  int v531; // [sp+ECh] [bp-84h]@6
  int v532; // [sp+F0h] [bp-80h]@6
  int v533; // [sp+F4h] [bp-7Ch]@6
  int v534; // [sp+F8h] [bp-78h]@6
  int v535; // [sp+FCh] [bp-74h]@6
  int v536; // [sp+100h] [bp-70h]@6
  int v537; // [sp+104h] [bp-6Ch]@6
  int v538; // [sp+108h] [bp-68h]@6
  int v539; // [sp+10Ch] [bp-64h]@6
  int v540; // [sp+110h] [bp-60h]@6
  int v541; // [sp+114h] [bp-5Ch]@6
  int v542; // [sp+118h] [bp-58h]@6
  int v543; // [sp+11Ch] [bp-54h]@6
  int v544; // [sp+120h] [bp-50h]@6
  int v545; // [sp+124h] [bp-4Ch]@6
  int v546; // [sp+128h] [bp-48h]@6
  int v547; // [sp+12Ch] [bp-44h]@6
  int v548; // [sp+130h] [bp-40h]@6
  int v549; // [sp+134h] [bp-3Ch]@6
  int v550; // [sp+138h] [bp-38h]@6
  int v551; // [sp+13Ch] [bp-34h]@6
  int v552; // [sp+140h] [bp-30h]@3
  int v553; // [sp+144h] [bp-2Ch]@6
  int v554; // [sp+148h] [bp-28h]@6
  int v555; // [sp+14Ch] [bp-24h]@6
  int v556; // [sp+150h] [bp-20h]@6
  int v557; // [sp+154h] [bp-1Ch]@6
  int v558; // [sp+158h] [bp-18h]@6
  int v559; // [sp+15Ch] [bp-14h]@6
  __int64 i; // [sp+160h] [bp-10h]@1

  v3 = off_69010[0];
  for ( i = *(_QWORD *)off_69010[0]; a2; --a2 )
  {
    v4 = 0LL;
    do
    {
      *(&v552 + v4) = *(_DWORD *)(a1 + 4 * v4);
      ++v4;
      v5 = 0LL;
    }
    while ( v4 != 8 );
    do
    {
      *(int *)((char *)&v488 + v5 * 4) = _byteswap_ulong(*(_DWORD *)(a3 + v5 * 4));
      ++v5;
      v6 = 16LL;
    }
    while ( v5 != 16 );
    do
    {
      v7 = __ROR4__(v487[v6], 17);
      v8 = __ROR4__(v487[v6], 19);
      v9 = v486[v6] + (v7 ^ v8 ^ ((unsigned int)v487[v6] >> 10));
      v10 = __ROR4__(v485[v6], 7);
      v11 = __ROR4__(v485[v6], 18);
      *(&v488 + v6) = (v10 ^ v11 ^ ((unsigned int)v485[v6] >> 3)) + *(&v484 + v6) + v9;
      ++v6;
    }
    while ( v6 != 64 );
    v12 = __ROR4__(v556, 6);
    v13 = __ROR4__(v556, 11);
    v14 = v12 ^ v13;
    v15 = __ROR4__(v556, 25);
    v16 = v14 ^ v15;
    v17 = v558 ^ v556 & (v558 ^ v557);
    v18 = v488 + v559 + v16;
    v19 = v18 + v17;
    v20 = v17 + v18 + 1116352408;
    v21 = __ROR4__(v552, 2);
    v22 = __ROR4__(v552, 13);
    v23 = __ROR4__(v552, 22);
    v24 = v555 + v20;
    v555 = v24;
    v559 = (v554 & (v552 | v553) | v552 & v553) + (v21 ^ v22 ^ v23) + v19 + 1116352408;
    v24 = __ROR4__(v24, 6);
    v25 = __ROR4__(v555, 11);
    v26 = __ROR4__(v555, 25);
    v27 = v489 + v558 + (v24 ^ v25 ^ v26) + (v557 ^ v555 & (v557 ^ v556));
    v28 = __ROR4__(v559, 2);
    v29 = __ROR4__(v559, 13);
    v30 = v28 ^ v29;
    v31 = __ROR4__(v559, 22);
    v32 = v554 + v27 + 1899447441;
    v554 = v32;
    v558 = (v553 & (v559 | v552) | v559 & v552) + (v30 ^ v31) + v27 + 1899447441;
    v32 = __ROR4__(v32, 6);
    v33 = __ROR4__(v554, 11);
    v34 = __ROR4__(v554, 25);
    v35 = v490 + v557 + (v32 ^ v33 ^ v34) + (v556 ^ v554 & (v556 ^ v555));
    v36 = __ROR4__(v558, 2);
    v37 = __ROR4__(v558, 13);
    v38 = __ROR4__(v558, 22);
    v39 = v553 + v35 - 1245643825;
    v553 = v39;
    v557 = (v552 & (v558 | v559) | v558 & v559) + (v36 ^ v37 ^ v38) + v35 - 1245643825;
    v39 = __ROR4__(v39, 6);
    v40 = __ROR4__(v553, 11);
    v41 = __ROR4__(v553, 25);
    v42 = v491 + v556 + (v39 ^ v40 ^ v41) + (v555 ^ v553 & (v555 ^ v554));
    v43 = __ROR4__(v557, 2);
    v44 = __ROR4__(v557, 13);
    v45 = v43 ^ v44;
    v46 = __ROR4__(v557, 22);
    v47 = v552 + v42 - 373957723;
    v552 = v47;
    v556 = (v559 & (v557 | v558) | v557 & v558) + (v45 ^ v46) + v42 - 373957723;
    v47 = __ROR4__(v47, 6);
    v48 = __ROR4__(v552, 11);
    v49 = __ROR4__(v552, 25);
    v50 = v492 + v555 + (v47 ^ v48 ^ v49) + (v554 ^ v552 & (v554 ^ v553));
    v51 = __ROR4__(v556, 2);
    v52 = __ROR4__(v556, 13);
    v53 = __ROR4__(v556, 22);
    v54 = v559 + v50 + 961987163;
    v559 = v54;
    v555 = (v558 & (v556 | v557) | v556 & v557) + (v51 ^ v52 ^ v53) + v50 + 961987163;
    v54 = __ROR4__(v54, 6);
    v55 = __ROR4__(v559, 11);
    v56 = __ROR4__(v559, 25);
    v57 = v493 + v554 + (v54 ^ v55 ^ v56) + (v553 ^ v559 & (v553 ^ v552));
    v58 = __ROR4__(v555, 2);
    v59 = __ROR4__(v555, 13);
    v60 = __ROR4__(v555, 22);
    v61 = v558 + v57 + 1508970993;
    v558 = v61;
    v554 = (v557 & (v555 | v556) | v555 & v556) + (v58 ^ v59 ^ v60) + v57 + 1508970993;
    v61 = __ROR4__(v61, 6);
    v62 = __ROR4__(v558, 11);
    v63 = __ROR4__(v558, 25);
    v64 = v494 + v553 + (v61 ^ v62 ^ v63) + (v552 ^ v558 & (v552 ^ v559));
    v65 = __ROR4__(v554, 2);
    v66 = __ROR4__(v554, 13);
    v67 = __ROR4__(v554, 22);
    v68 = v557 + v64 - 1841331548;
    v557 = v68;
    v553 = (v556 & (v554 | v555) | v554 & v555) + (v65 ^ v66 ^ v67) + v64 - 1841331548;
    v68 = __ROR4__(v68, 6);
    v69 = __ROR4__(v557, 11);
    v70 = __ROR4__(v557, 25);
    v71 = v495 + v552 + (v68 ^ v69 ^ v70) + (v559 ^ v557 & (v559 ^ v558));
    v72 = __ROR4__(v553, 2);
    v73 = __ROR4__(v553, 13);
    v74 = __ROR4__(v553, 22);
    v75 = v556 + v71 - 1424204075;
    v556 = v75;
    v552 = (v555 & (v553 | v554) | v553 & v554) + (v72 ^ v73 ^ v74) + v71 - 1424204075;
    v75 = __ROR4__(v75, 6);
    v76 = __ROR4__(v556, 11);
    v77 = __ROR4__(v556, 25);
    v78 = v496 + v559 + (v75 ^ v76 ^ v77) + (v558 ^ v556 & (v558 ^ v557));
    v79 = __ROR4__(v552, 2);
    v80 = __ROR4__(v552, 13);
    v81 = __ROR4__(v552, 22);
    v82 = v555 + v78 - 670586216;
    v555 = v82;
    v559 = (v554 & (v552 | v553) | v552 & v553) + (v79 ^ v80 ^ v81) + v78 - 670586216;
    v82 = __ROR4__(v82, 6);
    v83 = __ROR4__(v555, 11);
    v84 = __ROR4__(v555, 25);
    v85 = v497 + v558 + (v82 ^ v83 ^ v84) + (v557 ^ v555 & (v557 ^ v556));
    v86 = __ROR4__(v559, 2);
    v87 = __ROR4__(v559, 13);
    v88 = v86 ^ v87;
    v89 = __ROR4__(v559, 22);
    v90 = v554 + v85 + 310598401;
    v554 = v90;
    v558 = (v553 & (v559 | v552) | v559 & v552) + (v88 ^ v89) + v85 + 310598401;
    v90 = __ROR4__(v90, 6);
    v91 = __ROR4__(v554, 11);
    v92 = __ROR4__(v554, 25);
    v93 = v498 + v557 + (v90 ^ v91 ^ v92) + (v556 ^ v554 & (v556 ^ v555));
    v94 = __ROR4__(v558, 2);
    v95 = __ROR4__(v558, 13);
    v96 = __ROR4__(v558, 22);
    v97 = v553 + v93 + 607225278;
    v553 = v97;
    v557 = (v552 & (v558 | v559) | v558 & v559) + (v94 ^ v95 ^ v96) + v93 + 607225278;
    v97 = __ROR4__(v97, 6);
    v98 = __ROR4__(v553, 11);
    v99 = __ROR4__(v553, 25);
    v100 = v499 + v556 + (v97 ^ v98 ^ v99) + (v555 ^ v553 & (v555 ^ v554));
    v101 = __ROR4__(v557, 2);
    v102 = __ROR4__(v557, 13);
    v103 = v101 ^ v102;
    v104 = __ROR4__(v557, 22);
    v105 = v552 + v100 + 1426881987;
    v552 = v105;
    v556 = (v559 & (v557 | v558) | v557 & v558) + (v103 ^ v104) + v100 + 1426881987;
    v105 = __ROR4__(v105, 6);
    v106 = __ROR4__(v552, 11);
    v107 = __ROR4__(v552, 25);
    v108 = v500 + v555 + (v105 ^ v106 ^ v107) + (v554 ^ v552 & (v554 ^ v553));
    v109 = __ROR4__(v556, 2);
    v110 = __ROR4__(v556, 13);
    v111 = __ROR4__(v556, 22);
    v112 = v559 + v108 + 1925078388;
    v559 = v112;
    v555 = (v558 & (v556 | v557) | v556 & v557) + (v109 ^ v110 ^ v111) + v108 + 1925078388;
    v112 = __ROR4__(v112, 6);
    v113 = __ROR4__(v559, 11);
    v114 = __ROR4__(v559, 25);
    v115 = v501 + v554 + (v112 ^ v113 ^ v114) + (v553 ^ v559 & (v553 ^ v552));
    v116 = __ROR4__(v555, 2);
    v117 = __ROR4__(v555, 13);
    v118 = __ROR4__(v555, 22);
    v119 = v558 + v115 - 2132889090;
    v558 = v119;
    v554 = (v557 & (v555 | v556) | v555 & v556) + (v116 ^ v117 ^ v118) + v115 - 2132889090;
    v119 = __ROR4__(v119, 6);
    v120 = __ROR4__(v558, 11);
    v121 = __ROR4__(v558, 25);
    v122 = v502 + v553 + (v119 ^ v120 ^ v121) + (v552 ^ v558 & (v552 ^ v559));
    v123 = __ROR4__(v554, 2);
    v124 = __ROR4__(v554, 13);
    v125 = __ROR4__(v554, 22);
    v126 = v557 + v122 - 1680079193;
    v557 = v126;
    v553 = (v556 & (v554 | v555) | v554 & v555) + (v123 ^ v124 ^ v125) + v122 - 1680079193;
    v126 = __ROR4__(v126, 6);
    v127 = __ROR4__(v557, 11);
    v128 = __ROR4__(v557, 25);
    v129 = v503 + v552 + (v126 ^ v127 ^ v128) + (v559 ^ v557 & (v559 ^ v558));
    v130 = __ROR4__(v553, 2);
    v131 = __ROR4__(v553, 13);
    v132 = __ROR4__(v553, 22);
    v133 = v556 + v129 - 1046744716;
    v556 = v133;
    v552 = (v555 & (v553 | v554) | v553 & v554) + (v130 ^ v131 ^ v132) + v129 - 1046744716;
    v133 = __ROR4__(v133, 6);
    v134 = __ROR4__(v556, 11);
    v135 = __ROR4__(v556, 25);
    v136 = v504 + v559 + (v133 ^ v134 ^ v135) + (v558 ^ v556 & (v558 ^ v557));
    v137 = __ROR4__(v552, 2);
    v138 = __ROR4__(v552, 13);
    v139 = __ROR4__(v552, 22);
    v140 = v555 + v136 - 459576895;
    v555 = v140;
    v559 = (v554 & (v552 | v553) | v552 & v553) + (v137 ^ v138 ^ v139) + v136 - 459576895;
    v140 = __ROR4__(v140, 6);
    v141 = __ROR4__(v555, 11);
    v142 = __ROR4__(v555, 25);
    v143 = v505 + v558 + (v140 ^ v141 ^ v142) + (v557 ^ v555 & (v557 ^ v556));
    v144 = __ROR4__(v559, 2);
    v145 = __ROR4__(v559, 13);
    v146 = v144 ^ v145;
    v147 = __ROR4__(v559, 22);
    v148 = v554 + v143 - 272742522;
    v554 = v148;
    v558 = (v553 & (v559 | v552) | v559 & v552) + (v146 ^ v147) + v143 - 272742522;
    v148 = __ROR4__(v148, 6);
    v149 = __ROR4__(v554, 11);
    v150 = __ROR4__(v554, 25);
    v151 = v506 + v557 + (v148 ^ v149 ^ v150) + (v556 ^ v554 & (v556 ^ v555));
    v152 = __ROR4__(v558, 2);
    v153 = __ROR4__(v558, 13);
    v154 = __ROR4__(v558, 22);
    v155 = v553 + v151 + 264347078;
    v553 = v155;
    v557 = (v552 & (v558 | v559) | v558 & v559) + (v152 ^ v153 ^ v154) + v151 + 264347078;
    v155 = __ROR4__(v155, 6);
    v156 = __ROR4__(v553, 11);
    v157 = __ROR4__(v553, 25);
    v158 = v507 + v556 + (v155 ^ v156 ^ v157) + (v555 ^ v553 & (v555 ^ v554));
    v159 = __ROR4__(v557, 2);
    v160 = __ROR4__(v557, 13);
    v161 = v159 ^ v160;
    v162 = __ROR4__(v557, 22);
    v163 = v552 + v158 + 604807628;
    v552 = v163;
    v556 = (v559 & (v557 | v558) | v557 & v558) + (v161 ^ v162) + v158 + 604807628;
    v163 = __ROR4__(v163, 6);
    v164 = __ROR4__(v552, 11);
    v165 = __ROR4__(v552, 25);
    v166 = v508 + v555 + (v163 ^ v164 ^ v165) + (v554 ^ v552 & (v554 ^ v553));
    v167 = __ROR4__(v556, 2);
    v168 = __ROR4__(v556, 13);
    v169 = __ROR4__(v556, 22);
    v170 = v559 + v166 + 770255983;
    v559 = v170;
    v555 = (v558 & (v556 | v557) | v556 & v557) + (v167 ^ v168 ^ v169) + v166 + 770255983;
    v170 = __ROR4__(v170, 6);
    v171 = __ROR4__(v559, 11);
    v172 = __ROR4__(v559, 25);
    v173 = v509 + v554 + (v170 ^ v171 ^ v172) + (v553 ^ v559 & (v553 ^ v552));
    v174 = __ROR4__(v555, 2);
    v175 = __ROR4__(v555, 13);
    v176 = __ROR4__(v555, 22);
    v177 = v558 + v173 + 1249150122;
    v558 = v177;
    v554 = (v557 & (v555 | v556) | v555 & v556) + (v174 ^ v175 ^ v176) + v173 + 1249150122;
    v177 = __ROR4__(v177, 6);
    v178 = __ROR4__(v558, 11);
    v179 = __ROR4__(v558, 25);
    v180 = v510 + v553 + (v177 ^ v178 ^ v179) + (v552 ^ v558 & (v552 ^ v559));
    v181 = __ROR4__(v554, 2);
    v182 = __ROR4__(v554, 13);
    v183 = __ROR4__(v554, 22);
    v184 = v557 + v180 + 1555081692;
    v557 = v184;
    v553 = (v556 & (v554 | v555) | v554 & v555) + (v181 ^ v182 ^ v183) + v180 + 1555081692;
    v184 = __ROR4__(v184, 6);
    v185 = __ROR4__(v557, 11);
    v186 = __ROR4__(v557, 25);
    v187 = v511 + v552 + (v184 ^ v185 ^ v186) + (v559 ^ v557 & (v559 ^ v558));
    v188 = __ROR4__(v553, 2);
    v189 = __ROR4__(v553, 13);
    v190 = __ROR4__(v553, 22);
    v191 = v556 + v187 + 1996064986;
    v556 = v191;
    v552 = (v555 & (v553 | v554) | v553 & v554) + (v188 ^ v189 ^ v190) + v187 + 1996064986;
    v191 = __ROR4__(v191, 6);
    v192 = __ROR4__(v556, 11);
    v193 = __ROR4__(v556, 25);
    v194 = v512 + v559 + (v191 ^ v192 ^ v193) + (v558 ^ v556 & (v558 ^ v557));
    v195 = __ROR4__(v552, 2);
    v196 = __ROR4__(v552, 13);
    v197 = __ROR4__(v552, 22);
    v198 = v555 + v194 - 1740746414;
    v555 = v198;
    v559 = (v554 & (v552 | v553) | v552 & v553) + (v195 ^ v196 ^ v197) + v194 - 1740746414;
    v198 = __ROR4__(v198, 6);
    v199 = __ROR4__(v555, 11);
    v200 = __ROR4__(v555, 25);
    v201 = v513 + v558 + (v198 ^ v199 ^ v200) + (v557 ^ v555 & (v557 ^ v556));
    v202 = __ROR4__(v559, 2);
    v203 = __ROR4__(v559, 13);
    v204 = v202 ^ v203;
    v205 = __ROR4__(v559, 22);
    v206 = v554 + v201 - 1473132947;
    v554 = v206;
    v558 = (v553 & (v559 | v552) | v559 & v552) + (v204 ^ v205) + v201 - 1473132947;
    v206 = __ROR4__(v206, 6);
    v207 = __ROR4__(v554, 11);
    v208 = __ROR4__(v554, 25);
    v209 = v514 + v557 + (v206 ^ v207 ^ v208) + (v556 ^ v554 & (v556 ^ v555));
    v210 = __ROR4__(v558, 2);
    v211 = __ROR4__(v558, 13);
    v212 = __ROR4__(v558, 22);
    v213 = v553 + v209 - 1341970488;
    v553 = v213;
    v557 = (v552 & (v558 | v559) | v558 & v559) + (v210 ^ v211 ^ v212) + v209 - 1341970488;
    v213 = __ROR4__(v213, 6);
    v214 = __ROR4__(v553, 11);
    v215 = __ROR4__(v553, 25);
    v216 = v515 + v556 + (v213 ^ v214 ^ v215) + (v555 ^ v553 & (v555 ^ v554));
    v217 = __ROR4__(v557, 2);
    v218 = __ROR4__(v557, 13);
    v219 = v217 ^ v218;
    v220 = __ROR4__(v557, 22);
    v221 = v552 + v216 - 1084653625;
    v552 = v221;
    v556 = (v559 & (v557 | v558) | v557 & v558) + (v219 ^ v220) + v216 - 1084653625;
    v221 = __ROR4__(v221, 6);
    v222 = __ROR4__(v552, 11);
    v223 = __ROR4__(v552, 25);
    v224 = v516 + v555 + (v221 ^ v222 ^ v223) + (v554 ^ v552 & (v554 ^ v553));
    v225 = __ROR4__(v556, 2);
    v226 = __ROR4__(v556, 13);
    v227 = __ROR4__(v556, 22);
    v228 = v559 + v224 - 958395405;
    v559 = v228;
    v555 = (v558 & (v556 | v557) | v556 & v557) + (v225 ^ v226 ^ v227) + v224 - 958395405;
    v228 = __ROR4__(v228, 6);
    v229 = __ROR4__(v559, 11);
    v230 = __ROR4__(v559, 25);
    v231 = v517 + v554 + (v228 ^ v229 ^ v230) + (v553 ^ v559 & (v553 ^ v552));
    v232 = __ROR4__(v555, 2);
    v233 = __ROR4__(v555, 13);
    v234 = __ROR4__(v555, 22);
    v235 = v558 + v231 - 710438585;
    v558 = v235;
    v554 = (v557 & (v555 | v556) | v555 & v556) + (v232 ^ v233 ^ v234) + v231 - 710438585;
    v235 = __ROR4__(v235, 6);
    v236 = __ROR4__(v558, 11);
    v237 = __ROR4__(v558, 25);
    v238 = v518 + v553 + (v235 ^ v236 ^ v237) + (v552 ^ v558 & (v552 ^ v559));
    v239 = __ROR4__(v554, 2);
    v240 = __ROR4__(v554, 13);
    v241 = __ROR4__(v554, 22);
    v242 = v557 + v238 + 113926993;
    v557 = v242;
    v553 = (v556 & (v554 | v555) | v554 & v555) + (v239 ^ v240 ^ v241) + v238 + 113926993;
    v242 = __ROR4__(v242, 6);
    v243 = __ROR4__(v557, 11);
    v244 = __ROR4__(v557, 25);
    v245 = v519 + v552 + (v242 ^ v243 ^ v244) + (v559 ^ v557 & (v559 ^ v558));
    v246 = __ROR4__(v553, 2);
    v247 = __ROR4__(v553, 13);
    v248 = __ROR4__(v553, 22);
    v249 = v556 + v245 + 338241895;
    v556 = v249;
    v552 = (v555 & (v553 | v554) | v553 & v554) + (v246 ^ v247 ^ v248) + v245 + 338241895;
    v249 = __ROR4__(v249, 6);
    v250 = __ROR4__(v556, 11);
    v251 = __ROR4__(v556, 25);
    v252 = v520 + v559 + (v249 ^ v250 ^ v251) + (v558 ^ v556 & (v558 ^ v557));
    v253 = __ROR4__(v552, 2);
    v254 = __ROR4__(v552, 13);
    v255 = __ROR4__(v552, 22);
    v256 = v555 + v252 + 666307205;
    v555 = v256;
    v559 = (v554 & (v552 | v553) | v552 & v553) + (v253 ^ v254 ^ v255) + v252 + 666307205;
    v256 = __ROR4__(v256, 6);
    v257 = __ROR4__(v555, 11);
    v258 = __ROR4__(v555, 25);
    v259 = v521 + v558 + (v256 ^ v257 ^ v258) + (v557 ^ v555 & (v557 ^ v556));
    v260 = __ROR4__(v559, 2);
    v261 = __ROR4__(v559, 13);
    v262 = v260 ^ v261;
    v263 = __ROR4__(v559, 22);
    v264 = v554 + v259 + 773529912;
    v554 = v264;
    v558 = (v553 & (v559 | v552) | v559 & v552) + (v262 ^ v263) + v259 + 773529912;
    v264 = __ROR4__(v264, 6);
    v265 = __ROR4__(v554, 11);
    v266 = __ROR4__(v554, 25);
    v267 = v522 + v557 + (v264 ^ v265 ^ v266) + (v556 ^ v554 & (v556 ^ v555));
    v268 = __ROR4__(v558, 2);
    v269 = __ROR4__(v558, 13);
    v270 = __ROR4__(v558, 22);
    v271 = v553 + v267 + 1294757372;
    v553 = v271;
    v557 = (v552 & (v558 | v559) | v558 & v559) + (v268 ^ v269 ^ v270) + v267 + 1294757372;
    v271 = __ROR4__(v271, 6);
    v272 = __ROR4__(v553, 11);
    v273 = __ROR4__(v553, 25);
    v274 = v523 + v556 + (v271 ^ v272 ^ v273) + (v555 ^ v553 & (v555 ^ v554));
    v275 = __ROR4__(v557, 2);
    v276 = __ROR4__(v557, 13);
    v277 = v275 ^ v276;
    v278 = __ROR4__(v557, 22);
    v279 = v552 + v274 + 1396182291;
    v552 = v279;
    v556 = (v559 & (v557 | v558) | v557 & v558) + (v277 ^ v278) + v274 + 1396182291;
    v279 = __ROR4__(v279, 6);
    v280 = __ROR4__(v552, 11);
    v281 = __ROR4__(v552, 25);
    v282 = v524 + v555 + (v279 ^ v280 ^ v281) + (v554 ^ v552 & (v554 ^ v553));
    v283 = __ROR4__(v556, 2);
    v284 = __ROR4__(v556, 13);
    v285 = __ROR4__(v556, 22);
    v286 = v559 + v282 + 1695183700;
    v559 = v286;
    v555 = (v558 & (v556 | v557) | v556 & v557) + (v283 ^ v284 ^ v285) + v282 + 1695183700;
    v286 = __ROR4__(v286, 6);
    v287 = __ROR4__(v559, 11);
    v288 = __ROR4__(v559, 25);
    v289 = v525 + v554 + (v286 ^ v287 ^ v288) + (v553 ^ v559 & (v553 ^ v552));
    v290 = __ROR4__(v555, 2);
    v291 = __ROR4__(v555, 13);
    v292 = __ROR4__(v555, 22);
    v293 = v558 + v289 + 1986661051;
    v558 = v293;
    v554 = (v557 & (v555 | v556) | v555 & v556) + (v290 ^ v291 ^ v292) + v289 + 1986661051;
    v293 = __ROR4__(v293, 6);
    v294 = __ROR4__(v558, 11);
    v295 = __ROR4__(v558, 25);
    v296 = v526 + v553 + (v293 ^ v294 ^ v295) + (v552 ^ v558 & (v552 ^ v559));
    v297 = __ROR4__(v554, 2);
    v298 = __ROR4__(v554, 13);
    v299 = __ROR4__(v554, 22);
    v300 = v557 + v296 - 2117940946;
    v557 = v300;
    v553 = (v556 & (v554 | v555) | v554 & v555) + (v297 ^ v298 ^ v299) + v296 - 2117940946;
    v300 = __ROR4__(v300, 6);
    v301 = __ROR4__(v557, 11);
    v302 = __ROR4__(v557, 25);
    v303 = v527 + v552 + (v300 ^ v301 ^ v302) + (v559 ^ v557 & (v559 ^ v558));
    v304 = __ROR4__(v553, 2);
    v305 = __ROR4__(v553, 13);
    v306 = __ROR4__(v553, 22);
    v307 = v556 + v303 - 1838011259;
    v556 = v307;
    v552 = (v555 & (v553 | v554) | v553 & v554) + (v304 ^ v305 ^ v306) + v303 - 1838011259;
    v307 = __ROR4__(v307, 6);
    v308 = __ROR4__(v556, 11);
    v309 = __ROR4__(v556, 25);
    v310 = v528 + v559 + (v307 ^ v308 ^ v309) + (v558 ^ v556 & (v558 ^ v557));
    v311 = __ROR4__(v552, 2);
    v312 = __ROR4__(v552, 13);
    v313 = __ROR4__(v552, 22);
    v314 = v555 + v310 - 1564481375;
    v555 = v314;
    v559 = (v554 & (v552 | v553) | v552 & v553) + (v311 ^ v312 ^ v313) + v310 - 1564481375;
    v314 = __ROR4__(v314, 6);
    v315 = __ROR4__(v555, 11);
    v316 = __ROR4__(v555, 25);
    v317 = v529 + v558 + (v314 ^ v315 ^ v316) + (v557 ^ v555 & (v557 ^ v556));
    v318 = __ROR4__(v559, 2);
    v319 = __ROR4__(v559, 13);
    v320 = v318 ^ v319;
    v321 = __ROR4__(v559, 22);
    v322 = v554 + v317 - 1474664885;
    v554 = v322;
    v558 = (v553 & (v559 | v552) | v559 & v552) + (v320 ^ v321) + v317 - 1474664885;
    v322 = __ROR4__(v322, 6);
    v323 = __ROR4__(v554, 11);
    v324 = __ROR4__(v554, 25);
    v325 = v530 + v557 + (v322 ^ v323 ^ v324) + (v556 ^ v554 & (v556 ^ v555));
    v326 = __ROR4__(v558, 2);
    v327 = __ROR4__(v558, 13);
    v328 = __ROR4__(v558, 22);
    v329 = v553 + v325 - 1035236496;
    v553 = v329;
    v557 = (v552 & (v558 | v559) | v558 & v559) + (v326 ^ v327 ^ v328) + v325 - 1035236496;
    v329 = __ROR4__(v329, 6);
    v330 = __ROR4__(v553, 11);
    v331 = __ROR4__(v553, 25);
    v332 = v531 + v556 + (v329 ^ v330 ^ v331) + (v555 ^ v553 & (v555 ^ v554));
    v333 = __ROR4__(v557, 2);
    v334 = __ROR4__(v557, 13);
    v335 = v333 ^ v334;
    v336 = __ROR4__(v557, 22);
    v337 = v552 + v332 - 949202525;
    v552 = v337;
    v556 = (v559 & (v557 | v558) | v557 & v558) + (v335 ^ v336) + v332 - 949202525;
    v337 = __ROR4__(v337, 6);
    v338 = __ROR4__(v552, 11);
    v339 = __ROR4__(v552, 25);
    v340 = v532 + v555 + (v337 ^ v338 ^ v339) + (v554 ^ v552 & (v554 ^ v553));
    v341 = __ROR4__(v556, 2);
    v342 = __ROR4__(v556, 13);
    v343 = __ROR4__(v556, 22);
    v344 = v559 + v340 - 778901479;
    v559 = v344;
    v555 = (v558 & (v556 | v557) | v556 & v557) + (v341 ^ v342 ^ v343) + v340 - 778901479;
    v344 = __ROR4__(v344, 6);
    v345 = __ROR4__(v559, 11);
    v346 = __ROR4__(v559, 25);
    v347 = v533 + v554 + (v344 ^ v345 ^ v346) + (v553 ^ v559 & (v553 ^ v552));
    v348 = __ROR4__(v555, 2);
    v349 = __ROR4__(v555, 13);
    v350 = __ROR4__(v555, 22);
    v351 = v558 + v347 - 694614492;
    v558 = v351;
    v554 = (v557 & (v555 | v556) | v555 & v556) + (v348 ^ v349 ^ v350) + v347 - 694614492;
    v351 = __ROR4__(v351, 6);
    v352 = __ROR4__(v558, 11);
    v353 = __ROR4__(v558, 25);
    v354 = v534 + v553 + (v351 ^ v352 ^ v353) + (v552 ^ v558 & (v552 ^ v559));
    v355 = __ROR4__(v554, 2);
    v356 = __ROR4__(v554, 13);
    v357 = __ROR4__(v554, 22);
    v358 = v557 + v354 - 200395387;
    v557 = v358;
    v553 = (v556 & (v554 | v555) | v554 & v555) + (v355 ^ v356 ^ v357) + v354 - 200395387;
    v358 = __ROR4__(v358, 6);
    v359 = __ROR4__(v557, 11);
    v360 = __ROR4__(v557, 25);
    v361 = v535 + v552 + (v358 ^ v359 ^ v360) + (v559 ^ v557 & (v559 ^ v558));
    v362 = __ROR4__(v553, 2);
    v363 = __ROR4__(v553, 13);
    v364 = __ROR4__(v553, 22);
    v365 = v556 + v361 + 275423344;
    v556 = v365;
    v552 = (v555 & (v553 | v554) | v553 & v554) + (v362 ^ v363 ^ v364) + v361 + 275423344;
    v365 = __ROR4__(v365, 6);
    v366 = __ROR4__(v556, 11);
    v367 = __ROR4__(v556, 25);
    v368 = v536 + v559 + (v365 ^ v366 ^ v367) + (v558 ^ v556 & (v558 ^ v557));
    v369 = __ROR4__(v552, 2);
    v370 = __ROR4__(v552, 13);
    v371 = __ROR4__(v552, 22);
    v372 = v555 + v368 + 430227734;
    v555 = v372;
    v559 = (v554 & (v552 | v553) | v552 & v553) + (v369 ^ v370 ^ v371) + v368 + 430227734;
    v372 = __ROR4__(v372, 6);
    v373 = __ROR4__(v555, 11);
    v374 = __ROR4__(v555, 25);
    v375 = v537 + v558 + (v372 ^ v373 ^ v374) + (v557 ^ v555 & (v557 ^ v556));
    v376 = __ROR4__(v559, 2);
    v377 = __ROR4__(v559, 13);
    v378 = v376 ^ v377;
    v379 = __ROR4__(v559, 22);
    v380 = v554 + v375 + 506948616;
    v554 = v380;
    v558 = (v553 & (v559 | v552) | v559 & v552) + (v378 ^ v379) + v375 + 506948616;
    v380 = __ROR4__(v380, 6);
    v381 = __ROR4__(v554, 11);
    v382 = __ROR4__(v554, 25);
    v383 = v538 + v557 + (v380 ^ v381 ^ v382) + (v556 ^ v554 & (v556 ^ v555));
    v384 = __ROR4__(v558, 2);
    v385 = __ROR4__(v558, 13);
    v386 = __ROR4__(v558, 22);
    v387 = v553 + v383 + 659060556;
    v553 = v387;
    v557 = (v552 & (v558 | v559) | v558 & v559) + (v384 ^ v385 ^ v386) + v383 + 659060556;
    v387 = __ROR4__(v387, 6);
    v388 = __ROR4__(v553, 11);
    v389 = __ROR4__(v553, 25);
    v390 = v539 + v556 + (v387 ^ v388 ^ v389) + (v555 ^ v553 & (v555 ^ v554));
    v391 = __ROR4__(v557, 2);
    v392 = __ROR4__(v557, 13);
    v393 = v391 ^ v392;
    v394 = __ROR4__(v557, 22);
    v395 = v552 + v390 + 883997877;
    v552 = v395;
    v556 = (v559 & (v557 | v558) | v557 & v558) + (v393 ^ v394) + v390 + 883997877;
    v395 = __ROR4__(v395, 6);
    v396 = __ROR4__(v552, 11);
    v397 = __ROR4__(v552, 25);
    v398 = v540 + v555 + (v395 ^ v396 ^ v397) + (v554 ^ v552 & (v554 ^ v553));
    v399 = __ROR4__(v556, 2);
    v400 = __ROR4__(v556, 13);
    v401 = __ROR4__(v556, 22);
    v402 = v559 + v398 + 958139571;
    v559 = v402;
    v555 = (v558 & (v556 | v557) | v556 & v557) + (v399 ^ v400 ^ v401) + v398 + 958139571;
    v402 = __ROR4__(v402, 6);
    v403 = __ROR4__(v559, 11);
    v404 = __ROR4__(v559, 25);
    v405 = v541 + v554 + (v402 ^ v403 ^ v404) + (v553 ^ v559 & (v553 ^ v552));
    v406 = __ROR4__(v555, 2);
    v407 = __ROR4__(v555, 13);
    v408 = __ROR4__(v555, 22);
    v409 = v558 + v405 + 1322822218;
    v558 = v409;
    v554 = (v557 & (v555 | v556) | v555 & v556) + (v406 ^ v407 ^ v408) + v405 + 1322822218;
    v409 = __ROR4__(v409, 6);
    v410 = __ROR4__(v558, 11);
    v411 = __ROR4__(v558, 25);
    v412 = v542 + v553 + (v409 ^ v410 ^ v411) + (v552 ^ v558 & (v552 ^ v559));
    v413 = __ROR4__(v554, 2);
    v414 = __ROR4__(v554, 13);
    v415 = __ROR4__(v554, 22);
    v416 = v557 + v412 + 1537002063;
    v557 = v416;
    v553 = (v556 & (v554 | v555) | v554 & v555) + (v413 ^ v414 ^ v415) + v412 + 1537002063;
    v416 = __ROR4__(v416, 6);
    v417 = __ROR4__(v557, 11);
    v418 = __ROR4__(v557, 25);
    v419 = v543 + v552 + (v416 ^ v417 ^ v418) + (v559 ^ v557 & (v559 ^ v558));
    v420 = __ROR4__(v553, 2);
    v421 = __ROR4__(v553, 13);
    v422 = __ROR4__(v553, 22);
    v423 = v556 + v419 + 1747873779;
    v556 = v423;
    v552 = (v555 & (v553 | v554) | v553 & v554) + (v420 ^ v421 ^ v422) + v419 + 1747873779;
    v423 = __ROR4__(v423, 6);
    v424 = __ROR4__(v556, 11);
    v425 = __ROR4__(v556, 25);
    v426 = v544 + v559 + (v423 ^ v424 ^ v425) + (v558 ^ v556 & (v558 ^ v557));
    v427 = __ROR4__(v552, 2);
    v428 = __ROR4__(v552, 13);
    v429 = __ROR4__(v552, 22);
    v430 = v555 + v426 + 1955562222;
    v555 = v430;
    v559 = (v554 & (v552 | v553) | v552 & v553) + (v427 ^ v428 ^ v429) + v426 + 1955562222;
    v430 = __ROR4__(v430, 6);
    v431 = __ROR4__(v555, 11);
    v432 = __ROR4__(v555, 25);
    v433 = v545 + v558 + (v430 ^ v431 ^ v432) + (v557 ^ v555 & (v557 ^ v556));
    v434 = __ROR4__(v559, 2);
    v435 = __ROR4__(v559, 13);
    v436 = v434 ^ v435;
    v437 = __ROR4__(v559, 22);
    v438 = v554 + v433 + 2024104815;
    v554 = v438;
    v558 = (v553 & (v559 | v552) | v559 & v552) + (v436 ^ v437) + v433 + 2024104815;
    v438 = __ROR4__(v438, 6);
    v439 = __ROR4__(v554, 11);
    v440 = __ROR4__(v554, 25);
    v441 = v546 + v557 + (v438 ^ v439 ^ v440) + (v556 ^ v554 & (v556 ^ v555));
    v442 = __ROR4__(v558, 2);
    v443 = __ROR4__(v558, 13);
    v444 = __ROR4__(v558, 22);
    v445 = v553 + v441 - 2067236844;
    v553 = v445;
    v557 = (v552 & (v558 | v559) | v558 & v559) + (v442 ^ v443 ^ v444) + v441 - 2067236844;
    v445 = __ROR4__(v445, 6);
    v446 = __ROR4__(v553, 11);
    v447 = __ROR4__(v553, 25);
    v448 = v547 + v556 + (v445 ^ v446 ^ v447) + (v555 ^ v553 & (v555 ^ v554));
    v449 = __ROR4__(v557, 2);
    v450 = __ROR4__(v557, 13);
    v451 = v449 ^ v450;
    v452 = __ROR4__(v557, 22);
    v453 = v552 + v448 - 1933114872;
    v552 = v453;
    v556 = (v559 & (v557 | v558) | v557 & v558) + (v451 ^ v452) + v448 - 1933114872;
    v453 = __ROR4__(v453, 6);
    v454 = __ROR4__(v552, 11);
    v455 = __ROR4__(v552, 25);
    v456 = v548 + v555 + (v453 ^ v454 ^ v455) + (v554 ^ v552 & (v554 ^ v553));
    v457 = __ROR4__(v556, 2);
    v458 = __ROR4__(v556, 13);
    v459 = __ROR4__(v556, 22);
    v460 = v559 + v456 - 1866530822;
    v559 = v460;
    v555 = (v558 & (v556 | v557) | v556 & v557) + (v457 ^ v458 ^ v459) + v456 - 1866530822;
    v460 = __ROR4__(v460, 6);
    v461 = __ROR4__(v559, 11);
    v462 = __ROR4__(v559, 25);
    v463 = v549 + v554 + (v460 ^ v461 ^ v462) + (v553 ^ v559 & (v553 ^ v552));
    v464 = __ROR4__(v555, 2);
    v465 = __ROR4__(v555, 13);
    v466 = __ROR4__(v555, 22);
    v467 = v558 + v463 - 1538233109;
    v558 = v467;
    v554 = (v557 & (v555 | v556) | v555 & v556) + (v464 ^ v465 ^ v466) + v463 - 1538233109;
    v467 = __ROR4__(v467, 6);
    v468 = __ROR4__(v558, 11);
    v469 = __ROR4__(v558, 25);
    v470 = v550 + v553 + (v467 ^ v468 ^ v469) + (v552 ^ v558 & (v552 ^ v559));
    v471 = __ROR4__(v554, 2);
    v472 = __ROR4__(v554, 13);
    v473 = __ROR4__(v554, 22);
    v474 = v557 + v470 - 1090935817;
    v557 = v474;
    v553 = (v556 & (v554 | v555) | v554 & v555) + (v471 ^ v472 ^ v473) + v470 - 1090935817;
    v474 = __ROR4__(v474, 6);
    v475 = __ROR4__(v557, 11);
    v476 = __ROR4__(v557, 25);
    v477 = v551 + v552 + (v474 ^ v475 ^ v476) + (v559 ^ v557 & (v559 ^ v558));
    v478 = __ROR4__(v553, 2);
    v479 = __ROR4__(v553, 13);
    v480 = __ROR4__(v553, 22);
    v556 += v477 - 965641998;
    v481 = (v555 & (v553 | v554) | v553 & v554) + (v478 ^ v479 ^ v480) + v477 - 965641998;
    v552 = v481;
    for ( j = 0LL; ; ++j )
    {
      *(_DWORD *)(a1 + 4 * j) += v481;
      if ( j == 7 )
        break;
      v481 = *(&v553 + j);
    }
    a3 += 64LL;
  }
  return *(_QWORD *)v3;
}
// 69010: using guessed type __int64 off_69010[2];
// 3AFA6: using guessed type int var_138[2];
// 3AFA6: using guessed type int var_14C[5];
// 3AFA6: using guessed type int var_16C[8];

//----- (000000000003CF7E) ----------------------------------------------------
__int64 __fastcall ccrsa_oaep_encode_parameter(__int64 a1, __int64 a2, __int64 a3, const void *a4, size_t a5, void *a6, unsigned __int64 a7, const void *a8)
{
  const void *v8; // rbx@1
  __int64 v9; // r10@1
  __int64 v10; // r11@1
  unsigned __int64 v11; // r15@1
  size_t v12; // r9@1
  unsigned __int64 v13; // r14@1
  unsigned __int64 v14; // rax@1
  char *v15; // r13@1
  char *v16; // rcx@1
  unsigned __int64 v17; // rsi@1
  char *v18; // r12@1
  signed __int64 v19; // rax@1
  size_t v20; // r15@2
  __int64 v21; // rax@2
  size_t v22; // rcx@2
  __int64 v23; // r15@2
  int v24; // eax@2
  __int64 v25; // rdi@3
  unsigned __int64 v26; // rcx@3
  void *v27; // r15@3
  __int64 i; // rax@5
  size_t v29; // r14@7
  size_t v30; // r14@7
  unsigned __int64 v31; // rsi@7
  char *v32; // rax@8
  bool v37; // cf@9
  __int64 result; // rax@13
  __int64 v41; // [sp+0h] [bp-90h]@1
  void *v42; // [sp+8h] [bp-88h]@2
  unsigned __int64 v43; // [sp+10h] [bp-80h]@2
  size_t v44; // [sp+18h] [bp-78h]@2
  __int64 v45; // [sp+20h] [bp-70h]@2
  __int64 v46; // [sp+28h] [bp-68h]@2
  size_t v47; // [sp+30h] [bp-60h]@2
  __int64 v48; // [sp+38h] [bp-58h]@2
  size_t v49; // [sp+40h] [bp-50h]@2
  void *v50; // [sp+48h] [bp-48h]@1
  __int64 v51; // [sp+50h] [bp-40h]@1
  __int64 v52; // [sp+58h] [bp-38h]@2
  __int64 v53; // [sp+60h] [bp-30h]@1

  v50 = a6;
  v8 = a4;
  v9 = a2;
  v10 = off_69010[0];
  v53 = *(_QWORD *)off_69010[0];
  v11 = *(_QWORD *)a1;
  v12 = a3 - 1 - *(_QWORD *)a1;
  v13 = (v12 + 7) >> 3;
  v14 = (8 * v13 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v15 = (char *)&v41 - v14;
  v16 = (char *)&v41 - v14;
  v17 = (unsigned __int64)(*(_QWORD *)a1 + 7LL) >> 3;
  v18 = (char *)&v41 - ((8 * v17 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v19 = a3 + -2LL * *(_QWORD *)a1 - 2;
  LODWORD(v51) = -1;
  if ( v19 >= a5 )
  {
    v42 = v16;
    v43 = v11;
    v45 = a3;
    v46 = v17;
    v44 = 8 * v13;
    v52 = a1;
    v20 = v12;
    v49 = v12;
    v48 = v9;
    v47 = a5;
    bzero(v15, 8 * v13);
    ccdigest(v21, a8, (__int64)v15, v52, a7);
    v22 = v47;
    *(&v15[v20] + ~v47) = 1;
    memcpy(&v15[v20 - v22], v50, v22);
    v23 = v52;
    v24 = (*(int (__fastcall **)(__int64, _QWORD, const void *))v48)(v48, *(_QWORD *)v52, v8);
    v10 = off_69010[0];
    if ( !v24 )
    {
      v25 = v23;
      v26 = *(_QWORD *)v23;
      v27 = v42;
      ccmgf(v25, v49, v42, v26, v8);
      for ( ; v13; --v13 )
        *(_QWORD *)&v15[8 * v13 - 8] ^= *((_QWORD *)v27 + v13 - 1);
      ccmgf(v52, v43, v18, v49, v15);
      for ( i = v46; i; --i )
        *((_QWORD *)v8 + i - 1) ^= *(_QWORD *)&v18[8 * i - 8];
      v51 = v45 + 7;
      v50 = (void *)((v45 + 7) & 0xFFFFFFFFFFFFFFF8LL);
      v29 = (size_t)((char *)v50 - v45);
      memmove((char *)v8 + v29 + 1, v8, *(_QWORD *)v52);
      bzero((void *)v8, v29);
      *((_BYTE *)v8 + v29) = 0;
      memcpy((char *)v8 + v29 + *(_QWORD *)v52 + 1, v15, v49);
      v30 = v44;
      cc_clear(v44, v15);
      cc_clear(v30, v27);
      cc_clear(8 * v46, v18);
      v31 = (unsigned __int64)v51 >> 3;
      if ( (signed __int64)(((unsigned __int64)v51 >> 3) - 1) > 0 )
      {
        v32 = (char *)v8 + (_QWORD)v50 - 16;
        do
        {
          _RCX = *(_QWORD *)v8;
          __asm { bswap   rcx }
          _RDX = *((_QWORD *)v32 + 1);
          __asm { bswap   rdx }
          *(_QWORD *)v8 = _RDX;
          *((_QWORD *)v32 + 1) = _RCX;
          v8 = (char *)v8 + 8;
          v37 = v8 < v32;
          v32 -= 8;
        }
        while ( v37 );
      }
      LODWORD(v51) = 0;
      if ( v31 & 1 )
      {
        _RAX = *(_QWORD *)v8;
        __asm { bswap   rax }
        *(_QWORD *)v8 = _RAX;
      }
      v10 = off_69010[0];
    }
  }
  result = *(_QWORD *)v10;
  if ( *(_QWORD *)v10 == v53 )
    result = (unsigned int)v51;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000003D208) ----------------------------------------------------
__int64 __usercall ccsha256_vng_intel_sse3_compress@<rax>(__int64 a1@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm4>, __m128i a9@<xmm5>, __m128i a10@<xmm6>, __m128i a11@<xmm7>)
{
  __m128i v11; // xmm0@1
  __m128i v12; // xmm1@1
  __m128i v13; // xmm2@1
  __m128i v14; // xmm3@1
  signed __int64 v15; // rdx@1
  __m128i v16; // xmm0@1
  __m128i v17; // xmm1@1
  __m128i v18; // xmm2@1
  __m128i v19; // xmm3@1
  const __m128i *v20; // rbx@1
  __m128i v21; // xmm5@1
  __m128i v22; // xmm6@1
  __m128i v23; // xmm7@1
  int v24; // er9@2
  int v25; // er10@2
  int v26; // er12@2
  int v27; // er13@2
  int v28; // er14@2
  int v29; // ecx@2
  int v30; // eax@2
  int v31; // eax@2
  int v32; // eax@2
  int v33; // er11@2
  int v34; // er15@2
  int v35; // eax@2
  int v36; // ecx@2
  int v37; // eax@2
  int v38; // er15@2
  int v39; // ecx@2
  int v40; // eax@2
  int v41; // eax@2
  int v42; // eax@2
  int v43; // er10@2
  int v44; // er14@2
  int v45; // eax@2
  int v46; // ecx@2
  int v47; // eax@2
  int v48; // er14@2
  int v49; // ecx@2
  int v50; // eax@2
  int v51; // eax@2
  int v52; // eax@2
  int v53; // er9@2
  int v54; // er13@2
  int v55; // eax@2
  int v56; // ecx@2
  int v57; // eax@2
  int v58; // er13@2
  int v59; // ecx@2
  int v60; // eax@2
  int v61; // eax@2
  int v62; // eax@2
  int v63; // er8@2
  int v64; // er12@2
  int v65; // eax@2
  int v66; // ecx@2
  int v67; // eax@2
  int v68; // er12@2
  __m128i v69; // xmm5@2
  const __m128i *v70; // rbx@2
  __m128i v71; // xmm7@2
  __m128i v72; // xmm4@2
  __m128i v73; // xmm6@2
  __m128i v74; // xmm7@2
  __m128i v75; // xmm0@2
  __m128i v76; // xmm7@2
  __m128i v77; // xmm6@2
  __m128i v78; // xmm4@2
  __m128i v79; // xmm7@2
  __m128i v80; // xmm0@2
  __m128i v81; // xmm7@2
  __m128i v82; // xmm6@2
  __m128i v83; // xmm4@2
  __m128i v84; // xmm7@2
  __m128i v85; // xmm0@2
  int v86; // ecx@2
  int v87; // eax@2
  int v88; // eax@2
  int v89; // eax@2
  int v90; // er15@2
  int v91; // er11@2
  int v92; // eax@2
  int v93; // ecx@2
  int v94; // eax@2
  int v95; // er11@2
  int v96; // ecx@2
  int v97; // eax@2
  int v98; // eax@2
  int v99; // eax@2
  int v100; // er14@2
  int v101; // er10@2
  int v102; // eax@2
  int v103; // ecx@2
  int v104; // eax@2
  int v105; // er10@2
  int v106; // ecx@2
  int v107; // eax@2
  int v108; // eax@2
  int v109; // eax@2
  int v110; // er13@2
  int v111; // er9@2
  int v112; // eax@2
  int v113; // ecx@2
  int v114; // eax@2
  int v115; // er9@2
  int v116; // ecx@2
  int v117; // eax@2
  int v118; // eax@2
  int v119; // eax@2
  int v120; // er12@2
  int v121; // er8@2
  int v122; // eax@2
  int v123; // ecx@2
  int v124; // eax@2
  int v125; // er8@2
  __m128i v126; // xmm5@2
  __m128i v127; // xmm7@2
  __m128i v128; // xmm4@2
  __m128i v129; // xmm6@2
  __m128i v130; // xmm7@2
  __m128i v131; // xmm1@2
  __m128i v132; // xmm7@2
  __m128i v133; // xmm6@2
  __m128i v134; // xmm4@2
  __m128i v135; // xmm7@2
  __m128i v136; // xmm1@2
  __m128i v137; // xmm7@2
  __m128i v138; // xmm6@2
  __m128i v139; // xmm4@2
  __m128i v140; // xmm7@2
  __m128i v141; // xmm1@2
  int v142; // ecx@2
  int v143; // eax@2
  int v144; // eax@2
  int v145; // eax@2
  int v146; // er11@2
  int v147; // er15@2
  int v148; // eax@2
  int v149; // ecx@2
  int v150; // eax@2
  int v151; // er15@2
  int v152; // ecx@2
  int v153; // eax@2
  int v154; // eax@2
  int v155; // eax@2
  int v156; // er10@2
  int v157; // er14@2
  int v158; // eax@2
  int v159; // ecx@2
  int v160; // eax@2
  int v161; // er14@2
  int v162; // ecx@2
  int v163; // eax@2
  int v164; // eax@2
  int v165; // eax@2
  int v166; // er9@2
  int v167; // er13@2
  int v168; // eax@2
  int v169; // ecx@2
  int v170; // eax@2
  int v171; // er13@2
  int v172; // ecx@2
  int v173; // eax@2
  int v174; // eax@2
  int v175; // eax@2
  int v176; // er8@2
  int v177; // er12@2
  int v178; // eax@2
  int v179; // ecx@2
  int v180; // eax@2
  int v181; // er12@2
  __m128i v182; // xmm5@2
  __m128i v183; // xmm7@2
  __m128i v184; // xmm4@2
  __m128i v185; // xmm6@2
  __m128i v186; // xmm7@2
  __m128i v187; // xmm2@2
  __m128i v188; // xmm7@2
  __m128i v189; // xmm6@2
  __m128i v190; // xmm4@2
  __m128i v191; // xmm7@2
  __m128i v192; // xmm2@2
  __m128i v193; // xmm7@2
  __m128i v194; // xmm6@2
  __m128i v195; // xmm4@2
  __m128i v196; // xmm7@2
  __m128i v197; // xmm2@2
  int v198; // ecx@2
  int v199; // eax@2
  int v200; // eax@2
  int v201; // eax@2
  int v202; // er15@2
  int v203; // er11@2
  int v204; // eax@2
  int v205; // ecx@2
  int v206; // eax@2
  int v207; // er11@2
  int v208; // ecx@2
  int v209; // eax@2
  int v210; // eax@2
  int v211; // eax@2
  int v212; // er14@2
  int v213; // er10@2
  int v214; // eax@2
  int v215; // ecx@2
  int v216; // eax@2
  int v217; // er10@2
  int v218; // ecx@2
  int v219; // eax@2
  int v220; // eax@2
  int v221; // eax@2
  int v222; // er13@2
  int v223; // er9@2
  int v224; // eax@2
  int v225; // ecx@2
  int v226; // eax@2
  int v227; // er9@2
  int v228; // ecx@2
  int v229; // eax@2
  int v230; // eax@2
  int v231; // eax@2
  int v232; // er12@2
  int v233; // er8@2
  int v234; // eax@2
  int v235; // ecx@2
  int v236; // eax@2
  int v237; // er8@2
  __m128i v238; // xmm5@2
  __m128i v239; // xmm7@2
  __m128i v240; // xmm4@2
  __m128i v241; // xmm6@2
  __m128i v242; // xmm7@2
  __m128i v243; // xmm3@2
  __m128i v244; // xmm7@2
  __m128i v245; // xmm6@2
  __m128i v246; // xmm4@2
  __m128i v247; // xmm7@2
  __m128i v248; // xmm3@2
  __m128i v249; // xmm7@2
  __m128i v250; // xmm6@2
  __m128i v251; // xmm4@2
  __m128i v252; // xmm7@2
  __m128i v253; // xmm3@2
  int v254; // ecx@2
  int v255; // eax@2
  int v256; // eax@2
  int v257; // eax@2
  int v258; // er11@2
  int v259; // er15@2
  int v260; // eax@2
  int v261; // ecx@2
  int v262; // eax@2
  int v263; // er15@2
  int v264; // ecx@2
  int v265; // eax@2
  int v266; // eax@2
  int v267; // eax@2
  int v268; // er10@2
  int v269; // er14@2
  int v270; // eax@2
  int v271; // ecx@2
  int v272; // eax@2
  int v273; // er14@2
  int v274; // ecx@2
  int v275; // eax@2
  int v276; // eax@2
  int v277; // eax@2
  int v278; // er9@2
  int v279; // er13@2
  int v280; // eax@2
  int v281; // ecx@2
  int v282; // eax@2
  int v283; // er13@2
  int v284; // ecx@2
  int v285; // eax@2
  int v286; // eax@2
  int v287; // eax@2
  int v288; // er8@2
  int v289; // er12@2
  int v290; // eax@2
  int v291; // ecx@2
  int v292; // eax@2
  int v293; // er12@2
  __m128i v294; // xmm5@2
  __m128i v295; // xmm7@2
  __m128i v296; // xmm4@2
  __m128i v297; // xmm6@2
  __m128i v298; // xmm7@2
  __m128i v299; // xmm0@2
  __m128i v300; // xmm7@2
  __m128i v301; // xmm6@2
  __m128i v302; // xmm4@2
  __m128i v303; // xmm7@2
  __m128i v304; // xmm0@2
  __m128i v305; // xmm7@2
  __m128i v306; // xmm6@2
  __m128i v307; // xmm4@2
  __m128i v308; // xmm7@2
  __m128i v309; // xmm0@2
  int v310; // ecx@2
  int v311; // eax@2
  int v312; // eax@2
  int v313; // eax@2
  int v314; // er15@2
  int v315; // er11@2
  int v316; // eax@2
  int v317; // ecx@2
  int v318; // eax@2
  int v319; // er11@2
  int v320; // ecx@2
  int v321; // eax@2
  int v322; // eax@2
  int v323; // eax@2
  int v324; // er14@2
  int v325; // er10@2
  int v326; // eax@2
  int v327; // ecx@2
  int v328; // eax@2
  int v329; // er10@2
  int v330; // ecx@2
  int v331; // eax@2
  int v332; // eax@2
  int v333; // eax@2
  int v334; // er13@2
  int v335; // er9@2
  int v336; // eax@2
  int v337; // ecx@2
  int v338; // eax@2
  int v339; // er9@2
  int v340; // ecx@2
  int v341; // eax@2
  int v342; // eax@2
  int v343; // eax@2
  int v344; // er12@2
  int v345; // er8@2
  int v346; // eax@2
  int v347; // ecx@2
  int v348; // eax@2
  int v349; // er8@2
  __m128i v350; // xmm5@2
  __m128i v351; // xmm7@2
  __m128i v352; // xmm4@2
  __m128i v353; // xmm6@2
  __m128i v354; // xmm7@2
  __m128i v355; // xmm1@2
  __m128i v356; // xmm7@2
  __m128i v357; // xmm6@2
  __m128i v358; // xmm4@2
  __m128i v359; // xmm7@2
  __m128i v360; // xmm1@2
  __m128i v361; // xmm7@2
  __m128i v362; // xmm6@2
  __m128i v363; // xmm4@2
  __m128i v364; // xmm7@2
  __m128i v365; // xmm1@2
  int v366; // ecx@2
  int v367; // eax@2
  int v368; // eax@2
  int v369; // eax@2
  int v370; // er11@2
  int v371; // er15@2
  int v372; // eax@2
  int v373; // ecx@2
  int v374; // eax@2
  int v375; // er15@2
  int v376; // ecx@2
  int v377; // eax@2
  int v378; // eax@2
  int v379; // eax@2
  int v380; // er10@2
  int v381; // er14@2
  int v382; // eax@2
  int v383; // ecx@2
  int v384; // eax@2
  int v385; // er14@2
  int v386; // ecx@2
  int v387; // eax@2
  int v388; // eax@2
  int v389; // eax@2
  int v390; // er9@2
  int v391; // er13@2
  int v392; // eax@2
  int v393; // ecx@2
  int v394; // eax@2
  int v395; // er13@2
  int v396; // ecx@2
  int v397; // eax@2
  int v398; // eax@2
  int v399; // eax@2
  int v400; // er8@2
  int v401; // er12@2
  int v402; // eax@2
  int v403; // ecx@2
  int v404; // eax@2
  int v405; // er12@2
  __m128i v406; // xmm5@2
  __m128i v407; // xmm7@2
  __m128i v408; // xmm4@2
  __m128i v409; // xmm6@2
  __m128i v410; // xmm7@2
  __m128i v411; // xmm2@2
  __m128i v412; // xmm7@2
  __m128i v413; // xmm6@2
  __m128i v414; // xmm4@2
  __m128i v415; // xmm7@2
  __m128i v416; // xmm2@2
  __m128i v417; // xmm7@2
  __m128i v418; // xmm6@2
  __m128i v419; // xmm4@2
  __m128i v420; // xmm7@2
  __m128i v421; // xmm2@2
  int v422; // ecx@2
  int v423; // eax@2
  int v424; // eax@2
  int v425; // eax@2
  int v426; // er15@2
  int v427; // er11@2
  int v428; // eax@2
  int v429; // ecx@2
  int v430; // eax@2
  int v431; // er11@2
  int v432; // ecx@2
  int v433; // eax@2
  int v434; // eax@2
  int v435; // eax@2
  int v436; // er14@2
  int v437; // er10@2
  int v438; // eax@2
  int v439; // ecx@2
  int v440; // eax@2
  int v441; // er10@2
  int v442; // ecx@2
  int v443; // eax@2
  int v444; // eax@2
  int v445; // eax@2
  int v446; // er13@2
  int v447; // er9@2
  int v448; // eax@2
  int v449; // ecx@2
  int v450; // eax@2
  int v451; // er9@2
  int v452; // ecx@2
  int v453; // eax@2
  int v454; // eax@2
  int v455; // eax@2
  int v456; // er12@2
  int v457; // er8@2
  int v458; // eax@2
  int v459; // ecx@2
  int v460; // eax@2
  int v461; // er8@2
  __m128i v462; // xmm5@2
  __m128i v463; // xmm7@2
  __m128i v464; // xmm4@2
  __m128i v465; // xmm6@2
  __m128i v466; // xmm7@2
  __m128i v467; // xmm3@2
  __m128i v468; // xmm7@2
  __m128i v469; // xmm6@2
  __m128i v470; // xmm4@2
  __m128i v471; // xmm7@2
  __m128i v472; // xmm3@2
  __m128i v473; // xmm7@2
  __m128i v474; // xmm6@2
  __m128i v475; // xmm4@2
  __m128i v476; // xmm7@2
  __m128i v477; // xmm3@2
  int v478; // ecx@2
  int v479; // eax@2
  int v480; // eax@2
  int v481; // eax@2
  int v482; // er11@2
  int v483; // er15@2
  int v484; // eax@2
  int v485; // ecx@2
  int v486; // eax@2
  int v487; // er15@2
  int v488; // ecx@2
  int v489; // eax@2
  int v490; // eax@2
  int v491; // eax@2
  int v492; // er10@2
  int v493; // er14@2
  int v494; // eax@2
  int v495; // ecx@2
  int v496; // eax@2
  int v497; // er14@2
  int v498; // ecx@2
  int v499; // eax@2
  int v500; // eax@2
  int v501; // eax@2
  int v502; // er9@2
  int v503; // er13@2
  int v504; // eax@2
  int v505; // ecx@2
  int v506; // eax@2
  int v507; // er13@2
  int v508; // ecx@2
  int v509; // eax@2
  int v510; // eax@2
  int v511; // eax@2
  int v512; // er8@2
  int v513; // er12@2
  int v514; // eax@2
  int v515; // ecx@2
  int v516; // eax@2
  int v517; // er12@2
  __m128i v518; // xmm5@2
  __m128i v519; // xmm7@2
  __m128i v520; // xmm4@2
  __m128i v521; // xmm6@2
  __m128i v522; // xmm7@2
  __m128i v523; // xmm0@2
  __m128i v524; // xmm7@2
  __m128i v525; // xmm6@2
  __m128i v526; // xmm4@2
  __m128i v527; // xmm7@2
  __m128i v528; // xmm0@2
  __m128i v529; // xmm7@2
  __m128i v530; // xmm6@2
  __m128i v531; // xmm4@2
  __m128i v532; // xmm7@2
  __m128i v533; // xmm0@2
  int v534; // ecx@2
  int v535; // eax@2
  int v536; // eax@2
  int v537; // eax@2
  int v538; // er15@2
  int v539; // er11@2
  int v540; // eax@2
  int v541; // ecx@2
  int v542; // eax@2
  int v543; // er11@2
  int v544; // ecx@2
  int v545; // eax@2
  int v546; // eax@2
  int v547; // eax@2
  int v548; // er14@2
  int v549; // er10@2
  int v550; // eax@2
  int v551; // ecx@2
  int v552; // eax@2
  int v553; // er10@2
  int v554; // ecx@2
  int v555; // eax@2
  int v556; // eax@2
  int v557; // eax@2
  int v558; // er13@2
  int v559; // er9@2
  int v560; // eax@2
  int v561; // ecx@2
  int v562; // eax@2
  int v563; // er9@2
  int v564; // ecx@2
  int v565; // eax@2
  int v566; // eax@2
  int v567; // eax@2
  int v568; // er12@2
  int v569; // er8@2
  int v570; // eax@2
  int v571; // ecx@2
  int v572; // eax@2
  int v573; // er8@2
  __m128i v574; // xmm5@2
  __m128i v575; // xmm7@2
  __m128i v576; // xmm4@2
  __m128i v577; // xmm6@2
  __m128i v578; // xmm7@2
  __m128i v579; // xmm1@2
  __m128i v580; // xmm7@2
  __m128i v581; // xmm6@2
  __m128i v582; // xmm4@2
  __m128i v583; // xmm7@2
  __m128i v584; // xmm1@2
  __m128i v585; // xmm7@2
  __m128i v586; // xmm6@2
  __m128i v587; // xmm4@2
  __m128i v588; // xmm7@2
  __m128i v589; // xmm1@2
  int v590; // ecx@2
  int v591; // eax@2
  int v592; // eax@2
  int v593; // eax@2
  int v594; // er11@2
  int v595; // er15@2
  int v596; // eax@2
  int v597; // ecx@2
  int v598; // eax@2
  int v599; // er15@2
  int v600; // ecx@2
  int v601; // eax@2
  int v602; // eax@2
  int v603; // eax@2
  int v604; // er10@2
  int v605; // er14@2
  int v606; // eax@2
  int v607; // ecx@2
  int v608; // eax@2
  int v609; // er14@2
  int v610; // ecx@2
  int v611; // eax@2
  int v612; // eax@2
  int v613; // eax@2
  int v614; // er9@2
  int v615; // er13@2
  int v616; // eax@2
  int v617; // ecx@2
  int v618; // eax@2
  int v619; // er13@2
  int v620; // ecx@2
  int v621; // eax@2
  int v622; // eax@2
  int v623; // eax@2
  int v624; // er8@2
  int v625; // er12@2
  int v626; // eax@2
  int v627; // ecx@2
  int v628; // eax@2
  int v629; // er12@2
  __m128i v630; // xmm5@2
  __m128i v631; // xmm7@2
  __m128i v632; // xmm4@2
  __m128i v633; // xmm6@2
  __m128i v634; // xmm7@2
  __m128i v635; // xmm2@2
  __m128i v636; // xmm7@2
  __m128i v637; // xmm6@2
  __m128i v638; // xmm4@2
  __m128i v639; // xmm7@2
  __m128i v640; // xmm2@2
  __m128i v641; // xmm7@2
  __m128i v642; // xmm6@2
  __m128i v643; // xmm4@2
  __m128i v644; // xmm7@2
  __m128i v645; // xmm2@2
  int v646; // ecx@2
  int v647; // eax@2
  int v648; // eax@2
  int v649; // eax@2
  int v650; // er15@2
  int v651; // er11@2
  int v652; // eax@2
  int v653; // ecx@2
  int v654; // eax@2
  int v655; // er11@2
  int v656; // ecx@2
  int v657; // eax@2
  int v658; // eax@2
  int v659; // eax@2
  int v660; // er14@2
  int v661; // er10@2
  int v662; // eax@2
  int v663; // ecx@2
  int v664; // eax@2
  int v665; // er10@2
  int v666; // ecx@2
  int v667; // eax@2
  int v668; // eax@2
  int v669; // eax@2
  int v670; // er13@2
  int v671; // er9@2
  int v672; // eax@2
  int v673; // ecx@2
  int v674; // eax@2
  int v675; // er9@2
  int v676; // ecx@2
  int v677; // eax@2
  int v678; // eax@2
  int v679; // eax@2
  int v680; // er12@2
  int v681; // er8@2
  int v682; // eax@2
  int v683; // ecx@2
  int v684; // eax@2
  int v685; // er8@2
  __m128i v686; // xmm7@2
  __m128i v687; // xmm4@2
  __m128i v688; // xmm6@2
  __m128i v689; // xmm7@2
  __m128i v690; // xmm3@2
  __m128i v691; // xmm7@2
  __m128i v692; // xmm6@2
  __m128i v693; // xmm4@2
  __m128i v694; // xmm7@2
  __m128i v695; // xmm3@2
  __m128i v696; // xmm7@2
  __m128i v697; // xmm6@2
  __m128i v698; // xmm4@2
  __m128i v699; // xmm7@2
  signed __int64 v700; // rbx@2
  int v701; // ecx@3
  int v702; // eax@3
  int v703; // eax@3
  int v704; // eax@3
  int v705; // er11@3
  int v706; // er15@3
  int v707; // eax@3
  int v708; // ecx@3
  int v709; // eax@3
  int v710; // er15@3
  int v711; // ecx@3
  int v712; // eax@3
  int v713; // eax@3
  int v714; // eax@3
  int v715; // er10@3
  int v716; // er14@3
  int v717; // eax@3
  int v718; // ecx@3
  int v719; // eax@3
  int v720; // er14@3
  int v721; // ecx@3
  int v722; // eax@3
  int v723; // eax@3
  int v724; // eax@3
  int v725; // er9@3
  int v726; // er13@3
  int v727; // eax@3
  int v728; // ecx@3
  int v729; // eax@3
  int v730; // er13@3
  int v731; // ecx@3
  int v732; // eax@3
  int v733; // eax@3
  int v734; // eax@3
  int v735; // er8@3
  int v736; // er12@3
  int v737; // eax@3
  int v738; // ecx@3
  int v739; // eax@3
  int v740; // er12@3
  int v741; // ecx@3
  int v742; // eax@3
  int v743; // eax@3
  int v744; // eax@3
  int v745; // er15@3
  int v746; // er11@3
  int v747; // eax@3
  int v748; // ecx@3
  int v749; // eax@3
  int v750; // er11@3
  int v751; // ecx@3
  int v752; // eax@3
  int v753; // eax@3
  int v754; // eax@3
  int v755; // er14@3
  int v756; // er10@3
  int v757; // eax@3
  int v758; // ecx@3
  int v759; // eax@3
  int v760; // er10@3
  int v761; // ecx@3
  int v762; // eax@3
  int v763; // eax@3
  int v764; // eax@3
  int v765; // er13@3
  int v766; // er9@3
  int v767; // eax@3
  int v768; // ecx@3
  int v769; // eax@3
  int v770; // er9@3
  int v771; // ecx@3
  int v772; // eax@3
  int v773; // eax@3
  int v774; // eax@3
  int v775; // er12@3
  int v776; // er8@3
  int v777; // eax@3
  int v778; // ecx@3
  int v779; // eax@3
  int v780; // er8@3
  int v781; // ecx@3
  int v782; // eax@3
  int v783; // eax@3
  int v784; // eax@3
  int v785; // er11@3
  int v786; // er15@3
  int v787; // eax@3
  int v788; // ecx@3
  int v789; // eax@3
  int v790; // er15@3
  int v791; // ecx@3
  int v792; // eax@3
  int v793; // eax@3
  int v794; // eax@3
  int v795; // er10@3
  int v796; // er14@3
  int v797; // eax@3
  int v798; // ecx@3
  int v799; // eax@3
  int v800; // er14@3
  int v801; // ecx@3
  int v802; // eax@3
  int v803; // eax@3
  int v804; // eax@3
  int v805; // er9@3
  int v806; // er13@3
  int v807; // eax@3
  int v808; // ecx@3
  int v809; // eax@3
  int v810; // er13@3
  int v811; // ecx@3
  int v812; // eax@3
  int v813; // eax@3
  int v814; // eax@3
  int v815; // er8@3
  int v816; // er12@3
  int v817; // eax@3
  int v818; // ecx@3
  int v819; // eax@3
  int v820; // er12@3
  int v821; // ecx@3
  int v822; // eax@3
  int v823; // eax@3
  int v824; // eax@3
  int v825; // er15@3
  int v826; // er11@3
  int v827; // eax@3
  int v828; // ecx@3
  int v829; // eax@3
  int v830; // er11@3
  int v831; // ecx@3
  int v832; // eax@3
  int v833; // eax@3
  int v834; // eax@3
  int v835; // er14@3
  int v836; // er10@3
  int v837; // eax@3
  int v838; // ecx@3
  int v839; // eax@3
  int v840; // er10@3
  int v841; // ecx@3
  int v842; // eax@3
  int v843; // eax@3
  int v844; // eax@3
  int v845; // er13@3
  int v846; // er9@3
  int v847; // eax@3
  int v848; // ecx@3
  int v849; // eax@3
  int v850; // er9@3
  int v851; // ecx@3
  int v852; // eax@3
  int v853; // eax@3
  int v854; // eax@3
  int v855; // er12@3
  int v856; // er8@3
  int v857; // eax@3
  int v858; // ecx@3
  int v859; // eax@3
  int v860; // ecx@4
  int v861; // eax@4
  int v862; // eax@4
  int v863; // eax@4
  int v864; // er11@4
  int v865; // er15@4
  int v866; // eax@4
  int v867; // ecx@4
  int v868; // eax@4
  int v869; // er15@4
  int v870; // ecx@4
  int v871; // eax@4
  int v872; // eax@4
  int v873; // eax@4
  int v874; // er10@4
  int v875; // er14@4
  int v876; // eax@4
  int v877; // ecx@4
  int v878; // eax@4
  int v879; // er14@4
  int v880; // ecx@4
  int v881; // eax@4
  int v882; // eax@4
  int v883; // eax@4
  int v884; // er9@4
  int v885; // er13@4
  int v886; // eax@4
  int v887; // ecx@4
  int v888; // eax@4
  int v889; // er13@4
  int v890; // ecx@4
  int v891; // eax@4
  int v892; // eax@4
  int v893; // eax@4
  int v894; // er8@4
  int v895; // er12@4
  int v896; // eax@4
  int v897; // ecx@4
  int v898; // eax@4
  int v899; // er12@4
  int v900; // ecx@4
  int v901; // eax@4
  int v902; // eax@4
  int v903; // eax@4
  int v904; // er15@4
  int v905; // er11@4
  int v906; // eax@4
  int v907; // ecx@4
  int v908; // eax@4
  int v909; // er11@4
  int v910; // ecx@4
  int v911; // eax@4
  int v912; // eax@4
  int v913; // eax@4
  int v914; // er14@4
  int v915; // er10@4
  int v916; // eax@4
  int v917; // ecx@4
  int v918; // eax@4
  int v919; // er10@4
  int v920; // ecx@4
  int v921; // eax@4
  int v922; // eax@4
  int v923; // eax@4
  int v924; // er13@4
  int v925; // er9@4
  int v926; // eax@4
  int v927; // ecx@4
  int v928; // eax@4
  int v929; // er9@4
  int v930; // ecx@4
  int v931; // eax@4
  int v932; // eax@4
  int v933; // eax@4
  int v934; // er12@4
  int v935; // er8@4
  int v936; // eax@4
  int v937; // ecx@4
  int v938; // eax@4
  int v939; // er8@4
  int v940; // ecx@4
  int v941; // eax@4
  int v942; // eax@4
  int v943; // eax@4
  int v944; // er11@4
  int v945; // er15@4
  int v946; // eax@4
  int v947; // ecx@4
  int v948; // eax@4
  int v949; // er15@4
  int v950; // ecx@4
  int v951; // eax@4
  int v952; // eax@4
  int v953; // eax@4
  int v954; // er10@4
  int v955; // er14@4
  int v956; // eax@4
  int v957; // ecx@4
  int v958; // eax@4
  int v959; // er14@4
  int v960; // ecx@4
  int v961; // eax@4
  int v962; // eax@4
  int v963; // eax@4
  int v964; // er9@4
  int v965; // er13@4
  int v966; // eax@4
  int v967; // ecx@4
  int v968; // eax@4
  int v969; // er13@4
  int v970; // ecx@4
  int v971; // eax@4
  int v972; // eax@4
  int v973; // eax@4
  int v974; // er8@4
  int v975; // er12@4
  int v976; // eax@4
  int v977; // ecx@4
  int v978; // eax@4
  int v979; // er12@4
  int v980; // ecx@4
  int v981; // eax@4
  int v982; // eax@4
  int v983; // eax@4
  int v984; // er15@4
  int v985; // er11@4
  int v986; // eax@4
  int v987; // ecx@4
  int v988; // eax@4
  int v989; // er11@4
  int v990; // ecx@4
  int v991; // eax@4
  int v992; // eax@4
  int v993; // eax@4
  int v994; // er14@4
  int v995; // er10@4
  int v996; // eax@4
  int v997; // ecx@4
  int v998; // eax@4
  int v999; // er10@4
  int v1000; // ecx@4
  int v1001; // eax@4
  int v1002; // eax@4
  int v1003; // eax@4
  int v1004; // er13@4
  int v1005; // er9@4
  int v1006; // eax@4
  int v1007; // ecx@4
  int v1008; // eax@4
  int v1009; // er9@4
  int v1010; // ecx@4
  int v1011; // eax@4
  int v1012; // eax@4
  int v1013; // eax@4
  int v1014; // er12@4
  int v1015; // er8@4
  int v1016; // eax@4
  int v1017; // ecx@4
  int v1018; // eax@4
  int v1019; // er8@4
  __int64 result; // rax@4
  __int128 v1021; // [sp+0h] [bp-108h]@1
  __int128 v1022; // [sp+10h] [bp-F8h]@1
  __int128 v1023; // [sp+20h] [bp-E8h]@1
  __int128 v1024; // [sp+30h] [bp-D8h]@1
  __m128i v1025; // [sp+40h] [bp-C8h]@1
  __int128 v1026; // [sp+50h] [bp-B8h]@1
  __int128 v1027; // [sp+60h] [bp-A8h]@1
  __int128 v1028; // [sp+70h] [bp-98h]@1
  __int128 v1029; // [sp+80h] [bp-88h]@1
  __int128 v1030; // [sp+90h] [bp-78h]@1
  __int128 v1031; // [sp+A0h] [bp-68h]@1
  __int128 v1032; // [sp+B0h] [bp-58h]@1
  __int128 v1033; // [sp+C0h] [bp-48h]@1

  _mm_store_si128((__m128i *)&v1026, a4);
  _mm_store_si128((__m128i *)&v1027, a5);
  _mm_store_si128((__m128i *)&v1028, a6);
  _mm_store_si128((__m128i *)&v1029, a7);
  _mm_store_si128((__m128i *)&v1030, a8);
  _mm_store_si128((__m128i *)&v1031, a9);
  _mm_store_si128((__m128i *)&v1032, a10);
  _mm_store_si128((__m128i *)&v1033, a11);
  _mm_store_si128(&v1025, _mm_load_si128((const __m128i *)qword_67020));
  v11 = _mm_loadu_si128((const __m128i *)a1);
  v12 = _mm_loadu_si128((const __m128i *)(a1 + 16));
  v13 = _mm_loadu_si128((const __m128i *)(a1 + 32));
  v14 = _mm_loadu_si128((const __m128i *)(a1 + 48));
  v15 = a1 + 64;
  v16 = _mm_shuffle_epi8(v11, v1025);
  v17 = _mm_shuffle_epi8(v12, v1025);
  v18 = _mm_shuffle_epi8(v13, v1025);
  v19 = _mm_shuffle_epi8(v14, v1025);
  v20 = (const __m128i *)&ccsha256_K[8];
  v21 = _mm_add_epi32(_mm_loadu_si128((const __m128i *)&ccsha256_K[2]), v17);
  v22 = _mm_add_epi32(_mm_loadu_si128((const __m128i *)&ccsha256_K[4]), v18);
  v23 = _mm_add_epi32(_mm_loadu_si128((const __m128i *)&ccsha256_K[6]), v19);
  _mm_store_si128((__m128i *)&v1021, _mm_add_epi32(_mm_loadu_si128((const __m128i *)ccsha256_K), v16));
  _mm_store_si128((__m128i *)&v1022, v21);
  _mm_store_si128((__m128i *)&v1023, v22);
  _mm_store_si128((__m128i *)&v1024, v23);
  while ( 1 )
  {
    v24 = *(_DWORD *)(a2 + 4);
    v25 = *(_DWORD *)(a2 + 8);
    v26 = *(_DWORD *)(a2 + 16);
    v27 = *(_DWORD *)(a2 + 20);
    v28 = *(_DWORD *)(a2 + 24);
    v29 = __ROR4__(*(_DWORD *)(a2 + 16), 6);
    v30 = v29;
    v29 = __ROR4__(v29, 5);
    v31 = v29 ^ v30;
    v29 = __ROR4__(v29, 14);
    v32 = v1021 + (v29 ^ v31) + *(_DWORD *)(a2 + 28) + (v27 & *(_DWORD *)(a2 + 16) ^ v28 & ~v26);
    v33 = v32 + *(_DWORD *)(a2 + 12);
    v34 = v32;
    v35 = __ROR4__(*(_DWORD *)a2, 2);
    v36 = __ROR4__(*(_DWORD *)a2, 13);
    v37 = v36 ^ v35;
    v36 = __ROR4__(v36, 9);
    v38 = (v24 & v25 ^ *(_DWORD *)a2 & (v25 ^ *(_DWORD *)(a2 + 4))) + (v36 ^ v37) + v34;
    v39 = __ROR4__(v33, 6);
    v40 = v39;
    v39 = __ROR4__(v39, 5);
    v41 = v39 ^ v40;
    v39 = __ROR4__(v39, 14);
    v42 = DWORD1(v1021) + (v39 ^ v41) + v28 + (v26 & v33 ^ v27 & ~v33);
    v43 = v42 + v25;
    v44 = v42;
    v45 = __ROR4__(v38, 2);
    v46 = __ROR4__(v38, 13);
    v47 = v46 ^ v45;
    v46 = __ROR4__(v46, 9);
    v48 = (*(_DWORD *)a2 & *(_DWORD *)(a2 + 4) ^ v38 & (v24 ^ *(_DWORD *)a2)) + (v46 ^ v47) + v44;
    v49 = __ROR4__(v43, 6);
    v50 = v49;
    v49 = __ROR4__(v49, 5);
    v51 = v49 ^ v50;
    v49 = __ROR4__(v49, 14);
    v52 = DWORD2(v1021) + (v49 ^ v51) + v27 + (v33 & v43 ^ v26 & ~v43);
    v53 = v52 + v24;
    v54 = v52;
    v55 = __ROR4__(v48, 2);
    v56 = __ROR4__(v48, 13);
    v57 = v56 ^ v55;
    v56 = __ROR4__(v56, 9);
    v58 = (v38 & *(_DWORD *)a2 ^ v48 & (*(_DWORD *)a2 ^ v38)) + (v56 ^ v57) + v54;
    v59 = __ROR4__(v53, 6);
    v60 = v59;
    v59 = __ROR4__(v59, 5);
    v61 = v59 ^ v60;
    v59 = __ROR4__(v59, 14);
    v62 = DWORD3(v1021) + (v59 ^ v61) + v26 + (v43 & v53 ^ v33 & ~v53);
    v63 = v62 + *(_DWORD *)a2;
    v64 = v62;
    v65 = __ROR4__(v58, 2);
    v66 = __ROR4__(v58, 13);
    v67 = v66 ^ v65;
    v66 = __ROR4__(v66, 9);
    v68 = (v48 & v38 ^ v58 & (v38 ^ v48)) + (v66 ^ v67) + v64;
    v69 = _mm_loadu_si128(v20);
    v70 = v20 + 1;
    v71 = _mm_alignr_epi8(v17, v16, 4);
    v72 = _mm_srl_epi32(v71, 3u);
    v73 = _mm_srl_epi32(v71, 7u);
    v74 = _mm_sll_epi32(v71, 0xEu);
    v75 = _mm_add_epi32(
            _mm_add_epi32(
              v16,
              _mm_xor_si128(
                _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v72, v73), v74), _mm_srl_epi32(v73, 0xBu)),
                _mm_sll_epi32(v74, 0xBu))),
            _mm_alignr_epi8(v19, v18, 4));
    v76 = _mm_srli_si128(v19, 8);
    v77 = _mm_srl_epi32(v76, 0x11u);
    v78 = _mm_xor_si128(_mm_srl_epi32(v76, 0xAu), v77);
    v79 = _mm_sll_epi32(v76, 0xDu);
    v80 = _mm_add_epi32(
            v75,
            _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v78, v79), _mm_srl_epi32(v77, 2u)), _mm_sll_epi32(v79, 2u)));
    v81 = _mm_slli_si128(v80, 8);
    v82 = _mm_srl_epi32(v81, 0x11u);
    v83 = _mm_xor_si128(_mm_srl_epi32(v81, 0xAu), v82);
    v84 = _mm_sll_epi32(v81, 0xDu);
    v85 = _mm_add_epi32(
            v80,
            _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v83, v84), _mm_srl_epi32(v82, 2u)), _mm_sll_epi32(v84, 2u)));
    _mm_store_si128((__m128i *)&v1021, _mm_add_epi32(v69, v85));
    v86 = __ROR4__(v63, 6);
    v87 = v86;
    v86 = __ROR4__(v86, 5);
    v88 = v86 ^ v87;
    v86 = __ROR4__(v86, 14);
    v89 = v1022 + (v86 ^ v88) + v33 + (v53 & v63 ^ v43 & ~v63);
    v90 = v89 + v38;
    v91 = v89;
    v92 = __ROR4__(v68, 2);
    v93 = __ROR4__(v68, 13);
    v94 = v93 ^ v92;
    v93 = __ROR4__(v93, 9);
    v95 = (v58 & v48 ^ v68 & (v48 ^ v58)) + (v93 ^ v94) + v91;
    v96 = __ROR4__(v90, 6);
    v97 = v96;
    v96 = __ROR4__(v96, 5);
    v98 = v96 ^ v97;
    v96 = __ROR4__(v96, 14);
    v99 = DWORD1(v1022) + (v96 ^ v98) + v43 + (v63 & v90 ^ v53 & ~v90);
    v100 = v99 + v48;
    v101 = v99;
    v102 = __ROR4__(v95, 2);
    v103 = __ROR4__(v95, 13);
    v104 = v103 ^ v102;
    v103 = __ROR4__(v103, 9);
    v105 = (v68 & v58 ^ v95 & (v58 ^ v68)) + (v103 ^ v104) + v101;
    v106 = __ROR4__(v100, 6);
    v107 = v106;
    v106 = __ROR4__(v106, 5);
    v108 = v106 ^ v107;
    v106 = __ROR4__(v106, 14);
    v109 = DWORD2(v1022) + (v106 ^ v108) + v53 + (v90 & v100 ^ v63 & ~v100);
    v110 = v109 + v58;
    v111 = v109;
    v112 = __ROR4__(v105, 2);
    v113 = __ROR4__(v105, 13);
    v114 = v113 ^ v112;
    v113 = __ROR4__(v113, 9);
    v115 = (v95 & v68 ^ v105 & (v68 ^ v95)) + (v113 ^ v114) + v111;
    v116 = __ROR4__(v110, 6);
    v117 = v116;
    v116 = __ROR4__(v116, 5);
    v118 = v116 ^ v117;
    v116 = __ROR4__(v116, 14);
    v119 = DWORD3(v1022) + (v116 ^ v118) + v63 + (v100 & v110 ^ v90 & ~v110);
    v120 = v119 + v68;
    v121 = v119;
    v122 = __ROR4__(v115, 2);
    v123 = __ROR4__(v115, 13);
    v124 = v123 ^ v122;
    v123 = __ROR4__(v123, 9);
    v125 = (v105 & v95 ^ v115 & (v95 ^ v105)) + (v123 ^ v124) + v121;
    v126 = _mm_loadu_si128(v70);
    ++v70;
    v127 = _mm_alignr_epi8(v18, v17, 4);
    v128 = _mm_srl_epi32(v127, 3u);
    v129 = _mm_srl_epi32(v127, 7u);
    v130 = _mm_sll_epi32(v127, 0xEu);
    v131 = _mm_add_epi32(
             _mm_add_epi32(
               v17,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v128, v129), v130), _mm_srl_epi32(v129, 0xBu)),
                 _mm_sll_epi32(v130, 0xBu))),
             _mm_alignr_epi8(v85, v19, 4));
    v132 = _mm_srli_si128(v85, 8);
    v133 = _mm_srl_epi32(v132, 0x11u);
    v134 = _mm_xor_si128(_mm_srl_epi32(v132, 0xAu), v133);
    v135 = _mm_sll_epi32(v132, 0xDu);
    v136 = _mm_add_epi32(
             v131,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v134, v135), _mm_srl_epi32(v133, 2u)), _mm_sll_epi32(v135, 2u)));
    v137 = _mm_slli_si128(v136, 8);
    v138 = _mm_srl_epi32(v137, 0x11u);
    v139 = _mm_xor_si128(_mm_srl_epi32(v137, 0xAu), v138);
    v140 = _mm_sll_epi32(v137, 0xDu);
    v141 = _mm_add_epi32(
             v136,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v139, v140), _mm_srl_epi32(v138, 2u)), _mm_sll_epi32(v140, 2u)));
    _mm_store_si128((__m128i *)&v1022, _mm_add_epi32(v126, v141));
    v142 = __ROR4__(v120, 6);
    v143 = v142;
    v142 = __ROR4__(v142, 5);
    v144 = v142 ^ v143;
    v142 = __ROR4__(v142, 14);
    v145 = v1023 + (v142 ^ v144) + v90 + (v110 & v120 ^ v100 & ~v120);
    v146 = v145 + v95;
    v147 = v145;
    v148 = __ROR4__(v125, 2);
    v149 = __ROR4__(v125, 13);
    v150 = v149 ^ v148;
    v149 = __ROR4__(v149, 9);
    v151 = (v115 & v105 ^ v125 & (v105 ^ v115)) + (v149 ^ v150) + v147;
    v152 = __ROR4__(v146, 6);
    v153 = v152;
    v152 = __ROR4__(v152, 5);
    v154 = v152 ^ v153;
    v152 = __ROR4__(v152, 14);
    v155 = DWORD1(v1023) + (v152 ^ v154) + v100 + (v120 & v146 ^ v110 & ~v146);
    v156 = v155 + v105;
    v157 = v155;
    v158 = __ROR4__(v151, 2);
    v159 = __ROR4__(v151, 13);
    v160 = v159 ^ v158;
    v159 = __ROR4__(v159, 9);
    v161 = (v125 & v115 ^ v151 & (v115 ^ v125)) + (v159 ^ v160) + v157;
    v162 = __ROR4__(v156, 6);
    v163 = v162;
    v162 = __ROR4__(v162, 5);
    v164 = v162 ^ v163;
    v162 = __ROR4__(v162, 14);
    v165 = DWORD2(v1023) + (v162 ^ v164) + v110 + (v146 & v156 ^ v120 & ~v156);
    v166 = v165 + v115;
    v167 = v165;
    v168 = __ROR4__(v161, 2);
    v169 = __ROR4__(v161, 13);
    v170 = v169 ^ v168;
    v169 = __ROR4__(v169, 9);
    v171 = (v151 & v125 ^ v161 & (v125 ^ v151)) + (v169 ^ v170) + v167;
    v172 = __ROR4__(v166, 6);
    v173 = v172;
    v172 = __ROR4__(v172, 5);
    v174 = v172 ^ v173;
    v172 = __ROR4__(v172, 14);
    v175 = DWORD3(v1023) + (v172 ^ v174) + v120 + (v156 & v166 ^ v146 & ~v166);
    v176 = v175 + v125;
    v177 = v175;
    v178 = __ROR4__(v171, 2);
    v179 = __ROR4__(v171, 13);
    v180 = v179 ^ v178;
    v179 = __ROR4__(v179, 9);
    v181 = (v161 & v151 ^ v171 & (v151 ^ v161)) + (v179 ^ v180) + v177;
    v182 = _mm_loadu_si128(v70);
    ++v70;
    v183 = _mm_alignr_epi8(v19, v18, 4);
    v184 = _mm_srl_epi32(v183, 3u);
    v185 = _mm_srl_epi32(v183, 7u);
    v186 = _mm_sll_epi32(v183, 0xEu);
    v187 = _mm_add_epi32(
             _mm_add_epi32(
               v18,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v184, v185), v186), _mm_srl_epi32(v185, 0xBu)),
                 _mm_sll_epi32(v186, 0xBu))),
             _mm_alignr_epi8(v141, v85, 4));
    v188 = _mm_srli_si128(v141, 8);
    v189 = _mm_srl_epi32(v188, 0x11u);
    v190 = _mm_xor_si128(_mm_srl_epi32(v188, 0xAu), v189);
    v191 = _mm_sll_epi32(v188, 0xDu);
    v192 = _mm_add_epi32(
             v187,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v190, v191), _mm_srl_epi32(v189, 2u)), _mm_sll_epi32(v191, 2u)));
    v193 = _mm_slli_si128(v192, 8);
    v194 = _mm_srl_epi32(v193, 0x11u);
    v195 = _mm_xor_si128(_mm_srl_epi32(v193, 0xAu), v194);
    v196 = _mm_sll_epi32(v193, 0xDu);
    v197 = _mm_add_epi32(
             v192,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v195, v196), _mm_srl_epi32(v194, 2u)), _mm_sll_epi32(v196, 2u)));
    _mm_store_si128((__m128i *)&v1023, _mm_add_epi32(v182, v197));
    v198 = __ROR4__(v176, 6);
    v199 = v198;
    v198 = __ROR4__(v198, 5);
    v200 = v198 ^ v199;
    v198 = __ROR4__(v198, 14);
    v201 = v1024 + (v198 ^ v200) + v146 + (v166 & v176 ^ v156 & ~v176);
    v202 = v201 + v151;
    v203 = v201;
    v204 = __ROR4__(v181, 2);
    v205 = __ROR4__(v181, 13);
    v206 = v205 ^ v204;
    v205 = __ROR4__(v205, 9);
    v207 = (v171 & v161 ^ v181 & (v161 ^ v171)) + (v205 ^ v206) + v203;
    v208 = __ROR4__(v202, 6);
    v209 = v208;
    v208 = __ROR4__(v208, 5);
    v210 = v208 ^ v209;
    v208 = __ROR4__(v208, 14);
    v211 = DWORD1(v1024) + (v208 ^ v210) + v156 + (v176 & v202 ^ v166 & ~v202);
    v212 = v211 + v161;
    v213 = v211;
    v214 = __ROR4__(v207, 2);
    v215 = __ROR4__(v207, 13);
    v216 = v215 ^ v214;
    v215 = __ROR4__(v215, 9);
    v217 = (v181 & v171 ^ v207 & (v171 ^ v181)) + (v215 ^ v216) + v213;
    v218 = __ROR4__(v212, 6);
    v219 = v218;
    v218 = __ROR4__(v218, 5);
    v220 = v218 ^ v219;
    v218 = __ROR4__(v218, 14);
    v221 = DWORD2(v1024) + (v218 ^ v220) + v166 + (v202 & v212 ^ v176 & ~v212);
    v222 = v221 + v171;
    v223 = v221;
    v224 = __ROR4__(v217, 2);
    v225 = __ROR4__(v217, 13);
    v226 = v225 ^ v224;
    v225 = __ROR4__(v225, 9);
    v227 = (v207 & v181 ^ v217 & (v181 ^ v207)) + (v225 ^ v226) + v223;
    v228 = __ROR4__(v222, 6);
    v229 = v228;
    v228 = __ROR4__(v228, 5);
    v230 = v228 ^ v229;
    v228 = __ROR4__(v228, 14);
    v231 = DWORD3(v1024) + (v228 ^ v230) + v176 + (v212 & v222 ^ v202 & ~v222);
    v232 = v231 + v181;
    v233 = v231;
    v234 = __ROR4__(v227, 2);
    v235 = __ROR4__(v227, 13);
    v236 = v235 ^ v234;
    v235 = __ROR4__(v235, 9);
    v237 = (v217 & v207 ^ v227 & (v207 ^ v217)) + (v235 ^ v236) + v233;
    v238 = _mm_loadu_si128(v70);
    ++v70;
    v239 = _mm_alignr_epi8(v85, v19, 4);
    v240 = _mm_srl_epi32(v239, 3u);
    v241 = _mm_srl_epi32(v239, 7u);
    v242 = _mm_sll_epi32(v239, 0xEu);
    v243 = _mm_add_epi32(
             _mm_add_epi32(
               v19,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v240, v241), v242), _mm_srl_epi32(v241, 0xBu)),
                 _mm_sll_epi32(v242, 0xBu))),
             _mm_alignr_epi8(v197, v141, 4));
    v244 = _mm_srli_si128(v197, 8);
    v245 = _mm_srl_epi32(v244, 0x11u);
    v246 = _mm_xor_si128(_mm_srl_epi32(v244, 0xAu), v245);
    v247 = _mm_sll_epi32(v244, 0xDu);
    v248 = _mm_add_epi32(
             v243,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v246, v247), _mm_srl_epi32(v245, 2u)), _mm_sll_epi32(v247, 2u)));
    v249 = _mm_slli_si128(v248, 8);
    v250 = _mm_srl_epi32(v249, 0x11u);
    v251 = _mm_xor_si128(_mm_srl_epi32(v249, 0xAu), v250);
    v252 = _mm_sll_epi32(v249, 0xDu);
    v253 = _mm_add_epi32(
             v248,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v251, v252), _mm_srl_epi32(v250, 2u)), _mm_sll_epi32(v252, 2u)));
    _mm_store_si128((__m128i *)&v1024, _mm_add_epi32(v238, v253));
    v254 = __ROR4__(v232, 6);
    v255 = v254;
    v254 = __ROR4__(v254, 5);
    v256 = v254 ^ v255;
    v254 = __ROR4__(v254, 14);
    v257 = v1021 + (v254 ^ v256) + v202 + (v222 & v232 ^ v212 & ~v232);
    v258 = v257 + v207;
    v259 = v257;
    v260 = __ROR4__(v237, 2);
    v261 = __ROR4__(v237, 13);
    v262 = v261 ^ v260;
    v261 = __ROR4__(v261, 9);
    v263 = (v227 & v217 ^ v237 & (v217 ^ v227)) + (v261 ^ v262) + v259;
    v264 = __ROR4__(v258, 6);
    v265 = v264;
    v264 = __ROR4__(v264, 5);
    v266 = v264 ^ v265;
    v264 = __ROR4__(v264, 14);
    v267 = DWORD1(v1021) + (v264 ^ v266) + v212 + (v232 & v258 ^ v222 & ~v258);
    v268 = v267 + v217;
    v269 = v267;
    v270 = __ROR4__(v263, 2);
    v271 = __ROR4__(v263, 13);
    v272 = v271 ^ v270;
    v271 = __ROR4__(v271, 9);
    v273 = (v237 & v227 ^ v263 & (v227 ^ v237)) + (v271 ^ v272) + v269;
    v274 = __ROR4__(v268, 6);
    v275 = v274;
    v274 = __ROR4__(v274, 5);
    v276 = v274 ^ v275;
    v274 = __ROR4__(v274, 14);
    v277 = DWORD2(v1021) + (v274 ^ v276) + v222 + (v258 & v268 ^ v232 & ~v268);
    v278 = v277 + v227;
    v279 = v277;
    v280 = __ROR4__(v273, 2);
    v281 = __ROR4__(v273, 13);
    v282 = v281 ^ v280;
    v281 = __ROR4__(v281, 9);
    v283 = (v263 & v237 ^ v273 & (v237 ^ v263)) + (v281 ^ v282) + v279;
    v284 = __ROR4__(v278, 6);
    v285 = v284;
    v284 = __ROR4__(v284, 5);
    v286 = v284 ^ v285;
    v284 = __ROR4__(v284, 14);
    v287 = DWORD3(v1021) + (v284 ^ v286) + v232 + (v268 & v278 ^ v258 & ~v278);
    v288 = v287 + v237;
    v289 = v287;
    v290 = __ROR4__(v283, 2);
    v291 = __ROR4__(v283, 13);
    v292 = v291 ^ v290;
    v291 = __ROR4__(v291, 9);
    v293 = (v273 & v263 ^ v283 & (v263 ^ v273)) + (v291 ^ v292) + v289;
    v294 = _mm_loadu_si128(v70);
    ++v70;
    v295 = _mm_alignr_epi8(v141, v85, 4);
    v296 = _mm_srl_epi32(v295, 3u);
    v297 = _mm_srl_epi32(v295, 7u);
    v298 = _mm_sll_epi32(v295, 0xEu);
    v299 = _mm_add_epi32(
             _mm_add_epi32(
               v85,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v296, v297), v298), _mm_srl_epi32(v297, 0xBu)),
                 _mm_sll_epi32(v298, 0xBu))),
             _mm_alignr_epi8(v253, v197, 4));
    v300 = _mm_srli_si128(v253, 8);
    v301 = _mm_srl_epi32(v300, 0x11u);
    v302 = _mm_xor_si128(_mm_srl_epi32(v300, 0xAu), v301);
    v303 = _mm_sll_epi32(v300, 0xDu);
    v304 = _mm_add_epi32(
             v299,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v302, v303), _mm_srl_epi32(v301, 2u)), _mm_sll_epi32(v303, 2u)));
    v305 = _mm_slli_si128(v304, 8);
    v306 = _mm_srl_epi32(v305, 0x11u);
    v307 = _mm_xor_si128(_mm_srl_epi32(v305, 0xAu), v306);
    v308 = _mm_sll_epi32(v305, 0xDu);
    v309 = _mm_add_epi32(
             v304,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v307, v308), _mm_srl_epi32(v306, 2u)), _mm_sll_epi32(v308, 2u)));
    _mm_store_si128((__m128i *)&v1021, _mm_add_epi32(v294, v309));
    v310 = __ROR4__(v288, 6);
    v311 = v310;
    v310 = __ROR4__(v310, 5);
    v312 = v310 ^ v311;
    v310 = __ROR4__(v310, 14);
    v313 = v1022 + (v310 ^ v312) + v258 + (v278 & v288 ^ v268 & ~v288);
    v314 = v313 + v263;
    v315 = v313;
    v316 = __ROR4__(v293, 2);
    v317 = __ROR4__(v293, 13);
    v318 = v317 ^ v316;
    v317 = __ROR4__(v317, 9);
    v319 = (v283 & v273 ^ v293 & (v273 ^ v283)) + (v317 ^ v318) + v315;
    v320 = __ROR4__(v314, 6);
    v321 = v320;
    v320 = __ROR4__(v320, 5);
    v322 = v320 ^ v321;
    v320 = __ROR4__(v320, 14);
    v323 = DWORD1(v1022) + (v320 ^ v322) + v268 + (v288 & v314 ^ v278 & ~v314);
    v324 = v323 + v273;
    v325 = v323;
    v326 = __ROR4__(v319, 2);
    v327 = __ROR4__(v319, 13);
    v328 = v327 ^ v326;
    v327 = __ROR4__(v327, 9);
    v329 = (v293 & v283 ^ v319 & (v283 ^ v293)) + (v327 ^ v328) + v325;
    v330 = __ROR4__(v324, 6);
    v331 = v330;
    v330 = __ROR4__(v330, 5);
    v332 = v330 ^ v331;
    v330 = __ROR4__(v330, 14);
    v333 = DWORD2(v1022) + (v330 ^ v332) + v278 + (v314 & v324 ^ v288 & ~v324);
    v334 = v333 + v283;
    v335 = v333;
    v336 = __ROR4__(v329, 2);
    v337 = __ROR4__(v329, 13);
    v338 = v337 ^ v336;
    v337 = __ROR4__(v337, 9);
    v339 = (v319 & v293 ^ v329 & (v293 ^ v319)) + (v337 ^ v338) + v335;
    v340 = __ROR4__(v334, 6);
    v341 = v340;
    v340 = __ROR4__(v340, 5);
    v342 = v340 ^ v341;
    v340 = __ROR4__(v340, 14);
    v343 = DWORD3(v1022) + (v340 ^ v342) + v288 + (v324 & v334 ^ v314 & ~v334);
    v344 = v343 + v293;
    v345 = v343;
    v346 = __ROR4__(v339, 2);
    v347 = __ROR4__(v339, 13);
    v348 = v347 ^ v346;
    v347 = __ROR4__(v347, 9);
    v349 = (v329 & v319 ^ v339 & (v319 ^ v329)) + (v347 ^ v348) + v345;
    v350 = _mm_loadu_si128(v70);
    ++v70;
    v351 = _mm_alignr_epi8(v197, v141, 4);
    v352 = _mm_srl_epi32(v351, 3u);
    v353 = _mm_srl_epi32(v351, 7u);
    v354 = _mm_sll_epi32(v351, 0xEu);
    v355 = _mm_add_epi32(
             _mm_add_epi32(
               v141,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v352, v353), v354), _mm_srl_epi32(v353, 0xBu)),
                 _mm_sll_epi32(v354, 0xBu))),
             _mm_alignr_epi8(v309, v253, 4));
    v356 = _mm_srli_si128(v309, 8);
    v357 = _mm_srl_epi32(v356, 0x11u);
    v358 = _mm_xor_si128(_mm_srl_epi32(v356, 0xAu), v357);
    v359 = _mm_sll_epi32(v356, 0xDu);
    v360 = _mm_add_epi32(
             v355,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v358, v359), _mm_srl_epi32(v357, 2u)), _mm_sll_epi32(v359, 2u)));
    v361 = _mm_slli_si128(v360, 8);
    v362 = _mm_srl_epi32(v361, 0x11u);
    v363 = _mm_xor_si128(_mm_srl_epi32(v361, 0xAu), v362);
    v364 = _mm_sll_epi32(v361, 0xDu);
    v365 = _mm_add_epi32(
             v360,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v363, v364), _mm_srl_epi32(v362, 2u)), _mm_sll_epi32(v364, 2u)));
    _mm_store_si128((__m128i *)&v1022, _mm_add_epi32(v350, v365));
    v366 = __ROR4__(v344, 6);
    v367 = v366;
    v366 = __ROR4__(v366, 5);
    v368 = v366 ^ v367;
    v366 = __ROR4__(v366, 14);
    v369 = v1023 + (v366 ^ v368) + v314 + (v334 & v344 ^ v324 & ~v344);
    v370 = v369 + v319;
    v371 = v369;
    v372 = __ROR4__(v349, 2);
    v373 = __ROR4__(v349, 13);
    v374 = v373 ^ v372;
    v373 = __ROR4__(v373, 9);
    v375 = (v339 & v329 ^ v349 & (v329 ^ v339)) + (v373 ^ v374) + v371;
    v376 = __ROR4__(v370, 6);
    v377 = v376;
    v376 = __ROR4__(v376, 5);
    v378 = v376 ^ v377;
    v376 = __ROR4__(v376, 14);
    v379 = DWORD1(v1023) + (v376 ^ v378) + v324 + (v344 & v370 ^ v334 & ~v370);
    v380 = v379 + v329;
    v381 = v379;
    v382 = __ROR4__(v375, 2);
    v383 = __ROR4__(v375, 13);
    v384 = v383 ^ v382;
    v383 = __ROR4__(v383, 9);
    v385 = (v349 & v339 ^ v375 & (v339 ^ v349)) + (v383 ^ v384) + v381;
    v386 = __ROR4__(v380, 6);
    v387 = v386;
    v386 = __ROR4__(v386, 5);
    v388 = v386 ^ v387;
    v386 = __ROR4__(v386, 14);
    v389 = DWORD2(v1023) + (v386 ^ v388) + v334 + (v370 & v380 ^ v344 & ~v380);
    v390 = v389 + v339;
    v391 = v389;
    v392 = __ROR4__(v385, 2);
    v393 = __ROR4__(v385, 13);
    v394 = v393 ^ v392;
    v393 = __ROR4__(v393, 9);
    v395 = (v375 & v349 ^ v385 & (v349 ^ v375)) + (v393 ^ v394) + v391;
    v396 = __ROR4__(v390, 6);
    v397 = v396;
    v396 = __ROR4__(v396, 5);
    v398 = v396 ^ v397;
    v396 = __ROR4__(v396, 14);
    v399 = DWORD3(v1023) + (v396 ^ v398) + v344 + (v380 & v390 ^ v370 & ~v390);
    v400 = v399 + v349;
    v401 = v399;
    v402 = __ROR4__(v395, 2);
    v403 = __ROR4__(v395, 13);
    v404 = v403 ^ v402;
    v403 = __ROR4__(v403, 9);
    v405 = (v385 & v375 ^ v395 & (v375 ^ v385)) + (v403 ^ v404) + v401;
    v406 = _mm_loadu_si128(v70);
    ++v70;
    v407 = _mm_alignr_epi8(v253, v197, 4);
    v408 = _mm_srl_epi32(v407, 3u);
    v409 = _mm_srl_epi32(v407, 7u);
    v410 = _mm_sll_epi32(v407, 0xEu);
    v411 = _mm_add_epi32(
             _mm_add_epi32(
               v197,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v408, v409), v410), _mm_srl_epi32(v409, 0xBu)),
                 _mm_sll_epi32(v410, 0xBu))),
             _mm_alignr_epi8(v365, v309, 4));
    v412 = _mm_srli_si128(v365, 8);
    v413 = _mm_srl_epi32(v412, 0x11u);
    v414 = _mm_xor_si128(_mm_srl_epi32(v412, 0xAu), v413);
    v415 = _mm_sll_epi32(v412, 0xDu);
    v416 = _mm_add_epi32(
             v411,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v414, v415), _mm_srl_epi32(v413, 2u)), _mm_sll_epi32(v415, 2u)));
    v417 = _mm_slli_si128(v416, 8);
    v418 = _mm_srl_epi32(v417, 0x11u);
    v419 = _mm_xor_si128(_mm_srl_epi32(v417, 0xAu), v418);
    v420 = _mm_sll_epi32(v417, 0xDu);
    v421 = _mm_add_epi32(
             v416,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v419, v420), _mm_srl_epi32(v418, 2u)), _mm_sll_epi32(v420, 2u)));
    _mm_store_si128((__m128i *)&v1023, _mm_add_epi32(v406, v421));
    v422 = __ROR4__(v400, 6);
    v423 = v422;
    v422 = __ROR4__(v422, 5);
    v424 = v422 ^ v423;
    v422 = __ROR4__(v422, 14);
    v425 = v1024 + (v422 ^ v424) + v370 + (v390 & v400 ^ v380 & ~v400);
    v426 = v425 + v375;
    v427 = v425;
    v428 = __ROR4__(v405, 2);
    v429 = __ROR4__(v405, 13);
    v430 = v429 ^ v428;
    v429 = __ROR4__(v429, 9);
    v431 = (v395 & v385 ^ v405 & (v385 ^ v395)) + (v429 ^ v430) + v427;
    v432 = __ROR4__(v426, 6);
    v433 = v432;
    v432 = __ROR4__(v432, 5);
    v434 = v432 ^ v433;
    v432 = __ROR4__(v432, 14);
    v435 = DWORD1(v1024) + (v432 ^ v434) + v380 + (v400 & v426 ^ v390 & ~v426);
    v436 = v435 + v385;
    v437 = v435;
    v438 = __ROR4__(v431, 2);
    v439 = __ROR4__(v431, 13);
    v440 = v439 ^ v438;
    v439 = __ROR4__(v439, 9);
    v441 = (v405 & v395 ^ v431 & (v395 ^ v405)) + (v439 ^ v440) + v437;
    v442 = __ROR4__(v436, 6);
    v443 = v442;
    v442 = __ROR4__(v442, 5);
    v444 = v442 ^ v443;
    v442 = __ROR4__(v442, 14);
    v445 = DWORD2(v1024) + (v442 ^ v444) + v390 + (v426 & v436 ^ v400 & ~v436);
    v446 = v445 + v395;
    v447 = v445;
    v448 = __ROR4__(v441, 2);
    v449 = __ROR4__(v441, 13);
    v450 = v449 ^ v448;
    v449 = __ROR4__(v449, 9);
    v451 = (v431 & v405 ^ v441 & (v405 ^ v431)) + (v449 ^ v450) + v447;
    v452 = __ROR4__(v446, 6);
    v453 = v452;
    v452 = __ROR4__(v452, 5);
    v454 = v452 ^ v453;
    v452 = __ROR4__(v452, 14);
    v455 = DWORD3(v1024) + (v452 ^ v454) + v400 + (v436 & v446 ^ v426 & ~v446);
    v456 = v455 + v405;
    v457 = v455;
    v458 = __ROR4__(v451, 2);
    v459 = __ROR4__(v451, 13);
    v460 = v459 ^ v458;
    v459 = __ROR4__(v459, 9);
    v461 = (v441 & v431 ^ v451 & (v431 ^ v441)) + (v459 ^ v460) + v457;
    v462 = _mm_loadu_si128(v70);
    ++v70;
    v463 = _mm_alignr_epi8(v309, v253, 4);
    v464 = _mm_srl_epi32(v463, 3u);
    v465 = _mm_srl_epi32(v463, 7u);
    v466 = _mm_sll_epi32(v463, 0xEu);
    v467 = _mm_add_epi32(
             _mm_add_epi32(
               v253,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v464, v465), v466), _mm_srl_epi32(v465, 0xBu)),
                 _mm_sll_epi32(v466, 0xBu))),
             _mm_alignr_epi8(v421, v365, 4));
    v468 = _mm_srli_si128(v421, 8);
    v469 = _mm_srl_epi32(v468, 0x11u);
    v470 = _mm_xor_si128(_mm_srl_epi32(v468, 0xAu), v469);
    v471 = _mm_sll_epi32(v468, 0xDu);
    v472 = _mm_add_epi32(
             v467,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v470, v471), _mm_srl_epi32(v469, 2u)), _mm_sll_epi32(v471, 2u)));
    v473 = _mm_slli_si128(v472, 8);
    v474 = _mm_srl_epi32(v473, 0x11u);
    v475 = _mm_xor_si128(_mm_srl_epi32(v473, 0xAu), v474);
    v476 = _mm_sll_epi32(v473, 0xDu);
    v477 = _mm_add_epi32(
             v472,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v475, v476), _mm_srl_epi32(v474, 2u)), _mm_sll_epi32(v476, 2u)));
    _mm_store_si128((__m128i *)&v1024, _mm_add_epi32(v462, v477));
    v478 = __ROR4__(v456, 6);
    v479 = v478;
    v478 = __ROR4__(v478, 5);
    v480 = v478 ^ v479;
    v478 = __ROR4__(v478, 14);
    v481 = v1021 + (v478 ^ v480) + v426 + (v446 & v456 ^ v436 & ~v456);
    v482 = v481 + v431;
    v483 = v481;
    v484 = __ROR4__(v461, 2);
    v485 = __ROR4__(v461, 13);
    v486 = v485 ^ v484;
    v485 = __ROR4__(v485, 9);
    v487 = (v451 & v441 ^ v461 & (v441 ^ v451)) + (v485 ^ v486) + v483;
    v488 = __ROR4__(v482, 6);
    v489 = v488;
    v488 = __ROR4__(v488, 5);
    v490 = v488 ^ v489;
    v488 = __ROR4__(v488, 14);
    v491 = DWORD1(v1021) + (v488 ^ v490) + v436 + (v456 & v482 ^ v446 & ~v482);
    v492 = v491 + v441;
    v493 = v491;
    v494 = __ROR4__(v487, 2);
    v495 = __ROR4__(v487, 13);
    v496 = v495 ^ v494;
    v495 = __ROR4__(v495, 9);
    v497 = (v461 & v451 ^ v487 & (v451 ^ v461)) + (v495 ^ v496) + v493;
    v498 = __ROR4__(v492, 6);
    v499 = v498;
    v498 = __ROR4__(v498, 5);
    v500 = v498 ^ v499;
    v498 = __ROR4__(v498, 14);
    v501 = DWORD2(v1021) + (v498 ^ v500) + v446 + (v482 & v492 ^ v456 & ~v492);
    v502 = v501 + v451;
    v503 = v501;
    v504 = __ROR4__(v497, 2);
    v505 = __ROR4__(v497, 13);
    v506 = v505 ^ v504;
    v505 = __ROR4__(v505, 9);
    v507 = (v487 & v461 ^ v497 & (v461 ^ v487)) + (v505 ^ v506) + v503;
    v508 = __ROR4__(v502, 6);
    v509 = v508;
    v508 = __ROR4__(v508, 5);
    v510 = v508 ^ v509;
    v508 = __ROR4__(v508, 14);
    v511 = DWORD3(v1021) + (v508 ^ v510) + v456 + (v492 & v502 ^ v482 & ~v502);
    v512 = v511 + v461;
    v513 = v511;
    v514 = __ROR4__(v507, 2);
    v515 = __ROR4__(v507, 13);
    v516 = v515 ^ v514;
    v515 = __ROR4__(v515, 9);
    v517 = (v497 & v487 ^ v507 & (v487 ^ v497)) + (v515 ^ v516) + v513;
    v518 = _mm_loadu_si128(v70);
    ++v70;
    v519 = _mm_alignr_epi8(v365, v309, 4);
    v520 = _mm_srl_epi32(v519, 3u);
    v521 = _mm_srl_epi32(v519, 7u);
    v522 = _mm_sll_epi32(v519, 0xEu);
    v523 = _mm_add_epi32(
             _mm_add_epi32(
               v309,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v520, v521), v522), _mm_srl_epi32(v521, 0xBu)),
                 _mm_sll_epi32(v522, 0xBu))),
             _mm_alignr_epi8(v477, v421, 4));
    v524 = _mm_srli_si128(v477, 8);
    v525 = _mm_srl_epi32(v524, 0x11u);
    v526 = _mm_xor_si128(_mm_srl_epi32(v524, 0xAu), v525);
    v527 = _mm_sll_epi32(v524, 0xDu);
    v528 = _mm_add_epi32(
             v523,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v526, v527), _mm_srl_epi32(v525, 2u)), _mm_sll_epi32(v527, 2u)));
    v529 = _mm_slli_si128(v528, 8);
    v530 = _mm_srl_epi32(v529, 0x11u);
    v531 = _mm_xor_si128(_mm_srl_epi32(v529, 0xAu), v530);
    v532 = _mm_sll_epi32(v529, 0xDu);
    v533 = _mm_add_epi32(
             v528,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v531, v532), _mm_srl_epi32(v530, 2u)), _mm_sll_epi32(v532, 2u)));
    _mm_store_si128((__m128i *)&v1021, _mm_add_epi32(v518, v533));
    v534 = __ROR4__(v512, 6);
    v535 = v534;
    v534 = __ROR4__(v534, 5);
    v536 = v534 ^ v535;
    v534 = __ROR4__(v534, 14);
    v537 = v1022 + (v534 ^ v536) + v482 + (v502 & v512 ^ v492 & ~v512);
    v538 = v537 + v487;
    v539 = v537;
    v540 = __ROR4__(v517, 2);
    v541 = __ROR4__(v517, 13);
    v542 = v541 ^ v540;
    v541 = __ROR4__(v541, 9);
    v543 = (v507 & v497 ^ v517 & (v497 ^ v507)) + (v541 ^ v542) + v539;
    v544 = __ROR4__(v538, 6);
    v545 = v544;
    v544 = __ROR4__(v544, 5);
    v546 = v544 ^ v545;
    v544 = __ROR4__(v544, 14);
    v547 = DWORD1(v1022) + (v544 ^ v546) + v492 + (v512 & v538 ^ v502 & ~v538);
    v548 = v547 + v497;
    v549 = v547;
    v550 = __ROR4__(v543, 2);
    v551 = __ROR4__(v543, 13);
    v552 = v551 ^ v550;
    v551 = __ROR4__(v551, 9);
    v553 = (v517 & v507 ^ v543 & (v507 ^ v517)) + (v551 ^ v552) + v549;
    v554 = __ROR4__(v548, 6);
    v555 = v554;
    v554 = __ROR4__(v554, 5);
    v556 = v554 ^ v555;
    v554 = __ROR4__(v554, 14);
    v557 = DWORD2(v1022) + (v554 ^ v556) + v502 + (v538 & v548 ^ v512 & ~v548);
    v558 = v557 + v507;
    v559 = v557;
    v560 = __ROR4__(v553, 2);
    v561 = __ROR4__(v553, 13);
    v562 = v561 ^ v560;
    v561 = __ROR4__(v561, 9);
    v563 = (v543 & v517 ^ v553 & (v517 ^ v543)) + (v561 ^ v562) + v559;
    v564 = __ROR4__(v558, 6);
    v565 = v564;
    v564 = __ROR4__(v564, 5);
    v566 = v564 ^ v565;
    v564 = __ROR4__(v564, 14);
    v567 = DWORD3(v1022) + (v564 ^ v566) + v512 + (v548 & v558 ^ v538 & ~v558);
    v568 = v567 + v517;
    v569 = v567;
    v570 = __ROR4__(v563, 2);
    v571 = __ROR4__(v563, 13);
    v572 = v571 ^ v570;
    v571 = __ROR4__(v571, 9);
    v573 = (v553 & v543 ^ v563 & (v543 ^ v553)) + (v571 ^ v572) + v569;
    v574 = _mm_loadu_si128(v70);
    ++v70;
    v575 = _mm_alignr_epi8(v421, v365, 4);
    v576 = _mm_srl_epi32(v575, 3u);
    v577 = _mm_srl_epi32(v575, 7u);
    v578 = _mm_sll_epi32(v575, 0xEu);
    v579 = _mm_add_epi32(
             _mm_add_epi32(
               v365,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v576, v577), v578), _mm_srl_epi32(v577, 0xBu)),
                 _mm_sll_epi32(v578, 0xBu))),
             _mm_alignr_epi8(v533, v477, 4));
    v580 = _mm_srli_si128(v533, 8);
    v581 = _mm_srl_epi32(v580, 0x11u);
    v582 = _mm_xor_si128(_mm_srl_epi32(v580, 0xAu), v581);
    v583 = _mm_sll_epi32(v580, 0xDu);
    v584 = _mm_add_epi32(
             v579,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v582, v583), _mm_srl_epi32(v581, 2u)), _mm_sll_epi32(v583, 2u)));
    v585 = _mm_slli_si128(v584, 8);
    v586 = _mm_srl_epi32(v585, 0x11u);
    v587 = _mm_xor_si128(_mm_srl_epi32(v585, 0xAu), v586);
    v588 = _mm_sll_epi32(v585, 0xDu);
    v589 = _mm_add_epi32(
             v584,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v587, v588), _mm_srl_epi32(v586, 2u)), _mm_sll_epi32(v588, 2u)));
    _mm_store_si128((__m128i *)&v1022, _mm_add_epi32(v574, v589));
    v590 = __ROR4__(v568, 6);
    v591 = v590;
    v590 = __ROR4__(v590, 5);
    v592 = v590 ^ v591;
    v590 = __ROR4__(v590, 14);
    v593 = v1023 + (v590 ^ v592) + v538 + (v558 & v568 ^ v548 & ~v568);
    v594 = v593 + v543;
    v595 = v593;
    v596 = __ROR4__(v573, 2);
    v597 = __ROR4__(v573, 13);
    v598 = v597 ^ v596;
    v597 = __ROR4__(v597, 9);
    v599 = (v563 & v553 ^ v573 & (v553 ^ v563)) + (v597 ^ v598) + v595;
    v600 = __ROR4__(v594, 6);
    v601 = v600;
    v600 = __ROR4__(v600, 5);
    v602 = v600 ^ v601;
    v600 = __ROR4__(v600, 14);
    v603 = DWORD1(v1023) + (v600 ^ v602) + v548 + (v568 & v594 ^ v558 & ~v594);
    v604 = v603 + v553;
    v605 = v603;
    v606 = __ROR4__(v599, 2);
    v607 = __ROR4__(v599, 13);
    v608 = v607 ^ v606;
    v607 = __ROR4__(v607, 9);
    v609 = (v573 & v563 ^ v599 & (v563 ^ v573)) + (v607 ^ v608) + v605;
    v610 = __ROR4__(v604, 6);
    v611 = v610;
    v610 = __ROR4__(v610, 5);
    v612 = v610 ^ v611;
    v610 = __ROR4__(v610, 14);
    v613 = DWORD2(v1023) + (v610 ^ v612) + v558 + (v594 & v604 ^ v568 & ~v604);
    v614 = v613 + v563;
    v615 = v613;
    v616 = __ROR4__(v609, 2);
    v617 = __ROR4__(v609, 13);
    v618 = v617 ^ v616;
    v617 = __ROR4__(v617, 9);
    v619 = (v599 & v573 ^ v609 & (v573 ^ v599)) + (v617 ^ v618) + v615;
    v620 = __ROR4__(v614, 6);
    v621 = v620;
    v620 = __ROR4__(v620, 5);
    v622 = v620 ^ v621;
    v620 = __ROR4__(v620, 14);
    v623 = DWORD3(v1023) + (v620 ^ v622) + v568 + (v604 & v614 ^ v594 & ~v614);
    v624 = v623 + v573;
    v625 = v623;
    v626 = __ROR4__(v619, 2);
    v627 = __ROR4__(v619, 13);
    v628 = v627 ^ v626;
    v627 = __ROR4__(v627, 9);
    v629 = (v609 & v599 ^ v619 & (v599 ^ v609)) + (v627 ^ v628) + v625;
    v630 = _mm_loadu_si128(v70);
    ++v70;
    v631 = _mm_alignr_epi8(v477, v421, 4);
    v632 = _mm_srl_epi32(v631, 3u);
    v633 = _mm_srl_epi32(v631, 7u);
    v634 = _mm_sll_epi32(v631, 0xEu);
    v635 = _mm_add_epi32(
             _mm_add_epi32(
               v421,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v632, v633), v634), _mm_srl_epi32(v633, 0xBu)),
                 _mm_sll_epi32(v634, 0xBu))),
             _mm_alignr_epi8(v589, v533, 4));
    v636 = _mm_srli_si128(v589, 8);
    v637 = _mm_srl_epi32(v636, 0x11u);
    v638 = _mm_xor_si128(_mm_srl_epi32(v636, 0xAu), v637);
    v639 = _mm_sll_epi32(v636, 0xDu);
    v640 = _mm_add_epi32(
             v635,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v638, v639), _mm_srl_epi32(v637, 2u)), _mm_sll_epi32(v639, 2u)));
    v641 = _mm_slli_si128(v640, 8);
    v642 = _mm_srl_epi32(v641, 0x11u);
    v643 = _mm_xor_si128(_mm_srl_epi32(v641, 0xAu), v642);
    v644 = _mm_sll_epi32(v641, 0xDu);
    v645 = _mm_add_epi32(
             v640,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v643, v644), _mm_srl_epi32(v642, 2u)), _mm_sll_epi32(v644, 2u)));
    _mm_store_si128((__m128i *)&v1023, _mm_add_epi32(v630, v645));
    v646 = __ROR4__(v624, 6);
    v647 = v646;
    v646 = __ROR4__(v646, 5);
    v648 = v646 ^ v647;
    v646 = __ROR4__(v646, 14);
    v649 = v1024 + (v646 ^ v648) + v594 + (v614 & v624 ^ v604 & ~v624);
    v650 = v649 + v599;
    v651 = v649;
    v652 = __ROR4__(v629, 2);
    v653 = __ROR4__(v629, 13);
    v654 = v653 ^ v652;
    v653 = __ROR4__(v653, 9);
    v655 = (v619 & v609 ^ v629 & (v609 ^ v619)) + (v653 ^ v654) + v651;
    v656 = __ROR4__(v650, 6);
    v657 = v656;
    v656 = __ROR4__(v656, 5);
    v658 = v656 ^ v657;
    v656 = __ROR4__(v656, 14);
    v659 = DWORD1(v1024) + (v656 ^ v658) + v604 + (v624 & v650 ^ v614 & ~v650);
    v660 = v659 + v609;
    v661 = v659;
    v662 = __ROR4__(v655, 2);
    v663 = __ROR4__(v655, 13);
    v664 = v663 ^ v662;
    v663 = __ROR4__(v663, 9);
    v665 = (v629 & v619 ^ v655 & (v619 ^ v629)) + (v663 ^ v664) + v661;
    v666 = __ROR4__(v660, 6);
    v667 = v666;
    v666 = __ROR4__(v666, 5);
    v668 = v666 ^ v667;
    v666 = __ROR4__(v666, 14);
    v669 = DWORD2(v1024) + (v666 ^ v668) + v614 + (v650 & v660 ^ v624 & ~v660);
    v670 = v669 + v619;
    v671 = v669;
    v672 = __ROR4__(v665, 2);
    v673 = __ROR4__(v665, 13);
    v674 = v673 ^ v672;
    v673 = __ROR4__(v673, 9);
    v675 = (v655 & v629 ^ v665 & (v629 ^ v655)) + (v673 ^ v674) + v671;
    v676 = __ROR4__(v670, 6);
    v677 = v676;
    v676 = __ROR4__(v676, 5);
    v678 = v676 ^ v677;
    v676 = __ROR4__(v676, 14);
    v679 = DWORD3(v1024) + (v676 ^ v678) + v624 + (v660 & v670 ^ v650 & ~v670);
    v680 = v679 + v629;
    v681 = v679;
    v682 = __ROR4__(v675, 2);
    v683 = __ROR4__(v675, 13);
    v684 = v683 ^ v682;
    v683 = __ROR4__(v683, 9);
    v685 = (v665 & v655 ^ v675 & (v655 ^ v665)) + (v683 ^ v684) + v681;
    v686 = _mm_alignr_epi8(v533, v477, 4);
    v687 = _mm_srl_epi32(v686, 3u);
    v688 = _mm_srl_epi32(v686, 7u);
    v689 = _mm_sll_epi32(v686, 0xEu);
    v690 = _mm_add_epi32(
             _mm_add_epi32(
               v477,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v687, v688), v689), _mm_srl_epi32(v688, 0xBu)),
                 _mm_sll_epi32(v689, 0xBu))),
             _mm_alignr_epi8(v645, v589, 4));
    v691 = _mm_srli_si128(v645, 8);
    v692 = _mm_srl_epi32(v691, 0x11u);
    v693 = _mm_xor_si128(_mm_srl_epi32(v691, 0xAu), v692);
    v694 = _mm_sll_epi32(v691, 0xDu);
    v695 = _mm_add_epi32(
             v690,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v693, v694), _mm_srl_epi32(v692, 2u)), _mm_sll_epi32(v694, 2u)));
    v696 = _mm_slli_si128(v695, 8);
    v697 = _mm_srl_epi32(v696, 0x11u);
    v698 = _mm_xor_si128(_mm_srl_epi32(v696, 0xAu), v697);
    v699 = _mm_sll_epi32(v696, 0xDu);
    _mm_store_si128(
      (__m128i *)&v1024,
      _mm_add_epi32(
        _mm_loadu_si128(v70),
        _mm_add_epi32(
          v695,
          _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v698, v699), _mm_srl_epi32(v697, 2u)), _mm_sll_epi32(v699, 2u)))));
    v700 = (signed __int64)&v70[-15];
    --a3;
    if ( !a3 )
      break;
    v701 = __ROR4__(v680, 6);
    v702 = v701;
    v701 = __ROR4__(v701, 5);
    v703 = v701 ^ v702;
    v701 = __ROR4__(v701, 14);
    v704 = v1021 + (v701 ^ v703) + v650 + (v670 & v680 ^ v660 & ~v680);
    v705 = v704 + v655;
    v706 = v704;
    v707 = __ROR4__(v685, 2);
    v708 = __ROR4__(v685, 13);
    v709 = v708 ^ v707;
    v708 = __ROR4__(v708, 9);
    v710 = (v675 & v665 ^ v685 & (v665 ^ v675)) + (v708 ^ v709) + v706;
    v711 = __ROR4__(v705, 6);
    v712 = v711;
    v711 = __ROR4__(v711, 5);
    v713 = v711 ^ v712;
    v711 = __ROR4__(v711, 14);
    v714 = DWORD1(v1021) + (v711 ^ v713) + v660 + (v680 & v705 ^ v670 & ~v705);
    v715 = v714 + v665;
    v716 = v714;
    v717 = __ROR4__(v710, 2);
    v718 = __ROR4__(v710, 13);
    v719 = v718 ^ v717;
    v718 = __ROR4__(v718, 9);
    v720 = (v685 & v675 ^ v710 & (v675 ^ v685)) + (v718 ^ v719) + v716;
    v721 = __ROR4__(v715, 6);
    v722 = v721;
    v721 = __ROR4__(v721, 5);
    v723 = v721 ^ v722;
    v721 = __ROR4__(v721, 14);
    v724 = DWORD2(v1021) + (v721 ^ v723) + v670 + (v705 & v715 ^ v680 & ~v715);
    v725 = v724 + v675;
    v726 = v724;
    v727 = __ROR4__(v720, 2);
    v728 = __ROR4__(v720, 13);
    v729 = v728 ^ v727;
    v728 = __ROR4__(v728, 9);
    v730 = (v710 & v685 ^ v720 & (v685 ^ v710)) + (v728 ^ v729) + v726;
    v731 = __ROR4__(v725, 6);
    v732 = v731;
    v731 = __ROR4__(v731, 5);
    v733 = v731 ^ v732;
    v731 = __ROR4__(v731, 14);
    v734 = DWORD3(v1021) + (v731 ^ v733) + v680 + (v715 & v725 ^ v705 & ~v725);
    v735 = v734 + v685;
    v736 = v734;
    v737 = __ROR4__(v730, 2);
    v738 = __ROR4__(v730, 13);
    v739 = v738 ^ v737;
    v738 = __ROR4__(v738, 9);
    v740 = (v720 & v710 ^ v730 & (v710 ^ v720)) + (v738 ^ v739) + v736;
    v16 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)v15), v1025);
    _mm_store_si128((__m128i *)&v1021, _mm_add_epi32(_mm_loadu_si128((const __m128i *)v700), v16));
    v741 = __ROR4__(v735, 6);
    v742 = v741;
    v741 = __ROR4__(v741, 5);
    v743 = v741 ^ v742;
    v741 = __ROR4__(v741, 14);
    v744 = v1022 + (v741 ^ v743) + v705 + (v725 & v735 ^ v715 & ~v735);
    v745 = v744 + v710;
    v746 = v744;
    v747 = __ROR4__(v740, 2);
    v748 = __ROR4__(v740, 13);
    v749 = v748 ^ v747;
    v748 = __ROR4__(v748, 9);
    v750 = (v730 & v720 ^ v740 & (v720 ^ v730)) + (v748 ^ v749) + v746;
    v751 = __ROR4__(v745, 6);
    v752 = v751;
    v751 = __ROR4__(v751, 5);
    v753 = v751 ^ v752;
    v751 = __ROR4__(v751, 14);
    v754 = DWORD1(v1022) + (v751 ^ v753) + v715 + (v735 & v745 ^ v725 & ~v745);
    v755 = v754 + v720;
    v756 = v754;
    v757 = __ROR4__(v750, 2);
    v758 = __ROR4__(v750, 13);
    v759 = v758 ^ v757;
    v758 = __ROR4__(v758, 9);
    v760 = (v740 & v730 ^ v750 & (v730 ^ v740)) + (v758 ^ v759) + v756;
    v761 = __ROR4__(v755, 6);
    v762 = v761;
    v761 = __ROR4__(v761, 5);
    v763 = v761 ^ v762;
    v761 = __ROR4__(v761, 14);
    v764 = DWORD2(v1022) + (v761 ^ v763) + v725 + (v745 & v755 ^ v735 & ~v755);
    v765 = v764 + v730;
    v766 = v764;
    v767 = __ROR4__(v760, 2);
    v768 = __ROR4__(v760, 13);
    v769 = v768 ^ v767;
    v768 = __ROR4__(v768, 9);
    v770 = (v750 & v740 ^ v760 & (v740 ^ v750)) + (v768 ^ v769) + v766;
    v771 = __ROR4__(v765, 6);
    v772 = v771;
    v771 = __ROR4__(v771, 5);
    v773 = v771 ^ v772;
    v771 = __ROR4__(v771, 14);
    v774 = DWORD3(v1022) + (v771 ^ v773) + v735 + (v755 & v765 ^ v745 & ~v765);
    v775 = v774 + v740;
    v776 = v774;
    v777 = __ROR4__(v770, 2);
    v778 = __ROR4__(v770, 13);
    v779 = v778 ^ v777;
    v778 = __ROR4__(v778, 9);
    v780 = (v760 & v750 ^ v770 & (v750 ^ v760)) + (v778 ^ v779) + v776;
    v17 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(v15 + 16)), v1025);
    _mm_store_si128((__m128i *)&v1022, _mm_add_epi32(_mm_loadu_si128((const __m128i *)(v700 + 16)), v17));
    v781 = __ROR4__(v775, 6);
    v782 = v781;
    v781 = __ROR4__(v781, 5);
    v783 = v781 ^ v782;
    v781 = __ROR4__(v781, 14);
    v784 = v1023 + (v781 ^ v783) + v745 + (v765 & v775 ^ v755 & ~v775);
    v785 = v784 + v750;
    v786 = v784;
    v787 = __ROR4__(v780, 2);
    v788 = __ROR4__(v780, 13);
    v789 = v788 ^ v787;
    v788 = __ROR4__(v788, 9);
    v790 = (v770 & v760 ^ v780 & (v760 ^ v770)) + (v788 ^ v789) + v786;
    v791 = __ROR4__(v785, 6);
    v792 = v791;
    v791 = __ROR4__(v791, 5);
    v793 = v791 ^ v792;
    v791 = __ROR4__(v791, 14);
    v794 = DWORD1(v1023) + (v791 ^ v793) + v755 + (v775 & v785 ^ v765 & ~v785);
    v795 = v794 + v760;
    v796 = v794;
    v797 = __ROR4__(v790, 2);
    v798 = __ROR4__(v790, 13);
    v799 = v798 ^ v797;
    v798 = __ROR4__(v798, 9);
    v800 = (v780 & v770 ^ v790 & (v770 ^ v780)) + (v798 ^ v799) + v796;
    v801 = __ROR4__(v795, 6);
    v802 = v801;
    v801 = __ROR4__(v801, 5);
    v803 = v801 ^ v802;
    v801 = __ROR4__(v801, 14);
    v804 = DWORD2(v1023) + (v801 ^ v803) + v765 + (v785 & v795 ^ v775 & ~v795);
    v805 = v804 + v770;
    v806 = v804;
    v807 = __ROR4__(v800, 2);
    v808 = __ROR4__(v800, 13);
    v809 = v808 ^ v807;
    v808 = __ROR4__(v808, 9);
    v810 = (v790 & v780 ^ v800 & (v780 ^ v790)) + (v808 ^ v809) + v806;
    v811 = __ROR4__(v805, 6);
    v812 = v811;
    v811 = __ROR4__(v811, 5);
    v813 = v811 ^ v812;
    v811 = __ROR4__(v811, 14);
    v814 = DWORD3(v1023) + (v811 ^ v813) + v775 + (v795 & v805 ^ v785 & ~v805);
    v815 = v814 + v780;
    v816 = v814;
    v817 = __ROR4__(v810, 2);
    v818 = __ROR4__(v810, 13);
    v819 = v818 ^ v817;
    v818 = __ROR4__(v818, 9);
    v820 = (v800 & v790 ^ v810 & (v790 ^ v800)) + (v818 ^ v819) + v816;
    v18 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(v15 + 32)), v1025);
    _mm_store_si128((__m128i *)&v1023, _mm_add_epi32(_mm_loadu_si128((const __m128i *)(v700 + 32)), v18));
    v821 = __ROR4__(v815, 6);
    v822 = v821;
    v821 = __ROR4__(v821, 5);
    v823 = v821 ^ v822;
    v821 = __ROR4__(v821, 14);
    v824 = v1024 + (v821 ^ v823) + v785 + (v805 & v815 ^ v795 & ~v815);
    v825 = v824 + v790;
    v826 = v824;
    v827 = __ROR4__(v820, 2);
    v828 = __ROR4__(v820, 13);
    v829 = v828 ^ v827;
    v828 = __ROR4__(v828, 9);
    v830 = (v810 & v800 ^ v820 & (v800 ^ v810)) + (v828 ^ v829) + v826;
    v831 = __ROR4__(v825, 6);
    v832 = v831;
    v831 = __ROR4__(v831, 5);
    v833 = v831 ^ v832;
    v831 = __ROR4__(v831, 14);
    v834 = DWORD1(v1024) + (v831 ^ v833) + v795 + (v815 & v825 ^ v805 & ~v825);
    v835 = v834 + v800;
    v836 = v834;
    v837 = __ROR4__(v830, 2);
    v838 = __ROR4__(v830, 13);
    v839 = v838 ^ v837;
    v838 = __ROR4__(v838, 9);
    v840 = (v820 & v810 ^ v830 & (v810 ^ v820)) + (v838 ^ v839) + v836;
    v841 = __ROR4__(v835, 6);
    v842 = v841;
    v841 = __ROR4__(v841, 5);
    v843 = v841 ^ v842;
    v841 = __ROR4__(v841, 14);
    v844 = DWORD2(v1024) + (v841 ^ v843) + v805 + (v825 & v835 ^ v815 & ~v835);
    v845 = v844 + v810;
    v846 = v844;
    v847 = __ROR4__(v840, 2);
    v848 = __ROR4__(v840, 13);
    v849 = v848 ^ v847;
    v848 = __ROR4__(v848, 9);
    v850 = (v830 & v820 ^ v840 & (v820 ^ v830)) + (v848 ^ v849) + v846;
    v851 = __ROR4__(v845, 6);
    v852 = v851;
    v851 = __ROR4__(v851, 5);
    v853 = v851 ^ v852;
    v851 = __ROR4__(v851, 14);
    v854 = DWORD3(v1024) + (v851 ^ v853) + v815 + (v835 & v845 ^ v825 & ~v845);
    v855 = v854 + v820;
    v856 = v854;
    v857 = __ROR4__(v850, 2);
    v858 = __ROR4__(v850, 13);
    v859 = v858 ^ v857;
    v858 = __ROR4__(v858, 9);
    v19 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(v15 + 48)), v1025);
    _mm_store_si128((__m128i *)&v1024, _mm_add_epi32(_mm_loadu_si128((const __m128i *)(v700 + 48)), v19));
    v20 = (const __m128i *)(v700 + 64);
    v15 += 64LL;
    *(_DWORD *)a2 += (v840 & v830 ^ v850 & (v830 ^ v840)) + (v858 ^ v859) + v856;
    *(_DWORD *)(a2 + 4) += v850;
    *(_DWORD *)(a2 + 8) += v840;
    *(_DWORD *)(a2 + 12) += v830;
    *(_DWORD *)(a2 + 16) += v855;
    *(_DWORD *)(a2 + 20) += v845;
    *(_DWORD *)(a2 + 24) += v835;
    *(_DWORD *)(a2 + 28) += v825;
  }
  v860 = __ROR4__(v680, 6);
  v861 = v860;
  v860 = __ROR4__(v860, 5);
  v862 = v860 ^ v861;
  v860 = __ROR4__(v860, 14);
  v863 = v1021 + (v860 ^ v862) + v650 + (v670 & v680 ^ v660 & ~v680);
  v864 = v863 + v655;
  v865 = v863;
  v866 = __ROR4__(v685, 2);
  v867 = __ROR4__(v685, 13);
  v868 = v867 ^ v866;
  v867 = __ROR4__(v867, 9);
  v869 = (v675 & v665 ^ v685 & (v665 ^ v675)) + (v867 ^ v868) + v865;
  v870 = __ROR4__(v864, 6);
  v871 = v870;
  v870 = __ROR4__(v870, 5);
  v872 = v870 ^ v871;
  v870 = __ROR4__(v870, 14);
  v873 = DWORD1(v1021) + (v870 ^ v872) + v660 + (v680 & v864 ^ v670 & ~v864);
  v874 = v873 + v665;
  v875 = v873;
  v876 = __ROR4__(v869, 2);
  v877 = __ROR4__(v869, 13);
  v878 = v877 ^ v876;
  v877 = __ROR4__(v877, 9);
  v879 = (v685 & v675 ^ v869 & (v675 ^ v685)) + (v877 ^ v878) + v875;
  v880 = __ROR4__(v874, 6);
  v881 = v880;
  v880 = __ROR4__(v880, 5);
  v882 = v880 ^ v881;
  v880 = __ROR4__(v880, 14);
  v883 = DWORD2(v1021) + (v880 ^ v882) + v670 + (v864 & v874 ^ v680 & ~v874);
  v884 = v883 + v675;
  v885 = v883;
  v886 = __ROR4__(v879, 2);
  v887 = __ROR4__(v879, 13);
  v888 = v887 ^ v886;
  v887 = __ROR4__(v887, 9);
  v889 = (v869 & v685 ^ v879 & (v685 ^ v869)) + (v887 ^ v888) + v885;
  v890 = __ROR4__(v884, 6);
  v891 = v890;
  v890 = __ROR4__(v890, 5);
  v892 = v890 ^ v891;
  v890 = __ROR4__(v890, 14);
  v893 = DWORD3(v1021) + (v890 ^ v892) + v680 + (v874 & v884 ^ v864 & ~v884);
  v894 = v893 + v685;
  v895 = v893;
  v896 = __ROR4__(v889, 2);
  v897 = __ROR4__(v889, 13);
  v898 = v897 ^ v896;
  v897 = __ROR4__(v897, 9);
  v899 = (v879 & v869 ^ v889 & (v869 ^ v879)) + (v897 ^ v898) + v895;
  v900 = __ROR4__(v894, 6);
  v901 = v900;
  v900 = __ROR4__(v900, 5);
  v902 = v900 ^ v901;
  v900 = __ROR4__(v900, 14);
  v903 = v1022 + (v900 ^ v902) + v864 + (v884 & v894 ^ v874 & ~v894);
  v904 = v903 + v869;
  v905 = v903;
  v906 = __ROR4__(v899, 2);
  v907 = __ROR4__(v899, 13);
  v908 = v907 ^ v906;
  v907 = __ROR4__(v907, 9);
  v909 = (v889 & v879 ^ v899 & (v879 ^ v889)) + (v907 ^ v908) + v905;
  v910 = __ROR4__(v904, 6);
  v911 = v910;
  v910 = __ROR4__(v910, 5);
  v912 = v910 ^ v911;
  v910 = __ROR4__(v910, 14);
  v913 = DWORD1(v1022) + (v910 ^ v912) + v874 + (v894 & v904 ^ v884 & ~v904);
  v914 = v913 + v879;
  v915 = v913;
  v916 = __ROR4__(v909, 2);
  v917 = __ROR4__(v909, 13);
  v918 = v917 ^ v916;
  v917 = __ROR4__(v917, 9);
  v919 = (v899 & v889 ^ v909 & (v889 ^ v899)) + (v917 ^ v918) + v915;
  v920 = __ROR4__(v914, 6);
  v921 = v920;
  v920 = __ROR4__(v920, 5);
  v922 = v920 ^ v921;
  v920 = __ROR4__(v920, 14);
  v923 = DWORD2(v1022) + (v920 ^ v922) + v884 + (v904 & v914 ^ v894 & ~v914);
  v924 = v923 + v889;
  v925 = v923;
  v926 = __ROR4__(v919, 2);
  v927 = __ROR4__(v919, 13);
  v928 = v927 ^ v926;
  v927 = __ROR4__(v927, 9);
  v929 = (v909 & v899 ^ v919 & (v899 ^ v909)) + (v927 ^ v928) + v925;
  v930 = __ROR4__(v924, 6);
  v931 = v930;
  v930 = __ROR4__(v930, 5);
  v932 = v930 ^ v931;
  v930 = __ROR4__(v930, 14);
  v933 = DWORD3(v1022) + (v930 ^ v932) + v894 + (v914 & v924 ^ v904 & ~v924);
  v934 = v933 + v899;
  v935 = v933;
  v936 = __ROR4__(v929, 2);
  v937 = __ROR4__(v929, 13);
  v938 = v937 ^ v936;
  v937 = __ROR4__(v937, 9);
  v939 = (v919 & v909 ^ v929 & (v909 ^ v919)) + (v937 ^ v938) + v935;
  v940 = __ROR4__(v934, 6);
  v941 = v940;
  v940 = __ROR4__(v940, 5);
  v942 = v940 ^ v941;
  v940 = __ROR4__(v940, 14);
  v943 = v1023 + (v940 ^ v942) + v904 + (v924 & v934 ^ v914 & ~v934);
  v944 = v943 + v909;
  v945 = v943;
  v946 = __ROR4__(v939, 2);
  v947 = __ROR4__(v939, 13);
  v948 = v947 ^ v946;
  v947 = __ROR4__(v947, 9);
  v949 = (v929 & v919 ^ v939 & (v919 ^ v929)) + (v947 ^ v948) + v945;
  v950 = __ROR4__(v944, 6);
  v951 = v950;
  v950 = __ROR4__(v950, 5);
  v952 = v950 ^ v951;
  v950 = __ROR4__(v950, 14);
  v953 = DWORD1(v1023) + (v950 ^ v952) + v914 + (v934 & v944 ^ v924 & ~v944);
  v954 = v953 + v919;
  v955 = v953;
  v956 = __ROR4__(v949, 2);
  v957 = __ROR4__(v949, 13);
  v958 = v957 ^ v956;
  v957 = __ROR4__(v957, 9);
  v959 = (v939 & v929 ^ v949 & (v929 ^ v939)) + (v957 ^ v958) + v955;
  v960 = __ROR4__(v954, 6);
  v961 = v960;
  v960 = __ROR4__(v960, 5);
  v962 = v960 ^ v961;
  v960 = __ROR4__(v960, 14);
  v963 = DWORD2(v1023) + (v960 ^ v962) + v924 + (v944 & v954 ^ v934 & ~v954);
  v964 = v963 + v929;
  v965 = v963;
  v966 = __ROR4__(v959, 2);
  v967 = __ROR4__(v959, 13);
  v968 = v967 ^ v966;
  v967 = __ROR4__(v967, 9);
  v969 = (v949 & v939 ^ v959 & (v939 ^ v949)) + (v967 ^ v968) + v965;
  v970 = __ROR4__(v964, 6);
  v971 = v970;
  v970 = __ROR4__(v970, 5);
  v972 = v970 ^ v971;
  v970 = __ROR4__(v970, 14);
  v973 = DWORD3(v1023) + (v970 ^ v972) + v934 + (v954 & v964 ^ v944 & ~v964);
  v974 = v973 + v939;
  v975 = v973;
  v976 = __ROR4__(v969, 2);
  v977 = __ROR4__(v969, 13);
  v978 = v977 ^ v976;
  v977 = __ROR4__(v977, 9);
  v979 = (v959 & v949 ^ v969 & (v949 ^ v959)) + (v977 ^ v978) + v975;
  v980 = __ROR4__(v974, 6);
  v981 = v980;
  v980 = __ROR4__(v980, 5);
  v982 = v980 ^ v981;
  v980 = __ROR4__(v980, 14);
  v983 = v1024 + (v980 ^ v982) + v944 + (v964 & v974 ^ v954 & ~v974);
  v984 = v983 + v949;
  v985 = v983;
  v986 = __ROR4__(v979, 2);
  v987 = __ROR4__(v979, 13);
  v988 = v987 ^ v986;
  v987 = __ROR4__(v987, 9);
  v989 = (v969 & v959 ^ v979 & (v959 ^ v969)) + (v987 ^ v988) + v985;
  v990 = __ROR4__(v984, 6);
  v991 = v990;
  v990 = __ROR4__(v990, 5);
  v992 = v990 ^ v991;
  v990 = __ROR4__(v990, 14);
  v993 = DWORD1(v1024) + (v990 ^ v992) + v954 + (v974 & v984 ^ v964 & ~v984);
  v994 = v993 + v959;
  v995 = v993;
  v996 = __ROR4__(v989, 2);
  v997 = __ROR4__(v989, 13);
  v998 = v997 ^ v996;
  v997 = __ROR4__(v997, 9);
  v999 = (v979 & v969 ^ v989 & (v969 ^ v979)) + (v997 ^ v998) + v995;
  v1000 = __ROR4__(v994, 6);
  v1001 = v1000;
  v1000 = __ROR4__(v1000, 5);
  v1002 = v1000 ^ v1001;
  v1000 = __ROR4__(v1000, 14);
  v1003 = DWORD2(v1024) + (v1000 ^ v1002) + v964 + (v984 & v994 ^ v974 & ~v994);
  v1004 = v1003 + v969;
  v1005 = v1003;
  v1006 = __ROR4__(v999, 2);
  v1007 = __ROR4__(v999, 13);
  v1008 = v1007 ^ v1006;
  v1007 = __ROR4__(v1007, 9);
  v1009 = (v989 & v979 ^ v999 & (v979 ^ v989)) + (v1007 ^ v1008) + v1005;
  v1010 = __ROR4__(v1004, 6);
  v1011 = v1010;
  v1010 = __ROR4__(v1010, 5);
  v1012 = v1010 ^ v1011;
  v1010 = __ROR4__(v1010, 14);
  v1013 = DWORD3(v1024) + (v1010 ^ v1012) + v974 + (v994 & v1004 ^ v984 & ~v1004);
  v1014 = v1013 + v979;
  v1015 = v1013;
  v1016 = __ROR4__(v1009, 2);
  v1017 = __ROR4__(v1009, 13);
  v1018 = v1017 ^ v1016;
  v1017 = __ROR4__(v1017, 9);
  v1019 = (v1017 ^ v1018) + v1015;
  result = v999 & v989 ^ v1009 & (v989 ^ (unsigned int)v999);
  *(_DWORD *)a2 += result + v1019;
  *(_DWORD *)(a2 + 4) += v1009;
  *(_DWORD *)(a2 + 8) += v999;
  *(_DWORD *)(a2 + 12) += v989;
  *(_DWORD *)(a2 + 16) += v1014;
  *(_DWORD *)(a2 + 20) += v1004;
  *(_DWORD *)(a2 + 24) += v994;
  *(_DWORD *)(a2 + 28) += v984;
  return result;
}
// 66F00: using guessed type __int64 ccsha256_K[32];
// 67020: using guessed type __int64 qword_67020[2];

//----- (000000000003FC48) ----------------------------------------------------
__int64 __usercall ccsha256_vng_intel_nossse3_compress@<rax>(__int64 a1@<rdx>, __int64 a2@<rdi>, __int64 a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm4>, __m128i a9@<xmm5>, __m128i a10@<xmm6>, __m128i a11@<xmm7>)
{
  unsigned __int32 v11; // er12@1
  unsigned __int32 v12; // er11@1
  unsigned __int32 v13; // er12@1
  unsigned __int32 v14; // er11@1
  unsigned __int32 v15; // er12@1
  unsigned __int32 v16; // er11@1
  unsigned __int32 v17; // er12@1
  unsigned __int32 v18; // er11@1
  signed __int64 v19; // rdx@1
  __m128i v20; // xmm0@1
  __m128i v21; // xmm1@1
  __m128i v22; // xmm2@1
  __m128i v23; // xmm3@1
  const __m128i *v24; // rbx@1
  __m128i v25; // xmm5@1
  __m128i v26; // xmm6@1
  __m128i v27; // xmm7@1
  int v28; // er9@2
  int v29; // er10@2
  int v30; // er12@2
  int v31; // er13@2
  int v32; // er14@2
  int v33; // ecx@2
  int v34; // eax@2
  int v35; // eax@2
  int v36; // eax@2
  int v37; // er11@2
  int v38; // er15@2
  int v39; // eax@2
  int v40; // ecx@2
  int v41; // eax@2
  int v42; // er15@2
  int v43; // ecx@2
  int v44; // eax@2
  int v45; // eax@2
  int v46; // eax@2
  int v47; // er10@2
  int v48; // er14@2
  int v49; // eax@2
  int v50; // ecx@2
  int v51; // eax@2
  int v52; // er14@2
  int v53; // ecx@2
  int v54; // eax@2
  int v55; // eax@2
  int v56; // eax@2
  int v57; // er9@2
  int v58; // er13@2
  int v59; // eax@2
  int v60; // ecx@2
  int v61; // eax@2
  int v62; // er13@2
  int v63; // ecx@2
  int v64; // eax@2
  int v65; // eax@2
  int v66; // eax@2
  int v67; // er8@2
  int v68; // er12@2
  int v69; // eax@2
  int v70; // ecx@2
  int v71; // eax@2
  int v72; // er12@2
  __m128i v73; // xmm5@2
  const __m128i *v74; // rbx@2
  __m128i v75; // xmm7@2
  __m128i v76; // xmm4@2
  __m128i v77; // xmm6@2
  __m128i v78; // xmm7@2
  __m128i v79; // xmm0@2
  __m128i v80; // xmm7@2
  __m128i v81; // xmm6@2
  __m128i v82; // xmm4@2
  __m128i v83; // xmm7@2
  __m128i v84; // xmm0@2
  __m128i v85; // xmm7@2
  __m128i v86; // xmm6@2
  __m128i v87; // xmm4@2
  __m128i v88; // xmm7@2
  __m128i v89; // xmm0@2
  int v90; // ecx@2
  int v91; // eax@2
  int v92; // eax@2
  int v93; // eax@2
  int v94; // er15@2
  int v95; // er11@2
  int v96; // eax@2
  int v97; // ecx@2
  int v98; // eax@2
  int v99; // er11@2
  int v100; // ecx@2
  int v101; // eax@2
  int v102; // eax@2
  int v103; // eax@2
  int v104; // er14@2
  int v105; // er10@2
  int v106; // eax@2
  int v107; // ecx@2
  int v108; // eax@2
  int v109; // er10@2
  int v110; // ecx@2
  int v111; // eax@2
  int v112; // eax@2
  int v113; // eax@2
  int v114; // er13@2
  int v115; // er9@2
  int v116; // eax@2
  int v117; // ecx@2
  int v118; // eax@2
  int v119; // er9@2
  int v120; // ecx@2
  int v121; // eax@2
  int v122; // eax@2
  int v123; // eax@2
  int v124; // er12@2
  int v125; // er8@2
  int v126; // eax@2
  int v127; // ecx@2
  int v128; // eax@2
  int v129; // er8@2
  __m128i v130; // xmm5@2
  __m128i v131; // xmm7@2
  __m128i v132; // xmm4@2
  __m128i v133; // xmm6@2
  __m128i v134; // xmm7@2
  __m128i v135; // xmm1@2
  __m128i v136; // xmm7@2
  __m128i v137; // xmm6@2
  __m128i v138; // xmm4@2
  __m128i v139; // xmm7@2
  __m128i v140; // xmm1@2
  __m128i v141; // xmm7@2
  __m128i v142; // xmm6@2
  __m128i v143; // xmm4@2
  __m128i v144; // xmm7@2
  __m128i v145; // xmm1@2
  int v146; // ecx@2
  int v147; // eax@2
  int v148; // eax@2
  int v149; // eax@2
  int v150; // er11@2
  int v151; // er15@2
  int v152; // eax@2
  int v153; // ecx@2
  int v154; // eax@2
  int v155; // er15@2
  int v156; // ecx@2
  int v157; // eax@2
  int v158; // eax@2
  int v159; // eax@2
  int v160; // er10@2
  int v161; // er14@2
  int v162; // eax@2
  int v163; // ecx@2
  int v164; // eax@2
  int v165; // er14@2
  int v166; // ecx@2
  int v167; // eax@2
  int v168; // eax@2
  int v169; // eax@2
  int v170; // er9@2
  int v171; // er13@2
  int v172; // eax@2
  int v173; // ecx@2
  int v174; // eax@2
  int v175; // er13@2
  int v176; // ecx@2
  int v177; // eax@2
  int v178; // eax@2
  int v179; // eax@2
  int v180; // er8@2
  int v181; // er12@2
  int v182; // eax@2
  int v183; // ecx@2
  int v184; // eax@2
  int v185; // er12@2
  __m128i v186; // xmm5@2
  __m128i v187; // xmm7@2
  __m128i v188; // xmm4@2
  __m128i v189; // xmm6@2
  __m128i v190; // xmm7@2
  __m128i v191; // xmm2@2
  __m128i v192; // xmm7@2
  __m128i v193; // xmm6@2
  __m128i v194; // xmm4@2
  __m128i v195; // xmm7@2
  __m128i v196; // xmm2@2
  __m128i v197; // xmm7@2
  __m128i v198; // xmm6@2
  __m128i v199; // xmm4@2
  __m128i v200; // xmm7@2
  __m128i v201; // xmm2@2
  int v202; // ecx@2
  int v203; // eax@2
  int v204; // eax@2
  __int32 v205; // eax@2
  int v206; // er15@2
  __int32 v207; // er11@2
  int v208; // eax@2
  int v209; // ecx@2
  int v210; // eax@2
  int v211; // er11@2
  int v212; // ecx@2
  int v213; // eax@2
  int v214; // eax@2
  int v215; // eax@2
  int v216; // er14@2
  int v217; // er10@2
  int v218; // eax@2
  int v219; // ecx@2
  int v220; // eax@2
  int v221; // er10@2
  int v222; // ecx@2
  int v223; // eax@2
  int v224; // eax@2
  int v225; // eax@2
  int v226; // er13@2
  int v227; // er9@2
  int v228; // eax@2
  int v229; // ecx@2
  int v230; // eax@2
  int v231; // er9@2
  int v232; // ecx@2
  int v233; // eax@2
  int v234; // eax@2
  int v235; // eax@2
  int v236; // er12@2
  int v237; // er8@2
  int v238; // eax@2
  int v239; // ecx@2
  int v240; // eax@2
  int v241; // er8@2
  __m128i v242; // xmm5@2
  __m128i v243; // xmm7@2
  __m128i v244; // xmm4@2
  __m128i v245; // xmm6@2
  __m128i v246; // xmm7@2
  __m128i v247; // xmm3@2
  __m128i v248; // xmm7@2
  __m128i v249; // xmm6@2
  __m128i v250; // xmm4@2
  __m128i v251; // xmm7@2
  __m128i v252; // xmm3@2
  __m128i v253; // xmm7@2
  __m128i v254; // xmm6@2
  __m128i v255; // xmm4@2
  __m128i v256; // xmm7@2
  __m128i v257; // xmm3@2
  int v258; // ecx@2
  int v259; // eax@2
  int v260; // eax@2
  int v261; // eax@2
  int v262; // er11@2
  int v263; // er15@2
  int v264; // eax@2
  int v265; // ecx@2
  int v266; // eax@2
  int v267; // er15@2
  int v268; // ecx@2
  int v269; // eax@2
  int v270; // eax@2
  int v271; // eax@2
  int v272; // er10@2
  int v273; // er14@2
  int v274; // eax@2
  int v275; // ecx@2
  int v276; // eax@2
  int v277; // er14@2
  int v278; // ecx@2
  int v279; // eax@2
  int v280; // eax@2
  int v281; // eax@2
  int v282; // er9@2
  int v283; // er13@2
  int v284; // eax@2
  int v285; // ecx@2
  int v286; // eax@2
  int v287; // er13@2
  int v288; // ecx@2
  int v289; // eax@2
  int v290; // eax@2
  int v291; // eax@2
  int v292; // er8@2
  int v293; // er12@2
  int v294; // eax@2
  int v295; // ecx@2
  int v296; // eax@2
  int v297; // er12@2
  __m128i v298; // xmm5@2
  __m128i v299; // xmm7@2
  __m128i v300; // xmm4@2
  __m128i v301; // xmm6@2
  __m128i v302; // xmm7@2
  __m128i v303; // xmm0@2
  __m128i v304; // xmm7@2
  __m128i v305; // xmm6@2
  __m128i v306; // xmm4@2
  __m128i v307; // xmm7@2
  __m128i v308; // xmm0@2
  __m128i v309; // xmm7@2
  __m128i v310; // xmm6@2
  __m128i v311; // xmm4@2
  __m128i v312; // xmm7@2
  __m128i v313; // xmm0@2
  int v314; // ecx@2
  int v315; // eax@2
  int v316; // eax@2
  int v317; // eax@2
  int v318; // er15@2
  int v319; // er11@2
  int v320; // eax@2
  int v321; // ecx@2
  int v322; // eax@2
  int v323; // er11@2
  int v324; // ecx@2
  int v325; // eax@2
  int v326; // eax@2
  int v327; // eax@2
  int v328; // er14@2
  int v329; // er10@2
  int v330; // eax@2
  int v331; // ecx@2
  int v332; // eax@2
  int v333; // er10@2
  int v334; // ecx@2
  int v335; // eax@2
  int v336; // eax@2
  int v337; // eax@2
  int v338; // er13@2
  int v339; // er9@2
  int v340; // eax@2
  int v341; // ecx@2
  int v342; // eax@2
  int v343; // er9@2
  int v344; // ecx@2
  int v345; // eax@2
  int v346; // eax@2
  int v347; // eax@2
  int v348; // er12@2
  int v349; // er8@2
  int v350; // eax@2
  int v351; // ecx@2
  int v352; // eax@2
  int v353; // er8@2
  __m128i v354; // xmm5@2
  __m128i v355; // xmm7@2
  __m128i v356; // xmm4@2
  __m128i v357; // xmm6@2
  __m128i v358; // xmm7@2
  __m128i v359; // xmm1@2
  __m128i v360; // xmm7@2
  __m128i v361; // xmm6@2
  __m128i v362; // xmm4@2
  __m128i v363; // xmm7@2
  __m128i v364; // xmm1@2
  __m128i v365; // xmm7@2
  __m128i v366; // xmm6@2
  __m128i v367; // xmm4@2
  __m128i v368; // xmm7@2
  __m128i v369; // xmm1@2
  int v370; // ecx@2
  int v371; // eax@2
  int v372; // eax@2
  int v373; // eax@2
  int v374; // er11@2
  int v375; // er15@2
  int v376; // eax@2
  int v377; // ecx@2
  int v378; // eax@2
  int v379; // er15@2
  int v380; // ecx@2
  int v381; // eax@2
  int v382; // eax@2
  int v383; // eax@2
  int v384; // er10@2
  int v385; // er14@2
  int v386; // eax@2
  int v387; // ecx@2
  int v388; // eax@2
  int v389; // er14@2
  int v390; // ecx@2
  int v391; // eax@2
  int v392; // eax@2
  int v393; // eax@2
  int v394; // er9@2
  int v395; // er13@2
  int v396; // eax@2
  int v397; // ecx@2
  int v398; // eax@2
  int v399; // er13@2
  int v400; // ecx@2
  int v401; // eax@2
  int v402; // eax@2
  int v403; // eax@2
  int v404; // er8@2
  int v405; // er12@2
  int v406; // eax@2
  int v407; // ecx@2
  int v408; // eax@2
  int v409; // er12@2
  __m128i v410; // xmm5@2
  __m128i v411; // xmm7@2
  __m128i v412; // xmm4@2
  __m128i v413; // xmm6@2
  __m128i v414; // xmm7@2
  __m128i v415; // xmm2@2
  __m128i v416; // xmm7@2
  __m128i v417; // xmm6@2
  __m128i v418; // xmm4@2
  __m128i v419; // xmm7@2
  __m128i v420; // xmm2@2
  __m128i v421; // xmm7@2
  __m128i v422; // xmm6@2
  __m128i v423; // xmm4@2
  __m128i v424; // xmm7@2
  __m128i v425; // xmm2@2
  int v426; // ecx@2
  int v427; // eax@2
  int v428; // eax@2
  __int32 v429; // eax@2
  int v430; // er15@2
  __int32 v431; // er11@2
  int v432; // eax@2
  int v433; // ecx@2
  int v434; // eax@2
  int v435; // er11@2
  int v436; // ecx@2
  int v437; // eax@2
  int v438; // eax@2
  int v439; // eax@2
  int v440; // er14@2
  int v441; // er10@2
  int v442; // eax@2
  int v443; // ecx@2
  int v444; // eax@2
  int v445; // er10@2
  int v446; // ecx@2
  int v447; // eax@2
  int v448; // eax@2
  int v449; // eax@2
  int v450; // er13@2
  int v451; // er9@2
  int v452; // eax@2
  int v453; // ecx@2
  int v454; // eax@2
  int v455; // er9@2
  int v456; // ecx@2
  int v457; // eax@2
  int v458; // eax@2
  int v459; // eax@2
  int v460; // er12@2
  int v461; // er8@2
  int v462; // eax@2
  int v463; // ecx@2
  int v464; // eax@2
  int v465; // er8@2
  __m128i v466; // xmm5@2
  __m128i v467; // xmm7@2
  __m128i v468; // xmm4@2
  __m128i v469; // xmm6@2
  __m128i v470; // xmm7@2
  __m128i v471; // xmm3@2
  __m128i v472; // xmm7@2
  __m128i v473; // xmm6@2
  __m128i v474; // xmm4@2
  __m128i v475; // xmm7@2
  __m128i v476; // xmm3@2
  __m128i v477; // xmm7@2
  __m128i v478; // xmm6@2
  __m128i v479; // xmm4@2
  __m128i v480; // xmm7@2
  __m128i v481; // xmm3@2
  int v482; // ecx@2
  int v483; // eax@2
  int v484; // eax@2
  int v485; // eax@2
  int v486; // er11@2
  int v487; // er15@2
  int v488; // eax@2
  int v489; // ecx@2
  int v490; // eax@2
  int v491; // er15@2
  int v492; // ecx@2
  int v493; // eax@2
  int v494; // eax@2
  int v495; // eax@2
  int v496; // er10@2
  int v497; // er14@2
  int v498; // eax@2
  int v499; // ecx@2
  int v500; // eax@2
  int v501; // er14@2
  int v502; // ecx@2
  int v503; // eax@2
  int v504; // eax@2
  int v505; // eax@2
  int v506; // er9@2
  int v507; // er13@2
  int v508; // eax@2
  int v509; // ecx@2
  int v510; // eax@2
  int v511; // er13@2
  int v512; // ecx@2
  int v513; // eax@2
  int v514; // eax@2
  int v515; // eax@2
  int v516; // er8@2
  int v517; // er12@2
  int v518; // eax@2
  int v519; // ecx@2
  int v520; // eax@2
  int v521; // er12@2
  __m128i v522; // xmm5@2
  __m128i v523; // xmm7@2
  __m128i v524; // xmm4@2
  __m128i v525; // xmm6@2
  __m128i v526; // xmm7@2
  __m128i v527; // xmm0@2
  __m128i v528; // xmm7@2
  __m128i v529; // xmm6@2
  __m128i v530; // xmm4@2
  __m128i v531; // xmm7@2
  __m128i v532; // xmm0@2
  __m128i v533; // xmm7@2
  __m128i v534; // xmm6@2
  __m128i v535; // xmm4@2
  __m128i v536; // xmm7@2
  __m128i v537; // xmm0@2
  int v538; // ecx@2
  int v539; // eax@2
  int v540; // eax@2
  int v541; // eax@2
  int v542; // er15@2
  int v543; // er11@2
  int v544; // eax@2
  int v545; // ecx@2
  int v546; // eax@2
  int v547; // er11@2
  int v548; // ecx@2
  int v549; // eax@2
  int v550; // eax@2
  int v551; // eax@2
  int v552; // er14@2
  int v553; // er10@2
  int v554; // eax@2
  int v555; // ecx@2
  int v556; // eax@2
  int v557; // er10@2
  int v558; // ecx@2
  int v559; // eax@2
  int v560; // eax@2
  int v561; // eax@2
  int v562; // er13@2
  int v563; // er9@2
  int v564; // eax@2
  int v565; // ecx@2
  int v566; // eax@2
  int v567; // er9@2
  int v568; // ecx@2
  int v569; // eax@2
  int v570; // eax@2
  int v571; // eax@2
  int v572; // er12@2
  int v573; // er8@2
  int v574; // eax@2
  int v575; // ecx@2
  int v576; // eax@2
  int v577; // er8@2
  __m128i v578; // xmm5@2
  __m128i v579; // xmm7@2
  __m128i v580; // xmm4@2
  __m128i v581; // xmm6@2
  __m128i v582; // xmm7@2
  __m128i v583; // xmm1@2
  __m128i v584; // xmm7@2
  __m128i v585; // xmm6@2
  __m128i v586; // xmm4@2
  __m128i v587; // xmm7@2
  __m128i v588; // xmm1@2
  __m128i v589; // xmm7@2
  __m128i v590; // xmm6@2
  __m128i v591; // xmm4@2
  __m128i v592; // xmm7@2
  __m128i v593; // xmm1@2
  int v594; // ecx@2
  int v595; // eax@2
  int v596; // eax@2
  int v597; // eax@2
  int v598; // er11@2
  int v599; // er15@2
  int v600; // eax@2
  int v601; // ecx@2
  int v602; // eax@2
  int v603; // er15@2
  int v604; // ecx@2
  int v605; // eax@2
  int v606; // eax@2
  int v607; // eax@2
  int v608; // er10@2
  int v609; // er14@2
  int v610; // eax@2
  int v611; // ecx@2
  int v612; // eax@2
  int v613; // er14@2
  int v614; // ecx@2
  int v615; // eax@2
  int v616; // eax@2
  int v617; // eax@2
  int v618; // er9@2
  int v619; // er13@2
  int v620; // eax@2
  int v621; // ecx@2
  int v622; // eax@2
  int v623; // er13@2
  int v624; // ecx@2
  int v625; // eax@2
  int v626; // eax@2
  int v627; // eax@2
  int v628; // er8@2
  int v629; // er12@2
  int v630; // eax@2
  int v631; // ecx@2
  int v632; // eax@2
  int v633; // er12@2
  __m128i v634; // xmm5@2
  __m128i v635; // xmm7@2
  __m128i v636; // xmm4@2
  __m128i v637; // xmm6@2
  __m128i v638; // xmm7@2
  __m128i v639; // xmm2@2
  __m128i v640; // xmm7@2
  __m128i v641; // xmm6@2
  __m128i v642; // xmm4@2
  __m128i v643; // xmm7@2
  __m128i v644; // xmm2@2
  __m128i v645; // xmm7@2
  __m128i v646; // xmm6@2
  __m128i v647; // xmm4@2
  __m128i v648; // xmm7@2
  __m128i v649; // xmm2@2
  int v650; // ecx@2
  int v651; // eax@2
  int v652; // eax@2
  __int32 v653; // eax@2
  int v654; // er15@2
  __int32 v655; // er11@2
  int v656; // eax@2
  int v657; // ecx@2
  int v658; // eax@2
  int v659; // er11@2
  int v660; // ecx@2
  int v661; // eax@2
  int v662; // eax@2
  int v663; // eax@2
  int v664; // er14@2
  int v665; // er10@2
  int v666; // eax@2
  int v667; // ecx@2
  int v668; // eax@2
  int v669; // er10@2
  int v670; // ecx@2
  int v671; // eax@2
  int v672; // eax@2
  int v673; // eax@2
  int v674; // er13@2
  int v675; // er9@2
  int v676; // eax@2
  int v677; // ecx@2
  int v678; // eax@2
  int v679; // er9@2
  int v680; // ecx@2
  int v681; // eax@2
  int v682; // eax@2
  int v683; // eax@2
  int v684; // er12@2
  int v685; // er8@2
  int v686; // eax@2
  int v687; // ecx@2
  int v688; // eax@2
  int v689; // er8@2
  __m128i v690; // xmm7@2
  __m128i v691; // xmm4@2
  __m128i v692; // xmm6@2
  __m128i v693; // xmm7@2
  __m128i v694; // xmm3@2
  __m128i v695; // xmm7@2
  __m128i v696; // xmm6@2
  __m128i v697; // xmm4@2
  __m128i v698; // xmm7@2
  __m128i v699; // xmm3@2
  __m128i v700; // xmm7@2
  __m128i v701; // xmm6@2
  __m128i v702; // xmm4@2
  __m128i v703; // xmm7@2
  signed __int64 v704; // rbx@2
  int v705; // ecx@3
  int v706; // eax@3
  int v707; // eax@3
  int v708; // eax@3
  int v709; // er11@3
  int v710; // er15@3
  int v711; // eax@3
  int v712; // ecx@3
  int v713; // eax@3
  int v714; // er15@3
  int v715; // ecx@3
  int v716; // eax@3
  int v717; // eax@3
  int v718; // eax@3
  int v719; // er10@3
  int v720; // er14@3
  int v721; // eax@3
  int v722; // ecx@3
  int v723; // eax@3
  int v724; // er14@3
  int v725; // ecx@3
  int v726; // eax@3
  int v727; // eax@3
  int v728; // eax@3
  int v729; // er9@3
  int v730; // er13@3
  int v731; // eax@3
  int v732; // ecx@3
  int v733; // eax@3
  int v734; // er13@3
  int v735; // ecx@3
  int v736; // eax@3
  int v737; // eax@3
  int v738; // eax@3
  int v739; // er8@3
  int v740; // er12@3
  int v741; // eax@3
  int v742; // ecx@3
  int v743; // eax@3
  int v744; // er12@3
  int v745; // ecx@3
  int v746; // eax@3
  int v747; // eax@3
  int v748; // eax@3
  int v749; // er15@3
  int v750; // er11@3
  int v751; // eax@3
  int v752; // ecx@3
  int v753; // eax@3
  int v754; // er11@3
  int v755; // ecx@3
  int v756; // eax@3
  int v757; // eax@3
  int v758; // eax@3
  int v759; // er14@3
  int v760; // er10@3
  int v761; // eax@3
  int v762; // ecx@3
  int v763; // eax@3
  int v764; // er10@3
  int v765; // ecx@3
  int v766; // eax@3
  int v767; // eax@3
  int v768; // eax@3
  int v769; // er13@3
  int v770; // er9@3
  int v771; // eax@3
  int v772; // ecx@3
  int v773; // eax@3
  int v774; // er9@3
  int v775; // ecx@3
  int v776; // eax@3
  int v777; // eax@3
  int v778; // eax@3
  int v779; // er12@3
  int v780; // er8@3
  int v781; // eax@3
  int v782; // ecx@3
  int v783; // eax@3
  int v784; // er8@3
  int v785; // ecx@3
  int v786; // eax@3
  int v787; // eax@3
  int v788; // eax@3
  int v789; // er11@3
  int v790; // er15@3
  int v791; // eax@3
  int v792; // ecx@3
  int v793; // eax@3
  int v794; // er15@3
  int v795; // ecx@3
  int v796; // eax@3
  int v797; // eax@3
  int v798; // eax@3
  int v799; // er10@3
  int v800; // er14@3
  int v801; // eax@3
  int v802; // ecx@3
  int v803; // eax@3
  int v804; // er14@3
  int v805; // ecx@3
  int v806; // eax@3
  int v807; // eax@3
  int v808; // eax@3
  int v809; // er9@3
  int v810; // er13@3
  int v811; // eax@3
  int v812; // ecx@3
  int v813; // eax@3
  int v814; // er13@3
  int v815; // ecx@3
  int v816; // eax@3
  int v817; // eax@3
  int v818; // eax@3
  int v819; // er8@3
  int v820; // er12@3
  int v821; // eax@3
  int v822; // ecx@3
  int v823; // eax@3
  int v824; // er12@3
  int v825; // ecx@3
  int v826; // eax@3
  int v827; // eax@3
  __int32 v828; // eax@3
  int v829; // er15@3
  __int32 v830; // er11@3
  int v831; // eax@3
  int v832; // ecx@3
  int v833; // eax@3
  int v834; // er11@3
  int v835; // ecx@3
  int v836; // eax@3
  int v837; // eax@3
  int v838; // eax@3
  int v839; // er14@3
  int v840; // er10@3
  int v841; // eax@3
  int v842; // ecx@3
  int v843; // eax@3
  int v844; // er10@3
  int v845; // ecx@3
  int v846; // eax@3
  int v847; // eax@3
  int v848; // eax@3
  int v849; // er13@3
  int v850; // er9@3
  int v851; // eax@3
  int v852; // ecx@3
  int v853; // eax@3
  int v854; // er9@3
  int v855; // ecx@3
  int v856; // eax@3
  int v857; // eax@3
  int v858; // eax@3
  int v859; // er12@3
  int v860; // er8@3
  int v861; // eax@3
  int v862; // ecx@3
  int v863; // eax@3
  int v864; // ecx@4
  int v865; // eax@4
  int v866; // eax@4
  int v867; // eax@4
  int v868; // er11@4
  int v869; // er15@4
  int v870; // eax@4
  int v871; // ecx@4
  int v872; // eax@4
  int v873; // er15@4
  int v874; // ecx@4
  int v875; // eax@4
  int v876; // eax@4
  int v877; // eax@4
  int v878; // er10@4
  int v879; // er14@4
  int v880; // eax@4
  int v881; // ecx@4
  int v882; // eax@4
  int v883; // er14@4
  int v884; // ecx@4
  int v885; // eax@4
  int v886; // eax@4
  int v887; // eax@4
  int v888; // er9@4
  int v889; // er13@4
  int v890; // eax@4
  int v891; // ecx@4
  int v892; // eax@4
  int v893; // er13@4
  int v894; // ecx@4
  int v895; // eax@4
  int v896; // eax@4
  int v897; // eax@4
  int v898; // er8@4
  int v899; // er12@4
  int v900; // eax@4
  int v901; // ecx@4
  int v902; // eax@4
  int v903; // er12@4
  int v904; // ecx@4
  int v905; // eax@4
  int v906; // eax@4
  int v907; // eax@4
  int v908; // er15@4
  int v909; // er11@4
  int v910; // eax@4
  int v911; // ecx@4
  int v912; // eax@4
  int v913; // er11@4
  int v914; // ecx@4
  int v915; // eax@4
  int v916; // eax@4
  int v917; // eax@4
  int v918; // er14@4
  int v919; // er10@4
  int v920; // eax@4
  int v921; // ecx@4
  int v922; // eax@4
  int v923; // er10@4
  int v924; // ecx@4
  int v925; // eax@4
  int v926; // eax@4
  int v927; // eax@4
  int v928; // er13@4
  int v929; // er9@4
  int v930; // eax@4
  int v931; // ecx@4
  int v932; // eax@4
  int v933; // er9@4
  int v934; // ecx@4
  int v935; // eax@4
  int v936; // eax@4
  int v937; // eax@4
  int v938; // er12@4
  int v939; // er8@4
  int v940; // eax@4
  int v941; // ecx@4
  int v942; // eax@4
  int v943; // er8@4
  int v944; // ecx@4
  int v945; // eax@4
  int v946; // eax@4
  int v947; // eax@4
  int v948; // er11@4
  int v949; // er15@4
  int v950; // eax@4
  int v951; // ecx@4
  int v952; // eax@4
  int v953; // er15@4
  int v954; // ecx@4
  int v955; // eax@4
  int v956; // eax@4
  int v957; // eax@4
  int v958; // er10@4
  int v959; // er14@4
  int v960; // eax@4
  int v961; // ecx@4
  int v962; // eax@4
  int v963; // er14@4
  int v964; // ecx@4
  int v965; // eax@4
  int v966; // eax@4
  int v967; // eax@4
  int v968; // er9@4
  int v969; // er13@4
  int v970; // eax@4
  int v971; // ecx@4
  int v972; // eax@4
  int v973; // er13@4
  int v974; // ecx@4
  int v975; // eax@4
  int v976; // eax@4
  int v977; // eax@4
  int v978; // er8@4
  int v979; // er12@4
  int v980; // eax@4
  int v981; // ecx@4
  int v982; // eax@4
  int v983; // er12@4
  int v984; // ecx@4
  int v985; // eax@4
  int v986; // eax@4
  __int32 v987; // eax@4
  int v988; // er15@4
  __int32 v989; // er11@4
  int v990; // eax@4
  int v991; // ecx@4
  int v992; // eax@4
  int v993; // er11@4
  int v994; // ecx@4
  int v995; // eax@4
  int v996; // eax@4
  int v997; // eax@4
  int v998; // er14@4
  int v999; // er10@4
  int v1000; // eax@4
  int v1001; // ecx@4
  int v1002; // eax@4
  int v1003; // er10@4
  int v1004; // ecx@4
  int v1005; // eax@4
  int v1006; // eax@4
  int v1007; // eax@4
  int v1008; // er13@4
  int v1009; // er9@4
  int v1010; // eax@4
  int v1011; // ecx@4
  int v1012; // eax@4
  int v1013; // er9@4
  int v1014; // ecx@4
  int v1015; // eax@4
  int v1016; // eax@4
  int v1017; // eax@4
  int v1018; // er12@4
  int v1019; // er8@4
  int v1020; // eax@4
  int v1021; // ecx@4
  int v1022; // eax@4
  int v1023; // er8@4
  __int64 result; // rax@4
  __int128 v1025; // [sp+0h] [bp-108h]@1
  __int128 v1026; // [sp+10h] [bp-F8h]@1
  __int128 v1027; // [sp+20h] [bp-E8h]@1
  const __m128i v1028; // [sp+30h] [bp-D8h]@1
  __int128 v1029; // [sp+50h] [bp-B8h]@1
  __int128 v1030; // [sp+60h] [bp-A8h]@1
  __int128 v1031; // [sp+70h] [bp-98h]@1
  __int128 v1032; // [sp+80h] [bp-88h]@1
  __int128 v1033; // [sp+90h] [bp-78h]@1
  __int128 v1034; // [sp+A0h] [bp-68h]@1
  __int128 v1035; // [sp+B0h] [bp-58h]@1
  __int128 v1036; // [sp+C0h] [bp-48h]@1

  _mm_store_si128((__m128i *)&v1029, a4);
  _mm_store_si128((__m128i *)&v1030, a5);
  _mm_store_si128((__m128i *)&v1031, a6);
  _mm_store_si128((__m128i *)&v1032, a7);
  _mm_store_si128((__m128i *)&v1033, a8);
  _mm_store_si128((__m128i *)&v1034, a9);
  _mm_store_si128((__m128i *)&v1035, a10);
  _mm_store_si128((__m128i *)&v1036, a11);
  v11 = _byteswap_ulong(*(_DWORD *)(a1 + 8));
  v12 = _byteswap_ulong(*(_DWORD *)(a1 + 12));
  *(_QWORD *)&v1025 = __PAIR__(_byteswap_ulong(*(_DWORD *)(a1 + 4)), _byteswap_ulong(*(_DWORD *)a1));
  *((_QWORD *)&v1025 + 1) = __PAIR__(v12, v11);
  v13 = _byteswap_ulong(*(_DWORD *)(a1 + 24));
  v14 = _byteswap_ulong(*(_DWORD *)(a1 + 28));
  *(_QWORD *)&v1026 = __PAIR__(_byteswap_ulong(*(_DWORD *)(a1 + 20)), _byteswap_ulong(*(_DWORD *)(a1 + 16)));
  *((_QWORD *)&v1026 + 1) = __PAIR__(v14, v13);
  v15 = _byteswap_ulong(*(_DWORD *)(a1 + 40));
  v16 = _byteswap_ulong(*(_DWORD *)(a1 + 44));
  *(_QWORD *)&v1027 = __PAIR__(_byteswap_ulong(*(_DWORD *)(a1 + 36)), _byteswap_ulong(*(_DWORD *)(a1 + 32)));
  *((_QWORD *)&v1027 + 1) = __PAIR__(v16, v15);
  v17 = _byteswap_ulong(*(_DWORD *)(a1 + 56));
  v18 = _byteswap_ulong(*(_DWORD *)(a1 + 60));
  v1028.m128i_i64[0] = __PAIR__(_byteswap_ulong(*(_DWORD *)(a1 + 52)), _byteswap_ulong(*(_DWORD *)(a1 + 48)));
  v1028.m128i_i64[1] = __PAIR__(v18, v17);
  v19 = a1 + 64;
  v20 = _mm_load_si128((const __m128i *)&v1025);
  v21 = _mm_load_si128((const __m128i *)&v1026);
  v22 = _mm_load_si128((const __m128i *)&v1027);
  v23 = _mm_load_si128(&v1028);
  v24 = (const __m128i *)&ccsha256_K[8];
  v25 = _mm_add_epi32(_mm_loadu_si128((const __m128i *)&ccsha256_K[2]), v21);
  v26 = _mm_add_epi32(_mm_loadu_si128((const __m128i *)&ccsha256_K[4]), v22);
  v27 = _mm_add_epi32(_mm_loadu_si128((const __m128i *)&ccsha256_K[6]), v23);
  _mm_store_si128((__m128i *)&v1025, _mm_add_epi32(_mm_loadu_si128((const __m128i *)ccsha256_K), v20));
  _mm_store_si128((__m128i *)&v1026, v25);
  _mm_store_si128((__m128i *)&v1027, v26);
  _mm_store_si128((__m128i *)&v1028, v27);
  while ( 1 )
  {
    v28 = *(_DWORD *)(a2 + 4);
    v29 = *(_DWORD *)(a2 + 8);
    v30 = *(_DWORD *)(a2 + 16);
    v31 = *(_DWORD *)(a2 + 20);
    v32 = *(_DWORD *)(a2 + 24);
    v33 = __ROR4__(*(_DWORD *)(a2 + 16), 6);
    v34 = v33;
    v33 = __ROR4__(v33, 5);
    v35 = v33 ^ v34;
    v33 = __ROR4__(v33, 14);
    v36 = v1025 + (v33 ^ v35) + *(_DWORD *)(a2 + 28) + (v31 & *(_DWORD *)(a2 + 16) ^ v32 & ~v30);
    v37 = v36 + *(_DWORD *)(a2 + 12);
    v38 = v36;
    v39 = __ROR4__(*(_DWORD *)a2, 2);
    v40 = __ROR4__(*(_DWORD *)a2, 13);
    v41 = v40 ^ v39;
    v40 = __ROR4__(v40, 9);
    v42 = (v28 & v29 ^ *(_DWORD *)a2 & (v29 ^ *(_DWORD *)(a2 + 4))) + (v40 ^ v41) + v38;
    v43 = __ROR4__(v37, 6);
    v44 = v43;
    v43 = __ROR4__(v43, 5);
    v45 = v43 ^ v44;
    v43 = __ROR4__(v43, 14);
    v46 = DWORD1(v1025) + (v43 ^ v45) + v32 + (v30 & v37 ^ v31 & ~v37);
    v47 = v46 + v29;
    v48 = v46;
    v49 = __ROR4__(v42, 2);
    v50 = __ROR4__(v42, 13);
    v51 = v50 ^ v49;
    v50 = __ROR4__(v50, 9);
    v52 = (*(_DWORD *)a2 & *(_DWORD *)(a2 + 4) ^ v42 & (v28 ^ *(_DWORD *)a2)) + (v50 ^ v51) + v48;
    v53 = __ROR4__(v47, 6);
    v54 = v53;
    v53 = __ROR4__(v53, 5);
    v55 = v53 ^ v54;
    v53 = __ROR4__(v53, 14);
    v56 = DWORD2(v1025) + (v53 ^ v55) + v31 + (v37 & v47 ^ v30 & ~v47);
    v57 = v56 + v28;
    v58 = v56;
    v59 = __ROR4__(v52, 2);
    v60 = __ROR4__(v52, 13);
    v61 = v60 ^ v59;
    v60 = __ROR4__(v60, 9);
    v62 = (v42 & *(_DWORD *)a2 ^ v52 & (*(_DWORD *)a2 ^ v42)) + (v60 ^ v61) + v58;
    v63 = __ROR4__(v57, 6);
    v64 = v63;
    v63 = __ROR4__(v63, 5);
    v65 = v63 ^ v64;
    v63 = __ROR4__(v63, 14);
    v66 = DWORD3(v1025) + (v63 ^ v65) + v30 + (v47 & v57 ^ v37 & ~v57);
    v67 = v66 + *(_DWORD *)a2;
    v68 = v66;
    v69 = __ROR4__(v62, 2);
    v70 = __ROR4__(v62, 13);
    v71 = v70 ^ v69;
    v70 = __ROR4__(v70, 9);
    v72 = (v52 & v42 ^ v62 & (v42 ^ v52)) + (v70 ^ v71) + v68;
    v73 = _mm_loadu_si128(v24);
    v74 = v24 + 1;
    v75 = _mm_or_si128(_mm_slli_si128(v21, 12), _mm_srli_si128(v20, 4));
    v76 = _mm_srl_epi32(v75, 3u);
    v77 = _mm_srl_epi32(v75, 7u);
    v78 = _mm_sll_epi32(v75, 0xEu);
    v79 = _mm_add_epi32(
            _mm_add_epi32(
              v20,
              _mm_xor_si128(
                _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v76, v77), v78), _mm_srl_epi32(v77, 0xBu)),
                _mm_sll_epi32(v78, 0xBu))),
            _mm_or_si128(_mm_slli_si128(v23, 12), _mm_srli_si128(v22, 4)));
    v80 = _mm_srli_si128(v23, 8);
    v81 = _mm_srl_epi32(v80, 0x11u);
    v82 = _mm_xor_si128(_mm_srl_epi32(v80, 0xAu), v81);
    v83 = _mm_sll_epi32(v80, 0xDu);
    v84 = _mm_add_epi32(
            v79,
            _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v82, v83), _mm_srl_epi32(v81, 2u)), _mm_sll_epi32(v83, 2u)));
    v85 = _mm_slli_si128(v84, 8);
    v86 = _mm_srl_epi32(v85, 0x11u);
    v87 = _mm_xor_si128(_mm_srl_epi32(v85, 0xAu), v86);
    v88 = _mm_sll_epi32(v85, 0xDu);
    v89 = _mm_add_epi32(
            v84,
            _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v87, v88), _mm_srl_epi32(v86, 2u)), _mm_sll_epi32(v88, 2u)));
    _mm_store_si128((__m128i *)&v1025, _mm_add_epi32(v73, v89));
    v90 = __ROR4__(v67, 6);
    v91 = v90;
    v90 = __ROR4__(v90, 5);
    v92 = v90 ^ v91;
    v90 = __ROR4__(v90, 14);
    v93 = v1026 + (v90 ^ v92) + v37 + (v57 & v67 ^ v47 & ~v67);
    v94 = v93 + v42;
    v95 = v93;
    v96 = __ROR4__(v72, 2);
    v97 = __ROR4__(v72, 13);
    v98 = v97 ^ v96;
    v97 = __ROR4__(v97, 9);
    v99 = (v62 & v52 ^ v72 & (v52 ^ v62)) + (v97 ^ v98) + v95;
    v100 = __ROR4__(v94, 6);
    v101 = v100;
    v100 = __ROR4__(v100, 5);
    v102 = v100 ^ v101;
    v100 = __ROR4__(v100, 14);
    v103 = DWORD1(v1026) + (v100 ^ v102) + v47 + (v67 & v94 ^ v57 & ~v94);
    v104 = v103 + v52;
    v105 = v103;
    v106 = __ROR4__(v99, 2);
    v107 = __ROR4__(v99, 13);
    v108 = v107 ^ v106;
    v107 = __ROR4__(v107, 9);
    v109 = (v72 & v62 ^ v99 & (v62 ^ v72)) + (v107 ^ v108) + v105;
    v110 = __ROR4__(v104, 6);
    v111 = v110;
    v110 = __ROR4__(v110, 5);
    v112 = v110 ^ v111;
    v110 = __ROR4__(v110, 14);
    v113 = DWORD2(v1026) + (v110 ^ v112) + v57 + (v94 & v104 ^ v67 & ~v104);
    v114 = v113 + v62;
    v115 = v113;
    v116 = __ROR4__(v109, 2);
    v117 = __ROR4__(v109, 13);
    v118 = v117 ^ v116;
    v117 = __ROR4__(v117, 9);
    v119 = (v99 & v72 ^ v109 & (v72 ^ v99)) + (v117 ^ v118) + v115;
    v120 = __ROR4__(v114, 6);
    v121 = v120;
    v120 = __ROR4__(v120, 5);
    v122 = v120 ^ v121;
    v120 = __ROR4__(v120, 14);
    v123 = DWORD3(v1026) + (v120 ^ v122) + v67 + (v104 & v114 ^ v94 & ~v114);
    v124 = v123 + v72;
    v125 = v123;
    v126 = __ROR4__(v119, 2);
    v127 = __ROR4__(v119, 13);
    v128 = v127 ^ v126;
    v127 = __ROR4__(v127, 9);
    v129 = (v109 & v99 ^ v119 & (v99 ^ v109)) + (v127 ^ v128) + v125;
    v130 = _mm_loadu_si128(v74);
    ++v74;
    v131 = _mm_or_si128(_mm_slli_si128(v22, 12), _mm_srli_si128(v21, 4));
    v132 = _mm_srl_epi32(v131, 3u);
    v133 = _mm_srl_epi32(v131, 7u);
    v134 = _mm_sll_epi32(v131, 0xEu);
    v135 = _mm_add_epi32(
             _mm_add_epi32(
               v21,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v132, v133), v134), _mm_srl_epi32(v133, 0xBu)),
                 _mm_sll_epi32(v134, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v89, 12), _mm_srli_si128(v23, 4)));
    v136 = _mm_srli_si128(v89, 8);
    v137 = _mm_srl_epi32(v136, 0x11u);
    v138 = _mm_xor_si128(_mm_srl_epi32(v136, 0xAu), v137);
    v139 = _mm_sll_epi32(v136, 0xDu);
    v140 = _mm_add_epi32(
             v135,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v138, v139), _mm_srl_epi32(v137, 2u)), _mm_sll_epi32(v139, 2u)));
    v141 = _mm_slli_si128(v140, 8);
    v142 = _mm_srl_epi32(v141, 0x11u);
    v143 = _mm_xor_si128(_mm_srl_epi32(v141, 0xAu), v142);
    v144 = _mm_sll_epi32(v141, 0xDu);
    v145 = _mm_add_epi32(
             v140,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v143, v144), _mm_srl_epi32(v142, 2u)), _mm_sll_epi32(v144, 2u)));
    _mm_store_si128((__m128i *)&v1026, _mm_add_epi32(v130, v145));
    v146 = __ROR4__(v124, 6);
    v147 = v146;
    v146 = __ROR4__(v146, 5);
    v148 = v146 ^ v147;
    v146 = __ROR4__(v146, 14);
    v149 = v1027 + (v146 ^ v148) + v94 + (v114 & v124 ^ v104 & ~v124);
    v150 = v149 + v99;
    v151 = v149;
    v152 = __ROR4__(v129, 2);
    v153 = __ROR4__(v129, 13);
    v154 = v153 ^ v152;
    v153 = __ROR4__(v153, 9);
    v155 = (v119 & v109 ^ v129 & (v109 ^ v119)) + (v153 ^ v154) + v151;
    v156 = __ROR4__(v150, 6);
    v157 = v156;
    v156 = __ROR4__(v156, 5);
    v158 = v156 ^ v157;
    v156 = __ROR4__(v156, 14);
    v159 = DWORD1(v1027) + (v156 ^ v158) + v104 + (v124 & v150 ^ v114 & ~v150);
    v160 = v159 + v109;
    v161 = v159;
    v162 = __ROR4__(v155, 2);
    v163 = __ROR4__(v155, 13);
    v164 = v163 ^ v162;
    v163 = __ROR4__(v163, 9);
    v165 = (v129 & v119 ^ v155 & (v119 ^ v129)) + (v163 ^ v164) + v161;
    v166 = __ROR4__(v160, 6);
    v167 = v166;
    v166 = __ROR4__(v166, 5);
    v168 = v166 ^ v167;
    v166 = __ROR4__(v166, 14);
    v169 = DWORD2(v1027) + (v166 ^ v168) + v114 + (v150 & v160 ^ v124 & ~v160);
    v170 = v169 + v119;
    v171 = v169;
    v172 = __ROR4__(v165, 2);
    v173 = __ROR4__(v165, 13);
    v174 = v173 ^ v172;
    v173 = __ROR4__(v173, 9);
    v175 = (v155 & v129 ^ v165 & (v129 ^ v155)) + (v173 ^ v174) + v171;
    v176 = __ROR4__(v170, 6);
    v177 = v176;
    v176 = __ROR4__(v176, 5);
    v178 = v176 ^ v177;
    v176 = __ROR4__(v176, 14);
    v179 = DWORD3(v1027) + (v176 ^ v178) + v124 + (v160 & v170 ^ v150 & ~v170);
    v180 = v179 + v129;
    v181 = v179;
    v182 = __ROR4__(v175, 2);
    v183 = __ROR4__(v175, 13);
    v184 = v183 ^ v182;
    v183 = __ROR4__(v183, 9);
    v185 = (v165 & v155 ^ v175 & (v155 ^ v165)) + (v183 ^ v184) + v181;
    v186 = _mm_loadu_si128(v74);
    ++v74;
    v187 = _mm_or_si128(_mm_slli_si128(v23, 12), _mm_srli_si128(v22, 4));
    v188 = _mm_srl_epi32(v187, 3u);
    v189 = _mm_srl_epi32(v187, 7u);
    v190 = _mm_sll_epi32(v187, 0xEu);
    v191 = _mm_add_epi32(
             _mm_add_epi32(
               v22,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v188, v189), v190), _mm_srl_epi32(v189, 0xBu)),
                 _mm_sll_epi32(v190, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v145, 12), _mm_srli_si128(v89, 4)));
    v192 = _mm_srli_si128(v145, 8);
    v193 = _mm_srl_epi32(v192, 0x11u);
    v194 = _mm_xor_si128(_mm_srl_epi32(v192, 0xAu), v193);
    v195 = _mm_sll_epi32(v192, 0xDu);
    v196 = _mm_add_epi32(
             v191,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v194, v195), _mm_srl_epi32(v193, 2u)), _mm_sll_epi32(v195, 2u)));
    v197 = _mm_slli_si128(v196, 8);
    v198 = _mm_srl_epi32(v197, 0x11u);
    v199 = _mm_xor_si128(_mm_srl_epi32(v197, 0xAu), v198);
    v200 = _mm_sll_epi32(v197, 0xDu);
    v201 = _mm_add_epi32(
             v196,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v199, v200), _mm_srl_epi32(v198, 2u)), _mm_sll_epi32(v200, 2u)));
    _mm_store_si128((__m128i *)&v1027, _mm_add_epi32(v186, v201));
    v202 = __ROR4__(v180, 6);
    v203 = v202;
    v202 = __ROR4__(v202, 5);
    v204 = v202 ^ v203;
    v202 = __ROR4__(v202, 14);
    v205 = v1028.m128i_i32[0] + (v202 ^ v204) + v150 + (v170 & v180 ^ v160 & ~v180);
    v206 = v205 + v155;
    v207 = v205;
    v208 = __ROR4__(v185, 2);
    v209 = __ROR4__(v185, 13);
    v210 = v209 ^ v208;
    v209 = __ROR4__(v209, 9);
    v211 = (v175 & v165 ^ v185 & (v165 ^ v175)) + (v209 ^ v210) + v207;
    v212 = __ROR4__(v206, 6);
    v213 = v212;
    v212 = __ROR4__(v212, 5);
    v214 = v212 ^ v213;
    v212 = __ROR4__(v212, 14);
    v215 = v1028.m128i_i32[1] + (v212 ^ v214) + v160 + (v180 & v206 ^ v170 & ~v206);
    v216 = v215 + v165;
    v217 = v215;
    v218 = __ROR4__(v211, 2);
    v219 = __ROR4__(v211, 13);
    v220 = v219 ^ v218;
    v219 = __ROR4__(v219, 9);
    v221 = (v185 & v175 ^ v211 & (v175 ^ v185)) + (v219 ^ v220) + v217;
    v222 = __ROR4__(v216, 6);
    v223 = v222;
    v222 = __ROR4__(v222, 5);
    v224 = v222 ^ v223;
    v222 = __ROR4__(v222, 14);
    v225 = v1028.m128i_i32[2] + (v222 ^ v224) + v170 + (v206 & v216 ^ v180 & ~v216);
    v226 = v225 + v175;
    v227 = v225;
    v228 = __ROR4__(v221, 2);
    v229 = __ROR4__(v221, 13);
    v230 = v229 ^ v228;
    v229 = __ROR4__(v229, 9);
    v231 = (v211 & v185 ^ v221 & (v185 ^ v211)) + (v229 ^ v230) + v227;
    v232 = __ROR4__(v226, 6);
    v233 = v232;
    v232 = __ROR4__(v232, 5);
    v234 = v232 ^ v233;
    v232 = __ROR4__(v232, 14);
    v235 = v1028.m128i_i32[3] + (v232 ^ v234) + v180 + (v216 & v226 ^ v206 & ~v226);
    v236 = v235 + v185;
    v237 = v235;
    v238 = __ROR4__(v231, 2);
    v239 = __ROR4__(v231, 13);
    v240 = v239 ^ v238;
    v239 = __ROR4__(v239, 9);
    v241 = (v221 & v211 ^ v231 & (v211 ^ v221)) + (v239 ^ v240) + v237;
    v242 = _mm_loadu_si128(v74);
    ++v74;
    v243 = _mm_or_si128(_mm_slli_si128(v89, 12), _mm_srli_si128(v23, 4));
    v244 = _mm_srl_epi32(v243, 3u);
    v245 = _mm_srl_epi32(v243, 7u);
    v246 = _mm_sll_epi32(v243, 0xEu);
    v247 = _mm_add_epi32(
             _mm_add_epi32(
               v23,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v244, v245), v246), _mm_srl_epi32(v245, 0xBu)),
                 _mm_sll_epi32(v246, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v201, 12), _mm_srli_si128(v145, 4)));
    v248 = _mm_srli_si128(v201, 8);
    v249 = _mm_srl_epi32(v248, 0x11u);
    v250 = _mm_xor_si128(_mm_srl_epi32(v248, 0xAu), v249);
    v251 = _mm_sll_epi32(v248, 0xDu);
    v252 = _mm_add_epi32(
             v247,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v250, v251), _mm_srl_epi32(v249, 2u)), _mm_sll_epi32(v251, 2u)));
    v253 = _mm_slli_si128(v252, 8);
    v254 = _mm_srl_epi32(v253, 0x11u);
    v255 = _mm_xor_si128(_mm_srl_epi32(v253, 0xAu), v254);
    v256 = _mm_sll_epi32(v253, 0xDu);
    v257 = _mm_add_epi32(
             v252,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v255, v256), _mm_srl_epi32(v254, 2u)), _mm_sll_epi32(v256, 2u)));
    _mm_store_si128((__m128i *)&v1028, _mm_add_epi32(v242, v257));
    v258 = __ROR4__(v236, 6);
    v259 = v258;
    v258 = __ROR4__(v258, 5);
    v260 = v258 ^ v259;
    v258 = __ROR4__(v258, 14);
    v261 = v1025 + (v258 ^ v260) + v206 + (v226 & v236 ^ v216 & ~v236);
    v262 = v261 + v211;
    v263 = v261;
    v264 = __ROR4__(v241, 2);
    v265 = __ROR4__(v241, 13);
    v266 = v265 ^ v264;
    v265 = __ROR4__(v265, 9);
    v267 = (v231 & v221 ^ v241 & (v221 ^ v231)) + (v265 ^ v266) + v263;
    v268 = __ROR4__(v262, 6);
    v269 = v268;
    v268 = __ROR4__(v268, 5);
    v270 = v268 ^ v269;
    v268 = __ROR4__(v268, 14);
    v271 = DWORD1(v1025) + (v268 ^ v270) + v216 + (v236 & v262 ^ v226 & ~v262);
    v272 = v271 + v221;
    v273 = v271;
    v274 = __ROR4__(v267, 2);
    v275 = __ROR4__(v267, 13);
    v276 = v275 ^ v274;
    v275 = __ROR4__(v275, 9);
    v277 = (v241 & v231 ^ v267 & (v231 ^ v241)) + (v275 ^ v276) + v273;
    v278 = __ROR4__(v272, 6);
    v279 = v278;
    v278 = __ROR4__(v278, 5);
    v280 = v278 ^ v279;
    v278 = __ROR4__(v278, 14);
    v281 = DWORD2(v1025) + (v278 ^ v280) + v226 + (v262 & v272 ^ v236 & ~v272);
    v282 = v281 + v231;
    v283 = v281;
    v284 = __ROR4__(v277, 2);
    v285 = __ROR4__(v277, 13);
    v286 = v285 ^ v284;
    v285 = __ROR4__(v285, 9);
    v287 = (v267 & v241 ^ v277 & (v241 ^ v267)) + (v285 ^ v286) + v283;
    v288 = __ROR4__(v282, 6);
    v289 = v288;
    v288 = __ROR4__(v288, 5);
    v290 = v288 ^ v289;
    v288 = __ROR4__(v288, 14);
    v291 = DWORD3(v1025) + (v288 ^ v290) + v236 + (v272 & v282 ^ v262 & ~v282);
    v292 = v291 + v241;
    v293 = v291;
    v294 = __ROR4__(v287, 2);
    v295 = __ROR4__(v287, 13);
    v296 = v295 ^ v294;
    v295 = __ROR4__(v295, 9);
    v297 = (v277 & v267 ^ v287 & (v267 ^ v277)) + (v295 ^ v296) + v293;
    v298 = _mm_loadu_si128(v74);
    ++v74;
    v299 = _mm_or_si128(_mm_slli_si128(v145, 12), _mm_srli_si128(v89, 4));
    v300 = _mm_srl_epi32(v299, 3u);
    v301 = _mm_srl_epi32(v299, 7u);
    v302 = _mm_sll_epi32(v299, 0xEu);
    v303 = _mm_add_epi32(
             _mm_add_epi32(
               v89,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v300, v301), v302), _mm_srl_epi32(v301, 0xBu)),
                 _mm_sll_epi32(v302, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v257, 12), _mm_srli_si128(v201, 4)));
    v304 = _mm_srli_si128(v257, 8);
    v305 = _mm_srl_epi32(v304, 0x11u);
    v306 = _mm_xor_si128(_mm_srl_epi32(v304, 0xAu), v305);
    v307 = _mm_sll_epi32(v304, 0xDu);
    v308 = _mm_add_epi32(
             v303,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v306, v307), _mm_srl_epi32(v305, 2u)), _mm_sll_epi32(v307, 2u)));
    v309 = _mm_slli_si128(v308, 8);
    v310 = _mm_srl_epi32(v309, 0x11u);
    v311 = _mm_xor_si128(_mm_srl_epi32(v309, 0xAu), v310);
    v312 = _mm_sll_epi32(v309, 0xDu);
    v313 = _mm_add_epi32(
             v308,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v311, v312), _mm_srl_epi32(v310, 2u)), _mm_sll_epi32(v312, 2u)));
    _mm_store_si128((__m128i *)&v1025, _mm_add_epi32(v298, v313));
    v314 = __ROR4__(v292, 6);
    v315 = v314;
    v314 = __ROR4__(v314, 5);
    v316 = v314 ^ v315;
    v314 = __ROR4__(v314, 14);
    v317 = v1026 + (v314 ^ v316) + v262 + (v282 & v292 ^ v272 & ~v292);
    v318 = v317 + v267;
    v319 = v317;
    v320 = __ROR4__(v297, 2);
    v321 = __ROR4__(v297, 13);
    v322 = v321 ^ v320;
    v321 = __ROR4__(v321, 9);
    v323 = (v287 & v277 ^ v297 & (v277 ^ v287)) + (v321 ^ v322) + v319;
    v324 = __ROR4__(v318, 6);
    v325 = v324;
    v324 = __ROR4__(v324, 5);
    v326 = v324 ^ v325;
    v324 = __ROR4__(v324, 14);
    v327 = DWORD1(v1026) + (v324 ^ v326) + v272 + (v292 & v318 ^ v282 & ~v318);
    v328 = v327 + v277;
    v329 = v327;
    v330 = __ROR4__(v323, 2);
    v331 = __ROR4__(v323, 13);
    v332 = v331 ^ v330;
    v331 = __ROR4__(v331, 9);
    v333 = (v297 & v287 ^ v323 & (v287 ^ v297)) + (v331 ^ v332) + v329;
    v334 = __ROR4__(v328, 6);
    v335 = v334;
    v334 = __ROR4__(v334, 5);
    v336 = v334 ^ v335;
    v334 = __ROR4__(v334, 14);
    v337 = DWORD2(v1026) + (v334 ^ v336) + v282 + (v318 & v328 ^ v292 & ~v328);
    v338 = v337 + v287;
    v339 = v337;
    v340 = __ROR4__(v333, 2);
    v341 = __ROR4__(v333, 13);
    v342 = v341 ^ v340;
    v341 = __ROR4__(v341, 9);
    v343 = (v323 & v297 ^ v333 & (v297 ^ v323)) + (v341 ^ v342) + v339;
    v344 = __ROR4__(v338, 6);
    v345 = v344;
    v344 = __ROR4__(v344, 5);
    v346 = v344 ^ v345;
    v344 = __ROR4__(v344, 14);
    v347 = DWORD3(v1026) + (v344 ^ v346) + v292 + (v328 & v338 ^ v318 & ~v338);
    v348 = v347 + v297;
    v349 = v347;
    v350 = __ROR4__(v343, 2);
    v351 = __ROR4__(v343, 13);
    v352 = v351 ^ v350;
    v351 = __ROR4__(v351, 9);
    v353 = (v333 & v323 ^ v343 & (v323 ^ v333)) + (v351 ^ v352) + v349;
    v354 = _mm_loadu_si128(v74);
    ++v74;
    v355 = _mm_or_si128(_mm_slli_si128(v201, 12), _mm_srli_si128(v145, 4));
    v356 = _mm_srl_epi32(v355, 3u);
    v357 = _mm_srl_epi32(v355, 7u);
    v358 = _mm_sll_epi32(v355, 0xEu);
    v359 = _mm_add_epi32(
             _mm_add_epi32(
               v145,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v356, v357), v358), _mm_srl_epi32(v357, 0xBu)),
                 _mm_sll_epi32(v358, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v313, 12), _mm_srli_si128(v257, 4)));
    v360 = _mm_srli_si128(v313, 8);
    v361 = _mm_srl_epi32(v360, 0x11u);
    v362 = _mm_xor_si128(_mm_srl_epi32(v360, 0xAu), v361);
    v363 = _mm_sll_epi32(v360, 0xDu);
    v364 = _mm_add_epi32(
             v359,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v362, v363), _mm_srl_epi32(v361, 2u)), _mm_sll_epi32(v363, 2u)));
    v365 = _mm_slli_si128(v364, 8);
    v366 = _mm_srl_epi32(v365, 0x11u);
    v367 = _mm_xor_si128(_mm_srl_epi32(v365, 0xAu), v366);
    v368 = _mm_sll_epi32(v365, 0xDu);
    v369 = _mm_add_epi32(
             v364,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v367, v368), _mm_srl_epi32(v366, 2u)), _mm_sll_epi32(v368, 2u)));
    _mm_store_si128((__m128i *)&v1026, _mm_add_epi32(v354, v369));
    v370 = __ROR4__(v348, 6);
    v371 = v370;
    v370 = __ROR4__(v370, 5);
    v372 = v370 ^ v371;
    v370 = __ROR4__(v370, 14);
    v373 = v1027 + (v370 ^ v372) + v318 + (v338 & v348 ^ v328 & ~v348);
    v374 = v373 + v323;
    v375 = v373;
    v376 = __ROR4__(v353, 2);
    v377 = __ROR4__(v353, 13);
    v378 = v377 ^ v376;
    v377 = __ROR4__(v377, 9);
    v379 = (v343 & v333 ^ v353 & (v333 ^ v343)) + (v377 ^ v378) + v375;
    v380 = __ROR4__(v374, 6);
    v381 = v380;
    v380 = __ROR4__(v380, 5);
    v382 = v380 ^ v381;
    v380 = __ROR4__(v380, 14);
    v383 = DWORD1(v1027) + (v380 ^ v382) + v328 + (v348 & v374 ^ v338 & ~v374);
    v384 = v383 + v333;
    v385 = v383;
    v386 = __ROR4__(v379, 2);
    v387 = __ROR4__(v379, 13);
    v388 = v387 ^ v386;
    v387 = __ROR4__(v387, 9);
    v389 = (v353 & v343 ^ v379 & (v343 ^ v353)) + (v387 ^ v388) + v385;
    v390 = __ROR4__(v384, 6);
    v391 = v390;
    v390 = __ROR4__(v390, 5);
    v392 = v390 ^ v391;
    v390 = __ROR4__(v390, 14);
    v393 = DWORD2(v1027) + (v390 ^ v392) + v338 + (v374 & v384 ^ v348 & ~v384);
    v394 = v393 + v343;
    v395 = v393;
    v396 = __ROR4__(v389, 2);
    v397 = __ROR4__(v389, 13);
    v398 = v397 ^ v396;
    v397 = __ROR4__(v397, 9);
    v399 = (v379 & v353 ^ v389 & (v353 ^ v379)) + (v397 ^ v398) + v395;
    v400 = __ROR4__(v394, 6);
    v401 = v400;
    v400 = __ROR4__(v400, 5);
    v402 = v400 ^ v401;
    v400 = __ROR4__(v400, 14);
    v403 = DWORD3(v1027) + (v400 ^ v402) + v348 + (v384 & v394 ^ v374 & ~v394);
    v404 = v403 + v353;
    v405 = v403;
    v406 = __ROR4__(v399, 2);
    v407 = __ROR4__(v399, 13);
    v408 = v407 ^ v406;
    v407 = __ROR4__(v407, 9);
    v409 = (v389 & v379 ^ v399 & (v379 ^ v389)) + (v407 ^ v408) + v405;
    v410 = _mm_loadu_si128(v74);
    ++v74;
    v411 = _mm_or_si128(_mm_slli_si128(v257, 12), _mm_srli_si128(v201, 4));
    v412 = _mm_srl_epi32(v411, 3u);
    v413 = _mm_srl_epi32(v411, 7u);
    v414 = _mm_sll_epi32(v411, 0xEu);
    v415 = _mm_add_epi32(
             _mm_add_epi32(
               v201,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v412, v413), v414), _mm_srl_epi32(v413, 0xBu)),
                 _mm_sll_epi32(v414, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v369, 12), _mm_srli_si128(v313, 4)));
    v416 = _mm_srli_si128(v369, 8);
    v417 = _mm_srl_epi32(v416, 0x11u);
    v418 = _mm_xor_si128(_mm_srl_epi32(v416, 0xAu), v417);
    v419 = _mm_sll_epi32(v416, 0xDu);
    v420 = _mm_add_epi32(
             v415,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v418, v419), _mm_srl_epi32(v417, 2u)), _mm_sll_epi32(v419, 2u)));
    v421 = _mm_slli_si128(v420, 8);
    v422 = _mm_srl_epi32(v421, 0x11u);
    v423 = _mm_xor_si128(_mm_srl_epi32(v421, 0xAu), v422);
    v424 = _mm_sll_epi32(v421, 0xDu);
    v425 = _mm_add_epi32(
             v420,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v423, v424), _mm_srl_epi32(v422, 2u)), _mm_sll_epi32(v424, 2u)));
    _mm_store_si128((__m128i *)&v1027, _mm_add_epi32(v410, v425));
    v426 = __ROR4__(v404, 6);
    v427 = v426;
    v426 = __ROR4__(v426, 5);
    v428 = v426 ^ v427;
    v426 = __ROR4__(v426, 14);
    v429 = v1028.m128i_i32[0] + (v426 ^ v428) + v374 + (v394 & v404 ^ v384 & ~v404);
    v430 = v429 + v379;
    v431 = v429;
    v432 = __ROR4__(v409, 2);
    v433 = __ROR4__(v409, 13);
    v434 = v433 ^ v432;
    v433 = __ROR4__(v433, 9);
    v435 = (v399 & v389 ^ v409 & (v389 ^ v399)) + (v433 ^ v434) + v431;
    v436 = __ROR4__(v430, 6);
    v437 = v436;
    v436 = __ROR4__(v436, 5);
    v438 = v436 ^ v437;
    v436 = __ROR4__(v436, 14);
    v439 = v1028.m128i_i32[1] + (v436 ^ v438) + v384 + (v404 & v430 ^ v394 & ~v430);
    v440 = v439 + v389;
    v441 = v439;
    v442 = __ROR4__(v435, 2);
    v443 = __ROR4__(v435, 13);
    v444 = v443 ^ v442;
    v443 = __ROR4__(v443, 9);
    v445 = (v409 & v399 ^ v435 & (v399 ^ v409)) + (v443 ^ v444) + v441;
    v446 = __ROR4__(v440, 6);
    v447 = v446;
    v446 = __ROR4__(v446, 5);
    v448 = v446 ^ v447;
    v446 = __ROR4__(v446, 14);
    v449 = v1028.m128i_i32[2] + (v446 ^ v448) + v394 + (v430 & v440 ^ v404 & ~v440);
    v450 = v449 + v399;
    v451 = v449;
    v452 = __ROR4__(v445, 2);
    v453 = __ROR4__(v445, 13);
    v454 = v453 ^ v452;
    v453 = __ROR4__(v453, 9);
    v455 = (v435 & v409 ^ v445 & (v409 ^ v435)) + (v453 ^ v454) + v451;
    v456 = __ROR4__(v450, 6);
    v457 = v456;
    v456 = __ROR4__(v456, 5);
    v458 = v456 ^ v457;
    v456 = __ROR4__(v456, 14);
    v459 = v1028.m128i_i32[3] + (v456 ^ v458) + v404 + (v440 & v450 ^ v430 & ~v450);
    v460 = v459 + v409;
    v461 = v459;
    v462 = __ROR4__(v455, 2);
    v463 = __ROR4__(v455, 13);
    v464 = v463 ^ v462;
    v463 = __ROR4__(v463, 9);
    v465 = (v445 & v435 ^ v455 & (v435 ^ v445)) + (v463 ^ v464) + v461;
    v466 = _mm_loadu_si128(v74);
    ++v74;
    v467 = _mm_or_si128(_mm_slli_si128(v313, 12), _mm_srli_si128(v257, 4));
    v468 = _mm_srl_epi32(v467, 3u);
    v469 = _mm_srl_epi32(v467, 7u);
    v470 = _mm_sll_epi32(v467, 0xEu);
    v471 = _mm_add_epi32(
             _mm_add_epi32(
               v257,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v468, v469), v470), _mm_srl_epi32(v469, 0xBu)),
                 _mm_sll_epi32(v470, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v425, 12), _mm_srli_si128(v369, 4)));
    v472 = _mm_srli_si128(v425, 8);
    v473 = _mm_srl_epi32(v472, 0x11u);
    v474 = _mm_xor_si128(_mm_srl_epi32(v472, 0xAu), v473);
    v475 = _mm_sll_epi32(v472, 0xDu);
    v476 = _mm_add_epi32(
             v471,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v474, v475), _mm_srl_epi32(v473, 2u)), _mm_sll_epi32(v475, 2u)));
    v477 = _mm_slli_si128(v476, 8);
    v478 = _mm_srl_epi32(v477, 0x11u);
    v479 = _mm_xor_si128(_mm_srl_epi32(v477, 0xAu), v478);
    v480 = _mm_sll_epi32(v477, 0xDu);
    v481 = _mm_add_epi32(
             v476,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v479, v480), _mm_srl_epi32(v478, 2u)), _mm_sll_epi32(v480, 2u)));
    _mm_store_si128((__m128i *)&v1028, _mm_add_epi32(v466, v481));
    v482 = __ROR4__(v460, 6);
    v483 = v482;
    v482 = __ROR4__(v482, 5);
    v484 = v482 ^ v483;
    v482 = __ROR4__(v482, 14);
    v485 = v1025 + (v482 ^ v484) + v430 + (v450 & v460 ^ v440 & ~v460);
    v486 = v485 + v435;
    v487 = v485;
    v488 = __ROR4__(v465, 2);
    v489 = __ROR4__(v465, 13);
    v490 = v489 ^ v488;
    v489 = __ROR4__(v489, 9);
    v491 = (v455 & v445 ^ v465 & (v445 ^ v455)) + (v489 ^ v490) + v487;
    v492 = __ROR4__(v486, 6);
    v493 = v492;
    v492 = __ROR4__(v492, 5);
    v494 = v492 ^ v493;
    v492 = __ROR4__(v492, 14);
    v495 = DWORD1(v1025) + (v492 ^ v494) + v440 + (v460 & v486 ^ v450 & ~v486);
    v496 = v495 + v445;
    v497 = v495;
    v498 = __ROR4__(v491, 2);
    v499 = __ROR4__(v491, 13);
    v500 = v499 ^ v498;
    v499 = __ROR4__(v499, 9);
    v501 = (v465 & v455 ^ v491 & (v455 ^ v465)) + (v499 ^ v500) + v497;
    v502 = __ROR4__(v496, 6);
    v503 = v502;
    v502 = __ROR4__(v502, 5);
    v504 = v502 ^ v503;
    v502 = __ROR4__(v502, 14);
    v505 = DWORD2(v1025) + (v502 ^ v504) + v450 + (v486 & v496 ^ v460 & ~v496);
    v506 = v505 + v455;
    v507 = v505;
    v508 = __ROR4__(v501, 2);
    v509 = __ROR4__(v501, 13);
    v510 = v509 ^ v508;
    v509 = __ROR4__(v509, 9);
    v511 = (v491 & v465 ^ v501 & (v465 ^ v491)) + (v509 ^ v510) + v507;
    v512 = __ROR4__(v506, 6);
    v513 = v512;
    v512 = __ROR4__(v512, 5);
    v514 = v512 ^ v513;
    v512 = __ROR4__(v512, 14);
    v515 = DWORD3(v1025) + (v512 ^ v514) + v460 + (v496 & v506 ^ v486 & ~v506);
    v516 = v515 + v465;
    v517 = v515;
    v518 = __ROR4__(v511, 2);
    v519 = __ROR4__(v511, 13);
    v520 = v519 ^ v518;
    v519 = __ROR4__(v519, 9);
    v521 = (v501 & v491 ^ v511 & (v491 ^ v501)) + (v519 ^ v520) + v517;
    v522 = _mm_loadu_si128(v74);
    ++v74;
    v523 = _mm_or_si128(_mm_slli_si128(v369, 12), _mm_srli_si128(v313, 4));
    v524 = _mm_srl_epi32(v523, 3u);
    v525 = _mm_srl_epi32(v523, 7u);
    v526 = _mm_sll_epi32(v523, 0xEu);
    v527 = _mm_add_epi32(
             _mm_add_epi32(
               v313,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v524, v525), v526), _mm_srl_epi32(v525, 0xBu)),
                 _mm_sll_epi32(v526, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v481, 12), _mm_srli_si128(v425, 4)));
    v528 = _mm_srli_si128(v481, 8);
    v529 = _mm_srl_epi32(v528, 0x11u);
    v530 = _mm_xor_si128(_mm_srl_epi32(v528, 0xAu), v529);
    v531 = _mm_sll_epi32(v528, 0xDu);
    v532 = _mm_add_epi32(
             v527,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v530, v531), _mm_srl_epi32(v529, 2u)), _mm_sll_epi32(v531, 2u)));
    v533 = _mm_slli_si128(v532, 8);
    v534 = _mm_srl_epi32(v533, 0x11u);
    v535 = _mm_xor_si128(_mm_srl_epi32(v533, 0xAu), v534);
    v536 = _mm_sll_epi32(v533, 0xDu);
    v537 = _mm_add_epi32(
             v532,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v535, v536), _mm_srl_epi32(v534, 2u)), _mm_sll_epi32(v536, 2u)));
    _mm_store_si128((__m128i *)&v1025, _mm_add_epi32(v522, v537));
    v538 = __ROR4__(v516, 6);
    v539 = v538;
    v538 = __ROR4__(v538, 5);
    v540 = v538 ^ v539;
    v538 = __ROR4__(v538, 14);
    v541 = v1026 + (v538 ^ v540) + v486 + (v506 & v516 ^ v496 & ~v516);
    v542 = v541 + v491;
    v543 = v541;
    v544 = __ROR4__(v521, 2);
    v545 = __ROR4__(v521, 13);
    v546 = v545 ^ v544;
    v545 = __ROR4__(v545, 9);
    v547 = (v511 & v501 ^ v521 & (v501 ^ v511)) + (v545 ^ v546) + v543;
    v548 = __ROR4__(v542, 6);
    v549 = v548;
    v548 = __ROR4__(v548, 5);
    v550 = v548 ^ v549;
    v548 = __ROR4__(v548, 14);
    v551 = DWORD1(v1026) + (v548 ^ v550) + v496 + (v516 & v542 ^ v506 & ~v542);
    v552 = v551 + v501;
    v553 = v551;
    v554 = __ROR4__(v547, 2);
    v555 = __ROR4__(v547, 13);
    v556 = v555 ^ v554;
    v555 = __ROR4__(v555, 9);
    v557 = (v521 & v511 ^ v547 & (v511 ^ v521)) + (v555 ^ v556) + v553;
    v558 = __ROR4__(v552, 6);
    v559 = v558;
    v558 = __ROR4__(v558, 5);
    v560 = v558 ^ v559;
    v558 = __ROR4__(v558, 14);
    v561 = DWORD2(v1026) + (v558 ^ v560) + v506 + (v542 & v552 ^ v516 & ~v552);
    v562 = v561 + v511;
    v563 = v561;
    v564 = __ROR4__(v557, 2);
    v565 = __ROR4__(v557, 13);
    v566 = v565 ^ v564;
    v565 = __ROR4__(v565, 9);
    v567 = (v547 & v521 ^ v557 & (v521 ^ v547)) + (v565 ^ v566) + v563;
    v568 = __ROR4__(v562, 6);
    v569 = v568;
    v568 = __ROR4__(v568, 5);
    v570 = v568 ^ v569;
    v568 = __ROR4__(v568, 14);
    v571 = DWORD3(v1026) + (v568 ^ v570) + v516 + (v552 & v562 ^ v542 & ~v562);
    v572 = v571 + v521;
    v573 = v571;
    v574 = __ROR4__(v567, 2);
    v575 = __ROR4__(v567, 13);
    v576 = v575 ^ v574;
    v575 = __ROR4__(v575, 9);
    v577 = (v557 & v547 ^ v567 & (v547 ^ v557)) + (v575 ^ v576) + v573;
    v578 = _mm_loadu_si128(v74);
    ++v74;
    v579 = _mm_or_si128(_mm_slli_si128(v425, 12), _mm_srli_si128(v369, 4));
    v580 = _mm_srl_epi32(v579, 3u);
    v581 = _mm_srl_epi32(v579, 7u);
    v582 = _mm_sll_epi32(v579, 0xEu);
    v583 = _mm_add_epi32(
             _mm_add_epi32(
               v369,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v580, v581), v582), _mm_srl_epi32(v581, 0xBu)),
                 _mm_sll_epi32(v582, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v537, 12), _mm_srli_si128(v481, 4)));
    v584 = _mm_srli_si128(v537, 8);
    v585 = _mm_srl_epi32(v584, 0x11u);
    v586 = _mm_xor_si128(_mm_srl_epi32(v584, 0xAu), v585);
    v587 = _mm_sll_epi32(v584, 0xDu);
    v588 = _mm_add_epi32(
             v583,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v586, v587), _mm_srl_epi32(v585, 2u)), _mm_sll_epi32(v587, 2u)));
    v589 = _mm_slli_si128(v588, 8);
    v590 = _mm_srl_epi32(v589, 0x11u);
    v591 = _mm_xor_si128(_mm_srl_epi32(v589, 0xAu), v590);
    v592 = _mm_sll_epi32(v589, 0xDu);
    v593 = _mm_add_epi32(
             v588,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v591, v592), _mm_srl_epi32(v590, 2u)), _mm_sll_epi32(v592, 2u)));
    _mm_store_si128((__m128i *)&v1026, _mm_add_epi32(v578, v593));
    v594 = __ROR4__(v572, 6);
    v595 = v594;
    v594 = __ROR4__(v594, 5);
    v596 = v594 ^ v595;
    v594 = __ROR4__(v594, 14);
    v597 = v1027 + (v594 ^ v596) + v542 + (v562 & v572 ^ v552 & ~v572);
    v598 = v597 + v547;
    v599 = v597;
    v600 = __ROR4__(v577, 2);
    v601 = __ROR4__(v577, 13);
    v602 = v601 ^ v600;
    v601 = __ROR4__(v601, 9);
    v603 = (v567 & v557 ^ v577 & (v557 ^ v567)) + (v601 ^ v602) + v599;
    v604 = __ROR4__(v598, 6);
    v605 = v604;
    v604 = __ROR4__(v604, 5);
    v606 = v604 ^ v605;
    v604 = __ROR4__(v604, 14);
    v607 = DWORD1(v1027) + (v604 ^ v606) + v552 + (v572 & v598 ^ v562 & ~v598);
    v608 = v607 + v557;
    v609 = v607;
    v610 = __ROR4__(v603, 2);
    v611 = __ROR4__(v603, 13);
    v612 = v611 ^ v610;
    v611 = __ROR4__(v611, 9);
    v613 = (v577 & v567 ^ v603 & (v567 ^ v577)) + (v611 ^ v612) + v609;
    v614 = __ROR4__(v608, 6);
    v615 = v614;
    v614 = __ROR4__(v614, 5);
    v616 = v614 ^ v615;
    v614 = __ROR4__(v614, 14);
    v617 = DWORD2(v1027) + (v614 ^ v616) + v562 + (v598 & v608 ^ v572 & ~v608);
    v618 = v617 + v567;
    v619 = v617;
    v620 = __ROR4__(v613, 2);
    v621 = __ROR4__(v613, 13);
    v622 = v621 ^ v620;
    v621 = __ROR4__(v621, 9);
    v623 = (v603 & v577 ^ v613 & (v577 ^ v603)) + (v621 ^ v622) + v619;
    v624 = __ROR4__(v618, 6);
    v625 = v624;
    v624 = __ROR4__(v624, 5);
    v626 = v624 ^ v625;
    v624 = __ROR4__(v624, 14);
    v627 = DWORD3(v1027) + (v624 ^ v626) + v572 + (v608 & v618 ^ v598 & ~v618);
    v628 = v627 + v577;
    v629 = v627;
    v630 = __ROR4__(v623, 2);
    v631 = __ROR4__(v623, 13);
    v632 = v631 ^ v630;
    v631 = __ROR4__(v631, 9);
    v633 = (v613 & v603 ^ v623 & (v603 ^ v613)) + (v631 ^ v632) + v629;
    v634 = _mm_loadu_si128(v74);
    ++v74;
    v635 = _mm_or_si128(_mm_slli_si128(v481, 12), _mm_srli_si128(v425, 4));
    v636 = _mm_srl_epi32(v635, 3u);
    v637 = _mm_srl_epi32(v635, 7u);
    v638 = _mm_sll_epi32(v635, 0xEu);
    v639 = _mm_add_epi32(
             _mm_add_epi32(
               v425,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v636, v637), v638), _mm_srl_epi32(v637, 0xBu)),
                 _mm_sll_epi32(v638, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v593, 12), _mm_srli_si128(v537, 4)));
    v640 = _mm_srli_si128(v593, 8);
    v641 = _mm_srl_epi32(v640, 0x11u);
    v642 = _mm_xor_si128(_mm_srl_epi32(v640, 0xAu), v641);
    v643 = _mm_sll_epi32(v640, 0xDu);
    v644 = _mm_add_epi32(
             v639,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v642, v643), _mm_srl_epi32(v641, 2u)), _mm_sll_epi32(v643, 2u)));
    v645 = _mm_slli_si128(v644, 8);
    v646 = _mm_srl_epi32(v645, 0x11u);
    v647 = _mm_xor_si128(_mm_srl_epi32(v645, 0xAu), v646);
    v648 = _mm_sll_epi32(v645, 0xDu);
    v649 = _mm_add_epi32(
             v644,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v647, v648), _mm_srl_epi32(v646, 2u)), _mm_sll_epi32(v648, 2u)));
    _mm_store_si128((__m128i *)&v1027, _mm_add_epi32(v634, v649));
    v650 = __ROR4__(v628, 6);
    v651 = v650;
    v650 = __ROR4__(v650, 5);
    v652 = v650 ^ v651;
    v650 = __ROR4__(v650, 14);
    v653 = v1028.m128i_i32[0] + (v650 ^ v652) + v598 + (v618 & v628 ^ v608 & ~v628);
    v654 = v653 + v603;
    v655 = v653;
    v656 = __ROR4__(v633, 2);
    v657 = __ROR4__(v633, 13);
    v658 = v657 ^ v656;
    v657 = __ROR4__(v657, 9);
    v659 = (v623 & v613 ^ v633 & (v613 ^ v623)) + (v657 ^ v658) + v655;
    v660 = __ROR4__(v654, 6);
    v661 = v660;
    v660 = __ROR4__(v660, 5);
    v662 = v660 ^ v661;
    v660 = __ROR4__(v660, 14);
    v663 = v1028.m128i_i32[1] + (v660 ^ v662) + v608 + (v628 & v654 ^ v618 & ~v654);
    v664 = v663 + v613;
    v665 = v663;
    v666 = __ROR4__(v659, 2);
    v667 = __ROR4__(v659, 13);
    v668 = v667 ^ v666;
    v667 = __ROR4__(v667, 9);
    v669 = (v633 & v623 ^ v659 & (v623 ^ v633)) + (v667 ^ v668) + v665;
    v670 = __ROR4__(v664, 6);
    v671 = v670;
    v670 = __ROR4__(v670, 5);
    v672 = v670 ^ v671;
    v670 = __ROR4__(v670, 14);
    v673 = v1028.m128i_i32[2] + (v670 ^ v672) + v618 + (v654 & v664 ^ v628 & ~v664);
    v674 = v673 + v623;
    v675 = v673;
    v676 = __ROR4__(v669, 2);
    v677 = __ROR4__(v669, 13);
    v678 = v677 ^ v676;
    v677 = __ROR4__(v677, 9);
    v679 = (v659 & v633 ^ v669 & (v633 ^ v659)) + (v677 ^ v678) + v675;
    v680 = __ROR4__(v674, 6);
    v681 = v680;
    v680 = __ROR4__(v680, 5);
    v682 = v680 ^ v681;
    v680 = __ROR4__(v680, 14);
    v683 = v1028.m128i_i32[3] + (v680 ^ v682) + v628 + (v664 & v674 ^ v654 & ~v674);
    v684 = v683 + v633;
    v685 = v683;
    v686 = __ROR4__(v679, 2);
    v687 = __ROR4__(v679, 13);
    v688 = v687 ^ v686;
    v687 = __ROR4__(v687, 9);
    v689 = (v669 & v659 ^ v679 & (v659 ^ v669)) + (v687 ^ v688) + v685;
    v690 = _mm_or_si128(_mm_slli_si128(v537, 12), _mm_srli_si128(v481, 4));
    v691 = _mm_srl_epi32(v690, 3u);
    v692 = _mm_srl_epi32(v690, 7u);
    v693 = _mm_sll_epi32(v690, 0xEu);
    v694 = _mm_add_epi32(
             _mm_add_epi32(
               v481,
               _mm_xor_si128(
                 _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v691, v692), v693), _mm_srl_epi32(v692, 0xBu)),
                 _mm_sll_epi32(v693, 0xBu))),
             _mm_or_si128(_mm_slli_si128(v649, 12), _mm_srli_si128(v593, 4)));
    v695 = _mm_srli_si128(v649, 8);
    v696 = _mm_srl_epi32(v695, 0x11u);
    v697 = _mm_xor_si128(_mm_srl_epi32(v695, 0xAu), v696);
    v698 = _mm_sll_epi32(v695, 0xDu);
    v699 = _mm_add_epi32(
             v694,
             _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v697, v698), _mm_srl_epi32(v696, 2u)), _mm_sll_epi32(v698, 2u)));
    v700 = _mm_slli_si128(v699, 8);
    v701 = _mm_srl_epi32(v700, 0x11u);
    v702 = _mm_xor_si128(_mm_srl_epi32(v700, 0xAu), v701);
    v703 = _mm_sll_epi32(v700, 0xDu);
    _mm_store_si128(
      (__m128i *)&v1028,
      _mm_add_epi32(
        _mm_loadu_si128(v74),
        _mm_add_epi32(
          v699,
          _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v702, v703), _mm_srl_epi32(v701, 2u)), _mm_sll_epi32(v703, 2u)))));
    v704 = (signed __int64)&v74[-15];
    --a3;
    if ( !a3 )
      break;
    v705 = __ROR4__(v684, 6);
    v706 = v705;
    v705 = __ROR4__(v705, 5);
    v707 = v705 ^ v706;
    v705 = __ROR4__(v705, 14);
    v708 = v1025 + (v705 ^ v707) + v654 + (v674 & v684 ^ v664 & ~v684);
    v709 = v708 + v659;
    v710 = v708;
    v711 = __ROR4__(v689, 2);
    v712 = __ROR4__(v689, 13);
    v713 = v712 ^ v711;
    v712 = __ROR4__(v712, 9);
    v714 = (v679 & v669 ^ v689 & (v669 ^ v679)) + (v712 ^ v713) + v710;
    v715 = __ROR4__(v709, 6);
    v716 = v715;
    v715 = __ROR4__(v715, 5);
    v717 = v715 ^ v716;
    v715 = __ROR4__(v715, 14);
    v718 = DWORD1(v1025) + (v715 ^ v717) + v664 + (v684 & v709 ^ v674 & ~v709);
    v719 = v718 + v669;
    v720 = v718;
    v721 = __ROR4__(v714, 2);
    v722 = __ROR4__(v714, 13);
    v723 = v722 ^ v721;
    v722 = __ROR4__(v722, 9);
    v724 = (v689 & v679 ^ v714 & (v679 ^ v689)) + (v722 ^ v723) + v720;
    v725 = __ROR4__(v719, 6);
    v726 = v725;
    v725 = __ROR4__(v725, 5);
    v727 = v725 ^ v726;
    v725 = __ROR4__(v725, 14);
    v728 = DWORD2(v1025) + (v725 ^ v727) + v674 + (v709 & v719 ^ v684 & ~v719);
    v729 = v728 + v679;
    v730 = v728;
    v731 = __ROR4__(v724, 2);
    v732 = __ROR4__(v724, 13);
    v733 = v732 ^ v731;
    v732 = __ROR4__(v732, 9);
    v734 = (v714 & v689 ^ v724 & (v689 ^ v714)) + (v732 ^ v733) + v730;
    v735 = __ROR4__(v729, 6);
    v736 = v735;
    v735 = __ROR4__(v735, 5);
    v737 = v735 ^ v736;
    v735 = __ROR4__(v735, 14);
    v738 = DWORD3(v1025) + (v735 ^ v737) + v684 + (v719 & v729 ^ v709 & ~v729);
    v739 = v738 + v689;
    v740 = v738;
    v741 = __ROR4__(v734, 2);
    v742 = __ROR4__(v734, 13);
    v743 = v742 ^ v741;
    v742 = __ROR4__(v742, 9);
    v744 = (v724 & v714 ^ v734 & (v714 ^ v724)) + (v742 ^ v743) + v740;
    LODWORD(v1025) = _byteswap_ulong(*(_DWORD *)v19);
    DWORD1(v1025) = _byteswap_ulong(*(_DWORD *)(v19 + 4));
    DWORD2(v1025) = _byteswap_ulong(*(_DWORD *)(v19 + 8));
    DWORD3(v1025) = _byteswap_ulong(*(_DWORD *)(v19 + 12));
    v20 = _mm_load_si128((const __m128i *)&v1025);
    _mm_store_si128((__m128i *)&v1025, _mm_add_epi32(_mm_loadu_si128((const __m128i *)v704), v20));
    v745 = __ROR4__(v739, 6);
    v746 = v745;
    v745 = __ROR4__(v745, 5);
    v747 = v745 ^ v746;
    v745 = __ROR4__(v745, 14);
    v748 = v1026 + (v745 ^ v747) + v709 + (v729 & v739 ^ v719 & ~v739);
    v749 = v748 + v714;
    v750 = v748;
    v751 = __ROR4__(v744, 2);
    v752 = __ROR4__(v744, 13);
    v753 = v752 ^ v751;
    v752 = __ROR4__(v752, 9);
    v754 = (v734 & v724 ^ v744 & (v724 ^ v734)) + (v752 ^ v753) + v750;
    v755 = __ROR4__(v749, 6);
    v756 = v755;
    v755 = __ROR4__(v755, 5);
    v757 = v755 ^ v756;
    v755 = __ROR4__(v755, 14);
    v758 = DWORD1(v1026) + (v755 ^ v757) + v719 + (v739 & v749 ^ v729 & ~v749);
    v759 = v758 + v724;
    v760 = v758;
    v761 = __ROR4__(v754, 2);
    v762 = __ROR4__(v754, 13);
    v763 = v762 ^ v761;
    v762 = __ROR4__(v762, 9);
    v764 = (v744 & v734 ^ v754 & (v734 ^ v744)) + (v762 ^ v763) + v760;
    v765 = __ROR4__(v759, 6);
    v766 = v765;
    v765 = __ROR4__(v765, 5);
    v767 = v765 ^ v766;
    v765 = __ROR4__(v765, 14);
    v768 = DWORD2(v1026) + (v765 ^ v767) + v729 + (v749 & v759 ^ v739 & ~v759);
    v769 = v768 + v734;
    v770 = v768;
    v771 = __ROR4__(v764, 2);
    v772 = __ROR4__(v764, 13);
    v773 = v772 ^ v771;
    v772 = __ROR4__(v772, 9);
    v774 = (v754 & v744 ^ v764 & (v744 ^ v754)) + (v772 ^ v773) + v770;
    v775 = __ROR4__(v769, 6);
    v776 = v775;
    v775 = __ROR4__(v775, 5);
    v777 = v775 ^ v776;
    v775 = __ROR4__(v775, 14);
    v778 = DWORD3(v1026) + (v775 ^ v777) + v739 + (v759 & v769 ^ v749 & ~v769);
    v779 = v778 + v744;
    v780 = v778;
    v781 = __ROR4__(v774, 2);
    v782 = __ROR4__(v774, 13);
    v783 = v782 ^ v781;
    v782 = __ROR4__(v782, 9);
    v784 = (v764 & v754 ^ v774 & (v754 ^ v764)) + (v782 ^ v783) + v780;
    LODWORD(v1026) = _byteswap_ulong(*(_DWORD *)(v19 + 16));
    DWORD1(v1026) = _byteswap_ulong(*(_DWORD *)(v19 + 20));
    DWORD2(v1026) = _byteswap_ulong(*(_DWORD *)(v19 + 24));
    DWORD3(v1026) = _byteswap_ulong(*(_DWORD *)(v19 + 28));
    v21 = _mm_load_si128((const __m128i *)&v1026);
    _mm_store_si128((__m128i *)&v1026, _mm_add_epi32(_mm_loadu_si128((const __m128i *)(v704 + 16)), v21));
    v785 = __ROR4__(v779, 6);
    v786 = v785;
    v785 = __ROR4__(v785, 5);
    v787 = v785 ^ v786;
    v785 = __ROR4__(v785, 14);
    v788 = v1027 + (v785 ^ v787) + v749 + (v769 & v779 ^ v759 & ~v779);
    v789 = v788 + v754;
    v790 = v788;
    v791 = __ROR4__(v784, 2);
    v792 = __ROR4__(v784, 13);
    v793 = v792 ^ v791;
    v792 = __ROR4__(v792, 9);
    v794 = (v774 & v764 ^ v784 & (v764 ^ v774)) + (v792 ^ v793) + v790;
    v795 = __ROR4__(v789, 6);
    v796 = v795;
    v795 = __ROR4__(v795, 5);
    v797 = v795 ^ v796;
    v795 = __ROR4__(v795, 14);
    v798 = DWORD1(v1027) + (v795 ^ v797) + v759 + (v779 & v789 ^ v769 & ~v789);
    v799 = v798 + v764;
    v800 = v798;
    v801 = __ROR4__(v794, 2);
    v802 = __ROR4__(v794, 13);
    v803 = v802 ^ v801;
    v802 = __ROR4__(v802, 9);
    v804 = (v784 & v774 ^ v794 & (v774 ^ v784)) + (v802 ^ v803) + v800;
    v805 = __ROR4__(v799, 6);
    v806 = v805;
    v805 = __ROR4__(v805, 5);
    v807 = v805 ^ v806;
    v805 = __ROR4__(v805, 14);
    v808 = DWORD2(v1027) + (v805 ^ v807) + v769 + (v789 & v799 ^ v779 & ~v799);
    v809 = v808 + v774;
    v810 = v808;
    v811 = __ROR4__(v804, 2);
    v812 = __ROR4__(v804, 13);
    v813 = v812 ^ v811;
    v812 = __ROR4__(v812, 9);
    v814 = (v794 & v784 ^ v804 & (v784 ^ v794)) + (v812 ^ v813) + v810;
    v815 = __ROR4__(v809, 6);
    v816 = v815;
    v815 = __ROR4__(v815, 5);
    v817 = v815 ^ v816;
    v815 = __ROR4__(v815, 14);
    v818 = DWORD3(v1027) + (v815 ^ v817) + v779 + (v799 & v809 ^ v789 & ~v809);
    v819 = v818 + v784;
    v820 = v818;
    v821 = __ROR4__(v814, 2);
    v822 = __ROR4__(v814, 13);
    v823 = v822 ^ v821;
    v822 = __ROR4__(v822, 9);
    v824 = (v804 & v794 ^ v814 & (v794 ^ v804)) + (v822 ^ v823) + v820;
    LODWORD(v1027) = _byteswap_ulong(*(_DWORD *)(v19 + 32));
    DWORD1(v1027) = _byteswap_ulong(*(_DWORD *)(v19 + 36));
    DWORD2(v1027) = _byteswap_ulong(*(_DWORD *)(v19 + 40));
    DWORD3(v1027) = _byteswap_ulong(*(_DWORD *)(v19 + 44));
    v22 = _mm_load_si128((const __m128i *)&v1027);
    _mm_store_si128((__m128i *)&v1027, _mm_add_epi32(_mm_loadu_si128((const __m128i *)(v704 + 32)), v22));
    v825 = __ROR4__(v819, 6);
    v826 = v825;
    v825 = __ROR4__(v825, 5);
    v827 = v825 ^ v826;
    v825 = __ROR4__(v825, 14);
    v828 = v1028.m128i_i32[0] + (v825 ^ v827) + v789 + (v809 & v819 ^ v799 & ~v819);
    v829 = v828 + v794;
    v830 = v828;
    v831 = __ROR4__(v824, 2);
    v832 = __ROR4__(v824, 13);
    v833 = v832 ^ v831;
    v832 = __ROR4__(v832, 9);
    v834 = (v814 & v804 ^ v824 & (v804 ^ v814)) + (v832 ^ v833) + v830;
    v835 = __ROR4__(v829, 6);
    v836 = v835;
    v835 = __ROR4__(v835, 5);
    v837 = v835 ^ v836;
    v835 = __ROR4__(v835, 14);
    v838 = v1028.m128i_i32[1] + (v835 ^ v837) + v799 + (v819 & v829 ^ v809 & ~v829);
    v839 = v838 + v804;
    v840 = v838;
    v841 = __ROR4__(v834, 2);
    v842 = __ROR4__(v834, 13);
    v843 = v842 ^ v841;
    v842 = __ROR4__(v842, 9);
    v844 = (v824 & v814 ^ v834 & (v814 ^ v824)) + (v842 ^ v843) + v840;
    v845 = __ROR4__(v839, 6);
    v846 = v845;
    v845 = __ROR4__(v845, 5);
    v847 = v845 ^ v846;
    v845 = __ROR4__(v845, 14);
    v848 = v1028.m128i_i32[2] + (v845 ^ v847) + v809 + (v829 & v839 ^ v819 & ~v839);
    v849 = v848 + v814;
    v850 = v848;
    v851 = __ROR4__(v844, 2);
    v852 = __ROR4__(v844, 13);
    v853 = v852 ^ v851;
    v852 = __ROR4__(v852, 9);
    v854 = (v834 & v824 ^ v844 & (v824 ^ v834)) + (v852 ^ v853) + v850;
    v855 = __ROR4__(v849, 6);
    v856 = v855;
    v855 = __ROR4__(v855, 5);
    v857 = v855 ^ v856;
    v855 = __ROR4__(v855, 14);
    v858 = v1028.m128i_i32[3] + (v855 ^ v857) + v819 + (v839 & v849 ^ v829 & ~v849);
    v859 = v858 + v824;
    v860 = v858;
    v861 = __ROR4__(v854, 2);
    v862 = __ROR4__(v854, 13);
    v863 = v862 ^ v861;
    v862 = __ROR4__(v862, 9);
    v1028.m128i_i32[0] = _byteswap_ulong(*(_DWORD *)(v19 + 48));
    v1028.m128i_i32[1] = _byteswap_ulong(*(_DWORD *)(v19 + 52));
    v1028.m128i_i32[2] = _byteswap_ulong(*(_DWORD *)(v19 + 56));
    v1028.m128i_i32[3] = _byteswap_ulong(*(_DWORD *)(v19 + 60));
    v23 = _mm_load_si128(&v1028);
    _mm_store_si128((__m128i *)&v1028, _mm_add_epi32(_mm_loadu_si128((const __m128i *)(v704 + 48)), v23));
    v24 = (const __m128i *)(v704 + 64);
    v19 += 64LL;
    *(_DWORD *)a2 += (v844 & v834 ^ v854 & (v834 ^ v844)) + (v862 ^ v863) + v860;
    *(_DWORD *)(a2 + 4) += v854;
    *(_DWORD *)(a2 + 8) += v844;
    *(_DWORD *)(a2 + 12) += v834;
    *(_DWORD *)(a2 + 16) += v859;
    *(_DWORD *)(a2 + 20) += v849;
    *(_DWORD *)(a2 + 24) += v839;
    *(_DWORD *)(a2 + 28) += v829;
  }
  v864 = __ROR4__(v684, 6);
  v865 = v864;
  v864 = __ROR4__(v864, 5);
  v866 = v864 ^ v865;
  v864 = __ROR4__(v864, 14);
  v867 = v1025 + (v864 ^ v866) + v654 + (v674 & v684 ^ v664 & ~v684);
  v868 = v867 + v659;
  v869 = v867;
  v870 = __ROR4__(v689, 2);
  v871 = __ROR4__(v689, 13);
  v872 = v871 ^ v870;
  v871 = __ROR4__(v871, 9);
  v873 = (v679 & v669 ^ v689 & (v669 ^ v679)) + (v871 ^ v872) + v869;
  v874 = __ROR4__(v868, 6);
  v875 = v874;
  v874 = __ROR4__(v874, 5);
  v876 = v874 ^ v875;
  v874 = __ROR4__(v874, 14);
  v877 = DWORD1(v1025) + (v874 ^ v876) + v664 + (v684 & v868 ^ v674 & ~v868);
  v878 = v877 + v669;
  v879 = v877;
  v880 = __ROR4__(v873, 2);
  v881 = __ROR4__(v873, 13);
  v882 = v881 ^ v880;
  v881 = __ROR4__(v881, 9);
  v883 = (v689 & v679 ^ v873 & (v679 ^ v689)) + (v881 ^ v882) + v879;
  v884 = __ROR4__(v878, 6);
  v885 = v884;
  v884 = __ROR4__(v884, 5);
  v886 = v884 ^ v885;
  v884 = __ROR4__(v884, 14);
  v887 = DWORD2(v1025) + (v884 ^ v886) + v674 + (v868 & v878 ^ v684 & ~v878);
  v888 = v887 + v679;
  v889 = v887;
  v890 = __ROR4__(v883, 2);
  v891 = __ROR4__(v883, 13);
  v892 = v891 ^ v890;
  v891 = __ROR4__(v891, 9);
  v893 = (v873 & v689 ^ v883 & (v689 ^ v873)) + (v891 ^ v892) + v889;
  v894 = __ROR4__(v888, 6);
  v895 = v894;
  v894 = __ROR4__(v894, 5);
  v896 = v894 ^ v895;
  v894 = __ROR4__(v894, 14);
  v897 = DWORD3(v1025) + (v894 ^ v896) + v684 + (v878 & v888 ^ v868 & ~v888);
  v898 = v897 + v689;
  v899 = v897;
  v900 = __ROR4__(v893, 2);
  v901 = __ROR4__(v893, 13);
  v902 = v901 ^ v900;
  v901 = __ROR4__(v901, 9);
  v903 = (v883 & v873 ^ v893 & (v873 ^ v883)) + (v901 ^ v902) + v899;
  v904 = __ROR4__(v898, 6);
  v905 = v904;
  v904 = __ROR4__(v904, 5);
  v906 = v904 ^ v905;
  v904 = __ROR4__(v904, 14);
  v907 = v1026 + (v904 ^ v906) + v868 + (v888 & v898 ^ v878 & ~v898);
  v908 = v907 + v873;
  v909 = v907;
  v910 = __ROR4__(v903, 2);
  v911 = __ROR4__(v903, 13);
  v912 = v911 ^ v910;
  v911 = __ROR4__(v911, 9);
  v913 = (v893 & v883 ^ v903 & (v883 ^ v893)) + (v911 ^ v912) + v909;
  v914 = __ROR4__(v908, 6);
  v915 = v914;
  v914 = __ROR4__(v914, 5);
  v916 = v914 ^ v915;
  v914 = __ROR4__(v914, 14);
  v917 = DWORD1(v1026) + (v914 ^ v916) + v878 + (v898 & v908 ^ v888 & ~v908);
  v918 = v917 + v883;
  v919 = v917;
  v920 = __ROR4__(v913, 2);
  v921 = __ROR4__(v913, 13);
  v922 = v921 ^ v920;
  v921 = __ROR4__(v921, 9);
  v923 = (v903 & v893 ^ v913 & (v893 ^ v903)) + (v921 ^ v922) + v919;
  v924 = __ROR4__(v918, 6);
  v925 = v924;
  v924 = __ROR4__(v924, 5);
  v926 = v924 ^ v925;
  v924 = __ROR4__(v924, 14);
  v927 = DWORD2(v1026) + (v924 ^ v926) + v888 + (v908 & v918 ^ v898 & ~v918);
  v928 = v927 + v893;
  v929 = v927;
  v930 = __ROR4__(v923, 2);
  v931 = __ROR4__(v923, 13);
  v932 = v931 ^ v930;
  v931 = __ROR4__(v931, 9);
  v933 = (v913 & v903 ^ v923 & (v903 ^ v913)) + (v931 ^ v932) + v929;
  v934 = __ROR4__(v928, 6);
  v935 = v934;
  v934 = __ROR4__(v934, 5);
  v936 = v934 ^ v935;
  v934 = __ROR4__(v934, 14);
  v937 = DWORD3(v1026) + (v934 ^ v936) + v898 + (v918 & v928 ^ v908 & ~v928);
  v938 = v937 + v903;
  v939 = v937;
  v940 = __ROR4__(v933, 2);
  v941 = __ROR4__(v933, 13);
  v942 = v941 ^ v940;
  v941 = __ROR4__(v941, 9);
  v943 = (v923 & v913 ^ v933 & (v913 ^ v923)) + (v941 ^ v942) + v939;
  v944 = __ROR4__(v938, 6);
  v945 = v944;
  v944 = __ROR4__(v944, 5);
  v946 = v944 ^ v945;
  v944 = __ROR4__(v944, 14);
  v947 = v1027 + (v944 ^ v946) + v908 + (v928 & v938 ^ v918 & ~v938);
  v948 = v947 + v913;
  v949 = v947;
  v950 = __ROR4__(v943, 2);
  v951 = __ROR4__(v943, 13);
  v952 = v951 ^ v950;
  v951 = __ROR4__(v951, 9);
  v953 = (v933 & v923 ^ v943 & (v923 ^ v933)) + (v951 ^ v952) + v949;
  v954 = __ROR4__(v948, 6);
  v955 = v954;
  v954 = __ROR4__(v954, 5);
  v956 = v954 ^ v955;
  v954 = __ROR4__(v954, 14);
  v957 = DWORD1(v1027) + (v954 ^ v956) + v918 + (v938 & v948 ^ v928 & ~v948);
  v958 = v957 + v923;
  v959 = v957;
  v960 = __ROR4__(v953, 2);
  v961 = __ROR4__(v953, 13);
  v962 = v961 ^ v960;
  v961 = __ROR4__(v961, 9);
  v963 = (v943 & v933 ^ v953 & (v933 ^ v943)) + (v961 ^ v962) + v959;
  v964 = __ROR4__(v958, 6);
  v965 = v964;
  v964 = __ROR4__(v964, 5);
  v966 = v964 ^ v965;
  v964 = __ROR4__(v964, 14);
  v967 = DWORD2(v1027) + (v964 ^ v966) + v928 + (v948 & v958 ^ v938 & ~v958);
  v968 = v967 + v933;
  v969 = v967;
  v970 = __ROR4__(v963, 2);
  v971 = __ROR4__(v963, 13);
  v972 = v971 ^ v970;
  v971 = __ROR4__(v971, 9);
  v973 = (v953 & v943 ^ v963 & (v943 ^ v953)) + (v971 ^ v972) + v969;
  v974 = __ROR4__(v968, 6);
  v975 = v974;
  v974 = __ROR4__(v974, 5);
  v976 = v974 ^ v975;
  v974 = __ROR4__(v974, 14);
  v977 = DWORD3(v1027) + (v974 ^ v976) + v938 + (v958 & v968 ^ v948 & ~v968);
  v978 = v977 + v943;
  v979 = v977;
  v980 = __ROR4__(v973, 2);
  v981 = __ROR4__(v973, 13);
  v982 = v981 ^ v980;
  v981 = __ROR4__(v981, 9);
  v983 = (v963 & v953 ^ v973 & (v953 ^ v963)) + (v981 ^ v982) + v979;
  v984 = __ROR4__(v978, 6);
  v985 = v984;
  v984 = __ROR4__(v984, 5);
  v986 = v984 ^ v985;
  v984 = __ROR4__(v984, 14);
  v987 = v1028.m128i_i32[0] + (v984 ^ v986) + v948 + (v968 & v978 ^ v958 & ~v978);
  v988 = v987 + v953;
  v989 = v987;
  v990 = __ROR4__(v983, 2);
  v991 = __ROR4__(v983, 13);
  v992 = v991 ^ v990;
  v991 = __ROR4__(v991, 9);
  v993 = (v973 & v963 ^ v983 & (v963 ^ v973)) + (v991 ^ v992) + v989;
  v994 = __ROR4__(v988, 6);
  v995 = v994;
  v994 = __ROR4__(v994, 5);
  v996 = v994 ^ v995;
  v994 = __ROR4__(v994, 14);
  v997 = v1028.m128i_i32[1] + (v994 ^ v996) + v958 + (v978 & v988 ^ v968 & ~v988);
  v998 = v997 + v963;
  v999 = v997;
  v1000 = __ROR4__(v993, 2);
  v1001 = __ROR4__(v993, 13);
  v1002 = v1001 ^ v1000;
  v1001 = __ROR4__(v1001, 9);
  v1003 = (v983 & v973 ^ v993 & (v973 ^ v983)) + (v1001 ^ v1002) + v999;
  v1004 = __ROR4__(v998, 6);
  v1005 = v1004;
  v1004 = __ROR4__(v1004, 5);
  v1006 = v1004 ^ v1005;
  v1004 = __ROR4__(v1004, 14);
  v1007 = v1028.m128i_i32[2] + (v1004 ^ v1006) + v968 + (v988 & v998 ^ v978 & ~v998);
  v1008 = v1007 + v973;
  v1009 = v1007;
  v1010 = __ROR4__(v1003, 2);
  v1011 = __ROR4__(v1003, 13);
  v1012 = v1011 ^ v1010;
  v1011 = __ROR4__(v1011, 9);
  v1013 = (v993 & v983 ^ v1003 & (v983 ^ v993)) + (v1011 ^ v1012) + v1009;
  v1014 = __ROR4__(v1008, 6);
  v1015 = v1014;
  v1014 = __ROR4__(v1014, 5);
  v1016 = v1014 ^ v1015;
  v1014 = __ROR4__(v1014, 14);
  v1017 = v1028.m128i_i32[3] + (v1014 ^ v1016) + v978 + (v998 & v1008 ^ v988 & ~v1008);
  v1018 = v1017 + v983;
  v1019 = v1017;
  v1020 = __ROR4__(v1013, 2);
  v1021 = __ROR4__(v1013, 13);
  v1022 = v1021 ^ v1020;
  v1021 = __ROR4__(v1021, 9);
  v1023 = (v1021 ^ v1022) + v1019;
  result = v1003 & v993 ^ v1013 & (v993 ^ (unsigned int)v1003);
  *(_DWORD *)a2 += result + v1023;
  *(_DWORD *)(a2 + 4) += v1013;
  *(_DWORD *)(a2 + 8) += v1003;
  *(_DWORD *)(a2 + 12) += v993;
  *(_DWORD *)(a2 + 16) += v1018;
  *(_DWORD *)(a2 + 20) += v1008;
  *(_DWORD *)(a2 + 24) += v998;
  *(_DWORD *)(a2 + 28) += v988;
  return result;
}
// 66F00: using guessed type __int64 ccsha256_K[32];

//----- (00000000000428C0) ----------------------------------------------------
unsigned __int64 __fastcall ccec_compact_export(int a1, void *a2, __int64 **a3)
{
  __int64 **v3; // r14@1
  __int64 *v4; // rbx@1
  unsigned __int64 v5; // r12@1
  __int64 v6; // rbx@1
  __int64 v7; // r13@1
  unsigned __int64 v8; // rax@1
  size_t v9; // r14@1
  unsigned __int64 result; // rax@3
  __int64 v11; // r14@4
  unsigned __int64 v12; // rax@4
  size_t v13; // rbx@4
  __int64 **v14; // [sp+8h] [bp-38h]@1
  __int64 v15; // [sp+10h] [bp-30h]@4

  v3 = a3;
  v14 = a3;
  v4 = *a3;
  v5 = (unsigned __int64)(ccn_bitlen(**a3, (__int64)(*a3 + 2)) + 7) >> 3;
  v6 = *v4;
  v7 = (__int64)(v3 + 2);
  v8 = ccn_write_uint_size(v6, (__int64)(v3 + 2));
  v9 = 0LL;
  if ( v5 > v8 )
    v9 = v5 - v8;
  bzero(a2, v9);
  result = ccn_write_uint(v6, v7, v5 - v9, (__int64)((char *)a2 + v9));
  if ( a1 )
  {
    v11 = v7 + 24 * **v14;
    v12 = ccn_write_uint_size(v6, v11);
    v15 = v6;
    v13 = 0LL;
    if ( v5 > v12 )
      v13 = v5 - v12;
    bzero((char *)a2 + v5, v13);
    result = ccn_write_uint(v15, v11, v5 - v13, (__int64)((char *)a2 + v5 + v13));
  }
  return result;
}

//----- (00000000000429B1) ----------------------------------------------------
__int64 __fastcall ccsha512_ltc_compress(__int64 a1, signed __int64 a2, signed __int64 a3)
{
  __int64 v3; // rcx@2
  __int64 v4; // rax@3
  __int64 v7; // r8@5
  __int64 v8; // r11@5
  __int64 v9; // r10@6
  __int64 v10; // rcx@6
  unsigned __int64 v11; // rbx@6
  __int64 v12; // r10@6
  __int64 v13; // r8@6
  __int64 v14; // rcx@6
  __int64 v15; // rcx@7
  __int64 v16; // r9@7
  __int64 v17; // r13@7
  __int64 v18; // r11@7
  __int64 v19; // r12@7
  __int64 v20; // rdx@7
  __int64 v21; // r15@7
  __int64 v22; // r14@7
  __int64 v23; // r10@7
  __int64 v24; // r8@7
  __int64 v25; // rbx@8
  __int64 v26; // rax@8
  __int64 v27; // r8@8
  __int64 v28; // r8@8
  __int64 v29; // rbx@8
  __int64 v30; // rbx@8
  __int64 v31; // rbx@8
  __int64 v32; // rcx@8
  __int64 v33; // rax@8
  __int64 v34; // rax@8
  __int64 v35; // rcx@8
  __int64 v36; // r14@8
  __int64 v37; // rcx@8
  __int64 v38; // rax@8
  __int64 v39; // rsi@8
  __int64 v40; // rbx@8
  __int64 v41; // rbx@8
  __int64 v42; // rax@8
  __int64 v43; // rsi@8
  __int64 v44; // r13@8
  __int64 v45; // r15@8
  __int64 v46; // r13@8
  __int64 v47; // rax@8
  __int64 v48; // rsi@8
  __int64 v49; // rbx@8
  __int64 v50; // rbx@8
  __int64 v51; // rax@8
  __int64 v52; // rsi@8
  __int64 v53; // r11@8
  __int64 v54; // rdx@8
  __int64 v55; // r11@8
  __int64 v56; // rax@8
  __int64 v57; // rsi@8
  __int64 v58; // rbx@8
  __int64 v59; // rbx@8
  __int64 v60; // rax@8
  __int64 v61; // rsi@8
  __int64 v62; // r9@8
  __int64 v63; // r12@8
  __int64 v64; // r9@8
  __int64 v65; // rax@8
  __int64 v66; // rsi@8
  __int64 v67; // rbx@8
  __int64 v68; // rbx@8
  __int64 v69; // rax@8
  __int64 v70; // rsi@8
  __int64 v71; // r14@8
  __int64 v72; // rax@8
  __int64 v73; // rsi@8
  __int64 v74; // rbx@8
  __int64 v75; // rbx@8
  __int64 v76; // rax@8
  __int64 v77; // rsi@8
  __int64 v78; // r15@8
  __int64 v79; // rax@8
  __int64 v80; // rsi@8
  __int64 v81; // rbx@8
  __int64 v82; // rbx@8
  __int64 v83; // rax@8
  __int64 v84; // rsi@8
  __int64 v85; // rdx@8
  __int64 v86; // rax@8
  __int64 v87; // rsi@8
  __int64 v88; // rbx@8
  __int64 v89; // rbx@8
  __int64 v90; // rax@8
  __int64 v91; // rsi@8
  __int64 v92; // r12@8
  __int64 i; // rcx@9
  signed __int64 v95; // [sp+0h] [bp-300h]@5
  signed __int64 v96; // [sp+8h] [bp-2F8h]@5
  __int64 v97; // [sp+10h] [bp-2F0h]@4
  __int64 v98; // [sp+18h] [bp-2E8h]@6
  __int64 v99; // [sp+20h] [bp-2E0h]@8
  __int64 v100; // [sp+28h] [bp-2D8h]@8
  __int64 v101; // [sp+30h] [bp-2D0h]@8
  __int64 v102; // [sp+38h] [bp-2C8h]@8
  __int64 v103; // [sp+40h] [bp-2C0h]@8
  __int64 v104[2]; // [sp+48h] [bp-2B8h]@8
  __int64 v105[5]; // [sp+58h] [bp-2A8h]@6
  __int64 v106[2]; // [sp+80h] [bp-280h]@6
  __int64 v107[64]; // [sp+90h] [bp-270h]@6
  __int64 v108; // [sp+290h] [bp-70h]@3
  __int64 v109; // [sp+298h] [bp-68h]@7
  __int64 v110; // [sp+2A0h] [bp-60h]@7
  __int64 v111; // [sp+2A8h] [bp-58h]@7
  __int64 v112; // [sp+2B0h] [bp-50h]@7
  __int64 v113; // [sp+2B8h] [bp-48h]@7
  __int64 v114; // [sp+2C0h] [bp-40h]@7
  __int64 v115; // [sp+2C8h] [bp-38h]@7
  __int64 v116; // [sp+2D0h] [bp-30h]@1

  v116 = *(_QWORD *)off_69010[0];
  if ( a2 )
  {
    do
    {
      v3 = 0LL;
      do
      {
        *(&v108 + v3) = *(_QWORD *)(a1 + 8 * v3);
        ++v3;
        v4 = 0LL;
      }
      while ( v3 != 8 );
      do
      {
        _RCX = *(_QWORD *)(a3 + v4 * 8);
        __asm { bswap   rcx }
        *(__int64 *)((char *)&v97 + v4 * 8) = _RCX;
        ++v4;
      }
      while ( v4 != 16 );
      v95 = a2;
      v96 = a3;
      v7 = v97;
      v8 = 0LL;
      do
      {
        v9 = __ROR8__(v106[v8], 19);
        v10 = __ROR8__(v106[v8], 61);
        v11 = v7 + v105[v8] + (v9 ^ v10 ^ ((unsigned __int64)v106[v8] >> 6));
        v12 = *(&v98 + v8);
        v13 = __ROR8__(*(&v98 + v8), 1);
        v14 = __ROR8__(*(&v98 + v8), 8);
        v107[v8] = (v13 ^ v14 ^ ((unsigned __int64)*(&v98 + v8) >> 7)) + v11;
        ++v8;
        v7 = v12;
      }
      while ( v8 != 64 );
      v15 = v115;
      v16 = v112;
      v17 = v114;
      v18 = v113;
      v19 = v108;
      v20 = v109;
      v21 = v110;
      v22 = v111;
      v23 = 0LL;
      v24 = (__int64)K;
      do
      {
        v25 = __ROR8__(v16, 14);
        v26 = v24;
        v27 = __ROR8__(v16, 18);
        v28 = v25 ^ v27;
        v29 = __ROR8__(v16, 41);
        v30 = v28 ^ v29;
        v24 = v26;
        v31 = (v17 ^ v16 & (v17 ^ v18)) + *(&v97 + v23) + *(_QWORD *)(v26 + 8 * v23) + v15 + v30;
        v32 = __ROR8__(v19, 28);
        v33 = __ROR8__(v19, 34);
        v34 = v32 ^ v33;
        v35 = __ROR8__(v19, 39);
        v36 = v31 + v22;
        v37 = (v21 & (v19 | v20) | v19 & v20) + v31 + (v34 ^ v35);
        v38 = __ROR8__(v36, 14);
        v39 = __ROR8__(v36, 18);
        v40 = __ROR8__(v36, 41);
        v41 = (v18 ^ v36 & (v18 ^ v16)) + *(&v98 + v23) + *(_QWORD *)(v24 + 8 * v23 + 8) + v17 + (v38 ^ v39 ^ v40);
        v42 = __ROR8__(v37, 28);
        v43 = __ROR8__(v37, 34);
        v44 = __ROR8__(v37, 39);
        v45 = v41 + v21;
        v46 = (v20 & (v37 | v19) | v37 & v19) + v41 + (v42 ^ v43 ^ v44);
        v47 = __ROR8__(v45, 14);
        v48 = __ROR8__(v45, 18);
        v49 = __ROR8__(v45, 41);
        v50 = (v16 ^ v45 & (v16 ^ v36)) + *(&v99 + v23) + *(_QWORD *)(v24 + 8 * v23 + 16) + v18 + (v47 ^ v48 ^ v49);
        v51 = __ROR8__(v46, 28);
        v52 = __ROR8__(v46, 34);
        v53 = __ROR8__(v46, 39);
        v54 = v50 + v20;
        v55 = (v19 & (v46 | v37) | v46 & v37) + v50 + (v51 ^ v52 ^ v53);
        v56 = __ROR8__(v54, 14);
        v57 = __ROR8__(v54, 18);
        v58 = __ROR8__(v54, 41);
        v59 = (v36 ^ v54 & (v36 ^ v45)) + *(&v100 + v23) + *(_QWORD *)(v24 + 8 * v23 + 24) + v16 + (v56 ^ v57 ^ v58);
        v60 = __ROR8__(v55, 28);
        v61 = __ROR8__(v55, 34);
        v62 = __ROR8__(v55, 39);
        v63 = v59 + v19;
        v64 = (v37 & (v55 | v46) | v55 & v46) + v59 + (v60 ^ v61 ^ v62);
        v65 = __ROR8__(v63, 14);
        v66 = __ROR8__(v63, 18);
        v67 = __ROR8__(v63, 41);
        v68 = (v45 ^ v63 & (v45 ^ v54)) + *(&v101 + v23) + *(_QWORD *)(v24 + 8 * v23 + 32) + v36 + (v65 ^ v66 ^ v67);
        v69 = __ROR8__(v64, 28);
        v70 = __ROR8__(v64, 34);
        v71 = __ROR8__(v64, 39);
        v15 = v68 + v37;
        v22 = (v46 & (v64 | v55) | v64 & v55) + v68 + (v69 ^ v70 ^ v71);
        v72 = __ROR8__(v15, 14);
        v73 = __ROR8__(v15, 18);
        v74 = __ROR8__(v15, 41);
        v75 = (v54 ^ v15 & (v54 ^ v63)) + *(&v102 + v23) + *(_QWORD *)(v24 + 8 * v23 + 40) + v45 + (v72 ^ v73 ^ v74);
        v76 = __ROR8__(v22, 28);
        v77 = __ROR8__(v22, 34);
        v78 = __ROR8__(v22, 39);
        v17 = v75 + v46;
        v21 = (v55 & (v22 | v64) | v22 & v64) + v75 + (v76 ^ v77 ^ v78);
        v79 = __ROR8__(v17, 14);
        v80 = __ROR8__(v17, 18);
        v81 = __ROR8__(v17, 41);
        v82 = (v63 ^ v17 & (v63 ^ v15)) + *(&v103 + v23) + *(_QWORD *)(v24 + 8 * v23 + 48) + v54 + (v79 ^ v80 ^ v81);
        v83 = __ROR8__(v21, 28);
        v84 = __ROR8__(v21, 34);
        v85 = __ROR8__(v21, 39);
        v18 = v82 + v55;
        v20 = (v64 & (v21 | v22) | v21 & v22) + v82 + (v83 ^ v84 ^ v85);
        v86 = __ROR8__(v18, 14);
        v87 = __ROR8__(v18, 18);
        v88 = __ROR8__(v18, 41);
        v89 = (v15 ^ v18 & (v15 ^ v17)) + v104[v23] + *(_QWORD *)(v24 + 8 * v23 + 56) + v63 + (v86 ^ v87 ^ v88);
        v90 = __ROR8__(v20, 28);
        v91 = __ROR8__(v20, 34);
        v92 = __ROR8__(v20, 39);
        v16 = v89 + v64;
        v19 = (v22 & (v20 | v21) | v21 & v20) + v89 + (v90 ^ v91 ^ v92);
        v23 += 8LL;
      }
      while ( (signed int)v23 < 80 );
      v115 = v15;
      v112 = v16;
      v114 = v17;
      v113 = v18;
      v108 = v19;
      v109 = v20;
      v110 = v21;
      v111 = v22;
      for ( i = 0LL; ; ++i )
      {
        *(_QWORD *)(a1 + 8 * i) += v19;
        if ( i == 7 )
          break;
        v19 = *(&v109 + i);
      }
      a3 = v96 + 128;
      a2 = v95 - 1;
    }
    while ( v95 != 1 );
  }
  return *(_QWORD *)off_69010[0];
}
// 670B0: using guessed type __int64 K[80];
// 69010: using guessed type __int64 off_69010[2];
// 429B1: using guessed type __int64 var_280[2];
// 429B1: using guessed type __int64 var_2A8[5];
// 429B1: using guessed type __int64 var_270[64];
// 429B1: using guessed type __int64 var_2B8[2];

//----- (0000000000042EC7) ----------------------------------------------------
__int64 __fastcall ccsha512_final(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // rbx@1
  __int64 v4; // r14@1
  __int64 v5; // rax@1
  signed __int64 v6; // rcx@1
  __int64 v7; // r15@1
  __int64 v8; // rdx@1
  __int64 v9; // rdx@1
  unsigned __int64 v10; // rax@1
  signed __int64 v11; // rcx@1
  unsigned __int64 v12; // rsi@1
  int v13; // eax@3
  unsigned __int64 v14; // rax@4
  signed __int64 v15; // rsi@4
  bool v16; // cf@4
  unsigned __int64 v17; // rcx@6
  signed __int64 v18; // rsi@7
  int v19; // edi@7
  __int64 result; // rax@9
  signed int v24; // ecx@10
  unsigned __int64 v25; // rdx@10

  v3 = a2;
  v4 = a1;
  v5 = *(_QWORD *)(a1 + 8);
  v6 = *(_QWORD *)(a1 + 16) + v5 + 8;
  *(_QWORD *)v3 += (unsigned int)(8 * *(_DWORD *)(a2 + v6));
  v7 = a3;
  v8 = *(_DWORD *)(a2 + v6);
  *(_DWORD *)(a2 + v6) = v8 + 1;
  *(_BYTE *)(a2 + v5 + v8 + 8) = -128;
  v9 = *(_QWORD *)(a1 + 8);
  v10 = *(_QWORD *)(a1 + 16);
  v11 = v9 + v10 + 8;
  v12 = *(_DWORD *)(a2 + v11);
  if ( v12 > v10 - 16 )
  {
    if ( v12 < v10 )
    {
      v11 += v3;
      v13 = v12;
      do
      {
        *(_DWORD *)v11 = v13 + 1;
        *(_BYTE *)(v3 + v12 + v9 + 8) = 0;
        v9 = *(_QWORD *)(a1 + 8);
        v14 = *(_QWORD *)(a1 + 16);
        v15 = v9 + v14 + 8;
        v11 = v3 + v15;
        v12 = *(_DWORD *)(v3 + v15);
        v16 = v12 < v14;
        v13 = v12;
      }
      while ( v16 );
    }
    (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(a1 + 48))(
      v3 + 8,
      1LL,
      v3 + v9 + 8,
      v11);
    v9 = *(_QWORD *)(a1 + 8);
    v10 = *(_QWORD *)(a1 + 16);
    *(_DWORD *)(v3 + v9 + v10 + 8) = 0;
  }
  v17 = *(_DWORD *)(v3 + v9 + v10 + 8);
  if ( v17 < v10 - 8 )
  {
    v18 = v3 + v9 + v10 + 8;
    v19 = v17;
    do
    {
      *(_DWORD *)v18 = v19 + 1;
      *(_BYTE *)(v3 + v17 + v9 + 8) = 0;
      v9 = *(_QWORD *)(v4 + 8);
      v10 = *(_QWORD *)(v4 + 16);
      v18 = v3 + v9 + v10 + 8;
      v17 = *(_DWORD *)v18;
      v19 = *(_DWORD *)v18;
    }
    while ( v17 < v10 - 8 );
  }
  _RCX = *(_QWORD *)v3;
  __asm { bswap   rcx }
  *(_QWORD *)(v3 + v9 + v10) = _RCX;
  __asm { bswap   rcx }
  (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, __int64))(v4 + 48))(
    v3 + 8,
    1LL,
    *(_QWORD *)(v4 + 8) + v3 + 8,
    _RCX);
  result = 0LL;
  if ( *(_QWORD *)v4 >= 8uLL )
  {
    v24 = 1;
    v25 = 0LL;
    do
    {
      _RDX = *(_QWORD *)(v3 + 8 * v25 + 8);
      __asm { bswap   rdx }
      *(_QWORD *)(v7 + (unsigned int)result) = _RDX;
      __asm { bswap   rdx }
      v25 = (unsigned int)v24++;
      result = (unsigned int)(result + 8);
    }
    while ( v25 < *(_QWORD *)v4 >> 3 );
  }
  return result;
}

//----- (000000000004301F) ----------------------------------------------------
__int64 __fastcall cczp_add(__int64 *a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  __int64 v5; // rbx@1
  __int64 result; // rax@2

  v4 = *a1;
  v5 = (__int64)(a1 + 2);
  if ( ccn_add(*a1, a2, a3, a4) || (result = ccn_cmp(v4, a2, v5), (signed int)result >= 0) )
    result = ccn_sub(v4, a2, a2, v5);
  return result;
}

//----- (000000000004307B) ----------------------------------------------------
unsigned __int64 __fastcall ccecies_pub_key_size(__int64 **a1, __int64 a2)
{
  unsigned __int64 result; // rax@1

  result = 0LL;
  if ( *(_BYTE *)(a2 + 32) & 2 )
    result = ((unsigned __int64)(ccn_bitlen(**a1, (__int64)(*a1 + 2)) + 7) >> 2) | 1;
  return result;
}

//----- (00000000000430A4) ----------------------------------------------------
__int64 __fastcall cczp_div(unsigned __int64 a1, __int64 a2, void *a3, unsigned __int64 a4)
{
  __int64 v4; // r15@1
  signed __int64 v5; // r12@1
  __int64 v6; // r14@1
  __int64 v7; // r13@1
  size_t v8; // r15@1
  __int64 v9; // rax@2
  unsigned __int64 *v10; // r15@5
  __int64 v11; // r15@5
  signed __int64 v12; // r15@5
  __int64 v14; // [sp+0h] [bp-80h]@1
  __int64 v15; // [sp+8h] [bp-78h]@1
  __int64 v16; // [sp+10h] [bp-70h]@1
  void *v17; // [sp+18h] [bp-68h]@1
  unsigned __int64 v18; // [sp+20h] [bp-60h]@1
  unsigned __int64 v19; // [sp+28h] [bp-58h]@1
  __int64 v20; // [sp+30h] [bp-50h]@1
  __int64 v21; // [sp+38h] [bp-48h]@1
  __int64 *v22; // [sp+40h] [bp-40h]@1
  __int64 v23; // [sp+48h] [bp-38h]@1
  __int64 v24; // [sp+50h] [bp-30h]@1

  v19 = a4;
  v17 = a3;
  v18 = a1;
  v24 = *(_QWORD *)off_69010[0];
  v4 = *(_QWORD *)a1;
  v23 = v4;
  v16 = v4 + 1;
  v15 = a1 + 8 * v4 + 16;
  v21 = ccn_bitlen(v4 + 1, v15);
  v5 = 2 * v4;
  v20 = (__int64)(&v14 - 2 * v4);
  v22 = &v14 + -2 * v4 - 2;
  v6 = (__int64)(&v14 - 2 * v4);
  v7 = (__int64)(&v14 - 2 * v4);
  bzero(&v14 - 2 * v4, 16 * v4);
  ccn_set(v4, &v14 - 2 * v4, (const void *)(v18 + 16));
  ccn_set(2 * v4, &v14 - 2 * v4, (const void *)v19);
  v8 = 8 * v4;
  bzero((void *)a2, v8);
  if ( v21 == 1 )
  {
    bzero(v17, v8);
    v9 = off_69010[0];
  }
  else if ( (signed int)ccn_cmp(v5, v7, v6) < 0 )
  {
    ccn_set(v23, v17, (const void *)v7);
    v9 = off_69010[0];
  }
  else
  {
    v19 = 2 * v23 + 2;
    v18 = v21 - 2;
LABEL_5:
    v10 = (unsigned __int64 *)v20;
    ccn_shift_right_multi(v5, (void *)v20, (void *)v7, v18);
    ccn_mul(v16, (__int64)v22, v15, v10);
    ccn_shift_right_multi(v19, v22, v22, v21);
    v11 = v23;
    ccn_mul(v23, v20, v6, (unsigned __int64 *)v22);
    ccn_sub(v5, v7, v7, v20);
    ccn_add(v11, a2, a2, (__int64)v22);
    v12 = 1LL;
    while ( (signed int)ccn_cmp(v5, v7, v6) >= 0 )
    {
      ccn_sub(v5, v7, v7, v6);
      ccn_add1(v23, a2, a2, 1LL);
      ++v12;
      if ( (unsigned __int64)v12 >= 4 )
        goto LABEL_5;
    }
    ccn_set(v23, v17, (const void *)v7);
    v9 = off_69010[0];
  }
  return *(_QWORD *)v9;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000432D4) ----------------------------------------------------
__int64 __fastcall cczp_div2(__int64 *a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r14@1
  __int64 v4; // rbx@2
  __int64 result; // rax@2

  v3 = *a1;
  if ( *(_BYTE *)a3 & 1 )
  {
    v4 = ccn_add(*a1, a2, a3, (__int64)(a1 + 2));
    result = ccn_shift_right(v3, (void *)a2, (const void *)a2, 1LL);
    *(_QWORD *)(a2 + 8 * v3 - 8) |= v4 << 63;
  }
  else
  {
    result = ccn_shift_right(*a1, (void *)a2, (const void *)a3, 1LL);
  }
  return result;
}

//----- (0000000000043342) ----------------------------------------------------
__int64 __fastcall cczp_init(__int64 a1)
{
  __int64 v1; // rax@1
  __int64 v2; // rdi@1
  void *v3; // rsi@1
  __int64 v4; // rdx@1
  __int64 result; // rax@2

  v1 = a1;
  *(_QWORD *)(a1 + 8) = cczp_mod;
  v2 = *(_QWORD *)a1;
  v3 = (void *)(v1 + 8 * v2 + 16);
  v4 = v1 + 16;
  if ( (unsigned __int64)v2 < 0x1F )
    result = ccn_make_recip_newtonraphson(v2, v3, v4);
  else
    result = ccn_make_recip_shift_sub(v2, v3, v4);
  return result;
}

//----- (0000000000043375) ----------------------------------------------------
__int64 __fastcall ccn_make_recip_shift_sub(__int64 a1, void *a2, __int64 a3)
{
  signed __int64 v3; // rax@1
  unsigned __int64 v4; // r13@1
  __int64 v5; // rbx@1
  char *v6; // r12@1
  __int64 v7; // r15@1
  signed __int64 v8; // r14@4
  unsigned __int64 v9; // r13@4
  __int64 v10; // r12@4
  void *v11; // rbx@4
  __int64 v12; // rcx@5
  signed __int64 v13; // r15@5
  __int64 v14; // r14@5
  unsigned __int64 v15; // rbx@7
  __int64 v16; // rax@7
  signed __int64 v17; // rdx@7
  __int64 v19; // [sp+0h] [bp-70h]@1
  __int64 v20; // [sp+8h] [bp-68h]@2
  const void *v21; // [sp+10h] [bp-60h]@1
  void *v22; // [sp+18h] [bp-58h]@4
  void *v23; // [sp+20h] [bp-50h]@1
  unsigned __int64 v24; // [sp+28h] [bp-48h]@3
  char *v25; // [sp+30h] [bp-40h]@1
  char *v26; // [sp+38h] [bp-38h]@1
  __int64 v27; // [sp+40h] [bp-30h]@1

  v21 = (const void *)a3;
  v23 = a2;
  v27 = *(_QWORD *)off_69010[0];
  v3 = ccn_bitlen(a1, a3);
  v4 = v3;
  v5 = (unsigned __int64)(v3 + 64) >> 6;
  v6 = (char *)&v19 - ((24 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v7 = (__int64)&v6[8 * v5];
  v25 = &v6[8 * v5];
  v26 = &v6[16 * v5];
  if ( v5 >= (unsigned __int64)a1 )
  {
    v24 = v3;
    v20 = a1;
    ccn_set(a1, (char *)&v19 - ((24 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL), v21);
    v4 = v24;
    bzero(&v6[8 * a1], 8 * (v5 - a1));
  }
  else
  {
    v20 = a1;
    ccn_set(v5, (char *)&v19 - ((24 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL), v21);
  }
  v21 = (const void *)(8 * v5);
  v19 = (__int64)&v6[8 * v5];
  bzero(&v6[8 * v5], 8 * v5);
  v8 = 1LL << v4;
  v24 = v4;
  v9 = v4 >> 6;
  *(_QWORD *)&v6[8 * (v5 + v9)] |= v8;
  v22 = (char *)&v19 - ((24 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v10 = v5;
  ccn_sub(v5, v7, v7, (__int64)((char *)&v19 - ((24 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL)));
  v11 = v23;
  bzero(v23, 8 * v20 + 8);
  *((_QWORD *)v11 + v9) |= v8;
  if ( ((v24 - 1) & 0x8000000000000000LL) == 0LL )
  {
    v12 = v24 - 2;
    LODWORD(v13) = 0;
    v14 = v19;
    while ( 1 )
    {
      v24 = v12;
      v15 = v12 + 1;
      ccn_add(v10, v14, v14, v14);
      v16 = ccn_sub(v10, (__int64)(&v25)[8 * ((unsigned int)v13 ^ 1)], v14, (__int64)v22);
      v13 = v16 ^ (unsigned int)v13 ^ 1;
      v17 = 1LL << v15;
      if ( v16 == 1 )
        *((_QWORD *)v23 + (v15 >> 6)) &= ~v17;
      else
        *((_QWORD *)v23 + (v15 >> 6)) |= v17;
      if ( (v24 & 0x8000000000000000LL) != 0LL )
        break;
      v14 = (__int64)(&v25)[8 * (unsigned int)v13];
      v12 = v24 - 1;
    }
  }
  cc_clear((unsigned __int64)(3LL * (_QWORD)v21) >> 3, v22);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000043573) ----------------------------------------------------
__int64 __fastcall ccn_make_recip_newtonraphson(__int64 a1, void *a2, __int64 a3)
{
  __int64 v3; // r12@1
  __int64 v4; // rbx@1
  signed __int64 v5; // rax@2
  unsigned __int64 v6; // rdx@2
  signed __int64 v7; // rax@2
  __int64 v8; // rbx@2
  __int64 v9; // rax@2
  signed __int64 v10; // r13@2
  signed __int64 v11; // r15@3
  __int64 v12; // r14@3
  size_t v13; // rsi@3
  __int64 v14; // r14@4
  __int64 v15; // r12@4
  __int64 v16; // rdi@4
  unsigned __int64 *v17; // r12@5
  __int64 v18; // r12@6
  __int64 v19; // r15@7
  unsigned __int64 *v20; // r14@7
  __int64 v21; // r13@7
  __int64 v22; // r14@9
  __int64 v23; // r15@9
  __int64 v24; // r15@10
  __int64 v25; // r14@10
  signed __int64 v26; // rax@10
  __int64 v27; // r13@10
  bool v28; // cf@10
  bool v29; // zf@10
  __int64 v30; // rsi@10
  __int64 v31; // rdx@10
  __int64 v32; // r15@11
  signed __int64 v33; // rax@11
  __int64 v34; // r13@13
  void *v35; // r12@13
  __int64 v37; // [sp+0h] [bp-C0h]@2
  size_t v38; // [sp+10h] [bp-B0h]@5
  __int64 v39; // [sp+18h] [bp-A8h]@2
  __int64 v40; // [sp+20h] [bp-A0h]@2
  __int64 *v41; // [sp+28h] [bp-98h]@2
  void *v42; // [sp+30h] [bp-90h]@2
  __int64 v43; // [sp+38h] [bp-88h]@3
  __int64 v44; // [sp+40h] [bp-80h]@2
  void *v45; // [sp+48h] [bp-78h]@2
  __int64 v46; // [sp+50h] [bp-70h]@2
  __int64 v47; // [sp+58h] [bp-68h]@2
  void *v48; // [sp+60h] [bp-60h]@2
  __int64 v49; // [sp+68h] [bp-58h]@2
  __int64 v50; // [sp+70h] [bp-50h]@2
  __int64 v51; // [sp+78h] [bp-48h]@2
  __int64 v52; // [sp+80h] [bp-40h]@2
  __int64 v53; // [sp+88h] [bp-38h]@2
  __int64 v54; // [sp+90h] [bp-30h]@1

  v3 = a3;
  v4 = off_69010[0];
  v54 = *(_QWORD *)off_69010[0];
  if ( ccn_n(a1, a3) )
  {
    v42 = a2;
    v49 = v3;
    v5 = ccn_bitlen(a1, v3);
    v39 = v5;
    v41 = &v37;
    v45 = (void *)((unsigned __int64)v5 >> 6);
    v47 = v5 & 0x3F;
    v40 = v5 + 1;
    v6 = (unsigned __int64)(v5 + 65) >> 6;
    v51 = v6;
    v7 = 7 * v6 - ((unsigned __int64)v5 >> 6);
    v8 = (__int64)((char *)&v37 - ((8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL));
    v53 = v8 + 8 * v7;
    v44 = 2 * v6 - (_QWORD)v45;
    v46 = v8 + 8 * v44;
    v9 = v44 + 2 * v6;
    v50 = v8 + 8 * v9;
    v10 = v9 + 2 * v6;
    v48 = (void *)(v8 + 8 * v10);
    v52 = v8 + 8 * (v10 + v6);
    if ( v6 <= a1 )
    {
      v43 = a1;
      v14 = v51;
      v11 = 2 * v6;
      v15 = 8 * v51;
      bzero((void *)(v8 + 8 * (v51 + v9)), 8 * v51);
      v16 = v14;
      v12 = v50;
      ccn_set(v16, (void *)v50, (const void *)v49);
      v13 = v15;
    }
    else
    {
      v11 = 2 * v6;
      bzero((void *)(v8 + 8 * (a1 + v9)), 8 * (2 * v6 - a1));
      v43 = a1;
      v12 = v50;
      ccn_set(a1, (void *)v50, (const void *)v49);
      v13 = 8 * v51;
    }
    v38 = v13;
    v50 = v12;
    v17 = (unsigned __int64 *)v48;
    bzero(v48, v13);
    *(_QWORD *)(v8 + 8 * (((unsigned __int64)v40 >> 6) + v10)) |= 1LL << v40;
    ccn_sub(v51, (__int64)v17, (__int64)v17, v12);
    v49 = v8 + 8 * v11;
    v40 = v11;
    v45 = (void *)(v8 + 8LL * (_QWORD)v45);
    do
    {
      ccn_mul_ws(v51, v46, (__int64)v17, v17);
      v18 = v47;
      if ( v47 )
      {
        v19 = v44;
        v20 = (unsigned __int64 *)v49;
        ccn_shift_right(v44, (void *)v49, (const void *)v49, v47);
        ccn_mul_ws(v51, v8, v50, v20);
        v21 = (__int64)v45;
        ccn_shift_right(v19, v45, v45, v18);
      }
      else
      {
        ccn_mul_ws(v51, v8, v50, (unsigned __int64 *)v49);
        v21 = (__int64)v45;
      }
      v22 = v51;
      v17 = (unsigned __int64 *)v48;
      v23 = ccn_sub(v51, v21, v21, (__int64)v48);
      ccn_sub(v22, (__int64)v17, (__int64)v17, v21);
    }
    while ( v23 );
    ccn_mul_ws(v51, v8, (__int64)v17, (unsigned __int64 *)v50);
    v24 = v40;
    *(_QWORD *)(v8 + 8 * v40) = 1LL;
    bzero((void *)(v8 + 8 * (v24 | 1)), v38 - 8);
    v25 = v24;
    v26 = ccn_bitlen(v24, v8);
    v27 = 2 * v39;
    v28 = v26 < (unsigned __int64)(2 * v39);
    v29 = v26 == 2 * v39;
    v30 = v51;
    v31 = (__int64)v17;
    while ( !v28 && !v29 )
    {
      v32 = v31;
      ccn_sub(v30, v31, v31, v49);
      ccn_sub(v25, v8, v8, v50);
      v33 = ccn_bitlen(v25, v8);
      v28 = v33 < (unsigned __int64)v27;
      v29 = v33 == v27;
      v31 = v32;
    }
    v34 = v43 + 1;
    v35 = v42;
    ccn_set(v30, v42, (const void *)v31);
    bzero((char *)v35 + 8 * v30, 8 * (v34 - v30));
    v52 = v8;
    cc_clear((v53 - v8) >> 3, (void *)v8);
    v52 = 0LL;
    v53 = 0LL;
    v4 = off_69010[0];
  }
  return *(_QWORD *)v4;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004392F) ----------------------------------------------------
signed __int64 __fastcall ccec_affine_point_from_x(__int64 *a1, void *a2, const void *a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // r15@1
  __int64 v6; // r12@1
  __int64 v7; // r13@1

  v4 = a4;
  v5 = (__int64)a3;
  v6 = *a1;
  v7 = a4 + 8 * *a1;
  ccn_set(*a1, a2, a3);
  cczp_sqr(a1, v4, v5);
  cczp_mul(a1, v4, v4, (unsigned __int64 *)v5);
  cczp_add(a1, v7, v5, v5);
  cczp_add(a1, v7, v7, v5);
  cczp_sub(a1, v4, v4, v7);
  cczp_add(a1, v4, v4, (__int64)&a1[v6 + 2]);
  return cczp_sqrt((__int64)a1, (__int64)((char *)a2 + 8 * v6), v4);
}

//----- (00000000000439DF) ----------------------------------------------------
void __fastcall cczp_mod(signed __int64 *a1, void *a2, __int64 a3, void **a4)
{
  signed __int64 *v4; // r12@1
  __int64 v5; // r15@1
  signed __int64 v6; // r14@1
  __int64 v7; // r13@1
  signed __int64 v8; // rax@1
  char v9; // ST18_1@4
  unsigned __int64 v10; // ST10_8@4
  char v11; // ST28_1@4
  void *v12; // rbx@4
  __int64 v13; // ST08_8@4
  __int64 v14; // ST20_8@4
  __int64 v15; // r15@4
  signed __int64 v16; // ST00_8@4
  __int64 v17; // rdi@4
  __int64 v18; // r14@4
  signed int v19; // ebx@4
  signed __int64 i; // r12@4
  __int64 v21; // rax@6
  __int64 v22; // rax@7
  void *v23; // [sp+30h] [bp-70h]@4
  __int64 v24; // [sp+40h] [bp-60h]@1
  __int64 v25; // [sp+48h] [bp-58h]@1
  signed __int64 v26; // [sp+50h] [bp-50h]@4
  void **v27; // [sp+58h] [bp-48h]@1
  void *v28; // [sp+60h] [bp-40h]@4
  __int64 v29; // [sp+68h] [bp-38h]@4
  __int64 v30; // [sp+70h] [bp-30h]@1

  v27 = a4;
  v25 = a3;
  v4 = a1;
  v5 = off_69010[0];
  v30 = *(_QWORD *)off_69010[0];
  v6 = *a1;
  v7 = *a1 + 1;
  v24 = (__int64)&a1[*a1 + 2];
  v8 = ccn_bitlen(v7, v24);
  if ( v8 == 1 )
  {
    if ( *(_QWORD *)v5 == v30 )
      bzero(a2, 8 * v6);
  }
  else
  {
    v9 = v8 - 2;
    v10 = (unsigned __int64)(v8 - 2) >> 6;
    v11 = v8;
    v12 = *v27;
    v23 = v12;
    v26 = v6;
    v13 = 2 * v6;
    v14 = 2 * v6 + 2 - ((unsigned __int64)v8 >> 6);
    v15 = (__int64)((char *)*v27 + 8 * v14);
    v16 = v14 + 2 * v6 + 2;
    v17 = v6;
    v18 = (__int64)((char *)*v27 + 8 * v16);
    *v27 = (char *)*v27 + 40 * v7;
    v28 = v12;
    v29 = v15;
    ccn_set(v17, (void *)v18, v4 + 2);
    bzero((char *)v12 + 8 * (v26 + v16), 8uLL);
    ccn_shift_right(v13 - v10, v12, (const void *)(v25 + 8 * v10), v9 & 0x3F);
    ccn_mul_ws(v7, v15, v24, (unsigned __int64 *)v12);
    ccn_shift_right(v14, v12, (char *)v12 + 8 * v13 + 16, v11 & 0x3F);
    ccn_mul_ws(v26, v15, v18, (unsigned __int64 *)v12);
    ccn_sub(v13, v15, v25, v15);
    v19 = 3;
    for ( i = 0LL; ; i = v21 ^ 1 )
    {
      v21 = i ^ ccn_sub(v7, (__int64)*(&v28 + i), v15, v18);
      v15 = (__int64)*(&v28 + v21);
      --v19;
      if ( !v19 )
        break;
    }
    ccn_set(v26, a2, *(&v28 + v21));
    *v27 = v23;
    v22 = *(_QWORD *)off_69010[0];
  }
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000043BEB) ----------------------------------------------------
__int64 __fastcall cczp_modn(unsigned __int64 a1, void *a2, unsigned __int64 a3, __int64 a4)
{
  unsigned __int64 v4; // r12@1
  __int64 v5; // rbx@1
  __int64 v6; // r13@1
  signed __int64 v7; // rax@1
  signed __int64 v8; // r15@1
  unsigned __int64 v9; // rax@5
  __int64 v10; // r14@5
  __int64 v11; // rbx@5
  char *v12; // r12@5
  __int64 v13; // r13@5
  void *v14; // rbx@6
  signed __int64 v15; // rbx@6
  __int64 *v17; // [sp+0h] [bp-80h]@5
  void *v18; // [sp+8h] [bp-78h]@5
  __int64 **v19; // [sp+10h] [bp-70h]@5
  __int64 *v20; // [sp+18h] [bp-68h]@5
  unsigned __int64 v21; // [sp+20h] [bp-60h]@5
  unsigned __int64 v22; // [sp+28h] [bp-58h]@1
  __int64 v23; // [sp+30h] [bp-50h]@1
  unsigned __int64 v24; // [sp+38h] [bp-48h]@5
  __int64 v25; // [sp+40h] [bp-40h]@1
  __int64 v26; // [sp+48h] [bp-38h]@1
  __int64 v27; // [sp+50h] [bp-30h]@1

  v23 = a4;
  v4 = a3;
  v22 = a1;
  v5 = off_69010[0];
  v27 = *(_QWORD *)off_69010[0];
  v6 = *(_QWORD *)a1;
  v26 = *(_QWORD *)a1 + 1LL;
  v25 = a1 + 8 * v6 + 16;
  v7 = ccn_bitlen(v26, v25);
  v8 = v4;
  if ( 2 * v6 > v4 )
    v8 = 2 * v6;
  if ( v7 == 1 )
  {
    bzero(a2, 8 * v6);
  }
  else
  {
    v18 = a2;
    v17 = (__int64 *)&v17;
    v24 = v7;
    v9 = (8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL;
    v20 = (__int64 *)((char *)&v17 - v9);
    v21 = 2 * v6 + 2;
    v19 = (__int64 **)((char *)&v17 - ((16 * v6 + 31) & 0xFFFFFFFFFFFFFFF0LL));
    v10 = (__int64)((char *)&v17 - v9);
    v11 = v4;
    v12 = (char *)&v17 - v9;
    ccn_set(v6, (char *)&v17 - v9, (const void *)(v22 + 16));
    bzero((void *)(v10 + 8 * v6), 8 * (v8 - v6));
    ccn_set(v11, v12, (const void *)v23);
    bzero(&v12[8 * v11], 8 * (v8 - v11));
    v22 = v24 - 2;
    v23 = v6;
    v13 = (__int64)v20;
LABEL_6:
    ccn_shift_right_multi(v8, (void *)v13, v12, v22);
    v14 = v19;
    ccn_mul(v26, (__int64)v19, v25, (unsigned __int64 *)v13);
    ccn_shift_right_multi(v21, v14, v14, v24);
    ccn_mul(v23, v13, v10, (unsigned __int64 *)v14);
    ccn_sub(v8, (__int64)v12, (__int64)v12, v13);
    v15 = 1LL;
    while ( (signed int)ccn_cmp(v8, (__int64)v12, v10) >= 0 )
    {
      ccn_sub(v8, (__int64)v12, (__int64)v12, v10);
      ++v15;
      if ( (unsigned __int64)v15 >= 4 )
        goto LABEL_6;
    }
    ccn_set(v23, v18, v12);
    v5 = off_69010[0];
  }
  return *(_QWORD *)v5;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000043DE1) ----------------------------------------------------
__int64 __fastcall ccrsa_import_pub(__int64 a1, __int64 a2, unsigned __int64 a3)
{
  return ccder_decode_rsa_pub(a1, a3, a3 + a2) == 0;
}

//----- (0000000000043E01) ----------------------------------------------------
signed __int64 __fastcall cczp_mod_inv(__int64 a1, void *a2, const void *a3)
{
  __int64 v3; // r12@1
  unsigned __int64 v4; // rax@1
  char *v5; // r14@1
  char *v6; // r15@1
  __int64 v7; // r13@1
  __int64 v8; // rbx@1
  signed __int64 v9; // rax@3
  signed __int64 v10; // rdx@3
  signed __int64 v11; // rbx@5
  bool v12; // zf@5
  __int64 v13; // rax@5
  __int64 *v14; // rbx@8
  __int64 *v15; // r13@11
  int v16; // ecx@13
  __int64 *v17; // rdi@14
  __int64 v18; // rsi@14
  __int64 v19; // rdx@14
  __int64 v20; // rcx@14
  signed __int64 result; // rax@15
  __int64 v22; // rcx@21
  __int64 v23; // [sp+0h] [bp-60h]@1
  void *v24; // [sp+8h] [bp-58h]@1
  size_t v25; // [sp+10h] [bp-50h]@1
  __int64 v26; // [sp+18h] [bp-48h]@1
  __int64 v27; // [sp+20h] [bp-40h]@1
  __int64 v28; // [sp+28h] [bp-38h]@1
  __int64 v29; // [sp+30h] [bp-30h]@1

  v24 = a2;
  v28 = a1;
  v29 = *(_QWORD *)off_69010[0];
  v3 = *(_QWORD *)a1;
  v4 = (8LL * *(_QWORD *)a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v5 = (char *)&v23 - v4;
  v6 = (char *)&v23 - v4;
  v7 = (__int64)((char *)&v23 - v4);
  v26 = (__int64)((char *)&v23 - v4);
  v8 = (__int64)((char *)&v23 - v4);
  v27 = (__int64)((char *)&v23 - v4);
  ccn_set(v3, (char *)&v23 - v4, a3);
  ccn_set(v3, v6, (const void *)(v28 + 16));
  *(_QWORD *)v7 = 1LL;
  v25 = 8 * v3;
  bzero((void *)(v7 + 8), 8 * v3 - 8);
  bzero((void *)v8, v25);
  while ( 1 )
  {
    v9 = ccn_n(v3, (__int64)v5);
    v10 = 1LL;
    if ( v9 == 1 && *(_QWORD *)v5 == 1LL )
      break;
    v11 = *(_QWORD *)v5;
    v12 = ccn_n(v3, (__int64)v6) == 1;
    v13 = *(_QWORD *)v6;
    if ( v12 && v13 == 1 )
    {
      v10 = v11;
      v8 = v27;
      break;
    }
    if ( !(v11 & 1) )
    {
      v14 = (__int64 *)v28;
      do
      {
        ccn_shift_right(v3, v5, v5, 1LL);
        cczp_div2(v14, v7, v7);
      }
      while ( !(*v5 & 1) );
      v13 = *(_QWORD *)v6;
    }
    v15 = (__int64 *)v28;
    v8 = v27;
    if ( !(v13 & 1) )
    {
      do
      {
        ccn_shift_right(v3, v6, v6, 1LL);
        cczp_div2(v15, v8, v8);
      }
      while ( !(*v6 & 1) );
    }
    v16 = ccn_cmp(v3, (__int64)v5, (__int64)v6);
    if ( v16 <= 0 )
    {
      result = 0xFFFFFFFFLL;
      v7 = v26;
      if ( v16 >= 0 )
        goto LABEL_21;
      ccn_sub(v3, (__int64)v6, (__int64)v6, (__int64)v5);
      v17 = (__int64 *)v28;
      v18 = v8;
      v19 = v8;
      v20 = v7;
    }
    else
    {
      ccn_sub(v3, (__int64)v5, (__int64)v5, (__int64)v6);
      v17 = (__int64 *)v28;
      v7 = v26;
      v18 = v26;
      v19 = v26;
      v20 = v8;
    }
    cczp_sub(v17, v18, v19, v20);
  }
  if ( ccn_n(v3, (__int64)v5) == 1 && v10 == 1 )
    v8 = v7;
  ccn_set(v3, v24, (const void *)v8);
  result = 0LL;
LABEL_21:
  v22 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004402C) ----------------------------------------------------
__int64 __fastcall cczp_mod_inv_slow(unsigned __int64 a1, __int64 a2, const void *a3)
{
  return cczp_mod_inv_slown(a1, a2, *(_QWORD *)a1, a3);
}

//----- (000000000004403F) ----------------------------------------------------
__int64 __fastcall cczp_mod_inv_slown(unsigned __int64 a1, __int64 a2, __int64 a3, const void *a4)
{
  __int64 v4; // r12@1
  __int64 v5; // r14@1
  unsigned __int64 v6; // rcx@1
  __int64 v7; // rbx@1
  unsigned __int64 v8; // r13@1
  unsigned __int64 v9; // rcx@1
  unsigned __int64 v10; // r15@1
  int v11; // eax@1
  __int64 v12; // r14@3
  signed __int64 v13; // rax@3
  const void *v14; // r12@6
  signed __int64 v15; // r13@6
  unsigned __int64 v16; // rdi@6
  __int64 v17; // rbx@6
  __int64 v18; // r14@6
  void *v19; // rbx@6
  __int64 v20; // r13@6
  __int64 v22; // [sp+0h] [bp-90h]@1
  void *v23; // [sp+8h] [bp-88h]@1
  size_t v24; // [sp+10h] [bp-80h]@1
  __int64 v25; // [sp+18h] [bp-78h]@1
  void *v26; // [sp+20h] [bp-70h]@1
  unsigned __int64 v27; // [sp+28h] [bp-68h]@1
  __int64 v28; // [sp+30h] [bp-60h]@1
  unsigned __int64 v29; // [sp+38h] [bp-58h]@1
  const void *v30; // [sp+40h] [bp-50h]@1
  __int64 v31; // [sp+48h] [bp-48h]@1
  __int64 *v32; // [sp+50h] [bp-40h]@1
  __int64 *v33; // [sp+58h] [bp-38h]@1
  __int64 v34; // [sp+60h] [bp-30h]@1

  v30 = a4;
  v28 = a3;
  v4 = a2;
  v27 = a1;
  v34 = *(_QWORD *)off_69010[0];
  v5 = *(_QWORD *)a1;
  v31 = v5;
  v29 = 2 * v5;
  v6 = (16 * v5 + 39) & 0xFFFFFFFFFFFFFFF0LL;
  v7 = (__int64)((char *)&v22 - v6);
  v8 = (unsigned __int64)((char *)&v22 - v6);
  *(__int64 *)((char *)&v22 - v6) = v5;
  *(__int64 *)((char *)&v22 - v6) = v5;
  v9 = (8 * v5 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v26 = (char *)&v22 - v9;
  v25 = (__int64)((char *)&v22 + -v9 - v9);
  v10 = (unsigned __int64)(&v22 - 2 * v5);
  v32 = &v22 + -6 * v5 - 6;
  v33 = &v22;
  bzero((void *)(v10 + 8 * a3), 8 * (2 * v5 - a3));
  ccn_set(v28, &v22 - 2 * v5, v30);
  cczp_modn(v27, (void *)(v7 + 16), v29, (__int64)(&v22 - 2 * v5));
  v22 = v27 + 16;
  ccn_set(v5, (void *)(v8 + 16), (const void *)(v27 + 16));
  v24 = 8 * v5;
  bzero(v26, 8 * v5);
  *(_QWORD *)a2 = 1LL;
  bzero((void *)(a2 + 8), 8 * v5 - 8);
  v23 = (void *)(v10 + 8 * v5);
  v11 = 0;
  while ( 1 )
  {
    LODWORD(v30) = v11;
    v12 = v4;
    v13 = ccn_n(v31, v7 + 16);
    if ( !v13 || v13 == 1 && *(_QWORD *)(v7 + 16) == 1LL )
      break;
    cczp_init(v7);
    bzero(v23, v24);
    v29 = v7;
    v28 = v8;
    v14 = (const void *)(v8 + 16);
    v15 = v31;
    ccn_set(v31, (void *)v10, v14);
    v16 = v7;
    v17 = v25;
    cczp_div(v16, v25, (void *)v14, v10);
    v4 = v12;
    v18 = v15;
    ccn_mul(v15, v10, v17, (unsigned __int64 *)v4);
    cczp_mod((signed __int64 *)v27, (void *)v10, v10, (void **)&v32);
    v19 = v26;
    v20 = ccn_add(v15, v10, (__int64)v26, v10);
    ccn_set(v18, v19, (const void *)v4);
    if ( v20 )
      ccn_sub(v18, v4, v10, v22);
    else
      ccn_set(v18, (void *)v4, (const void *)v10);
    v11 = (_DWORD)v30 + 1;
    v8 = v29;
    v7 = v28;
  }
  if ( (unsigned __int8)v30 & 1 )
    ccn_sub(v31, v4, v22, v4);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000442C9) ----------------------------------------------------
int __fastcall cczp_mul_ws(signed __int64 *a1, __int64 a2, __int64 a3, unsigned __int64 *a4, __int64 *a5)
{
  __int64 *v5; // rbx@1
  signed __int64 *v6; // r15@1
  __int64 v7; // r12@1
  signed __int64 v8; // rdi@1
  int result; // eax@1

  v5 = a5;
  v6 = a1;
  v7 = *a5;
  v8 = *a1;
  *a5 += 16 * v8;
  ccn_mul_ws(v8, v7, a3, a4);
  result = ((int (__fastcall *)(signed __int64 *, __int64, __int64, __int64 *))v6[1])(v6, a2, v7, v5);
  *v5 -= 16 * v8;
  return result;
}

//----- (000000000004431E) ----------------------------------------------------
__int64 __fastcall cczp_mul(signed __int64 *a1, __int64 a2, __int64 a3, unsigned __int64 *a4)
{
  signed __int64 *v4; // r15@1
  __int64 v5; // r13@1
  signed __int64 v6; // rdi@1
  __int64 *v7; // rbx@1
  __int64 *v9; // [sp+0h] [bp-40h]@1
  __int64 *v10; // [sp+8h] [bp-38h]@1
  __int64 v11; // [sp+10h] [bp-30h]@1

  v4 = a1;
  v5 = off_69010[0];
  v11 = *(_QWORD *)off_69010[0];
  v6 = *a1;
  v7 = (__int64 *)&(&v9)[-8 * v6 - 6];
  v10 = (__int64 *)&v9;
  v9 = &v7[2 * v6];
  ccn_mul_ws(v6, (__int64)&(&v9)[-8 * v6 - 6], a3, a4);
  ((void (__fastcall *)(signed __int64 *, __int64, __int64 *, __int64 *))v4[1])(v4, a2, v7, &v9);
  return *(_QWORD *)v5;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000443B6) ----------------------------------------------------
char __usercall ccmode_gcm_gmac@<al>(__int64 a1@<rdx>, __int64 a2@<rdi>, unsigned __int64 a3@<rsi>, __m128i a4@<xmm0>, __m128i a5@<xmm1>, __m128i a6@<xmm2>, __m128i a7@<xmm3>, __m128i a8@<xmm6>, __m128i a9@<xmm7>)
{
  __int64 v9; // r15@1
  unsigned __int64 v10; // r14@1
  unsigned __int64 v11; // rax@1
  unsigned int v12; // ecx@2
  signed __int64 v13; // r13@3
  int v14; // ecx@6
  unsigned __int64 v15; // rax@9
  unsigned __int64 v16; // rcx@10
  signed __int64 v20; // rax@13
  __int64 v21; // rax@17
  __int64 v22; // rax@18
  __m128i v23; // xmm4@18
  __m128i v24; // xmm5@18
  unsigned __int64 v25; // r12@23
  unsigned __int64 v26; // rcx@24
  unsigned int v27; // ecx@30
  unsigned __int64 v28; // r14@30
  signed __int64 v30; // [sp+8h] [bp-38h]@19

  v9 = a1;
  v10 = a3;
  LODWORD(v11) = *(_DWORD *)(a2 + 92);
  if ( (_DWORD)v11 != 1 )
  {
    if ( (_DWORD)v11 )
      return v11;
    v14 = *(_DWORD *)(a2 + 96);
    if ( *(_DWORD *)(a2 + 88) || v14 != 12 )
    {
      v15 = 0LL;
      if ( v14 )
      {
        do
        {
          *(_BYTE *)(a2 + v15 + 16) ^= *(_BYTE *)(a2 + v15 + 64);
          ++v15;
          v16 = *(_DWORD *)(a2 + 96);
        }
        while ( v15 < v16 );
        if ( (_DWORD)v16 )
        {
          *(_QWORD *)(a2 + 104) += 8 * v16;
          ccmode_gcm_mult_h(a2, (__m128i *)(a2 + 16), a4, a5, a6, a7);
        }
      }
      bzero((void *)(a2 + 64), 8uLL);
      _RAX = *(_QWORD *)(a2 + 104);
      __asm { bswap   rax }
      *(_QWORD *)(a2 + 72) = _RAX;
      __asm { bswap   rax }
      v20 = 64LL;
      do
      {
        *(_BYTE *)(a2 + v20 - 48) ^= *(_BYTE *)(a2 + v20);
        ++v20;
      }
      while ( v20 != 80 );
      ccmode_gcm_mult_h(a2, (__m128i *)(a2 + 16), a4, a5, a6, a7);
      memcpy((void *)(a2 + 32), (const void *)(a2 + 16), 0x10uLL);
      bzero((void *)(a2 + 16), 0x10uLL);
    }
    else
    {
      memcpy((void *)(a2 + 32), (const void *)(a2 + 64), 0xCuLL);
      *(_DWORD *)(a2 + 44) = 0x1000000;
    }
    memcpy((void *)(a2 + 48), (const void *)(a2 + 32), 0x10uLL);
    v13 = a2 + 96;
    *(_DWORD *)(a2 + 96) = 0;
    *(_QWORD *)(a2 + 104) = 0LL;
    *(_DWORD *)(a2 + 92) = 1;
LABEL_17:
    LODWORD(v21) = cpuid_features();
    if ( _bittest((const unsigned __int64 *)&v21, 0x39u)
      && (LODWORD(v22) = cpuid_features(), _bittest((const unsigned __int64 *)&v22, 0x29u)) )
    {
      v30 = v13;
      if ( a3 >= 0x10 )
      {
        gcm_ghash(v9, a3 & 0xFFFFFFFFFFFFFFF0LL, (const __m128i *)(a2 + 16), a2 + 128, a4, a5, a6, a7, v23, v24, a8, a9);
        v9 += a3 & 0xFFFFFFFFFFFFFFF0LL;
        v10 = a3 - (a3 & 0xFFFFFFFFFFFFFFF0LL);
        *(_QWORD *)(a2 + 104) += 8 * (a3 & 0xFFFFFFFFFFFFFFF0LL);
      }
    }
    else
    {
      v30 = v13;
    }
    v11 = 0LL;
    if ( v10 & 0xFFFFFFFFFFFFFFF0LL )
    {
      v25 = 0LL;
      do
      {
        v26 = 0LL;
        do
        {
          *(_QWORD *)(a2 + v26 + 16) ^= *(_QWORD *)(v9 + v25 + v26);
          v26 += 8LL;
        }
        while ( v26 < 0x10 );
        ccmode_gcm_mult_h(a2, (__m128i *)(a2 + 16), a4, a5, a6, a7);
        *(_QWORD *)(a2 + 104) += 128LL;
        v25 += 16LL;
      }
      while ( v25 < (v10 & 0xFFFFFFFFFFFFFFF0LL) );
      v11 = v10 & 0xFFFFFFFFFFFFFFF0LL;
    }
    v9 += v11;
    v13 = v30;
    goto LABEL_29;
  }
  v12 = *(_DWORD *)(a2 + 96);
  if ( v12 > 0xF )
    return v11;
  v13 = a2 + 96;
  v11 = 0LL;
  if ( !v12 )
    goto LABEL_17;
LABEL_29:
  if ( v11 < v10 )
  {
    v27 = *(_DWORD *)v13;
    v28 = v10 - v11;
    do
    {
      LOBYTE(v11) = *(_BYTE *)v9;
      *(_DWORD *)v13 = v27 + 1;
      *(_BYTE *)(a2 + v27 + 16) ^= v11;
      v27 = *(_DWORD *)v13;
      if ( *(_DWORD *)v13 == 16 )
      {
        LOBYTE(v11) = ccmode_gcm_mult_h(a2, (__m128i *)(a2 + 16), a4, a5, a6, a7);
        *(_DWORD *)v13 = 0;
        *(_QWORD *)(a2 + 104) += 128LL;
        v27 = 0;
      }
      ++v9;
      --v28;
    }
    while ( v28 );
  }
  return v11;
}
// 4B8C8: using guessed type int cpuid_features(void);

//----- (00000000000445E9) ----------------------------------------------------
void __fastcall cczp_powern(signed __int64 *a1, __int64 a2, __int64 a3, unsigned __int64 a4, __int64 a5)
{
  __int64 v5; // r14@1
  unsigned __int64 v6; // r15@1
  __int64 v7; // rax@1
  unsigned __int64 v8; // rdx@1
  signed __int64 v9; // r12@3
  __int64 v10; // rax@4
  unsigned __int64 *v11; // [sp+0h] [bp-30h]@2

  v5 = a5;
  v6 = a4;
  v7 = a3;
  v8 = *a1;
  if ( a4 )
  {
    v11 = (unsigned __int64 *)v7;
    cczp_modn((unsigned __int64)a1, (void *)a2, v8, v7);
    if ( v6 != 1 )
    {
      v9 = v6 - 2;
      if ( v6 >= 2 )
      {
        do
        {
          cczp_sqr(a1, a2, a2);
          v10 = *(_QWORD *)(v5 + 8 * ((unsigned __int64)v9 >> 6));
          if ( _bittest((const unsigned __int64 *)&v10, v9) )
            cczp_mul(a1, a2, a2, v11);
          --v9;
        }
        while ( v9 < v6 );
      }
    }
  }
  else
  {
    *(_QWORD *)a2 = 1LL;
    bzero((void *)(a2 + 8), 8 * v8 - 8);
  }
}

//----- (00000000000446A1) ----------------------------------------------------
__int64 __fastcall cczp_power(__int64 *a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r14@1
  __int64 v5; // r13@1
  char *v6; // r15@2
  __int64 v7; // r14@8
  __int64 v8; // rax@9
  __int64 v10; // [sp+0h] [bp-70h]@2
  char *v11; // [sp+8h] [bp-68h]@3
  __int64 *v12; // [sp+10h] [bp-60h]@2
  __int64 v13; // [sp+18h] [bp-58h]@1
  __int64 v14; // [sp+20h] [bp-50h]@1
  __int64 v15; // [sp+28h] [bp-48h]@1
  char *v16; // [sp+30h] [bp-40h]@2
  char *v17; // [sp+38h] [bp-38h]@2
  __int64 v18; // [sp+40h] [bp-30h]@1

  v14 = a4;
  v13 = a3;
  v4 = off_69010[0];
  v18 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v15 = ccn_bitlen(*a1, a4);
  if ( v15 )
  {
    v12 = &v10;
    v6 = (char *)&v10 - ((72 * v5 + 63) & 0xFFFFFFFFFFFFFFF0LL);
    v17 = &v6[72 * v5 + 48];
    v16 = &v6[8 * v5];
    if ( (signed int)ccn_cmp(v5, v13, (__int64)(a1 + 2)) < 0 )
    {
      if ( v13 != a2 )
        ccn_set(v5, (void *)a2, (const void *)v13);
    }
    else
    {
      v11 = &v6[8 * v5];
      v16 = &v6[24 * v5];
      bzero(&v6[16 * v5], 8 * v5);
      ccn_set(v5, v11, (const void *)v13);
      cczp_mod(a1, (void *)a2, (__int64)v11, (void **)&v16);
      v16 -= 16 * v5;
    }
    v13 = v5;
    ccn_set(v5, v6, (const void *)a2);
    if ( v15 != 1 )
    {
      v7 = v15 - 2;
      if ( (unsigned __int64)v15 >= 2 )
      {
        do
        {
          cczp_sqr_ws(a1, a2, a2, (__int64 *)&v16);
          v8 = *(_QWORD *)(v14 + 8 * ((unsigned __int64)v7 >> 6));
          if ( _bittest((const unsigned __int64 *)&v8, v7) )
            cczp_mul_ws(a1, a2, a2, (unsigned __int64 *)v6, (__int64 *)&v16);
          --v7;
        }
        while ( v7 < (unsigned __int64)v15 );
      }
    }
    v16 -= 8 * v13;
    cc_clear((v17 - v16) >> 3, v16);
    v16 = 0LL;
    v17 = 0LL;
    v4 = off_69010[0];
  }
  else
  {
    *(_QWORD *)a2 = 1LL;
    bzero((void *)(a2 + 8), 8 * v5 - 8);
  }
  return *(_QWORD *)v4;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000044884) ----------------------------------------------------
__int64 __fastcall cczp_rabin_miller(__int64 a1, __int64 a2)
{
  unsigned __int64 v2; // r13@1
  unsigned __int64 v3; // rax@1
  char *v4; // r15@1
  __int64 v5; // rbx@1
  unsigned int v6; // eax@1
  signed int v7; // er12@1
  signed __int64 v8; // r12@2
  __int64 v9; // r14@5
  __int64 v10; // rcx@5
  size_t v11; // rdx@5
  __int64 v12; // rax@5
  __int64 v13; // rsi@5
  unsigned __int64 v14; // r14@7
  __int64 result; // rax@17
  size_t v16; // [sp+0h] [bp-70h]@1
  __int64 v17; // [sp+8h] [bp-68h]@5
  unsigned __int64 v18; // [sp+10h] [bp-60h]@4
  __int64 v19; // [sp+18h] [bp-58h]@6
  __int64 *v20; // [sp+20h] [bp-50h]@1
  __int64 *v21; // [sp+28h] [bp-48h]@1
  __int64 v22; // [sp+30h] [bp-40h]@1
  __int64 v23; // [sp+38h] [bp-38h]@1
  __int64 v24; // [sp+40h] [bp-30h]@1

  v22 = a2;
  v23 = a1;
  v24 = *(_QWORD *)off_69010[0];
  v2 = *(_QWORD *)a1;
  v3 = (8LL * *(_QWORD *)a1 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v4 = (char *)&v16 - v3;
  v21 = (__int64 *)((char *)&v16 - v3);
  v5 = (__int64)((char *)&v16 - v3);
  v20 = (__int64 *)((char *)&v16 - v3);
  LOBYTE(v6) = ccn_prime_sieve(v2, a1 + 16);
  v7 = v6;
  if ( v6 >= 2 )
  {
    v8 = 256LL;
    if ( (unsigned __int64)v22 <= 0x100 )
      v8 = v22;
    v18 = v8;
    ccn_sub1(v2, (__int64)v4, a1 + 16, 1LL);
    v22 = ccn_trailing_zeros(v2, (__int64)v4);
    ccn_shift_right_multi(v2, v21, v4, v22);
    cczp_init(v23);
    if ( v8 )
    {
      v9 = (__int64)v20;
      v10 = (__int64)(v20 + 1);
      v17 = (__int64)(v20 + 1);
      v11 = 8 * v2 - 8;
      v16 = 8 * v2 - 8;
      v12 = 0LL;
      v13 = (__int64)ccn_prime_table;
      v7 = 0;
      do
      {
        v19 = v12;
        *(_QWORD *)v9 = *(_WORD *)(v13 + 2 * v12);
        bzero((void *)v10, v11);
        cczp_power((__int64 *)v23, v5, v9, (__int64)v21);
        if ( ccn_n(v2, v5) != 1 )
        {
          v14 = 0LL;
          goto LABEL_10;
        }
        v14 = 0LL;
        if ( *(_QWORD *)v5 != 1LL )
        {
LABEL_10:
          while ( (unsigned int)ccn_cmp(v2, (__int64)v4, v5) )
          {
            ++v14;
            if ( v14 < v22 )
            {
              cczp_sqr((signed __int64 *)v23, v5, v5);
              if ( ccn_n(v2, v5) != 1 || *(_QWORD *)v5 != 1LL )
                continue;
            }
            goto LABEL_17;
          }
        }
        v12 = v19 + 1;
        v9 = (__int64)v20;
        v10 = v17;
        v11 = v16;
        v13 = (__int64)ccn_prime_table;
      }
      while ( v19 + 1 < v18 );
    }
    v7 = 1;
  }
LABEL_17:
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v24 )
    result = (unsigned int)v7;
  return result;
}
// 67330: using guessed type __int64 ccn_prime_table[64];
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000044A63) ----------------------------------------------------
char __fastcall ccn_prime_sieve(unsigned __int64 a1, __int64 a2)
{
  __int64 v2; // r14@1
  __int64 v3; // r12@1
  char result; // al@1
  __int64 v5; // rcx@4
  unsigned __int64 v6; // rdx@4
  int v7; // esi@4
  __int64 v8; // r15@8
  bool v9; // al@9
  __int64 v10[16]; // [sp+0h] [bp-B0h]@8
  __int64 v11; // [sp+80h] [bp-30h]@1

  v2 = a2;
  v3 = off_69010[0];
  v11 = *(_QWORD *)off_69010[0];
  result = 0;
  if ( !a1 )
  {
LABEL_23:
    *(_QWORD *)v3;
    return result;
  }
  if ( *(_QWORD *)a2 & 1 )
  {
    if ( (unsigned __int64)ccn_bitlen(a1, a2) > 0xB )
    {
LABEL_7:
      if ( a1 < 0x10 )
      {
        if ( a1 == 15 )
        {
          ccn_gcd(0xFuLL, &v10[-16], (__int64)composite, v2);
          if ( ccn_n(15LL, (__int64)&v10[-16]) == 1 )
            v9 = v10[-16] == 1;
          else
            v9 = 0;
        }
        else
        {
          bzero(&v10[a1], 8 * (15 - a1));
          ccn_set(a1, v10, (const void *)v2);
          ccn_gcd(0xFuLL, v10, (__int64)v10, (__int64)composite);
          if ( ccn_n(15LL, (__int64)v10) == 1 )
            v9 = v10[0] == 1;
          else
            v9 = 0;
        }
      }
      else
      {
        v8 = (__int64)((char *)v10 - ((8 * a1 + 15) & 0xFFFFFFFFFFFFFFF0LL));
        bzero((void *)(v8 + 120), 8 * a1 - 120);
        ccn_set(15LL, (void *)v8, composite);
        ccn_gcd(a1, (void *)v8, v8, v2);
        if ( ccn_n(15LL, v8) == 1 )
          v9 = *(_QWORD *)v8 == 1LL;
        else
          v9 = 0;
      }
      result = 2 * (v9 != 0);
    }
    else
    {
      v5 = (__int64)ccn_prime_table;
      v6 = 0LL;
      v7 = *(_WORD *)a2;
      while ( 1 )
      {
        result = 1;
        if ( *(_WORD *)v5 == v7 )
          break;
        ++v6;
        v5 += 2LL;
        if ( v6 > 0xFF )
          goto LABEL_7;
      }
    }
    goto LABEL_23;
  }
  result = a1 == 1;
  if ( *(_QWORD *)off_69010[0] == v11 )
    result &= *(_QWORD *)a2 == 2LL;
  return result;
}
// 67330: using guessed type __int64 ccn_prime_table[64];
// 67530: using guessed type __int64 composite[16];
// 69010: using guessed type __int64 off_69010[2];
// 44A63: using guessed type __int64 var_B0[16];

//----- (0000000000044C68) ----------------------------------------------------
__int64 __fastcall ccec_generate_key_internal(__int64 *a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r14@1
  signed __int64 *v4; // rbx@1
  __int64 v5; // r13@1
  char *v6; // r12@1
  __int64 v7; // r15@1
  __int64 v8; // rax@1
  int v9; // ecx@1
  __int64 v10; // rax@1
  __int64 v11; // rcx@2
  __int64 v12; // rdx@2
  __int64 v13; // r14@2
  __int64 v14; // rdi@4
  __int64 v15; // r13@4
  __int64 v16; // rsi@4
  __int64 v17; // r15@4
  __int64 v18; // rdx@6
  __int64 v19; // r13@6
  __int64 v20; // r14@6
  int v21; // eax@6
  int v22; // eax@8
  bool v23; // zf@8
  __int64 v24; // r15@8
  __int64 v25; // rbx@9
  signed __int64 v26; // rax@9
  __int64 result; // rax@10
  __int64 v28; // [sp+0h] [bp-50h]@1
  __int64 v29; // [sp+8h] [bp-48h]@1
  __int64 v30; // [sp+10h] [bp-40h]@1
  __int64 v31; // [sp+18h] [bp-38h]@1
  __int64 v32; // [sp+20h] [bp-30h]@1

  v3 = a3;
  v30 = a3;
  v31 = a2;
  v4 = a1;
  v32 = *(_QWORD *)off_69010[0];
  v5 = *a1;
  v6 = (char *)&v28 - ((24 * *a1 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  ccec_projectify(a1, (char *)&v28 - ((24 * *a1 + 15) & 0xFFFFFFFFFFFFFFF0LL), &a1[2 * *a1 + 2]);
  *(_QWORD *)v3 = a1;
  v29 = 3 * *a1;
  v7 = v3 + 8 * v29 + 16;
  v8 = ccn_bitlen(a1[4 * v5 + 2], (__int64)&a1[4 * v5 + 4]);
  v9 = ccn_random_bits(v8, v7, v31);
  v10 = off_69010[0];
  if ( !v9 )
  {
    v11 = (__int64)&a1[4 * v5 + 4];
    v12 = v30 + 16;
    v13 = v5;
    if ( v5 == 9 )
      *(_QWORD *)(v12 + 8 * v29 + 64) &= 0x1FFuLL;
    v31 = v12;
    v14 = v5;
    v15 = v7;
    v16 = v7;
    v17 = v11;
    if ( (signed int)ccn_cmp(v14, v16, v11) >= 0 )
      ccn_sub(v13, v15, v15, v17);
    v18 = v15;
    v19 = v13;
    v20 = v31;
    v21 = ccec_mult(v4, v31, v18, v6);
    v9 = -1;
    if ( v21 )
    {
      v10 = off_69010[0];
    }
    else
    {
      v22 = ccec_affinify(v4, v20, (unsigned __int64 *)v20);
      v9 = -1;
      v23 = v22 == 0;
      v10 = off_69010[0];
      v24 = v30;
      if ( v23 )
      {
        v25 = off_69010[0];
        v26 = 16LL * **(_QWORD **)v30;
        *(_QWORD *)(v26 + v30 + 16) = 1LL;
        bzero((void *)(v26 + v24 + 24), 8 * v19 - 8);
        v10 = v25;
        v9 = 0;
      }
    }
  }
  result = *(_QWORD *)v10;
  if ( result == v32 )
    result = (unsigned int)v9;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000044E00) ----------------------------------------------------
void __usercall ONE_3(__int64 a1@<rax>)
{
  *(_DWORD *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  TWO_1(a1);
}

//----- (0000000000044E10) ----------------------------------------------------
void __usercall TWO_1(__int64 a1@<rax>)
{
  LOBYTE(a1) = *(_BYTE *)a1 + a1;
  *(_BYTE *)a1 = a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  *(_BYTE *)a1 += a1;
  JUMPOUT(Lbswap_mask_3);
}

//----- (0000000000044E40) ----------------------------------------------------
// local variable allocation has failed, the output may be wrong!
__int64 __usercall gcmEncrypt_SupplementalSSE3@<rax>(__int64 a1@<rdx>, unsigned __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __int64 a6@<r9>, __m128i a7@<xmm0>, __m128i a8@<xmm1>, __m128i a9@<xmm2>, __m128i a10@<xmm3>, __m128i a11@<xmm4>, __m128i a12@<xmm5>, __m128i a13@<xmm6>, __m128i a14@<xmm7>, __m128i a15@<xmm8>, __m128i a16@<xmm9>, __m128i a17@<xmm10>, __m128i a18@<xmm11>, __m128i a19@<xmm12>, __m128i a20@<xmm13>, __m128i a21@<xmm14>, __m128i a22@<xmm15>, __int128 a23, __int128 a24, __int128 a25, __int128 a26, __int128 a27, int a28, __int128 a29, __int128 a30, int a31, int a32, int a33, int a34, int a35, int a36, int a37, int a38, int a39, __int128 a40, __int128 a41, __int128 a42, __int128 a43, __int128 a44, __int128 a45, __int128 a46, __int128 a47, __int128 a48, __int128 a49, __int128 a50, __int128 a51, __int128 a52, __int128 a53, __int128 a54, __int128 a55)
{
  unsigned int v55; // er10@1
  __m128i v56; // xmm15@1
  __m128i v57; // xmm0@1
  __m128i v58; // xmm5@1
  __m128i v59; // xmm6@1
  __m128i v60; // xmm4@1
  __m128i v61; // xmm8@1
  __m128i v62; // xmm9@1
  __m128i v63; // xmm10@1
  __m128i v65; // xmm11@1
  __m128i v67; // xmm12@1
  __m128i v69; // xmm13@1
  __m128i v71; // xmm14@1
  __m128i v73; // xmm15@1
  __m128i v158; // xmm6@1
  __m128i v195; // xmm4@4
  __int128 v197; // [sp+80h] [bp-100h]@1
  __int128 v198; // [sp+90h] [bp-F0h]@1
  __int128 v199; // [sp+A0h] [bp-E0h]@1
  __int128 v200; // [sp+B0h] [bp-D0h]@1
  __int128 v201; // [sp+C0h] [bp-C0h]@1
  __int128 v202; // [sp+D0h] [bp-B0h]@1
  __int128 v203; // [sp+E0h] [bp-A0h]@1
  __int128 v204; // [sp+F0h] [bp-90h]@1
  __int128 v205; // [sp+100h] [bp-80h]@1
  __int128 v206; // [sp+110h] [bp-70h]@1
  __int128 v207; // [sp+120h] [bp-60h]@1
  __int128 v208; // [sp+130h] [bp-50h]@1
  __int128 v209; // [sp+140h] [bp-40h]@1
  __int128 v210; // [sp+150h] [bp-30h]@1
  __int128 v211; // [sp+160h] [bp-20h]@1
  __int128 v212; // [sp+170h] [bp-10h]@1

  _mm_store_si128((__m128i *)&v197, a7);
  _mm_store_si128((__m128i *)&v198, a8);
  _mm_store_si128((__m128i *)&v199, a9);
  _mm_store_si128((__m128i *)&v200, a10);
  _mm_store_si128((__m128i *)&v201, a11);
  _mm_store_si128((__m128i *)&v202, a12);
  _mm_store_si128((__m128i *)&v203, a13);
  _mm_store_si128((__m128i *)&v204, a14);
  _mm_store_si128((__m128i *)&v205, a15);
  _mm_store_si128((__m128i *)&v206, a16);
  _mm_store_si128((__m128i *)&v207, a17);
  _mm_store_si128((__m128i *)&v208, a18);
  _mm_store_si128((__m128i *)&v209, a19);
  _mm_store_si128((__m128i *)&v210, a20);
  _mm_store_si128((__m128i *)&v211, a21);
  _mm_store_si128((__m128i *)&v212, a22);
  v55 = *(_DWORD *)(a6 + 240);
  v56 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(a1 + 32)), Lbswap_mask_3);
  v57 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i *)(a1 + 16)), Lbswap_mask_3);
  JUMPOUT(a2, 128LL, &loc_45D28);
  v58 = _mm_load_si128((const __m128i *)TWO_1);
  v59 = _mm_load_si128((const __m128i *)&Lbswap_mask_3);
  v60 = _mm_load_si128((const __m128i *)a6);
  v61 = _mm_add_epi32(v56, *(__m128i *)ONE_3);
  v62 = _mm_add_epi32(v56, v58);
  v63 = _mm_add_epi32(v61, v58);
  _XMM7 = _mm_xor_si128(_mm_shuffle_epi8(v56, v59), v60);
  v65 = _mm_add_epi32(v62, v58);
  _XMM8 = _mm_xor_si128(_mm_shuffle_epi8(v61, v59), v60);
  v67 = _mm_add_epi32(v63, v58);
  _XMM9 = _mm_xor_si128(_mm_shuffle_epi8(v62, v59), v60);
  v69 = _mm_add_epi32(v65, v58);
  _XMM10 = _mm_xor_si128(_mm_shuffle_epi8(v63, v59), v60);
  v71 = _mm_add_epi32(v67, v58);
  _XMM11 = _mm_xor_si128(_mm_shuffle_epi8(v65, v59), v60);
  v73 = _mm_add_epi32(v69, v58);
  _XMM12 = _mm_xor_si128(_mm_shuffle_epi8(v67, v59), v60);
  _XMM13 = _mm_xor_si128(_mm_shuffle_epi8(v69, v59), v60);
  _XMM14 = _mm_xor_si128(_mm_shuffle_epi8(v71, v59), v60);
  _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 16));
  __asm
  {
    aesenc  xmm7, xmm4
    aesenc  xmm8, xmm4
    aesenc  xmm9, xmm4
    aesenc  xmm10, xmm4
    aesenc  xmm11, xmm4
    aesenc  xmm12, xmm4
    aesenc  xmm13, xmm4
    aesenc  xmm14, xmm4
  }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 32));
  __asm
  {
    aesenc  xmm7, xmm4
    aesenc  xmm8, xmm4
    aesenc  xmm9, xmm4
    aesenc  xmm10, xmm4
    aesenc  xmm11, xmm4
    aesenc  xmm12, xmm4
    aesenc  xmm13, xmm4
    aesenc  xmm14, xmm4
  }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 48));
  __asm
  {
    aesenc  xmm7, xmm4
    aesenc  xmm8, xmm4
    aesenc  xmm9, xmm4
    aesenc  xmm10, xmm4
    aesenc  xmm11, xmm4
    aesenc  xmm12, xmm4
    aesenc  xmm13, xmm4
    aesenc  xmm14, xmm4
  }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 64));
  __asm
  {
    aesenc  xmm7, xmm4
    aesenc  xmm8, xmm4
    aesenc  xmm9, xmm4
    aesenc  xmm10, xmm4
    aesenc  xmm11, xmm4
    aesenc  xmm12, xmm4
    aesenc  xmm13, xmm4
    aesenc  xmm14, xmm4
  }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 80));
  __asm
  {
    aesenc  xmm7, xmm4
    aesenc  xmm8, xmm4
    aesenc  xmm9, xmm4
    aesenc  xmm10, xmm4
    aesenc  xmm11, xmm4
    aesenc  xmm12, xmm4
    aesenc  xmm13, xmm4
    aesenc  xmm14, xmm4
  }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 96));
  __asm
  {
    aesenc  xmm7, xmm4
    aesenc  xmm8, xmm4
    aesenc  xmm9, xmm4
    aesenc  xmm10, xmm4
    aesenc  xmm11, xmm4
    aesenc  xmm12, xmm4
    aesenc  xmm13, xmm4
    aesenc  xmm14, xmm4
  }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 112));
  __asm
  {
    aesenc  xmm7, xmm4
    aesenc  xmm8, xmm4
    aesenc  xmm9, xmm4
    aesenc  xmm10, xmm4
    aesenc  xmm11, xmm4
    aesenc  xmm12, xmm4
    aesenc  xmm13, xmm4
    aesenc  xmm14, xmm4
  }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 128));
  __asm
  {
    aesenc  xmm7, xmm4
    aesenc  xmm8, xmm4
    aesenc  xmm9, xmm4
    aesenc  xmm10, xmm4
    aesenc  xmm11, xmm4
    aesenc  xmm12, xmm4
    aesenc  xmm13, xmm4
    aesenc  xmm14, xmm4
  }
  _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 144));
  __asm
  {
    aesenc  xmm7, xmm4
    aesenc  xmm8, xmm4
    aesenc  xmm9, xmm4
    aesenc  xmm10, xmm4
    aesenc  xmm11, xmm4
    aesenc  xmm12, xmm4
    aesenc  xmm13, xmm4
    aesenc  xmm14, xmm4
  }
  v158 = _mm_loadu_si128((const __m128i *)(a6 + 160));
  if ( v55 > 0xA0 )
  {
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 160));
    __asm
    {
      aesenc  xmm7, xmm4
      aesenc  xmm8, xmm4
      aesenc  xmm9, xmm4
      aesenc  xmm10, xmm4
      aesenc  xmm11, xmm4
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
      aesenc  xmm14, xmm4
    }
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 176));
    __asm
    {
      aesenc  xmm7, xmm4
      aesenc  xmm8, xmm4
      aesenc  xmm9, xmm4
      aesenc  xmm10, xmm4
      aesenc  xmm11, xmm4
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
      aesenc  xmm14, xmm4
    }
    v158 = _mm_loadu_si128((const __m128i *)(a6 + 192));
    if ( v55 > 0xC0 )
    {
      _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 192));
      __asm
      {
        aesenc  xmm7, xmm4
        aesenc  xmm8, xmm4
        aesenc  xmm9, xmm4
        aesenc  xmm10, xmm4
        aesenc  xmm11, xmm4
        aesenc  xmm12, xmm4
        aesenc  xmm13, xmm4
        aesenc  xmm14, xmm4
      }
      _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 208));
      __asm
      {
        aesenc  xmm7, xmm4
        aesenc  xmm8, xmm4
        aesenc  xmm9, xmm4
        aesenc  xmm10, xmm4
        aesenc  xmm11, xmm4
        aesenc  xmm12, xmm4
        aesenc  xmm13, xmm4
        aesenc  xmm14, xmm4
      }
      v158 = _mm_loadu_si128((const __m128i *)(a6 + 224));
    }
  }
  _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)a3), v158);
  __asm { aesenclast xmm7, xmm4 }
  _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 16)), v158);
  __asm { aesenclast xmm8, xmm4 }
  _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 32)), v158);
  __asm { aesenclast xmm9, xmm4 }
  _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 48)), v158);
  __asm { aesenclast xmm10, xmm4 }
  _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 64)), v158);
  __asm { aesenclast xmm11, xmm4 }
  _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 80)), v158);
  __asm { aesenclast xmm12, xmm4 }
  _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 96)), v158);
  __asm { aesenclast xmm13, xmm4 }
  _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 112)), v158);
  __asm { aesenclast xmm14, xmm4 }
  v195 = _mm_loadu_si128((const __m128i *)&Lbswap_mask_3);
  _mm_storeu_si128((__m128i *)a4, _XMM7);
  _mm_storeu_si128((__m128i *)(a4 + 16), _XMM8);
  _mm_storeu_si128((__m128i *)(a4 + 32), _XMM9);
  _mm_storeu_si128((__m128i *)(a4 + 48), _XMM10);
  _mm_storeu_si128((__m128i *)(a4 + 64), _XMM11);
  _mm_storeu_si128((__m128i *)(a4 + 80), _XMM12);
  _mm_storeu_si128((__m128i *)(a4 + 96), _XMM13);
  _mm_storeu_si128((__m128i *)(a4 + 112), _XMM14);
  JUMPOUT(a2 < 0x100, End_Main_Encrypt_Loop_0);
  return Main_Encrypt_Loop_0(
           a1,
           a2 - 256,
           a3 + 128,
           a4 + 128,
           a5,
           a6,
           v55,
           v57,
           v195,
           _mm_shuffle_epi8(_XMM7, v195),
           _mm_shuffle_epi8(_XMM8, v195),
           _mm_shuffle_epi8(_XMM9, v195),
           _mm_shuffle_epi8(_XMM10, v195),
           _mm_shuffle_epi8(_XMM11, v195),
           _mm_shuffle_epi8(_XMM12, v195),
           _mm_shuffle_epi8(_XMM13, v195),
           _mm_shuffle_epi8(_XMM14, v195),
           v73,
           a23,
           a24,
           a25,
           a26,
           a27,
           a28,
           a29,
           a30,
           a31,
           a32,
           a33,
           a34,
           a35,
           a36,
           a37,
           a38,
           a39,
           a40,
           a41,
           a42,
           a43,
           a44,
           a45,
           a46,
           a47,
           a48,
           a49,
           a50,
           a51,
           a52,
           a53,
           a54,
           a55);
}
// 44E40: array has been used for an input argument
// 45B03: using guessed type __int64 __fastcall End_Main_Encrypt_Loop_0(_DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128);

//----- (0000000000045400) ----------------------------------------------------
// local variable allocation has failed, the output may be wrong!
__int64 __usercall Main_Encrypt_Loop_0@<rax>(int a1@<edx>, __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 _R8@<r8>, __int64 a6@<r9>, unsigned int a7@<r10d>, __m128i a8@<xmm0>, __m128i a9@<xmm4>, __m128i a10@<xmm7>, __m128i a11@<xmm8>, __m128i a12@<xmm9>, __m128i a13@<xmm10>, __m128i a14@<xmm11>, __m128i a15@<xmm12>, __m128i a16@<xmm13>, __m128i a17@<xmm14>, __m128i a18@<xmm15>, __int128 a19, __int128 a20, __int128 a21, __int128 a22, __int128 a23, int a24, __int128 a25, __int128 a26, int a27, int a28, int a29, int a30, int a31, int a32, int a33, int a34, int a35, __int128 a36, __int128 a37, __int128 a38, __int128 a39, __int128 a40, __int128 a41, __int128 a42, __int128 a43, __int128 a44, __int128 a45, __int128 a46, __int128 a47, __int128 a48, __int128 a49, __int128 a50, __int128 a51)
{
  __m128i v51; // xmm5@1
  __m128i v52; // xmm0@1
  __m128i v53; // xmm8@1
  __m128i v54; // xmm9@1
  __m128i v55; // xmm10@1
  __m128i v56; // xmm11@1
  __m128i v58; // xmm12@1
  __m128i v60; // xmm13@1
  __m128i v62; // xmm1@1
  __m128i v63; // xmm14@1
  __m128i v76; // xmm6@1
  __m128i v82; // xmm2@1
  __m128i v91; // xmm3@1
  __m128i v93; // xmm1@1
  __m128i v95; // xmm6@1
  __m128i v101; // xmm2@1
  __m128i v110; // xmm3@1
  __m128i v112; // xmm1@1
  __m128i v114; // xmm6@1
  __m128i v120; // xmm2@1
  __m128i v129; // xmm3@1
  __m128i v131; // xmm1@1
  __m128i v133; // xmm6@1
  __m128i v139; // xmm2@1
  __m128i v148; // xmm3@1
  __m128i v150; // xmm1@1
  __m128i v152; // xmm6@1
  __m128i v158; // xmm2@1
  __m128i v167; // xmm3@1
  __m128i v169; // xmm1@1
  __m128i v171; // xmm6@1
  __m128i v177; // xmm2@1
  __m128i v186; // xmm3@1
  __m128i v188; // xmm1@1
  __m128i v190; // xmm6@1
  __m128i v196; // xmm2@1
  __m128i v205; // xmm3@1
  __m128i v209; // xmm1@1
  __m128i v216; // xmm0@1
  __m128i v224; // xmm3@1
  __m128i v232; // xmm6@1
  unsigned __int8 v269; // of@4

  do
  {
    v51 = _mm_load_si128((const __m128i *)TWO_1);
    _mm_store_si128((__m128i *)&a26, _mm_xor_si128(a8, a10));
    v52 = _mm_loadu_si128((const __m128i *)a6);
    _mm_store_si128((__m128i *)&a25, a11);
    v53 = _mm_add_epi32(a18, *(__m128i *)ONE_3);
    _mm_store_si128((__m128i *)((char *)&a23 + 8), a12);
    v54 = _mm_add_epi32(a18, v51);
    _mm_store_si128((__m128i *)((char *)&a22 + 8), a13);
    v55 = _mm_add_epi32(v53, v51);
    _mm_store_si128((__m128i *)((char *)&a21 + 8), a14);
    v56 = _mm_add_epi32(v54, v51);
    _XMM7 = _mm_xor_si128(_mm_shuffle_epi8(a18, a9), v52);
    _mm_store_si128((__m128i *)((char *)&a20 + 8), a15);
    v58 = _mm_add_epi32(v55, v51);
    _XMM8 = _mm_xor_si128(_mm_shuffle_epi8(v53, a9), v52);
    _mm_store_si128((__m128i *)((char *)&a19 + 8), a16);
    v60 = _mm_add_epi32(v56, v51);
    _XMM9 = _mm_xor_si128(_mm_shuffle_epi8(v54, a9), v52);
    v62 = a17;
    v63 = _mm_add_epi32(v58, v51);
    _XMM2 = v62;
    _XMM10 = _mm_xor_si128(_mm_shuffle_epi8(v55, a9), v52);
    _XMM3 = v62;
    a18 = _mm_add_epi32(v60, v51);
    __asm { pclmulqdq xmm2, xmmword ptr [r8], 11h }
    _XMM11 = _mm_xor_si128(_mm_shuffle_epi8(v56, a9), v52);
    _XMM1 = _mm_xor_si128(v62, _mm_shuffle_epi32(v62, 78));
    __asm
    {
      pclmulqdq xmm3, xmmword ptr [r8], 0
      pclmulqdq xmm1, xmmword ptr [r8+80h], 0
    }
    _XMM12 = _mm_xor_si128(_mm_shuffle_epi8(v58, a9), v52);
    _XMM13 = _mm_xor_si128(_mm_shuffle_epi8(v60, a9), v52);
    _XMM14 = _mm_xor_si128(_mm_shuffle_epi8(v63, a9), v52);
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 16));
    v76 = _mm_loadu_si128((const __m128i *)((char *)&a19 + 8));
    __asm { aesenc  xmm7, xmm4 }
    _XMM0 = v76;
    __asm
    {
      aesenc  xmm8, xmm4
      pclmulqdq xmm0, xmmword ptr [r8+10h], 11h
      aesenc  xmm9, xmm4
    }
    v82 = _mm_xor_si128(_XMM2, _XMM0);
    __asm { aesenc  xmm10, xmm4 }
    _XMM0 = v76;
    __asm
    {
      pclmulqdq xmm0, xmmword ptr [r8+10h], 0
      aesenc  xmm11, xmm4
    }
    _XMM5 = _mm_xor_si128(_mm_shuffle_epi32(v76, 78), v76);
    __asm
    {
      pclmulqdq xmm5, xmmword ptr [r8+90h], 0
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
    }
    v91 = _mm_xor_si128(_XMM3, _XMM0);
    __asm { aesenc  xmm14, xmm4 }
    v93 = _mm_xor_si128(_XMM1, _XMM5);
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 32));
    v95 = _mm_loadu_si128((const __m128i *)((char *)&a20 + 8));
    __asm { aesenc  xmm7, xmm4 }
    _XMM0 = v95;
    __asm
    {
      aesenc  xmm8, xmm4
      pclmulqdq xmm0, xmmword ptr [r8+20h], 11h
      aesenc  xmm9, xmm4
    }
    v101 = _mm_xor_si128(v82, _XMM0);
    __asm { aesenc  xmm10, xmm4 }
    _XMM0 = v95;
    __asm
    {
      pclmulqdq xmm0, xmmword ptr [r8+20h], 0
      aesenc  xmm11, xmm4
    }
    _XMM5 = _mm_xor_si128(_mm_shuffle_epi32(v95, 78), v95);
    __asm
    {
      pclmulqdq xmm5, xmmword ptr [r8+0A0h], 0
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
    }
    v110 = _mm_xor_si128(v91, _XMM0);
    __asm { aesenc  xmm14, xmm4 }
    v112 = _mm_xor_si128(v93, _XMM5);
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 48));
    v114 = _mm_loadu_si128((const __m128i *)((char *)&a21 + 8));
    __asm { aesenc  xmm7, xmm4 }
    _XMM0 = v114;
    __asm
    {
      aesenc  xmm8, xmm4
      pclmulqdq xmm0, xmmword ptr [r8+30h], 11h
      aesenc  xmm9, xmm4
    }
    v120 = _mm_xor_si128(v101, _XMM0);
    __asm { aesenc  xmm10, xmm4 }
    _XMM0 = v114;
    __asm
    {
      pclmulqdq xmm0, xmmword ptr [r8+30h], 0
      aesenc  xmm11, xmm4
    }
    _XMM5 = _mm_xor_si128(_mm_shuffle_epi32(v114, 78), v114);
    __asm
    {
      pclmulqdq xmm5, xmmword ptr [r8+0B0h], 0
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
    }
    v129 = _mm_xor_si128(v110, _XMM0);
    __asm { aesenc  xmm14, xmm4 }
    v131 = _mm_xor_si128(v112, _XMM5);
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 64));
    v133 = _mm_loadu_si128((const __m128i *)((char *)&a22 + 8));
    __asm { aesenc  xmm7, xmm4 }
    _XMM0 = v133;
    __asm
    {
      aesenc  xmm8, xmm4
      pclmulqdq xmm0, xmmword ptr [r8+40h], 11h
      aesenc  xmm9, xmm4
    }
    v139 = _mm_xor_si128(v120, _XMM0);
    __asm { aesenc  xmm10, xmm4 }
    _XMM0 = v133;
    __asm
    {
      pclmulqdq xmm0, xmmword ptr [r8+40h], 0
      aesenc  xmm11, xmm4
    }
    _XMM5 = _mm_xor_si128(_mm_shuffle_epi32(v133, 78), v133);
    __asm
    {
      pclmulqdq xmm5, xmmword ptr [r8+0C0h], 0
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
    }
    v148 = _mm_xor_si128(v129, _XMM0);
    __asm { aesenc  xmm14, xmm4 }
    v150 = _mm_xor_si128(v131, _XMM5);
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 80));
    v152 = _mm_loadu_si128((const __m128i *)((char *)&a23 + 8));
    __asm { aesenc  xmm7, xmm4 }
    _XMM0 = v152;
    __asm
    {
      aesenc  xmm8, xmm4
      pclmulqdq xmm0, xmmword ptr [r8+50h], 11h
      aesenc  xmm9, xmm4
    }
    v158 = _mm_xor_si128(v139, _XMM0);
    __asm { aesenc  xmm10, xmm4 }
    _XMM0 = v152;
    __asm
    {
      pclmulqdq xmm0, xmmword ptr [r8+50h], 0
      aesenc  xmm11, xmm4
    }
    _XMM5 = _mm_xor_si128(_mm_shuffle_epi32(v152, 78), v152);
    __asm
    {
      pclmulqdq xmm5, xmmword ptr [r8+0D0h], 0
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
    }
    v167 = _mm_xor_si128(v148, _XMM0);
    __asm { aesenc  xmm14, xmm4 }
    v169 = _mm_xor_si128(v150, _XMM5);
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 96));
    v171 = _mm_loadu_si128((const __m128i *)&a25);
    __asm { aesenc  xmm7, xmm4 }
    _XMM0 = v171;
    __asm
    {
      aesenc  xmm8, xmm4
      pclmulqdq xmm0, xmmword ptr [r8+60h], 11h
      aesenc  xmm9, xmm4
    }
    v177 = _mm_xor_si128(v158, _XMM0);
    __asm { aesenc  xmm10, xmm4 }
    _XMM0 = v171;
    __asm
    {
      pclmulqdq xmm0, xmmword ptr [r8+60h], 0
      aesenc  xmm11, xmm4
    }
    _XMM5 = _mm_xor_si128(_mm_shuffle_epi32(v171, 78), v171);
    __asm
    {
      pclmulqdq xmm5, xmmword ptr [r8+0E0h], 0
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
    }
    v186 = _mm_xor_si128(v167, _XMM0);
    __asm { aesenc  xmm14, xmm4 }
    v188 = _mm_xor_si128(v169, _XMM5);
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 112));
    v190 = _mm_loadu_si128((const __m128i *)&a26);
    __asm { aesenc  xmm7, xmm4 }
    _XMM0 = v190;
    __asm
    {
      aesenc  xmm8, xmm4
      pclmulqdq xmm0, xmmword ptr [r8+70h], 11h
      aesenc  xmm9, xmm4
    }
    v196 = _mm_xor_si128(v177, _XMM0);
    __asm { aesenc  xmm10, xmm4 }
    _XMM0 = v190;
    __asm
    {
      pclmulqdq xmm0, xmmword ptr [r8+70h], 0
      aesenc  xmm11, xmm4
    }
    _XMM5 = _mm_xor_si128(_mm_shuffle_epi32(v190, 78), v190);
    __asm
    {
      pclmulqdq xmm5, xmmword ptr [r8+0F0h], 0
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
    }
    v205 = _mm_xor_si128(v186, _XMM0);
    __asm { aesenc  xmm14, xmm4 }
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 128));
    __asm { aesenc  xmm7, xmm4 }
    v209 = _mm_xor_si128(_mm_xor_si128(_mm_xor_si128(v188, _XMM5), v196), v205);
    __asm
    {
      aesenc  xmm8, xmm4
      aesenc  xmm9, xmm4
      aesenc  xmm10, xmm4
      aesenc  xmm11, xmm4
    }
    _XMM3 = _mm_xor_si128(v205, _mm_slli_si128(v209, 8));
    __asm { aesenc  xmm12, xmm4 }
    v216 = _mm_shuffle_epi32(_XMM3, 78);
    __asm
    {
      pclmulqdq xmm3, cs:xmmword_44E30, 0
      aesenc  xmm13, xmm4
      aesenc  xmm14, xmm4
    }
    _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 144));
    _XMM0 = _mm_xor_si128(v216, _XMM3);
    __asm
    {
      aesenc  xmm7, xmm4
      aesenc  xmm8, xmm4
    }
    v224 = _mm_shuffle_epi32(_XMM0, 78);
    __asm
    {
      pclmulqdq xmm0, cs:xmmword_44E30, 0
      aesenc  xmm9, xmm4
      aesenc  xmm10, xmm4
      aesenc  xmm11, xmm4
      aesenc  xmm12, xmm4
      aesenc  xmm13, xmm4
      aesenc  xmm14, xmm4
    }
    a8 = _mm_xor_si128(_mm_xor_si128(_XMM0, v224), _mm_xor_si128(_mm_srli_si128(v209, 8), v196));
    v232 = _mm_loadu_si128((const __m128i *)(a6 + 160));
    if ( a7 > 0xA0 )
    {
      _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 160));
      __asm
      {
        aesenc  xmm7, xmm4
        aesenc  xmm8, xmm4
        aesenc  xmm9, xmm4
        aesenc  xmm10, xmm4
        aesenc  xmm11, xmm4
        aesenc  xmm12, xmm4
        aesenc  xmm13, xmm4
        aesenc  xmm14, xmm4
      }
      _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 176));
      __asm
      {
        aesenc  xmm7, xmm4
        aesenc  xmm8, xmm4
        aesenc  xmm9, xmm4
        aesenc  xmm10, xmm4
        aesenc  xmm11, xmm4
        aesenc  xmm12, xmm4
        aesenc  xmm13, xmm4
        aesenc  xmm14, xmm4
      }
      v232 = _mm_loadu_si128((const __m128i *)(a6 + 192));
      if ( a7 > 0xC0 )
      {
        _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 192));
        __asm
        {
          aesenc  xmm7, xmm4
          aesenc  xmm8, xmm4
          aesenc  xmm9, xmm4
          aesenc  xmm10, xmm4
          aesenc  xmm11, xmm4
          aesenc  xmm12, xmm4
          aesenc  xmm13, xmm4
          aesenc  xmm14, xmm4
        }
        _XMM4 = _mm_loadu_si128((const __m128i *)(a6 + 208));
        __asm
        {
          aesenc  xmm7, xmm4
          aesenc  xmm8, xmm4
          aesenc  xmm9, xmm4
          aesenc  xmm10, xmm4
          aesenc  xmm11, xmm4
          aesenc  xmm12, xmm4
          aesenc  xmm13, xmm4
          aesenc  xmm14, xmm4
        }
        v232 = _mm_loadu_si128((const __m128i *)(a6 + 224));
      }
    }
    _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)a3), v232);
    __asm { aesenclast xmm7, xmm4 }
    _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 16)), v232);
    __asm { aesenclast xmm8, xmm4 }
    _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 32)), v232);
    __asm { aesenclast xmm9, xmm4 }
    _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 48)), v232);
    __asm { aesenclast xmm10, xmm4 }
    _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 64)), v232);
    __asm { aesenclast xmm11, xmm4 }
    _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 80)), v232);
    __asm { aesenclast xmm12, xmm4 }
    _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 96)), v232);
    __asm { aesenclast xmm13, xmm4 }
    _XMM4 = _mm_xor_si128(_mm_loadu_si128((const __m128i *)(a3 + 112)), v232);
    __asm { aesenclast xmm14, xmm4 }
    a9 = _mm_loadu_si128((const __m128i *)&Lbswap_mask_3);
    _mm_storeu_si128((__m128i *)a4, _XMM7);
    a10 = _mm_shuffle_epi8(_XMM7, a9);
    _mm_storeu_si128((__m128i *)(a4 + 16), _XMM8);
    a11 = _mm_shuffle_epi8(_XMM8, a9);
    _mm_storeu_si128((__m128i *)(a4 + 32), _XMM9);
    a12 = _mm_shuffle_epi8(_XMM9, a9);
    _mm_storeu_si128((__m128i *)(a4 + 48), _XMM10);
    a13 = _mm_shuffle_epi8(_XMM10, a9);
    _mm_storeu_si128((__m128i *)(a4 + 64), _XMM11);
    a14 = _mm_shuffle_epi8(_XMM11, a9);
    _mm_storeu_si128((__m128i *)(a4 + 80), _XMM12);
    a15 = _mm_shuffle_epi8(_XMM12, a9);
    _mm_storeu_si128((__m128i *)(a4 + 96), _XMM13);
    a16 = _mm_shuffle_epi8(_XMM13, a9);
    _mm_storeu_si128((__m128i *)(a4 + 112), _XMM14);
    a17 = _mm_shuffle_epi8(_XMM14, a9);
    a3 += 128LL;
    a4 += 128LL;
    v269 = __OFSUB__(a2, 128LL);
    a2 -= 128LL;
  }
  while ( !((a2 < 0) ^ v269) );
  return End_Main_Encrypt_Loop_0(
           a3,
           a4,
           a1,
           a2,
           _R8,
           a6,
           a19,
           DWORD2(a19),
           a20,
           DWORD2(a20),
           a21,
           DWORD2(a21),
           a22,
           DWORD2(a22),
           a23,
           DWORD2(a23),
           a24,
           a25,
           DWORD2(a25),
           a26,
           DWORD2(a26),
           a27,
           a28,
           a29,
           a30,
           a31,
           a32,
           a33,
           a34,
           a35,
           a36,
           a37,
           a38,
           a39,
           a40,
           a41,
           a42,
           a43,
           a44,
           a45,
           a46,
           a47,
           a48,
           a49,
           a50,
           a51);
}
// 45400: array has been used for an input argument
// 45B03: using guessed type __int64 __fastcall End_Main_Encrypt_Loop_0(_DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, _DWORD, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128, __int128);

//----- (0000000000045B03) ----------------------------------------------------
#error "460D2: positive sp value has been found (funcsize=0)"

//----- (00000000000460E0) ----------------------------------------------------
int __usercall gcmDecrypt_SupplementalSSE3@<eax>(__int64 a1@<rdx>, unsigned __int64 a2@<rcx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r9>, __m128i a6@<xmm0>, __m128i a7@<xmm1>, __m128i a8@<xmm2>, __m128i a9@<xmm3>, __m128i a10@<xmm4>, __m128i a11@<xmm5>, __m128i a12@<xmm6>, __m128i a13@<xmm7>, __m128i a14@<xmm8>, __m128i a15@<xmm9>, __m128i a16@<xmm10>, __m128i a17@<xmm11>, __m128i a18@<xmm12>, __m128i a19@<xmm13>, __m128i a20@<xmm14>, __m128i a21@<xmm15>)
{
  __int64 v21; // r10@1
  __int128 v23; // [sp+0h] [bp-110h]@1
  __int128 v24; // [sp+10h] [bp-100h]@1
  __int128 v25; // [sp+20h] [bp-F0h]@1
  __int128 v26; // [sp+30h] [bp-E0h]@1
  __int128 v27; // [sp+40h] [bp-D0h]@1
  __int128 v28; // [sp+50h] [bp-C0h]@1
  __int128 v29; // [sp+60h] [bp-B0h]@1
  __int128 v30; // [sp+70h] [bp-A0h]@1
  __int128 v31; // [sp+80h] [bp-90h]@1
  __int128 v32; // [sp+90h] [bp-80h]@1
  __int128 v33; // [sp+A0h] [bp-70h]@1
  __int128 v34; // [sp+B0h] [bp-60h]@1
  __int128 v35; // [sp+C0h] [bp-50h]@1
  __int128 v36; // [sp+D0h] [bp-40h]@1
  __int128 v37; // [sp+E0h] [bp-30h]@1
  __int128 v38; // [sp+F0h] [bp-20h]@1

  _mm_store_si128((__m128i *)&v23, a6);
  _mm_store_si128((__m128i *)&v24, a7);
  _mm_store_si128((__m128i *)&v25, a8);
  _mm_store_si128((__m128i *)&v26, a9);
  _mm_store_si128((__m128i *)&v27, a10);
  _mm_store_si128((__m128i *)&v28, a11);
  _mm_store_si128((__m128i *)&v29, a12);
  _mm_store_si128((__m128i *)&v30, a13);
  _mm_store_si128((__m128i *)&v31, a14);
  _mm_store_si128((__m128i *)&v32, a15);
  _mm_store_si128((__m128i *)&v33, a16);
  _mm_store_si128((__m128i *)&v34, a17);
  _mm_store_si128((__m128i *)&v35, a18);
  _mm_store_si128((__m128i *)&v36, a19);
  _mm_store_si128((__m128i *)&v37, a20);
  _mm_store_si128((__m128i *)&v38, a21);
  v21 = *(_DWORD *)(a5 + 240);
  JUMPOUT(a2 < 0x80, &loc_468D5);
  return Main_Decrypt_Loop_1(a3, a4, a1, a2 - 128);
}
// 461C0: using guessed type int __fastcall Main_Decrypt_Loop_1(_QWORD, _QWORD, _QWORD, _QWORD);

//----- (00000000000461C0) ----------------------------------------------------
#error "46C5C: positive sp value has been found (funcsize=0)"

//----- (0000000000046C5D) ----------------------------------------------------
signed __int64 __fastcall cczp_random_prime(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  char v5; // al@1
  unsigned __int64 v6; // r13@3
  __int64 v7; // r14@3
  unsigned __int64 v8; // r12@4
  unsigned __int64 v9; // r15@5
  int v10; // ecx@7
  signed __int64 result; // rax@7
  bool v12; // zf@7
  __int64 v13; // rdi@7
  __int64 v14; // rcx@7
  int v15; // eax@10
  __int64 v16; // rcx@12
  __int64 v17; // [sp+0h] [bp-80h]@5
  unsigned __int64 v18; // [sp+8h] [bp-78h]@4
  __int64 v19; // [sp+10h] [bp-70h]@4
  __int64 v20; // [sp+18h] [bp-68h]@1
  __int64 v21; // [sp+20h] [bp-60h]@1
  __int64 v22; // [sp+28h] [bp-58h]@3
  unsigned __int64 v23; // [sp+30h] [bp-50h]@3
  __int64 v24; // [sp+38h] [bp-48h]@1
  __int64 *v25; // [sp+40h] [bp-40h]@5
  unsigned __int64 v26; // [sp+48h] [bp-38h]@3
  __int64 v27; // [sp+50h] [bp-30h]@1

  v20 = a4;
  v24 = a3;
  v4 = a2;
  v21 = a2;
  v27 = *(_QWORD *)off_69010[0];
  v5 = 64;
  if ( a1 & 0x3F )
    v5 = a1 & 0x3F;
  v23 = 0xFFFFFFFFFFFFFFFFLL >> (64 - v5);
  v22 = 1LL << (v5 - 1);
  v6 = (unsigned __int64)(a1 + 63) >> 6;
  *(_QWORD *)a2 = v6;
  v26 = 8 * v6;
  v7 = a2 + 16;
  if ( !(*(int (__fastcall **)(__int64, unsigned __int64, __int64))a4)(a4, 8 * v6, a2 + 16) )
  {
    v19 = 1LL << ((unsigned __int8)a1 - 2);
    v8 = (unsigned __int64)(a1 - 2) >> 6;
    v18 = (unsigned __int64)(a1 - 2) >> 6;
    do
    {
      v25 = &v17;
      *(_QWORD *)(v4 + 8 * v6 + 8) = v22 | v23 & *(_QWORD *)(v4 + 8 * v6 + 8);
      *(_QWORD *)(v4 + 8 * v8 + 16) |= v19;
      *(_BYTE *)(v4 + 16) |= 1u;
      v9 = (8 * v6 + 15) & 0xFFFFFFFFFFFFFFF0LL;
      ccn_sub1(v6, (__int64)((char *)&v17 - v9), v7, 1LL);
      ccn_gcd(v6, (char *)&v17 - v9, (__int64)((char *)&v17 - v9), v24);
      if ( ccn_n(v6, (__int64)((char *)&v17 - v9)) != 1 || *(__int64 *)((char *)&v17 - v9) != 1 )
      {
        v13 = v20;
        v4 = v21;
        v14 = v26;
      }
      else
      {
        v4 = v21;
        v10 = cczp_rabin_miller(v21, 16LL);
        result = 0LL;
        v12 = v10 == 0;
        v13 = v20;
        v14 = v26;
        if ( !v12 )
          goto LABEL_12;
      }
      v26 = v14;
      v15 = (*(int (__fastcall **)(__int64, __int64, __int64))v13)(v13, v14, v7);
      v8 = v18;
    }
    while ( !v15 );
  }
  result = 0xFFFFFFFFLL;
LABEL_12:
  v16 = *(_QWORD *)off_69010[0];
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000046E01) ----------------------------------------------------
int __fastcall cczp_sqr_ws(signed __int64 *a1, __int64 a2, __int64 a3, __int64 *a4)
{
  return cczp_mul_ws(a1, a2, a3, (unsigned __int64 *)a3, a4);
}

//----- (0000000000046E14) ----------------------------------------------------
__int64 __fastcall cczp_sqr(signed __int64 *a1, __int64 a2, __int64 a3)
{
  return cczp_mul(a1, a2, a3, (unsigned __int64 *)a3);
}

//----- (0000000000046E21) ----------------------------------------------------
__int64 __fastcall cczp_sub(__int64 *a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  __int64 result; // rax@1

  v4 = *a1;
  result = ccn_sub(*a1, a2, a3, a4);
  if ( result )
    result = ccn_add(v4, a2, (__int64)(a1 + 2), a2);
  return result;
}

//----- (0000000000046E6B) ----------------------------------------------------
__int64 __usercall corecrypto_kext_start@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>, char *a3@<rsi>)
{
  char *v3; // r14@1
  signed int v4; // er15@1
  char *v5; // rsi@1
  signed int v6; // ebx@1
  char v7; // r13@5
  __int64 v8; // rdi@6
  __int64 v10; // [sp-4h] [bp-30h]@1
  __int64 v11; // [sp-4h] [bp-30h]@11

  v10 = a1;
  v3 = a3;
  v4 = 0;
  kprintf("corecrypto_kext_start called\n", a3);
  HIDWORD(v10) = 0;
  v5 = (char *)&v10 + 4;
  v6 = 2;
  if ( PE_parse_boot_argn("fips_mode", (char *)&v10 + 4, 4LL) )
    v6 = HIDWORD(v10);
  if ( !v6 )
  {
    v8 = (__int64)"Bypassing FIPS mode for kernel space!\n";
LABEL_14:
    kprintf(v8, v5);
    goto LABEL_15;
  }
  if ( (unsigned int)(v6 - 1) < 2 )
  {
    v7 = 0;
    v4 = 0;
    if ( v6 == 1 )
    {
      kprintf("fips_mode being set to 1, verbose is set to 1!\n", (char *)&v10 + 4);
      v6 = 1;
      v7 = 1;
      v4 = 1;
    }
  }
  else
  {
    v7 = 0;
    kprintf("fips_mode forced to 2!\n", (char *)&v10 + 4);
    v6 = 2;
    v4 = 0;
  }
  kprintf("kext fips mode set to %d\n", (unsigned int)v6);
  v5 = v3;
  if ( (unsigned int)KEXT_FIPSPost(a2, v3) )
  {
    if ( v6 == 2 )
    {
      panic("FIPS Kernel POST Failed!", v3, v11);
      goto LABEL_15;
    }
    if ( v7 )
    {
      v8 = (__int64)"KEXT_FIPS Post failed but the failure is being ignored\n";
      goto LABEL_14;
    }
  }
LABEL_15:
  kpis = (__int64)ccdigest_init;
  qword_6BC78 = (__int64)ccdigest_update;
  qword_6BC88 = (__int64)ccdigest;
  qword_6BC90 = ccmd5_di();
  qword_6BC98 = (__int64)ccsha1_di();
  qword_6BCA0 = (__int64)ccsha256_di();
  qword_6BCA8 = ccsha384_di();
  qword_6BCB0 = ccsha512_di();
  qword_6BCB8 = (__int64)cchmac_init;
  qword_6BCC0 = (__int64)cchmac_update;
  qword_6BCC8 = (__int64)cchmac_final;
  qword_6BCD0 = (__int64)cchmac;
  qword_6BCD8 = (__int64)ccaes_ecb_encrypt_mode();
  qword_6BCE0 = (__int64)ccaes_ecb_decrypt_mode();
  qword_6BCE8 = (__int64)ccaes_cbc_encrypt_mode();
  qword_6BCF0 = (__int64)ccaes_cbc_decrypt_mode();
  qword_6BCF8 = (__int64)ccaes_xts_encrypt_mode();
  qword_6BD00 = (__int64)ccaes_xts_decrypt_mode();
  qword_6BD08 = ccdes_ecb_encrypt_mode();
  qword_6BD10 = ccdes_ecb_decrypt_mode();
  qword_6BD18 = (__int64)ccdes_cbc_encrypt_mode();
  qword_6BD20 = (__int64)ccdes_cbc_decrypt_mode();
  qword_6BD28 = ccdes3_ecb_encrypt_mode();
  qword_6BD30 = ccdes3_ecb_decrypt_mode();
  qword_6BD38 = (__int64)ccdes3_cbc_encrypt_mode();
  qword_6BD40 = (__int64)ccdes3_cbc_decrypt_mode();
  qword_6BD48 = (__int64)ccrc4();
  qword_6BD50 = ccblowfish_ecb_encrypt_mode();
  qword_6BD58 = ccblowfish_ecb_decrypt_mode();
  qword_6BD60 = cccast_ecb_encrypt_mode();
  qword_6BD68 = cccast_ecb_decrypt_mode();
  qword_6BD70 = (__int64)ccdes_key_is_weak;
  qword_6BD78 = (__int64)ccdes_key_set_odd_parity;
  qword_6BD80 = 0LL;
  qword_6BD88 = 0LL;
  register_crypto_functions(&kpis, v5);
  if ( v4 )
    kprintf("corecrypto_kext_start completed sucessfully\n", v5);
  return 0LL;
}
// 4B8B6: using guessed type int __fastcall PE_parse_boot_argn(_QWORD, _QWORD, _QWORD);
// 4B8D4: using guessed type int __fastcall kprintf(_QWORD, _QWORD);
// 4B904: using guessed type int __fastcall register_crypto_functions(_QWORD, _QWORD);
// 6BC70: using guessed type __int64 kpis;
// 6BC78: using guessed type __int64 qword_6BC78;
// 6BC88: using guessed type __int64 qword_6BC88;
// 6BC90: using guessed type __int64 qword_6BC90;
// 6BC98: using guessed type __int64 qword_6BC98;
// 6BCA0: using guessed type __int64 qword_6BCA0;
// 6BCA8: using guessed type __int64 qword_6BCA8;
// 6BCB0: using guessed type __int64 qword_6BCB0;
// 6BCB8: using guessed type __int64 qword_6BCB8;
// 6BCC0: using guessed type __int64 qword_6BCC0;
// 6BCC8: using guessed type __int64 qword_6BCC8;
// 6BCD0: using guessed type __int64 qword_6BCD0;
// 6BCD8: using guessed type __int64 qword_6BCD8;
// 6BCE0: using guessed type __int64 qword_6BCE0;
// 6BCE8: using guessed type __int64 qword_6BCE8;
// 6BCF0: using guessed type __int64 qword_6BCF0;
// 6BCF8: using guessed type __int64 qword_6BCF8;
// 6BD00: using guessed type __int64 qword_6BD00;
// 6BD08: using guessed type __int64 qword_6BD08;
// 6BD10: using guessed type __int64 qword_6BD10;
// 6BD18: using guessed type __int64 qword_6BD18;
// 6BD20: using guessed type __int64 qword_6BD20;
// 6BD28: using guessed type __int64 qword_6BD28;
// 6BD30: using guessed type __int64 qword_6BD30;
// 6BD38: using guessed type __int64 qword_6BD38;
// 6BD40: using guessed type __int64 qword_6BD40;
// 6BD48: using guessed type __int64 qword_6BD48;
// 6BD50: using guessed type __int64 qword_6BD50;
// 6BD58: using guessed type __int64 qword_6BD58;
// 6BD60: using guessed type __int64 qword_6BD60;
// 6BD68: using guessed type __int64 qword_6BD68;
// 6BD70: using guessed type __int64 qword_6BD70;
// 6BD78: using guessed type __int64 qword_6BD78;
// 6BD80: using guessed type __int64 qword_6BD80;
// 6BD88: using guessed type __int64 qword_6BD88;

//----- (0000000000047142) ----------------------------------------------------
__int64 corecrypto_kext_stop()
{
  return 0LL;
}

//----- (000000000004714A) ----------------------------------------------------
__int64 __fastcall KEXT_FIPSPost(__int64 a1, char *a2)
{
  __int64 v2; // rbx@1
  int v3; // er15@1
  signed int v4; // ebx@3
  const void **v5; // r13@3
  __int64 (*v6)[2]; // r14@3
  __int64 (*v7)[2]; // r12@3
  __int64 v8; // rdi@4
  __int64 v9; // r14@8
  __int64 v10; // rax@8
  __int64 v11; // rdi@10
  __int64 v12; // rdi@12
  __int64 v14; // rax@16
  __int64 v15; // rax@17
  int v16; // eax@17
  signed __int64 v17; // rax@19
  __int64 v18; // rbx@21
  __int64 v19; // rbx@22
  signed int v20; // er14@25
  __int64 v21; // r15@25
  __int64 (*v22)[4]; // rbx@25
  __int64 (*v23)[4]; // r12@25
  __int64 v24; // ST00_8@25
  __int64 v25; // ST08_8@25
  __int64 v26; // rax@25
  __int64 *v27; // rbx@26
  const void *v28; // rdi@28
  size_t v29; // rdx@28
  signed int v30; // er15@30
  __int64 v31; // r13@30
  __int64 (*v32)[4]; // r12@30
  __int64 (*v33)[4]; // ST00_8@30
  __int64 v34; // ST08_8@30
  __int64 v35; // rax@30
  const void *v36; // rdx@30
  const void *v37; // r8@30
  __int64 *v38; // rbx@30
  __int64 v39; // rax@30
  unsigned __int64 v40; // rsi@30
  unsigned __int64 v41; // rcx@30
  __int64 v42; // rcx@35
  int v43; // er14@39
  char v44[16]; // [sp+10h] [bp-1C0h]@6
  __int64 v45; // [sp+20h] [bp-1B0h]@3
  __int64 v46; // [sp+28h] [bp-1A8h]@3
  void *v47; // [sp+30h] [bp-1A0h]@3
  size_t v48; // [sp+38h] [bp-198h]@3
  __int64 (*v49)[4]; // [sp+40h] [bp-190h]@3
  __int64 v50; // [sp+48h] [bp-188h]@3
  __int64 v51; // [sp+50h] [bp-180h]@3
  __int64 (*v52)[4]; // [sp+58h] [bp-178h]@3
  __int64 (*v53)[4]; // [sp+60h] [bp-170h]@3
  __int64 v54; // [sp+68h] [bp-168h]@3
  __int64 v55; // [sp+70h] [bp-160h]@3
  __int64 v56; // [sp+78h] [bp-158h]@3
  __int64 v57; // [sp+80h] [bp-150h]@3
  __int64 v58; // [sp+88h] [bp-148h]@3
  __int64 (*v59)[4]; // [sp+90h] [bp-140h]@3
  __int64 v60; // [sp+98h] [bp-138h]@3
  __int64 v61; // [sp+A0h] [bp-130h]@3
  int *v62; // [sp+A8h] [bp-128h]@3
  __int64 v63; // [sp+B0h] [bp-120h]@3
  __int64 v64; // [sp+B8h] [bp-118h]@3
  __int64 v65; // [sp+C0h] [bp-110h]@3
  __int64 v66; // [sp+C8h] [bp-108h]@3
  __int64 v67; // [sp+D0h] [bp-100h]@3
  __int64 v68; // [sp+D8h] [bp-F8h]@3
  __int64 v69; // [sp+E0h] [bp-F0h]@3
  __int64 v70; // [sp+E8h] [bp-E8h]@3
  __int64 v71; // [sp+F0h] [bp-E0h]@3
  __int64 v72; // [sp+F8h] [bp-D8h]@3
  __int64 v73; // [sp+100h] [bp-D0h]@3
  __int64 v74; // [sp+108h] [bp-C8h]@3
  __int64 v75; // [sp+110h] [bp-C0h]@3
  __int64 v76; // [sp+118h] [bp-B8h]@3
  __int64 v77; // [sp+120h] [bp-B0h]@3
  __int64 v78; // [sp+128h] [bp-A8h]@3
  __int64 v79; // [sp+130h] [bp-A0h]@3
  __int64 v80; // [sp+138h] [bp-98h]@3
  char v81; // [sp+148h] [bp-88h]@8
  char v82; // [sp+150h] [bp-80h]@17
  char v83; // [sp+160h] [bp-70h]@25
  __int64 v84; // [sp+1A0h] [bp-30h]@1

  v2 = off_69010[0];
  v84 = *(_QWORD *)off_69010[0];
  v3 = Integrity_POST(a1, a2);
  if ( !v3 )
  {
    v4 = 0;
    kprintf("corecrypto.kext FIPS integrity POST test passed!\n", a2);
    v45 = 16LL;
    LODWORD(v46) = 1;
    v47 = "4I\x1B&mL\\{\t";
    v48 = (size_t)"\v)Qr;=";
    v49 = (__int64 (*)[4])"\x06qcU>(Wt";
    v5 = (const void **)&v50;
    v50 = (__int64)"2]\x14)\b\x053";
    v51 = 16LL;
    LODWORD(v52) = 0;
    v53 = (__int64 (*)[4])"N*\x11";
    v54 = (__int64)"\x11\":hL\x18";
    v55 = (__int64)"6W\fr(\x16{p\x12";
    v56 = (__int64)"f\x1D9 r@f";
    v57 = 24LL;
    LODWORD(v58) = 1;
    v59 = (__int64 (*)[4])"\x05T*M}S\x13\f\x1CBhk";
    v60 = (__int64)"]!<~)<Do\x18";
    v61 = (__int64)"[a\x18U-(#!K";
    v62 = (int *)"4gq\rMo(";
    v63 = 24LL;
    LODWORD(v64) = 0;
    v65 = (__int64)"\x05\t\f\x017?\x14B$IKS";
    v66 = (__int64)"i\x18\x10J\x19Up";
    v67 = (__int64)"\x12\f{jU?o";
    v68 = (__int64)"~\x01`^}u";
    v69 = 32LL;
    LODWORD(v70) = 1;
    v71 = (__int64)"Se\x19EeLK\x16$)>\x02y'+G";
    v72 = (__int64)"\rRCW1N(x2";
    v73 = (__int64)"\x17Us\v\x1Ce\vB\x13p[>";
    v74 = (__int64)"HG(\x19\x11\n#7I70";
    v75 = 32LL;
    LODWORD(v76) = 0;
    v77 = (__int64)"\x10B**lA%c)\x10\x1C' \x04";
    v78 = (__int64)"\v^g\x1Fq<\x06\x15L";
    v79 = (__int64)"\x1BXxa^P\x12";
    v80 = (__int64)"ajmJI";
    v6 = ccaes_cbc_encrypt_mode();
    v7 = ccaes_cbc_decrypt_mode();
    do
    {
      v8 = (__int64)v6;
      if ( !*((_DWORD *)v5 - 8) )
        v8 = (__int64)v7;
      cccbc_one_shot(v8, (__int64)*(v5 - 5), (__int64)*(v5 - 3), *(v5 - 2), (__int64)*(v5 - 1), (__int64)v44);
      a2 = v44;
      if ( memcmp(*v5, v44, 0x10uLL) )
      {
        v12 = (__int64)"corecrypto.kext FIPS AES CBC POST test failed\n";
        goto LABEL_13;
      }
      ++v4;
      v5 += 6;
    }
    while ( v4 < 6 );
    kprintf("corecrypto.kext FIPS AES CBC POST test passed!\n", v44);
    v9 = (__int64)ccdes3_cbc_decrypt_mode();
    v10 = (__int64)ccdes3_cbc_encrypt_mode();
    cccbc_one_shot(
      v10,
      24LL,
      (__int64)"\x01OQ^\x01OQ^\x01OQ^",
      "}\x19j",
      (__int64)"nf'*",
      (__int64)&v81);
    a2 = &v81;
    v3 = memcmp("T\fG&@=", &v81, 8uLL);
    if ( !v3 )
    {
      cccbc_one_shot(v9, 24LL, (__int64)"\x15y^d\x15y^d\x15y^d", "", (__int64)"G0x0", (__int64)&v81);
      a2 = &v81;
      v3 = memcmp(byte_67763, &v81, 8uLL);
      if ( !v3 )
        goto LABEL_16;
    }
    v11 = (__int64)"corecrypto.kext FIPS TDES CBC POST test failed\n";
    goto LABEL_11;
  }
  kprintf("corecrypto.kext FIPS integrity POST test failed\n", a2);
  while ( *(_QWORD *)v2 != v84 )
  {
LABEL_16:
    kprintf("corecrypto.kext FIPS TDES CBC POST test passed!\n", a2);
    v14 = (__int64)ccaes_ecb_encrypt_mode();
    ccecb_one_shot_0(v14, (__int64)v44, v14, (__int64)"\x0F*\\y");
    a2 = v44;
    v3 = -1;
    if ( memcmp("=06EF\x16qg\x10&", v44, 0x10uLL)
      || (v15 = (__int64)ccaes_ecb_decrypt_mode(),
          ccecb_one_shot_0(v15, (__int64)&v82, v15, (__int64)v44),
          a2 = &v82,
          v16 = memcmp("\x0F*\\y", &v82, 0x10uLL),
          v3 = -(v16 != 0),
          v16) )
    {
      v11 = (__int64)"corecrypto.kext FIPS AES ECB POST test failed\n";
      goto LABEL_11;
    }
    kprintf("corecrypto.kext FIPS AES ECB POST test passed!\n", &v82);
    LODWORD(v45) = 41;
    memset(v44, 0, 0x10uLL);
    v44[0] = 41;
    v17 = 1LL;
    do
    {
      v44[v17] = *((_BYTE *)&v45 + v17);
      ++v17;
    }
    while ( v17 != 4 );
    v18 = (__int64)ccaes_xts_encrypt_mode();
    memset(&v82, 0, 0x10uLL);
    ccxts_one_shot(v18, (__int64)v44, (__int64)"\x18\x14{\x05M\x1E8hG", (__int64)&v82);
    a2 = "\x1A8\x15\x1CZ_";
    v3 = -1;
    if ( !memcmp(&v82, "\x1A8\x15\x1CZ_", 0x10uLL) )
    {
      v19 = (__int64)ccaes_xts_decrypt_mode();
      memset(&v82, 0, 0x10uLL);
      ccxts_one_shot(v19, (__int64)v44, (__int64)"\x1A8\x15\x1CZ_", (__int64)&v82);
      a2 = "\x18\x14{\x05M\x1E8hG";
      v3 = -(memcmp(&v82, "\x18\x14{\x05M\x1E8hG", 0x10uLL) != 0);
    }
    if ( v3 )
    {
      v11 = (__int64)"corecrypto.kext FIPS AES XTS POST test failed\n";
LABEL_11:
      kprintf(v11, a2);
      v2 = off_69010[0];
    }
    else
    {
      v20 = 0;
      kprintf("corecrypto.kext FIPS AES XTS POST test passed!\n", a2);
      v21 = (__int64)ccsha1_di();
      v22 = ccsha224_di();
      v23 = ccsha256_di();
      v24 = ccsha384_di();
      v25 = ccsha512_di();
      memset(&v83, 0, 0x40uLL);
      v45 = v21;
      v46 = (__int64)"2~R";
      v47 = "1,+\x0F\x04\t:(";
      v48 = 20LL;
      v49 = v22;
      v50 = (__int64)"\x1E;j;(i*";
      v51 = (__int64)"\x15\x11u\x02:*bK{gB9-:+1\v";
      v52 = (__int64 (*)[4])&dword_1C;
      v53 = v23;
      v54 = (__int64)";J6FM\x14(";
      v55 = (__int64)"4\x1A#\x01wGFKAT\t?\\b";
      v56 = 32LL;
      v57 = v24;
      v58 = (__int64)"#\x15j";
      v59 = (__int64 (*)[4])"=~<?s\x16\x10\x1FO{aYS\rcd~K\x03.\x06\v";
      v60 = 48LL;
      v61 = v25;
      v62 = &dword_67724;
      v63 = (__int64)"\\!?\x1E\x11'8Tf|jiDi)UxRHjz\\wp|_\bEF\x02";
      v64 = 64LL;
      ccdigest(
        (__int64)"\\!?\x1E\x11'8Tf|jiDi)UxRHjz\\wp|_\bEF\x02",
        "2~R",
        (__int64)&v83,
        v21,
        0xAuLL);
      a2 = &v83;
      LODWORD(v26) = memcmp(v47, &v83, v48);
      if ( (_DWORD)v26 )
      {
LABEL_29:
        v12 = (__int64)"corecrypto.kext FIPS SHA POST test failed\n";
LABEL_13:
        kprintf(v12, a2);
        v3 = -1;
        v2 = off_69010[0];
      }
      else
      {
        v27 = (__int64 *)&v52;
        while ( 1 )
        {
          ++v20;
          if ( v20 > 4 )
            break;
          ccdigest(v26, (const void *)*(v27 - 2), (__int64)&v83, *(v27 - 3), 0xAuLL);
          v28 = (const void *)*(v27 - 1);
          v29 = *v27;
          v27 += 4;
          a2 = &v83;
          LODWORD(v26) = memcmp(v28, &v83, v29);
          if ( (_DWORD)v26 )
            goto LABEL_29;
        }
        v30 = 0;
        kprintf("corecrypto.kext FIPS SHA POST test passed!\n", a2);
        memset(&v83, 0, 0x40uLL);
        v31 = (__int64)ccsha1_di();
        v32 = ccsha224_di();
        v33 = ccsha256_di();
        v34 = ccsha384_di();
        v35 = ccsha512_di();
        v45 = v31;
        v46 = 50LL;
        v36 = "\x1B1*\x16\x1E\x05D[J6\x034:r'P`wO\x01";
        v47 = "\x1B1*\x16\x1E\x05D[J6\x034:r'P`wO\x01";
        v48 = 128LL;
        v37 = "k[`X'LM[hz92\x04!\b\t=\bN\x1Dw B\x19\r8U\x0F\"yr.\x0E,3\x02{7VA\x04gkC$\x03;\x1F?\x01O\x10A\a^\b2Aq;y\x10'!Q\ruUHnb\x10";
        v49 = (__int64 (*)[4])"k[`X'LM[hz92\x04!\b\t=\bN\x1Dw B\x19\r8U\x0F\"yr.\x0E,3\x02{7VA\x04gkC$\x03;\x1F?\x01O\x10A\a^\b2Aq;y\x10'!Q\ruUHnb\x10";
        v50 = 20LL;
        v51 = (__int64)"npk^\x12t$[";
        v52 = v32;
        v53 = (__int64 (*)[4])(&stru_20 + 18);
        v54 = (__int64)"vg~d6D>T{\vk\x1FQ>Y\x12v\bz:9\f";
        v55 = 128LL;
        v38 = &v56;
        v56 = (__int64)byte_675F1;
        v57 = 28LL;
        v58 = (__int64)"[$M\x1C\v\x1F\"vb .G";
        v59 = v33;
        v60 = 50LL;
        v61 = (__int64)"\x1Cx.\x1DM_#h;)-:tR\x0482:-_\x19-%M]`je";
        v62 = (_DWORD *)(&stru_68 + 24);
        v63 = (__int64)word_67672;
        v64 = 32LL;
        v65 = (__int64)"\x1E\x16uk\b\b\x02TO6\x18)4";
        v66 = v34;
        v67 = 110LL;
        v68 = (__int64)"h9\x02+/H\x0Ei\x1EGK\x03&\x03\x19XD Rh\x14g\x1Fb\x16b3\x04(\x02F):;xcx7>\\\\Y\x1FJ\x1Dzy.\x01J|";
        v69 = 128LL;
        v70 = (__int64)"i|t\x14+z= \x17~\x19m!9~\x13Qw:$\x1E&+C6\x15jH}\"Q_R@s\x18O\x03BegG)piup\b0c@`J.\x02";
        v71 = 48LL;
        v72 = (__int64)byte_676F3;
        v73 = v35;
        v74 = 110LL;
        v75 = (__int64)"\x14b7<<e\"0`3D`O)V\x18fH\x0F\x12cC]\x1D\x18\x06\x1EWF\rND&QK\x19a\x1FE\r\x01\x03mL\x03\b,&URkx@\x0FO\x03_\r(AP\x19\x0F\x1A";
        v76 = 128LL;
        v77 = (__int64)"\x14zF~\v^$\n\x140MX\x02Tw\x06T'kCg_\x03E\x1BX\x0E\x12\x0E\x031aT\x1A*:\x06A)V\x03\x1C5qwi*4uB\x19\x0F1i@\x1C\rs~\b";
        v78 = 64LL;
        v39 = (__int64)"W\x05K|2A@hb\x1ECmaX;EaG\x19\x04y\"YLp0iAr4In\x12";
        v79 = (__int64)"W\x05K|2A@hb\x1ECmaX;EaG\x19\x04y\"YLp0iAr4In\x12";
        v40 = 50LL;
        v41 = 128LL;
        while ( 1 )
        {
          cchmac(v39, v36, v41, v31, v40, v37, (__int64)&v83);
          a2 = (char *)*(v38 - 5);
          LODWORD(v39) = memcmp(&v83, a2, *(v38 - 6));
          if ( (_DWORD)v39 )
          {
            v12 = (__int64)"corecrypto.kext FIPS HMAC POST test failed\n";
            goto LABEL_13;
          }
          ++v30;
          if ( v30 > 4 )
            break;
          v31 = *(v38 - 4);
          v40 = *(v38 - 3);
          v36 = (const void *)*(v38 - 2);
          v41 = *(v38 - 1);
          v37 = (const void *)*v38;
          v38 += 7;
        }
        kprintf("corecrypto.kext FIPS HMAC POST test passed!\n", a2);
        v3 = ECDSA_POST();
        if ( v3 )
        {
          v42 = (__int64)"corecrypto.kext FIPS ECDSA POST test failed\n";
        }
        else
        {
          kprintf("corecrypto.kext FIPS ECDSA POST test passed!\n", a2);
          v3 = DRBG_POST();
          if ( v3 )
          {
            v42 = (__int64)"corecrypto.kext FIPS DRBG POST test failed\n";
          }
          else
          {
            v3 = 0;
            kprintf("corecrypto.kext FIPS DRBG POST test passed!\n", a2);
            v43 = RSA_POST();
            if ( v43 )
            {
              kprintf("corecrypto.kext FIPS RSA POST test failed\n", a2);
              v3 = v43;
              goto LABEL_43;
            }
            kprintf("corecrypto.kext FIPS RSA POST test passed!\n", a2);
            v42 = (__int64)"corecrypto.kext FIPS POST passed!\n";
          }
        }
        kprintf(v42, a2);
LABEL_43:
        v2 = off_69010[0];
      }
    }
  }
  return (unsigned int)v3;
}
// 1C: using guessed type int;
// 20: using guessed type segment_command_64;
// 68: using guessed type section_64;
// 4B8D4: using guessed type int __fastcall kprintf(_QWORD, _QWORD);
// 67672: using guessed type __int16 word_67672[3];
// 67724: using guessed type int dword_67724;
// 69010: using guessed type __int64 off_69010[2];
// 4714A: using guessed type char var_1C0[16];

//----- (0000000000047B29) ----------------------------------------------------
int __fastcall Integrity_POST(__int64 a1, void *a2)
{
  __int64 v2; // r15@1
  __int64 *v3; // r13@5
  __int64 v4; // rbx@5
  signed __int64 v5; // r14@6
  __int64 v6; // rdi@7
  int result; // eax@9
  unsigned __int64 v8; // r12@14
  unsigned __int64 v9; // r13@15
  __int64 v10; // rcx@15
  const char *v11; // r14@16
  __int64 v12; // r15@16
  size_t v13; // rax@16
  unsigned __int64 v14; // r15@21
  char *v15; // rbx@21
  unsigned __int64 v16; // r14@21
  unsigned __int64 v17; // rbx@27
  __int64 v18; // r15@28
  unsigned __int64 v19; // r14@28
  char v20; // r12@35
  unsigned __int64 v21; // rdx@37
  signed __int64 v22; // rax@37
  const void *v23; // rcx@37
  void *v24; // rax@42
  __int64 v25; // [sp+0h] [bp-80h]@5
  __int64 *v26; // [sp+8h] [bp-78h]@15
  __int64 v27; // [sp+10h] [bp-70h]@14
  __int64 v28; // [sp+18h] [bp-68h]@14
  __int64 *v29; // [sp+20h] [bp-60h]@6
  char *v30; // [sp+28h] [bp-58h]@15
  __int64 v31; // [sp+30h] [bp-50h]@14
  __int64 v32; // [sp+38h] [bp-48h]@20
  unsigned __int64 v33; // [sp+40h] [bp-40h]@15
  char v34; // [sp+4Fh] [bp-31h]@14
  __int64 v35; // [sp+50h] [bp-30h]@1

  v2 = off_69010[0];
  v35 = *(_QWORD *)off_69010[0];
  if ( !a2 )
  {
    v6 = (__int64)"The AppleTEXTHash_t pointer was NOT passed to the Integrity_POST function\n";
    goto LABEL_9;
  }
  if ( *(_DWORD *)a2 != 1 || *((_DWORD *)a2 + 1) != 32 )
  {
    v6 = (__int64)"The AppleTEXTHash_t pointer passed to Integrity_POST function, is invalid\n";
    goto LABEL_9;
  }
  if ( !*((_QWORD *)a2 + 1) )
    goto LABEL_11;
  v3 = &v25 - 10;
  a2 = bytesToHexString(*((_QWORD *)a2 + 1), *((_DWORD *)a2 + 1), &v25 - 10);
  kprintf("Plist hmac value is    %s\n", a2);
  v4 = *(_QWORD *)(a1 + 156);
  if ( *(_DWORD *)v4 == -17958194 )
  {
    v29 = &v25;
    v5 = v4 + 28;
  }
  else
  {
    if ( *(_DWORD *)v4 != -17958193 )
    {
      kprintf("pLoadCommand is NULL!\n", a2);
      result = -1;
      goto LABEL_10;
    }
    v29 = &v25;
    v5 = v4 + 32;
  }
  v31 = v5;
  v8 = *(_DWORD *)(v4 + 16);
  v28 = (__int64)ccsha256_di();
  v34 = 0;
  v27 = (__int64)((char *)&v25
                - ((((*(_QWORD *)(v28 + 16) + 2LL * *(_QWORD *)(v28 + 8) + 19) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL));
  cchmac_init(v28, v27, 1uLL, &v34);
  if ( !v8 )
  {
LABEL_35:
    v20 = 1;
    goto LABEL_40;
  }
  v25 = v4;
  v26 = &v25 - 10;
  v30 = (char *)(v5 + 56);
  v9 = 0LL;
  v10 = v5;
  v31 = v5;
  v33 = v8;
  while ( 1 )
  {
    v11 = (const char *)(v10 + 8);
    v12 = v10;
    v13 = strlen("__TEXT");
    if ( strncmp("__TEXT", v11, v13) )
    {
      v10 = v12;
      if ( *(_DWORD *)v12 == 1 || *(_DWORD *)v12 == 25 )
        v10 = *(_DWORD *)(v12 + 4) + v12;
      goto LABEL_33;
    }
    v32 = v12;
    if ( *(_DWORD *)v31 == 1 )
    {
      v14 = *(_DWORD *)(v31 + 48);
      v15 = v30;
      v16 = 0LL;
      if ( *(_DWORD *)(v31 + 48) )
      {
        while ( strcmp(v15, "__text") || strcmp(v15 + 16, "__TEXT") )
        {
          v15 += 68;
          ++v16;
          if ( v16 >= v14 )
            goto LABEL_32;
        }
        v21 = *((_DWORD *)v15 + 9);
        v22 = *((_DWORD *)v15 + 10);
        v23 = (const void *)(v22 + v25);
        goto LABEL_39;
      }
LABEL_32:
      v10 = v32;
      goto LABEL_33;
    }
    if ( *(_DWORD *)v31 != 25 )
      goto LABEL_32;
    v10 = v32;
    v17 = *(_DWORD *)(v32 + 64);
    if ( *(_DWORD *)(v32 + 64) )
      break;
LABEL_33:
    ++v9;
    if ( v9 >= v33 )
    {
      v2 = off_69010[0];
      v3 = v26;
      goto LABEL_35;
    }
  }
  v18 = v32 + 72;
  v19 = 0LL;
  while ( strcmp((const char *)v18, "__text") || strcmp((const char *)(v18 + 16), "__TEXT") )
  {
    v18 += 80LL;
    ++v19;
    if ( v19 >= v17 )
      goto LABEL_32;
  }
  v22 = *(_DWORD *)(v18 + 48);
  v23 = (const void *)(v22 + v25);
  v21 = *(_QWORD *)(v18 + 40);
LABEL_39:
  cchmac_update(v22, v21, v23, v28, v27);
  v20 = 0;
  v2 = off_69010[0];
  v3 = v26;
LABEL_40:
  a2 = 0LL;
  memset(&v25 - 4, 0, 0x20uLL);
  if ( v20 )
  {
    kprintf("Integrity_POST: WARNING! could not create the hash!\n", 0LL);
    result = -1;
  }
  else
  {
    cchmac_final(v28, v27, (__int64)(&v25 - 4));
    v24 = bytesToHexString((__int64)(&v25 - 4), 0x20uLL, &v25 - 10);
    kprintf("Computed hmac value is %s\n", v24);
    a2 = v3;
    result = memcmp(&v25 - 10, v3, 0x41uLL);
  }
LABEL_10:
  while ( *(_QWORD *)v2 != v35 )
  {
LABEL_11:
    v6 = (__int64)"The AppleTEXTHash_t pointer passed to Integrity_POST function,has a null HASH pointer\n";
LABEL_9:
    kprintf(v6, a2);
    result = -1;
  }
  return result;
}
// 4B8D4: using guessed type int __fastcall kprintf(_QWORD, _QWORD);
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000047E66) ----------------------------------------------------
__int64 RSA_POST()
{
  __int64 v0; // r15@1
  signed int v1; // ebx@1
  void *v2; // r13@9
  __int64 result; // rax@14
  __int64 v4; // [sp+10h] [bp-1D0h]@7
  __int64 v5; // [sp+90h] [bp-150h]@7
  void *v6; // [sp+98h] [bp-148h]@7
  char v7; // [sp+A7h] [bp-139h]@11
  unsigned __int64 v8; // [sp+A8h] [bp-138h]@9
  __int64 v9; // [sp+B0h] [bp-130h]@7
  __int64 v10; // [sp+B8h] [bp-128h]@7
  __int64 v11; // [sp+C0h] [bp-120h]@7
  __int64 v12; // [sp+C8h] [bp-118h]@7
  char v13; // [sp+D0h] [bp-110h]@6
  char v14; // [sp+110h] [bp-D0h]@5
  char v15; // [sp+120h] [bp-C0h]@4
  char v16; // [sp+130h] [bp-B0h]@3
  char v17; // [sp+170h] [bp-70h]@2
  char v18; // [sp+180h] [bp-60h]@1
  char v19; // [sp+190h] [bp-50h]@9
  __int64 v20; // [sp+1A8h] [bp-38h]@1
  __int64 v21; // [sp+1B0h] [bp-30h]@1

  v0 = off_69010[0];
  v21 = *(_QWORD *)off_69010[0];
  v20 = 3LL;
  v1 = -1;
  if ( !(unsigned int)ccn_read_uint(2uLL, (__int64)&v18, 0xDuLL, (unsigned __int64)"\x13\x16D^\"6r")
    && !(unsigned int)ccn_read_uint(2uLL, (__int64)&v17, 0xDuLL, (unsigned __int64)"\x1A\b[\vs*\x1F2b")
    && !(unsigned int)ccn_read_uint(
                        8uLL,
                        (__int64)&v16,
                        0x40uLL,
                        (unsigned __int64)"Z3P;\x06b+>\\3\x117|+#RSD\vFA|\t\f\x1E\t\x06\x1B\x04")
    && !(unsigned int)ccn_read_uint(2uLL, (__int64)&v15, 0xDuLL, (unsigned __int64)"\x17\r}!Y\x1D\x18")
    && !(unsigned int)ccn_read_uint(2uLL, (__int64)&v14, 0xDuLL, (unsigned __int64)"\x175X:A*\x14")
    && !(unsigned int)ccn_read_uint(8uLL, (__int64)&v13, 0x40uLL, (unsigned __int64)qword_675B0) )
  {
    v12 = 16LL;
    v11 = 16LL;
    v10 = 16LL;
    v9 = 16LL;
    v6 = &v5 - 976;
    *(_QWORD *)v6 = 16LL;
    if ( (unsigned int)ccrsa_make_931_key(
                         0x400uLL,
                         1LL,
                         (__int64)&v20,
                         2LL,
                         (__int64)&v18,
                         2LL,
                         (__int64)&v17,
                         8LL,
                         (__int64)&v16,
                         2LL,
                         (__int64)&v15,
                         2LL,
                         (__int64)&v14,
                         8LL,
                         (__int64)&v13,
                         (__int64)(&v5 - 976),
                         (__int64)&v12,
                         &v4,
                         (__int64)&v11,
                         &v4,
                         (__int64)&v10,
                         &v4,
                         (__int64)&v9,
                         &v4) )
    {
      cc_clear(0x1E80uLL, v6);
    }
    else
    {
      memcpy(&v19, "ABCEDFGHIJKLMNOPRSTU", 0x14uLL);
      v8 = 128LL;
      v2 = v6;
      if ( (unsigned int)ccrsa_sign_pkcs1v15(
                           (__int64 *)v6,
                           (__int64)"\x06\x05+\x0E\x03\x02\x1A",
                           0x14uLL,
                           &v19,
                           (__int64)&v8,
                           &v4) )
      {
        cc_clear(0x1E80uLL, v2);
      }
      else if ( (unsigned int)ccrsa_verify_pkcs1v15(
                                (__int64 *)v2,
                                (__int64)"\x06\x05+\x0E\x03\x02\x1A",
                                0x14uLL,
                                &v19,
                                v8,
                                (unsigned __int64)&v4,
                                (__int64)&v7)
             || (v1 = 0, !v7) )
      {
        cc_clear(0x1E80uLL, v2);
        v1 = -1;
      }
    }
  }
  result = *(_QWORD *)v0;
  if ( *(_QWORD *)v0 == v21 )
    result = (unsigned int)v1;
  return result;
}
// 675B0: using guessed type __int64 qword_675B0[8];
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000481AD) ----------------------------------------------------
__int64 DRBG_POST()
{
  __int64 v0; // r15@1
  char *v1; // rbx@1
  int v2; // er14@1
  __int64 result; // rax@7
  __int64 v4; // [sp+10h] [bp-50h]@1
  __int64 (*v5)[2]; // [sp+18h] [bp-48h]@1
  __int64 v6; // [sp+20h] [bp-40h]@1
  int v7; // [sp+28h] [bp-38h]@1
  int v8; // [sp+2Ch] [bp-34h]@1
  char v9; // [sp+30h] [bp-30h]@1
  __int64 v10; // [sp+38h] [bp-28h]@1
  __int64 v11; // [sp+40h] [bp-20h]@1

  v0 = off_69010[0];
  v11 = *(_QWORD *)off_69010[0];
  memset(&v9, 0, 0x10uLL);
  v5 = ccaes_ecb_encrypt_mode();
  v6 = 16LL;
  v7 = 0;
  v8 = 1;
  ccdrbg_factory_nistctr((__int64)&DRBG_POST_info, (__int64)&v5);
  v1 = (char *)&v4 - ((DRBG_POST_info + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v11 = (__int64)"k\a{\x13\f\x10";
  v10 = 16LL;
  v2 = -1;
  if ( !qword_6BD98(
          &DRBG_POST_info,
          (char *)&v4 - ((DRBG_POST_info + 15) & 0xFFFFFFFFFFFFFFF0LL),
          16LL,
          "tz\x1F=1R\x13m",
          8LL,
          "")
    && !qword_6BDA0(v1, 16LL, "4.+\x10Xr", 16LL, "\x1A \v\x13g\x1A")
    && !qword_6BDA8(v1, 16LL, &v9, 0LL, 0LL)
    && !qword_6BDA0(v1, 16LL, "&#j)I", 16LL, "E^\tecm")
    && !qword_6BDA8(v1, 16LL, &v9, 0LL, 0LL) )
    v2 = memcmp(&v9, "RXKBIWQt", 0x10uLL);
  result = *(_QWORD *)v0;
  if ( *(_QWORD *)v0 == v11 )
    result = (unsigned int)v2;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];
// 6BD90: using guessed type __int64 DRBG_POST_info;
// 6BD98: using guessed type int (__fastcall *qword_6BD98)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD, _QWORD);
// 6BDA0: using guessed type int (__fastcall *qword_6BDA0)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD);
// 6BDA8: using guessed type int (__fastcall *qword_6BDA8)(_QWORD, _QWORD, _QWORD, _QWORD, _QWORD);

//----- (0000000000048325) ----------------------------------------------------
__int64 ECDSA_POST()
{
  __int64 v0; // r13@1
  __int64 *v1; // rax@1
  __int64 *v2; // r15@1
  signed __int64 v3; // rbx@1
  signed __int64 v4; // rax@1
  int v5; // er14@1
  __int64 result; // rax@1
  char v7[16]; // [sp+0h] [bp-40h]@1
  __int64 v8; // [sp+10h] [bp-30h]@1

  v0 = off_69010[0];
  v8 = *(_QWORD *)off_69010[0];
  ccrng_system_init((__int64)v7);
  v1 = ccec_get_cp(256LL);
  v2 = v1;
  v3 = 32 * *v1 | 0x10;
  v4 = ccn_bitlen(*v1, (__int64)(v1 + 2));
  bzero(
    &v7[-((2 * ((unsigned __int64)(v4 + 7) >> 3) + 24) & 0xFFFFFFFFFFFFFFF0LL)],
    2 * ((unsigned __int64)(v4 + 7) >> 3) + 9);
  v5 = ccec_generate_key_fips(v2, (__int64)v7, (__int64)&v7[-v3]);
  cc_clear(v3, &v7[-v3]);
  result = *(_QWORD *)v0;
  if ( *(_QWORD *)v0 == v8 )
    result = (unsigned int)v5;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];
// 48325: using guessed type char var_40[16];

//----- (00000000000483DF) ----------------------------------------------------
__int64 __fastcall ccxts_one_shot(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  char *v5; // rbx@1
  char *v6; // r14@1
  __int64 v8; // [sp+0h] [bp-40h]@1
  __int64 v9; // [sp+8h] [bp-38h]@1
  __int64 v10; // [sp+10h] [bp-30h]@1

  v9 = a4;
  v4 = a3;
  v10 = *(_QWORD *)off_69010[0];
  v5 = (char *)&v8 - ((*(_QWORD *)a1 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  v6 = (char *)&v8 - ((*(_QWORD *)(a1 + 8) + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  (*(void (__fastcall **)(__int64, char *, signed __int64, char *, _QWORD))(a1 + 24))(
    a1,
    (char *)&v8 - ((*(_QWORD *)a1 + 15LL) & 0xFFFFFFFFFFFFFFF0LL),
    16LL,
    &byte_6772F,
    "O6!rgTt\b-X");
  (*(void (__fastcall **)(char *, char *, __int64))(a1 + 32))(v5, v6, a2);
  (*(void (__fastcall **)(char *, char *, signed __int64, __int64, __int64))(a1 + 40))(v5, v6, 1LL, v4, v9);
  cc_clear(*(_QWORD *)a1, v5);
  cc_clear(*(_QWORD *)(a1 + 8), v6);
  return *(_QWORD *)off_69010[0];
}
// 6772F: using guessed type char byte_6772F;
// 69010: using guessed type __int64 off_69010[2];

//----- (00000000000484B3) ----------------------------------------------------
__int64 __usercall ccecb_one_shot_0@<rax>(__int64 a1@<rax>, __int64 a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>)
{
  __int64 v4; // r14@1
  __int64 v5; // r13@1
  char *v6; // rbx@1
  __int64 v8; // [sp+0h] [bp-30h]@1

  v8 = a1;
  v4 = a2;
  v5 = off_69010[0];
  v8 = *(_QWORD *)off_69010[0];
  v6 = (char *)&v8 - ((*(_QWORD *)a3 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  (*(void (__fastcall **)(__int64, char *, signed __int64, _QWORD))(a3 + 16))(a3, v6, 16LL, qword_67740);
  (*(void (__fastcall **)(char *, signed __int64, __int64, __int64))(a3 + 24))(v6, 1LL, a4, v4);
  cc_clear(*(_QWORD *)a3, v6);
  return *(_QWORD *)v5;
}
// 67740: using guessed type __int64 qword_67740[2];
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004853F) ----------------------------------------------------
__int64 __fastcall cccbc_one_shot(__int64 a1, __int64 a2, __int64 a3, const void *a4, __int64 a5, __int64 a6)
{
  __int64 v6; // r15@1
  const void *v7; // r13@1
  char *v8; // r12@1
  char *v9; // r14@1
  __int64 v11; // [sp+0h] [bp-40h]@1
  __int64 v12; // [sp+8h] [bp-38h]@1
  __int64 v13; // [sp+10h] [bp-30h]@1

  v12 = a6;
  v6 = a5;
  v7 = a4;
  v13 = *(_QWORD *)off_69010[0];
  v8 = (char *)&v11 - ((*(_QWORD *)a1 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  v9 = (char *)&v11 - ((*(_QWORD *)(a1 + 8) + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  (*(void (__fastcall **)(__int64, char *, __int64, __int64))(a1 + 16))(
    a1,
    (char *)&v11 - ((*(_QWORD *)a1 + 15LL) & 0xFFFFFFFFFFFFFFF0LL),
    a2,
    a3);
  if ( v7 )
    memcpy(v9, v7, *(_QWORD *)(a1 + 8));
  else
    bzero(v9, *(_QWORD *)(a1 + 8));
  (*(void (__fastcall **)(char *, char *, signed __int64, __int64, __int64))(a1 + 24))(v8, v9, 1LL, v6, v12);
  cc_clear(*(_QWORD *)a1, v8);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000048608) ----------------------------------------------------
void *__fastcall bytesToHexString(__int64 a1, unsigned __int64 a2, void *a3)
{
  void *v3; // r15@1
  void *result; // rax@1
  __int64 v5; // rbx@2

  v3 = a3;
  result = 0LL;
  if ( a2 <= 0x20 )
  {
    v5 = 0LL;
    memset(a3, 0, 0x41uLL);
    if ( a2 )
    {
      do
      {
        *((_BYTE *)v3 + 2 * v5) = *((_BYTE *)byteMap + (unsigned __int8)(*(_BYTE *)(a1 + v5) >> 4));
        *((_BYTE *)v3 + 2 * v5 + 1) = *((_BYTE *)byteMap + (*(_BYTE *)(a1 + v5) & 0xF));
        ++v5;
      }
      while ( a2 != v5 );
    }
    result = v3;
  }
  return result;
}
// 67770: using guessed type __int64 byteMap[2];

//----- (0000000000048677) ----------------------------------------------------
__int64 __fastcall deskey(__int64 a1, unsigned __int16 a2, void *a3)
{
  __int64 v3; // rax@1
  int v4; // ecx@2
  __int64 v5; // r12@3
  int v6; // er11@3
  int v7; // eax@4
  __int64 v8; // r9@6
  __int64 v9; // r8@6
  __int64 v10; // rsi@6
  __int64 v11; // rdx@6
  char v12; // bl@8
  __int64 v13; // rax@11
  signed __int64 v14; // rcx@12
  __int64 v15; // rdx@15
  __int64 v16; // rsi@16
  int v17; // ecx@16
  int v18; // eax@16
  __int64 v19; // rax@23
  int v20; // ecx@24
  unsigned int v21; // edx@24
  void *v23; // [sp+8h] [bp-1B8h]@1
  char v24[28]; // [sp+10h] [bp-1B0h]@10
  char v25[36]; // [sp+2Ch] [bp-194h]@15
  char v26[64]; // [sp+50h] [bp-170h]@2
  int v27; // [sp+90h] [bp-130h]@6
  int v28[31]; // [sp+94h] [bp-12Ch]@24
  int v29; // [sp+110h] [bp-B0h]@24
  int v30[31]; // [sp+114h] [bp-ACh]@24
  __int64 v31; // [sp+190h] [bp-30h]@1

  v23 = a3;
  v31 = *(_QWORD *)off_69010[0];
  v3 = 0LL;
  do
  {
    v4 = *((_DWORD *)bytebit + (*((_BYTE *)pc1 + v3) & 7));
    v26[v3] = (unsigned __int8)(v4 & *(_BYTE *)(a1 + ((unsigned __int64)*((_BYTE *)pc1 + v3) >> 3))) == v4;
    ++v3;
  }
  while ( v3 != 56 );
  v5 = 0LL;
  v6 = a2;
  do
  {
    v7 = 15 - v5;
    if ( v6 != 1 )
      v7 = v5;
    v8 = (unsigned int)(2 * v7);
    v9 = (unsigned int)(2 * v7 + 1);
    *(&v27 + v9) = 0;
    *(&v27 + v8) = 0;
    v10 = *((_BYTE *)totrot + v5);
    v11 = 0LL;
    do
    {
      if ( (unsigned int)(v10 + v11) > 0x1B )
        v12 = v26[(v10 + 4294967268LL + v11) & 0xFFFFFFFFLL];
      else
        v12 = *(&v26[v10] + v11);
      v24[v11++] = v12;
    }
    while ( v11 != 28 );
    v13 = 0LL;
    do
    {
      v14 = v10 + v13 + 28;
      if ( (unsigned int)v14 > 0x37 )
        v14 = (unsigned int)(v10 + v13);
      else
        v14 = (unsigned int)v14;
      v25[v13++] = v26[v14];
      v15 = 0LL;
    }
    while ( v13 != 28 );
    v16 = (__int64)pc2;
    v17 = 0;
    v18 = 0;
    do
    {
      if ( v24[*(_BYTE *)v16] )
      {
        v17 |= *(_DWORD *)((char *)bigbyte + v15);
        *(&v27 + v8) = v17;
      }
      if ( v24[*(_BYTE *)(v16 + 24)] )
      {
        v18 |= *(_DWORD *)((char *)bigbyte + v15);
        *(&v27 + v9) = v18;
      }
      ++v16;
      v15 += 4LL;
    }
    while ( v15 != 96 );
    ++v5;
  }
  while ( v5 != 16 );
  v19 = 0LL;
  do
  {
    v20 = *(&v27 + 2 * v19);
    v21 = v28[2 * v19];
    *(&v29 + 2 * v19) = (*(&v27 + 2 * v19) << 6) & 0x3F000000 | (*(&v27 + 2 * v19) << 10) & 0x3F0000 | ((unsigned int)v28[2 * v19] >> 10) & 0x3F00 | ((unsigned int)v28[2 * v19] >> 6) & 0x3F;
    v30[2 * v19++] = (v21 >> 4) & 0x3F00 | (v20 << 12) & 0x3F000000 | ((v20 & 0x3F) << 16) | v21 & 0x3F;
  }
  while ( (_DWORD)v19 != 16 );
  memcpy(v23, &v29, 0x80uLL);
  return *(_QWORD *)off_69010[0];
}
// 67780: using guessed type __int64 pc1[8];
// 677C0: using guessed type __int64 bytebit[4];
// 677E0: using guessed type __int64 totrot[2];
// 677F0: using guessed type __int64 pc2[6];
// 67820: using guessed type __int64 bigbyte[12];
// 69010: using guessed type __int64 off_69010[2];
// 48677: using guessed type char var_170[64];
// 48677: using guessed type char var_1B0[28];
// 48677: using guessed type char var_194[36];
// 48677: using guessed type int var_12C[31];
// 48677: using guessed type int var_AC[31];

//----- (00000000000488AC) ----------------------------------------------------
__int64 __fastcall desfunc(__int64 a1, __int64 a2)
{
  int v2; // ecx@1
  int v3; // edx@1
  int v4; // ecx@1
  int v5; // edx@1
  int v6; // eax@1
  int v7; // ecx@1
  int v8; // eax@1
  int v9; // edx@1
  int v10; // eax@1
  int v11; // edx@1
  int v12; // edi@1
  int v13; // eax@1
  int v14; // edi@1
  unsigned int v15; // ecx@1
  int v16; // eax@1
  int v17; // ecx@1
  unsigned int v18; // eax@1
  __int64 v19; // r9@1
  __int64 v20; // rdx@1
  int v21; // er8@2
  int v22; // er8@2
  int v23; // edx@2
  int v24; // edx@2
  int v25; // ecx@2
  int v26; // ecx@3
  unsigned int v27; // edx@3
  unsigned int v28; // esi@3
  int v29; // ecx@3
  int v30; // edx@3
  int v31; // ecx@3
  int v32; // eax@3
  int v33; // edx@3
  int v34; // eax@3
  int v35; // esi@3
  int v36; // eax@3
  int v37; // esi@3
  int v38; // ecx@3
  __int64 result; // rax@3
  __int64 v40; // [sp+10h] [bp-30h]@1

  v40 = a1;
  v2 = *(_DWORD *)(a1 + 4);
  v3 = (v2 ^ (*(_DWORD *)a1 >> 4)) & 0xF0F0F0F;
  v4 = v3 ^ v2;
  v5 = *(_DWORD *)a1 ^ 16 * v3;
  v6 = ((unsigned int)v5 >> 16) ^ (unsigned __int16)v4;
  v7 = v6 ^ v4;
  v8 = v5 ^ (v6 << 16);
  v9 = (v8 ^ ((unsigned int)v7 >> 2)) & 0x33333333;
  v10 = v9 ^ v8;
  v11 = v7 ^ 4 * v9;
  v12 = (v10 ^ ((unsigned int)v11 >> 8)) & 0xFF00FF;
  v13 = v12 ^ v10;
  v14 = __ROL4__(v11 ^ (v12 << 8), 1);
  v15 = (v14 ^ v13) & 0xAAAAAAAA;
  v16 = v15 ^ v13;
  v17 = v14 ^ v15;
  v18 = __ROL4__(v16, 1);
  v19 = 0LL;
  v20 = (__int64)SP2;
  do
  {
    v21 = __ROR4__(v17, 4);
    v22 = *(_DWORD *)(a2 + v19) ^ v21;
    v18 ^= *(_DWORD *)(v20 + 4LL * ((((unsigned int)v17 ^ *(_DWORD *)(a2 + v19 + 4)) >> 24) & 0x3F)) ^ *((_DWORD *)SP4 + ((((unsigned int)v17 ^ *(_DWORD *)(a2 + v19 + 4)) >> 16) & 0x3F)) ^ *((_DWORD *)SP6 + ((((unsigned int)v17 ^ *(_DWORD *)(a2 + v19 + 4)) >> 8) & 0x3F)) ^ *((_DWORD *)SP8 + (((unsigned __int8)v17 ^ *(_BYTE *)(a2 + v19 + 4)) & 0x3F)) ^ *((_DWORD *)SP1 + (((unsigned int)v22 >> 24) & 0x3F)) ^ *((_DWORD *)SP3 + (((unsigned int)v22 >> 16) & 0x3F)) ^ *((_DWORD *)SP5 + (((unsigned int)v22 >> 8) & 0x3F)) ^ *((_DWORD *)SP7 + (v22 & 0x3F));
    v23 = __ROR4__(v18, 4);
    v24 = *(_DWORD *)(a2 + v19 + 8) ^ v23;
    v25 = *((_DWORD *)SP4 + (((*(_DWORD *)(a2 + v19 + 12) ^ v18) >> 16) & 0x3F)) ^ *((_DWORD *)SP6
                                                                                   + (((*(_DWORD *)(a2 + v19 + 12) ^ v18) >> 8) & 0x3F)) ^ *((_DWORD *)SP8 + ((*(_BYTE *)(a2 + v19 + 12) ^ (unsigned __int8)v18) & 0x3F)) ^ *((_DWORD *)SP1 + (((unsigned int)v24 >> 24) & 0x3F)) ^ *((_DWORD *)SP3 + (((unsigned int)v24 >> 16) & 0x3F)) ^ *((_DWORD *)SP5 + (((unsigned int)v24 >> 8) & 0x3F)) ^ *((_DWORD *)SP7 + (v24 & 0x3F)) ^ v17;
    v20 = (__int64)SP2;
    v17 = *((_DWORD *)SP2 + (((*(_DWORD *)(a2 + v19 + 12) ^ v18) >> 24) & 0x3F)) ^ v25;
    v19 += 16LL;
  }
  while ( (_DWORD)v19 != 128 );
  v26 = __ROR4__(v17, 1);
  v27 = v26 ^ (v18 ^ v26) & 0xAAAAAAAA;
  v28 = __ROR4__(v18 ^ (v18 ^ v26) & 0xAAAAAAAA, 1);
  v29 = (v27 ^ (v28 >> 8)) & 0xFF00FF;
  v30 = v29 ^ v27;
  v31 = v28 ^ (v29 << 8);
  v32 = (v30 ^ ((unsigned int)v31 >> 2)) & 0x33333333;
  v33 = v32 ^ v30;
  v34 = v31 ^ 4 * v32;
  v35 = ((unsigned int)v33 >> 16) ^ (unsigned __int16)v34;
  v36 = v35 ^ v34;
  v37 = v33 ^ (v35 << 16);
  v38 = (v36 ^ ((unsigned int)v37 >> 4)) & 0xF0F0F0F;
  result = v38 ^ (unsigned int)v36;
  *(_DWORD *)v40 = v37 ^ 16 * v38;
  *(_DWORD *)(v40 + 4) = result;
  return result;
}
// 67880: using guessed type __int64 SP7[32];
// 67980: using guessed type __int64 SP5[32];
// 67A80: using guessed type __int64 SP3[32];
// 67B80: using guessed type __int64 SP1[32];
// 67C80: using guessed type __int64 SP8[32];
// 67D80: using guessed type __int64 SP6[32];
// 67E80: using guessed type __int64 SP4[32];
// 67F80: using guessed type __int64 SP2[32];

//----- (0000000000048AD2) ----------------------------------------------------
__int64 __fastcall desfunc3(__int64 a1, __int64 a2)
{
  int v2; // ecx@1
  int v3; // edx@1
  int v4; // ecx@1
  int v5; // edx@1
  int v6; // eax@1
  int v7; // ecx@1
  int v8; // eax@1
  int v9; // edx@1
  int v10; // eax@1
  int v11; // edx@1
  int v12; // edi@1
  int v13; // eax@1
  int v14; // edi@1
  unsigned int v15; // ecx@1
  int v16; // eax@1
  int v17; // ecx@1
  unsigned int v18; // eax@1
  __int64 v19; // r9@1
  int v20; // er8@2
  int v21; // er8@2
  int v22; // ebx@2
  int v23; // ebx@2
  __int64 v24; // r8@3
  int v25; // edx@4
  int v26; // edx@4
  int v27; // ebx@4
  int v28; // ebx@4
  __int64 v29; // r8@5
  int v30; // ebx@6
  int v31; // ebx@6
  int v32; // ebx@6
  int v33; // ebx@6
  int v34; // ecx@7
  unsigned int v35; // edx@7
  unsigned int v36; // esi@7
  int v37; // ecx@7
  int v38; // edx@7
  int v39; // ecx@7
  int v40; // eax@7
  int v41; // edx@7
  int v42; // eax@7
  int v43; // esi@7
  int v44; // eax@7
  int v45; // esi@7
  int v46; // ecx@7
  __int64 result; // rax@7
  __int64 v48; // [sp+8h] [bp-30h]@1

  v48 = a1;
  v2 = *(_DWORD *)(a1 + 4);
  v3 = (v2 ^ (*(_DWORD *)a1 >> 4)) & 0xF0F0F0F;
  v4 = v3 ^ v2;
  v5 = *(_DWORD *)a1 ^ 16 * v3;
  v6 = ((unsigned int)v5 >> 16) ^ (unsigned __int16)v4;
  v7 = v6 ^ v4;
  v8 = v5 ^ (v6 << 16);
  v9 = (v8 ^ ((unsigned int)v7 >> 2)) & 0x33333333;
  v10 = v9 ^ v8;
  v11 = v7 ^ 4 * v9;
  v12 = (v10 ^ ((unsigned int)v11 >> 8)) & 0xFF00FF;
  v13 = v12 ^ v10;
  v14 = __ROL4__(v11 ^ (v12 << 8), 1);
  v15 = (v14 ^ v13) & 0xAAAAAAAA;
  v16 = v15 ^ v13;
  v17 = v14 ^ v15;
  v18 = __ROL4__(v16, 1);
  v19 = 0LL;
  do
  {
    v20 = __ROR4__(v17, 4);
    v21 = *(_DWORD *)(a2 + v19) ^ v20;
    v18 ^= *((_DWORD *)SP2 + ((((unsigned int)v17 ^ *(_DWORD *)(a2 + v19 + 4)) >> 24) & 0x3F)) ^ *((_DWORD *)SP4
                                                                                                 + ((((unsigned int)v17 ^ *(_DWORD *)(a2 + v19 + 4)) >> 16) & 0x3F)) ^ *((_DWORD *)SP6 + ((((unsigned int)v17 ^ *(_DWORD *)(a2 + v19 + 4)) >> 8) & 0x3F)) ^ *((_DWORD *)SP8 + (((unsigned __int8)v17 ^ *(_BYTE *)(a2 + v19 + 4)) & 0x3F)) ^ *((_DWORD *)SP1 + (((unsigned int)v21 >> 24) & 0x3F)) ^ *((_DWORD *)SP3 + (((unsigned int)v21 >> 16) & 0x3F)) ^ *((_DWORD *)SP5 + (((unsigned int)v21 >> 8) & 0x3F)) ^ *((_DWORD *)SP7 + (v21 & 0x3F));
    v22 = __ROR4__(v18, 4);
    v23 = *(_DWORD *)(a2 + v19 + 8) ^ v22;
    v17 ^= *((_DWORD *)SP2 + (((*(_DWORD *)(a2 + v19 + 12) ^ v18) >> 24) & 0x3F)) ^ *((_DWORD *)SP4
                                                                                    + (((*(_DWORD *)(a2 + v19 + 12) ^ v18) >> 16) & 0x3F)) ^ *((_DWORD *)SP6 + (((*(_DWORD *)(a2 + v19 + 12) ^ v18) >> 8) & 0x3F)) ^ *((_DWORD *)SP8 + ((*(_BYTE *)(a2 + v19 + 12) ^ (unsigned __int8)v18) & 0x3F)) ^ *((_DWORD *)SP1 + (((unsigned int)v23 >> 24) & 0x3F)) ^ *((_DWORD *)SP3 + (((unsigned int)v23 >> 16) & 0x3F)) ^ *((_DWORD *)SP5 + (((unsigned int)v23 >> 8) & 0x3F)) ^ *((_DWORD *)SP7 + (v23 & 0x3F));
    v19 += 16LL;
  }
  while ( (_DWORD)v19 != 128 );
  v24 = 0LL;
  do
  {
    v25 = __ROR4__(v18, 4);
    v26 = *(_DWORD *)(a2 + v24 + 128) ^ v25;
    v17 ^= *((_DWORD *)SP2 + (((v18 ^ *(_DWORD *)(a2 + v24 + 132)) >> 24) & 0x3F)) ^ *((_DWORD *)SP4
                                                                                     + (((v18 ^ *(_DWORD *)(a2 + v24 + 132)) >> 16) & 0x3F)) ^ *((_DWORD *)SP6 + (((v18 ^ *(_DWORD *)(a2 + v24 + 132)) >> 8) & 0x3F)) ^ *((_DWORD *)SP8 + (((unsigned __int8)v18 ^ *(_BYTE *)(a2 + v24 + 132)) & 0x3F)) ^ *((_DWORD *)SP1 + (((unsigned int)v26 >> 24) & 0x3F)) ^ *((_DWORD *)SP3 + (((unsigned int)v26 >> 16) & 0x3F)) ^ *((_DWORD *)SP5 + (((unsigned int)v26 >> 8) & 0x3F)) ^ *((_DWORD *)SP7 + (v26 & 0x3F));
    v27 = __ROR4__(v17, 4);
    v28 = *(_DWORD *)(a2 + v24 + 136) ^ v27;
    v18 ^= *((_DWORD *)SP2 + (((*(_DWORD *)(a2 + v24 + 140) ^ (unsigned int)v17) >> 24) & 0x3F)) ^ *((_DWORD *)SP4 + (((*(_DWORD *)(a2 + v24 + 140) ^ (unsigned int)v17) >> 16) & 0x3F)) ^ *((_DWORD *)SP6 + (((*(_DWORD *)(a2 + v24 + 140) ^ (unsigned int)v17) >> 8) & 0x3F)) ^ *((_DWORD *)SP8 + ((*(_BYTE *)(a2 + v24 + 140) ^ (unsigned __int8)v17) & 0x3F)) ^ *((_DWORD *)SP1 + (((unsigned int)v28 >> 24) & 0x3F)) ^ *((_DWORD *)SP3 + (((unsigned int)v28 >> 16) & 0x3F)) ^ *((_DWORD *)SP5 + (((unsigned int)v28 >> 8) & 0x3F)) ^ *((_DWORD *)SP7 + (v28 & 0x3F));
    v24 += 16LL;
  }
  while ( (_DWORD)v24 != 128 );
  v29 = 0LL;
  do
  {
    v30 = __ROR4__(v17, 4);
    v31 = *(_DWORD *)(a2 + v29 + 256) ^ v30;
    v18 ^= *((_DWORD *)SP2 + ((((unsigned int)v17 ^ *(_DWORD *)(a2 + v29 + 260)) >> 24) & 0x3F)) ^ *((_DWORD *)SP4 + ((((unsigned int)v17 ^ *(_DWORD *)(a2 + v29 + 260)) >> 16) & 0x3F)) ^ *((_DWORD *)SP6 + ((((unsigned int)v17 ^ *(_DWORD *)(a2 + v29 + 260)) >> 8) & 0x3F)) ^ *((_DWORD *)SP8 + (((unsigned __int8)v17 ^ *(_BYTE *)(a2 + v29 + 260)) & 0x3F)) ^ *((_DWORD *)SP1 + (((unsigned int)v31 >> 24) & 0x3F)) ^ *((_DWORD *)SP3 + (((unsigned int)v31 >> 16) & 0x3F)) ^ *((_DWORD *)SP5 + (((unsigned int)v31 >> 8) & 0x3F)) ^ *((_DWORD *)SP7 + (v31 & 0x3F));
    v32 = __ROR4__(v18, 4);
    v33 = *(_DWORD *)(a2 + v29 + 264) ^ v32;
    v17 ^= *((_DWORD *)SP2 + (((*(_DWORD *)(a2 + v29 + 268) ^ v18) >> 24) & 0x3F)) ^ *((_DWORD *)SP4
                                                                                     + (((*(_DWORD *)(a2 + v29 + 268) ^ v18) >> 16) & 0x3F)) ^ *((_DWORD *)SP6 + (((*(_DWORD *)(a2 + v29 + 268) ^ v18) >> 8) & 0x3F)) ^ *((_DWORD *)SP8 + ((*(_BYTE *)(a2 + v29 + 268) ^ (unsigned __int8)v18) & 0x3F)) ^ *((_DWORD *)SP1 + (((unsigned int)v33 >> 24) & 0x3F)) ^ *((_DWORD *)SP3 + (((unsigned int)v33 >> 16) & 0x3F)) ^ *((_DWORD *)SP5 + (((unsigned int)v33 >> 8) & 0x3F)) ^ *((_DWORD *)SP7 + (v33 & 0x3F));
    v29 += 16LL;
  }
  while ( (_DWORD)v29 != 128 );
  v34 = __ROR4__(v17, 1);
  v35 = v34 ^ (v18 ^ v34) & 0xAAAAAAAA;
  v36 = __ROR4__(v18 ^ (v18 ^ v34) & 0xAAAAAAAA, 1);
  v37 = (v35 ^ (v36 >> 8)) & 0xFF00FF;
  v38 = v37 ^ v35;
  v39 = v36 ^ (v37 << 8);
  v40 = (v38 ^ ((unsigned int)v39 >> 2)) & 0x33333333;
  v41 = v40 ^ v38;
  v42 = v39 ^ 4 * v40;
  v43 = ((unsigned int)v41 >> 16) ^ (unsigned __int16)v42;
  v44 = v43 ^ v42;
  v45 = v41 ^ (v43 << 16);
  v46 = (v44 ^ ((unsigned int)v45 >> 4)) & 0xF0F0F0F;
  result = v46 ^ (unsigned int)v44;
  *(_DWORD *)v48 = v45 ^ 16 * v46;
  *(_DWORD *)(v48 + 4) = result;
  return result;
}
// 67880: using guessed type __int64 SP7[32];
// 67980: using guessed type __int64 SP5[32];
// 67A80: using guessed type __int64 SP3[32];
// 67B80: using guessed type __int64 SP1[32];
// 67C80: using guessed type __int64 SP8[32];
// 67D80: using guessed type __int64 SP6[32];
// 67E80: using guessed type __int64 SP4[32];
// 67F80: using guessed type __int64 SP2[32];

//----- (0000000000048F17) ----------------------------------------------------
__int64 __fastcall cccmac_init(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r14@1
  char v5; // [sp+0h] [bp-40h]@1
  __int64 v6; // [sp+10h] [bp-30h]@1

  v3 = a3;
  v6 = *(_QWORD *)off_69010[0];
  (*(void (__fastcall **)(__int64, signed __int64, signed __int64, __int64))(a1 + 16))(a1, a2 + 32, 16LL, a3);
  bzero(&v5, 0x10uLL);
  memcpy((void *)(*(_QWORD *)a1 + a2 + 32), &v5, *(_QWORD *)(a1 + 8));
  ccmac_generate_subkeys((__int64)ccaes_ltc_ecb_encrypt_mode, v3, a2, a2 + 16);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];
// 69368: using guessed type __int64 ccaes_ltc_ecb_encrypt_mode[2];

//----- (0000000000048FB2) ----------------------------------------------------
void __usercall vng_aes_xts_encrypt_opt(__m128i *a1@<rdx>, __m128i *a2@<rcx>, __m128i *a3@<rdi>, unsigned __int64 a4@<rsi>, __int64 a5@<r8>, __m128i *a6@<r15>)
{
  __m128i *v6; // r12@1
  unsigned __int64 v7; // rbx@1
  __m128i *v8; // r13@1
  __int64 v9; // r8@1
  unsigned __int64 v10; // r14@2
  __int64 v11; // r15@3
  signed __int64 v12; // r14@3
  __int64 v13; // r14@6
  int v14; // eax@6
  __int64 i; // rcx@7
  __int64 v16; // r14@9
  __int64 v17; // rax@9
  __m128i *v18; // [sp+0h] [bp-60h]@3
  __int64 v19; // [sp+8h] [bp-58h]@1
  char v20[16]; // [sp+10h] [bp-50h]@6
  char v21[16]; // [sp+20h] [bp-40h]@10
  __int64 v22; // [sp+30h] [bp-30h]@1

  v19 = a5;
  v6 = a1;
  v7 = a4;
  v8 = a3;
  v9 = off_69010[0];
  v22 = *(_QWORD *)off_69010[0];
  if ( a4 >> 4 )
  {
    v7 = a4 & 0xF;
    v10 = (a4 >> 4) - ((a4 & 0xF) != 0);
    if ( a4 >> 4 == ((a4 & 0xF) != 0) )
    {
      v18 = a2;
    }
    else
    {
      v18 = a2;
      v11 = off_69010[0];
      aesxts_tweak_crypt_group_opt(a3, a1, a2, v19, v10);
      v9 = v11;
      v12 = v10;
      v6 = (__m128i *)((char *)v6 + v12 * 16);
      v8 = &a3[v12];
    }
    a6 = v18;
    if ( v7 )
    {
      v13 = v9;
      v14 = aesxts_tweak_crypt_opt(v8, (__m128i *)v20, v18, v19);
      v9 = v13;
      if ( !v14 )
        goto LABEL_9;
    }
  }
  for ( i = *(_QWORD *)v9; i != v22; i = *(_QWORD *)v16 )
  {
LABEL_9:
    v16 = v9;
    v17 = 0LL;
    do
    {
      v21[v17] = v8[1].m128i_i8[v17];
      v6[1].m128i_i8[v17] = v20[v17];
      ++v17;
    }
    while ( v7 != v17 );
    do
    {
      v21[v7] = v20[v7];
      ++v7;
    }
    while ( v7 != 16 );
    aesxts_tweak_crypt_opt((__m128i *)v21, v6, a6, v19);
  }
}
// 69010: using guessed type __int64 off_69010[2];
// 48FB2: using guessed type char var_50[16];
// 48FB2: using guessed type char var_40[16];

//----- (00000000000490B5) ----------------------------------------------------
signed __int64 __usercall vng_aes_xts_encrypt_aesni@<rax>(__int64 a1@<rdx>, __m128i *a2@<rcx>, __int64 a3@<rdi>, unsigned __int64 a4@<rsi>, __int64 a5@<r8>, __m128i *a6@<r15>)
{
  __m128i *v6; // r12@1
  unsigned __int64 v7; // rbx@1
  __m128i *v8; // r13@1
  __int64 v9; // r8@1
  signed __int64 result; // rax@1
  unsigned __int64 v11; // r14@2
  __int64 v12; // r15@3
  signed __int64 v13; // r14@3
  __int64 v14; // r14@6
  __int64 i; // rcx@7
  __int64 v16; // r14@9
  __int64 v17; // rax@9
  __m128i *v18; // [sp+0h] [bp-60h]@3
  __int64 v19; // [sp+8h] [bp-58h]@1
  char v20[16]; // [sp+10h] [bp-50h]@6
  char v21[16]; // [sp+20h] [bp-40h]@10
  __int64 v22; // [sp+30h] [bp-30h]@1

  v19 = a5;
  v6 = (__m128i *)a1;
  v7 = a4;
  v8 = (__m128i *)a3;
  v9 = off_69010[0];
  v22 = *(_QWORD *)off_69010[0];
  result = 16LL;
  if ( a4 >> 4 )
  {
    v7 = a4 & 0xF;
    result = (a4 & 0xF) != 0;
    v11 = (a4 >> 4) - result;
    if ( a4 >> 4 == result )
    {
      v18 = a2;
    }
    else
    {
      v18 = a2;
      v12 = off_69010[0];
      result = aesxts_tweak_crypt_group_aesni(a3, a1, a2, v19, v11);
      v9 = v12;
      v13 = 16 * v11;
      v6 = (__m128i *)((char *)v6 + v13);
      v8 = (__m128i *)(v13 + a3);
    }
    a6 = v18;
    if ( v7 )
    {
      v14 = v9;
      result = aesxts_tweak_crypt_aesni(v8, (__m128i *)v20, v18, v19);
      v9 = v14;
      if ( !(_DWORD)result )
        goto LABEL_9;
    }
  }
  for ( i = *(_QWORD *)v9; i != v22; i = *(_QWORD *)v16 )
  {
LABEL_9:
    v16 = v9;
    v17 = 0LL;
    do
    {
      v21[v17] = v8[1].m128i_i8[v17];
      v6[1].m128i_i8[v17] = v20[v17];
      ++v17;
    }
    while ( v7 != v17 );
    do
    {
      v21[v7] = v20[v7];
      ++v7;
    }
    while ( v7 != 16 );
    result = aesxts_tweak_crypt_aesni((__m128i *)v21, v6, a6, v19);
  }
  return result;
}
// 69010: using guessed type __int64 off_69010[2];
// 490B5: using guessed type char var_50[16];
// 490B5: using guessed type char var_40[16];

//----- (00000000000491B8) ----------------------------------------------------
void __fastcall vng_aes_xts_decrypt_opt(__m128i *a1, unsigned __int64 a2, __m128i *a3, __m128i *a4, __int64 a5)
{
  __m128i *v5; // r12@1
  __m128i *v6; // r13@1
  __int64 v7; // rdx@1
  unsigned __int64 v8; // rbx@5
  unsigned __int64 v9; // r14@5
  __int64 v10; // r15@6
  int v11; // eax@6
  signed __int64 v12; // r14@7
  __int64 v13; // r15@9
  int v14; // eax@9
  __int64 v15; // r14@10
  __int64 v16; // rax@10
  int v17; // eax@13
  __int64 v18; // rcx@14
  const void *v19; // [sp+8h] [bp-58h]@1
  char v20[16]; // [sp+10h] [bp-50h]@9
  char v21[16]; // [sp+20h] [bp-40h]@9
  __int64 v22; // [sp+30h] [bp-30h]@1

  v19 = a4;
  v5 = a3;
  v6 = a1;
  v7 = off_69010[0];
  v22 = *(_QWORD *)off_69010[0];
  if ( v5 && a1 && a5 && a2 >> 4 )
  {
    v8 = a2 & 0xF;
    v9 = (a2 >> 4) - ((a2 & 0xF) != 0);
    if ( a2 >> 4 != ((a2 & 0xF) != 0) )
    {
      v10 = a5;
      aesxts_tweak_uncrypt_group_opt(a1, v5, a4, a5, v9);
      a5 = v10;
      v7 = off_69010[0];
      if ( v11 )
        goto LABEL_14;
      v12 = v9;
      v6 = &a1[v12];
      v5 = (__m128i *)((char *)v5 + v12 * 16);
    }
    if ( v8 )
    {
      v13 = a5;
      memcpy(v20, v19, 0x10uLL);
      aesxts_mult_x((__m128i *)v20);
      v14 = aesxts_tweak_uncrypt_opt(v6, (__m128i *)v21, (__m128i *)v20, v13);
      v7 = off_69010[0];
      if ( !v14 )
      {
        v15 = off_69010[0];
        v16 = 0LL;
        do
        {
          v20[v16] = v6[1].m128i_i8[v16];
          v5[1].m128i_i8[v16] = v21[v16];
          ++v16;
        }
        while ( v8 != v16 );
        do
        {
          v20[v8] = v21[v8];
          ++v8;
        }
        while ( v8 != 16 );
        v17 = aesxts_tweak_uncrypt_opt((__m128i *)v20, v5, (__m128i *)v19, v13);
        v7 = v15;
      }
    }
  }
LABEL_14:
  v18 = *(_QWORD *)v7;
}
// 69010: using guessed type __int64 off_69010[2];
// 491B8: using guessed type char var_50[16];
// 491B8: using guessed type char var_40[16];

//----- (000000000004930B) ----------------------------------------------------
signed __int64 __fastcall vng_aes_xts_decrypt_aesni(__int64 a1, unsigned __int64 a2, __int64 a3, __m128i *a4, __int64 a5)
{
  __int64 v5; // r12@1
  __m128i *v6; // r13@1
  __int64 v7; // rdx@1
  signed __int64 result; // rax@1
  unsigned __int64 v9; // rbx@5
  unsigned __int64 v10; // r14@5
  __int64 v11; // r15@6
  signed __int64 v12; // r14@7
  __int64 v13; // r15@9
  __int64 v14; // r14@10
  __int64 v15; // rax@10
  __int64 v16; // rcx@15
  const void *v17; // [sp+8h] [bp-58h]@1
  char v18[16]; // [sp+10h] [bp-50h]@9
  char v19[16]; // [sp+20h] [bp-40h]@9
  __int64 v20; // [sp+30h] [bp-30h]@1

  v17 = a4;
  v5 = a3;
  v6 = (__m128i *)a1;
  v7 = off_69010[0];
  v20 = *(_QWORD *)off_69010[0];
  result = 1LL;
  if ( v5 )
  {
    if ( a1 )
    {
      if ( a5 )
      {
        result = 16LL;
        if ( a2 >> 4 )
        {
          v9 = a2 & 0xF;
          v10 = (a2 >> 4) - ((a2 & 0xF) != 0);
          if ( a2 >> 4 != ((a2 & 0xF) != 0) )
          {
            v11 = a5;
            result = aesxts_tweak_uncrypt_group_aesni(a1, v5, a4, a5, v10);
            a5 = v11;
            v7 = off_69010[0];
            if ( (_DWORD)result )
              goto LABEL_15;
            v12 = 16 * v10;
            v6 = (__m128i *)(v12 + a1);
            v5 += v12;
          }
          if ( !v9 )
            goto LABEL_18;
          v13 = a5;
          memcpy(v18, v17, 0x10uLL);
          aesxts_mult_x((__m128i *)v18);
          result = aesxts_tweak_uncrypt_aesni(v6, (__m128i *)v19, (__m128i *)v18, v13);
          v7 = off_69010[0];
          if ( !(_DWORD)result )
          {
            v14 = off_69010[0];
            v15 = 0LL;
            do
            {
              v18[v15] = v6[1].m128i_i8[v15];
              *(_BYTE *)(v5 + v15 + 16) = v19[v15];
              ++v15;
            }
            while ( v9 != v15 );
            do
            {
              v18[v9] = v19[v9];
              ++v9;
            }
            while ( v9 != 16 );
            result = aesxts_tweak_uncrypt_aesni((__m128i *)v18, (__m128i *)v5, (__m128i *)v17, v13);
            v7 = v14;
            if ( !(_DWORD)result )
LABEL_18:
              result = 0LL;
          }
        }
      }
    }
  }
LABEL_15:
  v16 = *(_QWORD *)v7;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];
// 4930B: using guessed type char var_50[16];
// 4930B: using guessed type char var_40[16];

//----- (000000000004945E) ----------------------------------------------------
__int64 (*ccsha224_di())[4]
{
  __int64 v0; // rax@1
  bool v1; // zf@1
  __int64 (*result)[4]; // rax@1

  LODWORD(v0) = cpuid_features();
  v1 = (v0 & 0x20000000000uLL) >> 41 == 0;
  result = (__int64 (*)[4])ccsha224_vng_intel_NOSupplementalSSE3_di;
  if ( !v1 )
    result = off_69058[0];
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 69058: using guessed type __int64 (*off_69058[2])[4];
// 69BE8: using guessed type __int64 ccsha224_vng_intel_NOSupplementalSSE3_di[4];

//----- (0000000000049489) ----------------------------------------------------
__int64 (*ccsha256_di())[4]
{
  __int64 v0; // rax@1
  __int64 (*result)[4]; // rax@2
  __int64 v2; // rax@3
  __int64 v3; // rax@5
  bool v4; // zf@5

  LODWORD(v0) = cpuid_info();
  if ( *(_BYTE *)(v0 + 456) & 0x20 )
  {
    result = (__int64 (*)[4])ccsha256_vng_intel_AVX2_di;
  }
  else
  {
    LODWORD(v2) = cpuid_features();
    if ( _bittest((const unsigned __int64 *)&v2, 0x3Cu) )
    {
      result = (__int64 (*)[4])ccsha256_vng_intel_AVX1_di;
    }
    else
    {
      LODWORD(v3) = cpuid_features();
      v4 = (v3 & 0x20000000000uLL) >> 41 == 0;
      result = (__int64 (*)[4])ccsha256_vng_intel_NOSupplementalSSE3_di;
      if ( !v4 )
        result = off_69060;
    }
  }
  return result;
}
// 4B8C8: using guessed type int cpuid_features(void);
// 4B8CE: using guessed type int cpuid_info(void);
// 69060: using guessed type __int64 (*off_69060)[4];
// 69C68: using guessed type __int64 ccsha256_vng_intel_AVX2_di[4];
// 69CA8: using guessed type __int64 ccsha256_vng_intel_AVX1_di[4];
// 69D28: using guessed type __int64 ccsha256_vng_intel_NOSupplementalSSE3_di[4];

//----- (00000000000494E0) ----------------------------------------------------
__int64 ccsha384_di()
{
  return (__int64)ccsha384_ltc_di;
}
// 69D68: using guessed type __int64 ccsha384_ltc_di[4];

//----- (00000000000494ED) ----------------------------------------------------
__int64 ccsha512_di()
{
  return (__int64)ccsha512_ltc_di;
}
// 69DA8: using guessed type __int64 ccsha512_ltc_di[4];

//----- (00000000000494FA) ----------------------------------------------------
__int64 __fastcall ccn_zero_multi(__int64 a1, void *a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6, char a7)
{
  __int64 v7; // r14@1
  size_t v8; // rbx@1
  void *v9; // rdi@1
  __int64 v10; // rax@3
  char v12; // [sp+0h] [bp-60h]@1
  __int64 v13; // [sp+10h] [bp-50h]@1
  __int64 v14; // [sp+18h] [bp-48h]@1
  __int64 v15; // [sp+20h] [bp-40h]@1
  __int64 v16; // [sp+28h] [bp-38h]@1
  int v17; // [sp+30h] [bp-30h]@1
  int v18; // [sp+34h] [bp-2Ch]@1
  char *v19; // [sp+38h] [bp-28h]@1
  char *v20; // [sp+40h] [bp-20h]@1
  __int64 v21; // [sp+48h] [bp-18h]@1

  v16 = a6;
  v15 = a5;
  v14 = a4;
  v13 = a3;
  v7 = off_69010[0];
  v21 = *(_QWORD *)off_69010[0];
  v20 = &v12;
  v19 = &a7;
  v18 = 48;
  v17 = 16;
  v8 = 8 * a1;
  v9 = a2;
  do
  {
    bzero(v9, v8);
    if ( (unsigned __int64)v17 > 0x28 )
    {
      v10 = (__int64)v19;
      v19 += 8;
    }
    else
    {
      v10 = (__int64)&v20[v17];
      v17 += 8;
    }
    v9 = *(void **)v10;
  }
  while ( *(_QWORD *)v10 );
  return *(_QWORD *)v7;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000049597) ----------------------------------------------------
__int64 *ccsrp_gp_rfc5054_8192()
{
  return &rfc5054_8192;
}
// 69DE8: using guessed type __int64 rfc5054_8192;

//----- (00000000000495A4) ----------------------------------------------------
signed __int64 __fastcall init_0(__int64 a1, __int64 a2, unsigned __int64 a3, const void *a4, unsigned __int64 a5, const void *a6, unsigned __int64 a7, const void *a8)
{
  unsigned __int64 v8; // r13@1
  __int64 v9; // rbx@1
  size_t v10; // rsi@1
  signed __int64 result; // rax@1
  const void *v12; // r15@2
  unsigned __int64 v13; // r12@2
  const void *v14; // [sp+10h] [bp-30h]@2

  v8 = a3;
  v9 = a2;
  *(_QWORD *)(a2 + 8) = 0LL;
  *(_QWORD *)a2 = a1;
  v10 = ***(_QWORD ***)(a1 + 40);
  *(_QWORD *)(v9 + 24) = v10;
  *(_QWORD *)(v9 + 32) = v10;
  result = 0xFFFFFFFFLL;
  if ( a7 <= 0x400 )
  {
    v12 = a4;
    v13 = a5;
    v14 = a6;
    if ( !(a3 >> 32) )
    {
      bzero((void *)(v9 + 104), v10);
      memset((void *)(v9 + 40), 1, *(_QWORD *)(v9 + 24));
      hmac_dbrg_update(v9, v8, v12, v13, v14, a7, a8);
      *(_QWORD *)(v9 + 16) = 1LL;
      result = 0LL;
    }
  }
  return result;
}

//----- (0000000000049651) ----------------------------------------------------
__int64 __fastcall reseed(__int64 a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5)
{
  hmac_dbrg_update(a1, a2, a3, a4, a5, 0LL, 0LL);
  *(_QWORD *)(a1 + 16) = 1LL;
  return 0LL;
}

//----- (000000000004967B) ----------------------------------------------------
__int64 __fastcall generate(__int64 a1, unsigned __int64 a2, void *a3, unsigned __int64 a4, const void *a5)
{
  const void *v5; // r11@1
  unsigned __int64 v6; // r10@1
  void *v7; // r13@1
  unsigned __int64 v8; // rbx@1
  signed int v9; // er14@1
  __int64 v10; // r15@3
  unsigned __int64 v11; // r14@5
  const void *v12; // ST10_8@5
  __int64 v13; // rax@7
  size_t v14; // r14@7
  unsigned __int64 v16; // [sp+8h] [bp-48h]@6
  const void *v17; // [sp+10h] [bp-40h]@6
  __int64 v18; // [sp+20h] [bp-30h]@7

  v5 = a5;
  v6 = a4;
  v7 = a3;
  v8 = a2;
  v9 = -3;
  if ( a2 <= 0xFFFF )
  {
    v9 = -2;
    if ( !*(_WORD *)(a1 + 22) )
    {
      v10 = **(_QWORD **)(*(_QWORD *)a1 + 40LL);
      if ( a5 && a4 )
      {
        v11 = a4;
        v12 = a5;
        hmac_dbrg_update(a1, a4, a5, 0LL, 0LL, 0LL, 0LL);
        v5 = v12;
        v6 = v11;
      }
      v16 = v6;
      v17 = v5;
      if ( a2 )
      {
        v13 = a1 + 40;
        v18 = a1 + 40;
        v14 = *(_QWORD *)(a1 + 8);
        do
        {
          if ( !v14 )
          {
            cchmac(
              v13,
              (const void *)(a1 + 104),
              *(_QWORD *)(a1 + 24),
              v10,
              *(_QWORD *)(a1 + 32),
              (const void *)v18,
              v18);
            v14 = *(_QWORD *)v10;
            *(_QWORD *)(a1 + 8) = *(_QWORD *)v10;
          }
          if ( v8 <= v14 )
            v14 = v8;
          memcpy(v7, (const void *)v18, v14);
          v13 = *(_QWORD *)(a1 + 8) - v14;
          *(_QWORD *)(a1 + 8) = v13;
          v7 = (char *)v7 + v14;
          v8 -= v14;
          v14 = v13;
        }
        while ( v8 );
      }
      v9 = 0;
      hmac_dbrg_update(a1, v16, v17, 0LL, 0LL, 0LL, 0LL);
      ++*(_QWORD *)(a1 + 16);
    }
  }
  return (unsigned int)v9;
}

//----- (00000000000497BA) ----------------------------------------------------
void __fastcall done(__int64 a1)
{
  bzero((void *)(a1 + 40), 0x40uLL);
  bzero((void *)(a1 + 104), 0x40uLL);
}

//----- (00000000000497E8) ----------------------------------------------------
void (__fastcall *__fastcall ccdrbg_factory_nisthmac(__int64 a1, __int64 a2))(__int64 a1)
{
  void (__fastcall *result)(__int64); // rax@1

  *(_QWORD *)a1 = 184LL;
  *(_QWORD *)(a1 + 8) = init_0;
  *(_QWORD *)(a1 + 24) = generate;
  *(_QWORD *)(a1 + 16) = reseed;
  result = done;
  *(_QWORD *)(a1 + 32) = done;
  *(_QWORD *)(a1 + 40) = a2;
  return result;
}

//----- (0000000000049825) ----------------------------------------------------
__int64 __fastcall hmac_dbrg_update(__int64 a1, unsigned __int64 a2, const void *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, const void *a7)
{
  const void *v7; // r13@1
  __int64 v8; // rbx@1
  __int64 v9; // r12@1
  unsigned __int64 v10; // rdx@1
  signed __int64 v11; // rax@1
  unsigned __int64 v12; // rdx@1
  signed __int64 v13; // rax@1
  signed __int64 v14; // rax@1
  __int64 v15; // r15@7
  __int64 v16; // rax@7
  __int64 v17; // r13@7
  signed __int64 v18; // rax@8
  signed __int64 v19; // rax@8
  signed __int64 v20; // rax@8
  __int64 v21; // rax@14
  __int64 v23; // [sp+0h] [bp-80h]@1
  unsigned __int64 v24; // [sp+8h] [bp-78h]@1
  const void *v25; // [sp+10h] [bp-70h]@1
  __int64 v26; // [sp+18h] [bp-68h]@1
  __int64 v27; // [sp+20h] [bp-60h]@1
  bool v28; // [sp+2Dh] [bp-53h]@5
  bool v29; // [sp+2Eh] [bp-52h]@3
  bool v30; // [sp+2Fh] [bp-51h]@1
  unsigned __int64 v31; // [sp+30h] [bp-50h]@1
  const void *v32; // [sp+38h] [bp-48h]@1
  unsigned __int64 v33; // [sp+40h] [bp-40h]@1
  char v34; // [sp+4Eh] [bp-32h]@1
  char v35; // [sp+4Fh] [bp-31h]@1
  __int64 v36; // [sp+50h] [bp-30h]@1

  v33 = a6;
  v32 = a5;
  v31 = a4;
  v7 = a3;
  v25 = a3;
  v24 = a2;
  v36 = *(_QWORD *)off_69010[0];
  v8 = **(_QWORD **)(*(_QWORD *)a1 + 40LL);
  v35 = 0;
  v34 = 1;
  v9 = (__int64)((char *)&v23
               - ((((*(_QWORD *)(v8 + 16) + 2LL * *(_QWORD *)(v8 + 8) + 19) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v10 = *(_QWORD *)(a1 + 32);
  v27 = a1 + 104;
  v11 = cchmac_init(v8, v9, v10, (const void *)(a1 + 104));
  v12 = *(_QWORD *)(a1 + 24);
  v26 = a1 + 40;
  LODWORD(v13) = cchmac_update(v11, v12, (const void *)(a1 + 40), v8, v9);
  LODWORD(v14) = cchmac_update(v13, 1uLL, &v35, v8, v9);
  v30 = v7 != 0LL && a2 != 0;
  if ( v30 )
  {
    LOBYTE(v14) = v7 != 0LL;
    LODWORD(v14) = cchmac_update(v14, v24, v25, v8, v9);
  }
  v29 = v32 != 0LL && v31 != 0;
  if ( v29 )
  {
    LOBYTE(v14) = v32 != 0LL;
    LODWORD(v14) = cchmac_update(v14, v31, v32, v8, v9);
  }
  v28 = a7 != 0LL && v33 != 0;
  if ( v28 )
  {
    LOBYTE(v14) = a7 != 0LL;
    cchmac_update(v14, v33, a7, v8, v9);
  }
  v15 = v27;
  LODWORD(v16) = cchmac_final(v8, v9, v27);
  v17 = v26;
  cchmac(v16, (const void *)v15, *(_QWORD *)(a1 + 24), v8, *(_QWORD *)(a1 + 32), (const void *)v26, v26);
  if ( v28 || v29 || v30 )
  {
    v18 = cchmac_init(v8, v9, *(_QWORD *)(a1 + 32), (const void *)v15);
    LODWORD(v19) = cchmac_update(v18, *(_QWORD *)(a1 + 24), (const void *)v17, v8, v9);
    LODWORD(v20) = cchmac_update(v19, 1uLL, &v34, v8, v9);
    if ( v30 )
      LODWORD(v20) = cchmac_update(v20, v24, v25, v8, v9);
    if ( v29 )
      LODWORD(v20) = cchmac_update(v20, v31, v32, v8, v9);
    if ( v28 )
      cchmac_update(v20, v33, a7, v8, v9);
    LODWORD(v21) = cchmac_final(v8, v9, v15);
    cchmac(v21, (const void *)v15, *(_QWORD *)(a1 + 24), v8, *(_QWORD *)(a1 + 32), (const void *)v17, v17);
  }
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000049A60) ----------------------------------------------------
__int64 *ccsrp_gp_rfc5054_1024()
{
  return &rfc5054_1024;
}
// 6AA08: using guessed type __int64 rfc5054_1024;

//----- (0000000000049A6D) ----------------------------------------------------
__int64 *ccsrp_gp_rfc5054_2048()
{
  return &rfc5054_2048;
}
// 6ABA8: using guessed type __int64 rfc5054_2048;

//----- (0000000000049A7A) ----------------------------------------------------
int __usercall ccmode_ccm_finalize@<eax>(int result@<eax>, void *a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>)
{
  void *v4; // r14@1
  __int64 i; // rax@4

  v4 = a2;
  if ( *(_DWORD *)(a4 + 64) )
  {
    if ( *(_DWORD *)(a4 + 72) )
      (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(*(_QWORD *)a3 + 24LL))(
        a3 + 8,
        1LL,
        a4 + 16,
        a4 + 16);
    for ( i = *(_QWORD *)(*(_QWORD *)a3 + 8LL); i; --i )
      *(_BYTE *)(a4 + i + 31) ^= *(_BYTE *)(a4 + i + 15);
    result = (unsigned __int64)memcpy(v4, (const void *)(a4 + 32), *(_QWORD *)(a4 + 88));
  }
  return result;
}

//----- (0000000000049AF0) ----------------------------------------------------
__int64 *ccsrp_gp_rfc5054_4096()
{
  return &rfc5054_4096;
}
// 6AEC8: using guessed type __int64 rfc5054_4096;

//----- (0000000000049AFD) ----------------------------------------------------
int __fastcall ccmode_ccm_set_iv(__int64 a1, __int64 a2, size_t a3, const void *a4, unsigned __int64 a5, unsigned __int64 a6, unsigned __int64 a7)
{
  unsigned __int64 v7; // r14@1
  __int64 v8; // r15@1
  signed __int64 v9; // rax@1
  size_t v10; // r13@4
  unsigned __int64 v11; // r12@6
  signed __int64 v12; // rax@8
  size_t v13; // rcx@8
  const void *v15; // [sp+8h] [bp-48h]@1
  __int64 v16; // [sp+10h] [bp-40h]@1
  unsigned __int64 v17; // [sp+18h] [bp-38h]@1
  size_t v18; // [sp+20h] [bp-30h]@1

  v17 = a6;
  v7 = a5;
  v15 = a4;
  v18 = a3;
  v8 = *(_QWORD *)(*(_QWORD *)a1 + 8LL);
  v16 = (unsigned int)(v8 - 1);
  *(_DWORD *)(a2 + 64) = 0;
  bzero((void *)(a2 + 48), 0x10uLL);
  *(_DWORD *)(a2 + 68) = 0;
  LODWORD(v9) = v8;
  if ( !(v7 & 1) && (unsigned int)v8 >= v7 && v7 >= 4 )
  {
    v10 = v16 - v18;
    v9 = v16 - v18 - 2;
    if ( (unsigned __int64)v9 <= 5 && v17 <= 0xFEFF )
    {
      v11 = a7;
      *(_QWORD *)(a2 + 80) = v18;
      *(_QWORD *)(a2 + 88) = v7;
      *(_BYTE *)(a2 + 16) = ((4 * v7 & 0xF8) - 8) | ((v17 != 0) << 6) | (v8 - 2 - v18);
      memcpy((void *)(a2 + 17), v15, v18);
      v9 = 1LL << 8 * (unsigned __int8)v10;
      if ( v9 > a7 )
      {
        if ( v16 != v18 )
        {
          v12 = a2 + (unsigned int)(v8 - 1) + 16;
          v13 = 0LL;
          do
          {
            *(_BYTE *)v12 = v11;
            v11 >>= 8;
            ++v13;
            --v12;
          }
          while ( v13 < v10 );
        }
        (*(void (__fastcall **)(signed __int64, signed __int64, signed __int64, signed __int64))(*(_QWORD *)a1 + 24LL))(
          a1 + 8,
          1LL,
          a2 + 16,
          a2 + 16);
        *(_BYTE *)a2 = v10 - 1;
        memcpy((void *)(a2 + 1), v15, v18);
        bzero((void *)(v18 + a2 + 1), v10);
        LODWORD(v9) = (*(int (__fastcall **)(signed __int64, signed __int64, __int64, signed __int64))(*(_QWORD *)a1 + 24LL))(
                        a1 + 8,
                        1LL,
                        a2,
                        a2 + 32);
        if ( v17 )
        {
          *(_BYTE *)(a2 + 16) ^= BYTE1(v17);
          LODWORD(v9) = v17 ^ *(_BYTE *)(a2 + 17);
          *(_BYTE *)(a2 + 17) ^= v17;
          *(_DWORD *)(a2 + 72) = 2;
          *(_DWORD *)(a2 + 64) = 1;
        }
        else
        {
          *(_DWORD *)(a2 + 72) = 0;
          *(_DWORD *)(a2 + 64) = 2;
        }
      }
    }
  }
  return v9;
}

//----- (0000000000049CCE) ----------------------------------------------------
__int64 *ccaes_ccm_decrypt_mode()
{
  __int64 (*v0)[2]; // rax@1

  v0 = ccaes_ecb_encrypt_mode();
  ccm_aes_decrypt = (((*v0)[1] + 15) & 0xFFFFFFFFFFFFFFF8LL) + (((*v0)[0] + 7) & 0xFFFFFFFFFFFFFFF8LL);
  qword_6BDC8 = 96LL;
  qword_6BDD0 = 1LL;
  qword_6BDD8 = (__int64)ccmode_ccm_init;
  qword_6BDE0 = (__int64)ccmode_ccm_set_iv;
  qword_6BDE8 = (__int64)ccmode_ccm_cbcmac;
  qword_6BDF0 = (__int64)ccmode_ccm_decrypt_x86_64;
  qword_6BDF8 = (__int64)ccmode_ccm_finalize;
  qword_6BE00 = (__int64)ccmode_ccm_reset;
  qword_6BE08 = (__int64)v0;
  return &ccm_aes_decrypt;
}
// 6BDC0: using guessed type __int64 ccm_aes_decrypt;
// 6BDC8: using guessed type __int64 qword_6BDC8;
// 6BDD0: using guessed type __int64 qword_6BDD0;
// 6BDD8: using guessed type __int64 qword_6BDD8;
// 6BDE0: using guessed type __int64 qword_6BDE0;
// 6BDE8: using guessed type __int64 qword_6BDE8;
// 6BDF0: using guessed type __int64 qword_6BDF0;
// 6BDF8: using guessed type __int64 qword_6BDF8;
// 6BE00: using guessed type __int64 qword_6BE00;
// 6BE08: using guessed type __int64 qword_6BE08;

//----- (0000000000049D75) ----------------------------------------------------
__int64 __fastcall ccsrp_generate_salt_and_verification(void *a1, __int64 a2, char *a3, unsigned __int64 a4, const void *a5, __int64 a6, __int64 a7, void *a8)
{
  __int64 v8; // rbx@1
  unsigned __int64 v9; // r15@1
  char *v10; // r13@1
  int v11; // ecx@1
  __int64 result; // rax@1
  const void *v13; // [sp+10h] [bp-30h]@1

  v8 = a6;
  v13 = a5;
  v9 = a4;
  v10 = a3;
  v11 = (*(int (__fastcall **)(__int64, __int64))a2)(a2, a6);
  result = 0xFFFFFFFFLL;
  if ( !v11 )
  {
    ccsrp_generate_verifier(a1, v10, v9, v13, v8, a7, a8);
    result = 0LL;
  }
  return result;
}

//----- (0000000000049DE4) ----------------------------------------------------
__int64 __fastcall ccsrp_generate_verifier(void *a1, char *a2, unsigned __int64 a3, const void *a4, __int64 a5, __int64 a6, void *a7)
{
  const void *v7; // r15@1
  unsigned __int64 v8; // rax@1
  char *v9; // r14@1
  __int64 v10; // rbx@1
  __int64 v11; // r15@1
  unsigned __int64 v12; // rax@1
  size_t v13; // rbx@1
  __int64 v14; // r8@3
  __int64 v15; // r9@3
  __int64 result; // rax@3
  __int64 v17; // [sp+10h] [bp-60h]@1
  char *v18; // [sp+18h] [bp-58h]@1
  unsigned __int64 v19; // [sp+20h] [bp-50h]@1
  unsigned __int64 v20; // [sp+28h] [bp-48h]@1
  const void *v21; // [sp+30h] [bp-40h]@1
  __int64 v22; // [sp+38h] [bp-38h]@1
  __int64 v23; // [sp+40h] [bp-30h]@1
  char v24; // [sp+48h] [bp-28h]@1

  v21 = (const void *)a6;
  v19 = a5;
  v7 = a4;
  v20 = a3;
  v18 = a2;
  v23 = *(_QWORD *)off_69010[0];
  v22 = **((_QWORD **)a1 + 1);
  v8 = (8 * v22 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v9 = (char *)&v17 - v8;
  v17 = (__int64)((char *)&v17 - v8);
  v10 = (__int64)((char *)&v17 - v8);
  ccn_zero_multi(v22, (char *)&v17 - v8, (__int64)((char *)&v17 - v8), 0LL, a5, a6, v24);
  ccsrp_generate_x((__int64)a1, v10, v18, v19, v21, v20, v7);
  cczp_power(*((__int64 **)a1 + 1), (__int64)v9, 16LL * **((_QWORD **)a1 + 1) + *((_QWORD *)a1 + 1) + 24, v10);
  v11 = **((_QWORD **)a1 + 1);
  v12 = ccn_write_uint_size(v11, (__int64)v9);
  v13 = 8 * v11 - v12;
  if ( 8 * v11 <= v12 )
    v13 = 0LL;
  bzero(a7, v13);
  ccn_write_uint(v11, (__int64)v9, 8 * v11 - v13, (__int64)((char *)a7 + v13));
  ccn_zero_multi(v22, v9, v17, 0LL, v14, v15, v24);
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v23 )
    result = 0LL;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (0000000000049F26) ----------------------------------------------------
__int64 __fastcall ccsrp_generate_x(__int64 a1, __int64 a2, char *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, const void *a7)
{
  __int64 v7; // r14@1
  __int64 v8; // rbx@1
  char *v9; // r12@1
  __int64 v10; // r13@1
  __int64 v11; // rax@1
  char *v12; // r14@2
  size_t v13; // rax@2
  const void *v14; // rcx@2
  signed __int64 v15; // rax@3
  __int64 v16; // rax@3
  signed __int64 v17; // rax@3
  __int64 v19; // [sp+0h] [bp-60h]@1
  __int64 v20; // [sp+8h] [bp-58h]@2
  char *v21; // [sp+10h] [bp-50h]@1
  unsigned __int64 v22; // [sp+18h] [bp-48h]@1
  unsigned __int64 v23; // [sp+20h] [bp-40h]@1
  const void *v24; // [sp+28h] [bp-38h]@1
  __int64 v25; // [sp+30h] [bp-30h]@1

  v22 = a6;
  v24 = a5;
  v23 = a4;
  v21 = a3;
  v7 = a2;
  v25 = *(_QWORD *)off_69010[0];
  v8 = *(_QWORD *)a1;
  v9 = (char *)&v19 - ((**(_QWORD **)a1 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  v10 = (__int64)((char *)&v19
                - ((((*(_QWORD *)(*(_QWORD *)a1 + 8LL) + *(_QWORD *)(*(_QWORD *)a1 + 16LL) + 19LL) & 0xFFFFFFFFFFFFFFF8LL)
                  + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v11 = ccdigest_init(*(_QWORD *)a1, v10);
  if ( !(*(_BYTE *)(a1 + 16) & 2) )
  {
    v20 = a2;
    v12 = v21;
    v13 = strlen(v21);
    v14 = v12;
    v7 = v20;
    LODWORD(v11) = ccdigest_update(v13, v13, v14, v8, v10);
  }
  LODWORD(v15) = ccdigest_update(v11, 1uLL, ":", v8, v10);
  ccdigest_update(v15, v22, a7, v8, v10);
  (*(void (__fastcall **)(__int64, __int64, char *))(v8 + 56))(v8, v10, v9);
  v16 = ccdigest_init(v8, v10);
  LODWORD(v17) = ccdigest_update(v16, v23, v24, v8, v10);
  ccdigest_update(v17, *(_QWORD *)v8, v9, v8, v10);
  (*(void (__fastcall **)(__int64, __int64, char *))(v8 + 56))(v8, v10, v9);
  ccn_read_uint(**(_QWORD **)(a1 + 8), v7, *(_QWORD *)v8, (unsigned __int64)v9);
  cczp_modn(*(_QWORD *)(a1 + 8), (void *)v7, **(_QWORD **)(a1 + 8), v7);
  cc_clear(*(_QWORD *)v8, v9);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004A092) ----------------------------------------------------
__int64 __fastcall ccsrp_server_start_authentication(__int64 a1, void *a2, char *a3, unsigned __int64 a4, const void *a5, __int64 a6, unsigned __int64 a7, void *a8)
{
  __int64 v8; // r12@1
  __int64 v9; // r15@1
  unsigned __int64 v10; // rax@1
  __int64 v11; // r13@1
  __int64 v12; // rbx@1
  char *v13; // r14@1
  signed __int64 v14; // rax@1
  __int64 v15; // r8@1
  __int64 v16; // r9@1
  signed int v17; // ecx@1
  bool v18; // zf@1
  __int64 v19; // rax@1
  __int64 v20; // rdi@2
  __int64 *v21; // r13@2
  __int64 v22; // r14@2
  __int64 v23; // r15@2
  __int64 v24; // rbx@4
  __int64 v25; // rax@4
  __int64 v26; // r8@4
  __int64 v27; // r9@4
  __int64 v28; // r14@4
  unsigned __int64 v29; // rax@4
  size_t v30; // rbx@4
  __int64 v31; // r13@6
  __int64 v32; // r14@6
  __int64 v33; // rbx@6
  __int64 v34; // r15@6
  void *v35; // r13@6
  __int64 v36; // r9@6
  __int64 v37; // rax@6
  __int64 result; // rax@7
  __int64 v39; // [sp+20h] [bp-A0h]@1
  __int64 v40; // [sp+28h] [bp-98h]@2
  __int64 v41; // [sp+30h] [bp-90h]@2
  __int64 v42; // [sp+38h] [bp-88h]@1
  __int64 v43; // [sp+40h] [bp-80h]@1
  __int64 v44; // [sp+48h] [bp-78h]@1
  __int64 v45; // [sp+50h] [bp-70h]@1
  __int64 v46; // [sp+58h] [bp-68h]@1
  char *v47; // [sp+60h] [bp-60h]@1
  unsigned __int64 v48; // [sp+68h] [bp-58h]@1
  const void *v49; // [sp+70h] [bp-50h]@1
  __int64 v50; // [sp+78h] [bp-48h]@2
  __int64 v51; // [sp+80h] [bp-40h]@1
  __int64 v52; // [sp+88h] [bp-38h]@1
  __int64 v53; // [sp+90h] [bp-30h]@1
  char v54; // [sp+98h] [bp-28h]@2

  v52 = a6;
  v49 = a5;
  v48 = a4;
  v47 = a3;
  v42 = (__int64)a2;
  v8 = a1;
  v53 = *(_QWORD *)off_69010[0];
  v9 = **(_QWORD **)(a1 + 8);
  v10 = (8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v11 = (__int64)((char *)&v39 - v10);
  v12 = (__int64)((char *)&v39 - v10);
  v13 = (char *)&v39 - v10;
  v46 = (__int64)((char *)&v39 - v10);
  v45 = (__int64)((char *)&v39 - v10);
  v44 = (__int64)((char *)&v39 - v10);
  v43 = (__int64)((char *)&v39 - v10);
  v51 = (__int64)((char *)&v39 - v10);
  v52 = 0LL;
  ccn_zero_multi(
    v9,
    (char *)&v39 - v10,
    (__int64)((char *)&v39 - v10),
    (__int64)((char *)&v39 - v10),
    (__int64)((char *)&v39 - v10),
    (__int64)((char *)&v39 - v10),
    (char)((char *)&v39 - v10));
  ccn_read_uint(**(_QWORD **)(v8 + 8), v11, 8LL * **(_QWORD **)(v8 + 8), v52);
  ccn_read_uint(**(_QWORD **)(v8 + 8), v12, 8LL * **(_QWORD **)(v8 + 8), a7);
  *(_BYTE *)(v8 + 16) &= 0xFEu;
  cczp_modn(*(_QWORD *)(a1 + 8), v13, v9, v12);
  v14 = ccn_n(v9, (__int64)v13);
  v17 = -1;
  v18 = v14 == 0;
  v19 = off_69010[0];
  if ( !v18 )
  {
    v40 = v12;
    v50 = v11;
    v41 = v9;
    v20 = **(_QWORD **)(a1 + 8);
    v52 = v8 + 40;
    ccn_zero_multi(v20, (void *)(v8 + 8 * v20 + 40), v8 + 40, 0LL, v15, v16, v54);
    ccn_random_bits(**(_QWORD **)(v8 + 8) << 6, v8 + 8LL * **(_QWORD **)(v8 + 8) + 40, v42);
    v21 = *(__int64 **)(v8 + 8);
    v22 = *v21;
    v23 = v8 + 8 * *v21 + 40;
    if ( (signed int)ccn_cmp(*v21, v23, (__int64)(v21 + 2)) >= 0 )
    {
      ccn_sub(v22, v23, v23, (__int64)(v21 + 2));
      v21 = *(__int64 **)(v8 + 8);
      v22 = *v21;
    }
    v24 = v51;
    v25 = ccsrp_digest_ccn_ccn(v8, v51, (__int64)(v21 + 2), (__int64)&v21[2 * v22 + 3]);
    ccsrp_generate_server_pubkey(v25, (unsigned __int64 *)v50, v8, v24, v26, v27);
    v28 = **(_QWORD **)(v8 + 8);
    v29 = ccn_write_uint_size(v28, v52);
    v30 = 8 * v28 - v29;
    if ( 8 * v28 <= v29 )
      v30 = 0LL;
    bzero(a8, v30);
    v31 = v52;
    ccn_write_uint(v28, v52, 8 * v28 - v30, (__int64)((char *)a8 + v30));
    v32 = v44;
    v33 = v40;
    ccsrp_digest_ccn_ccn(v8, v44, v40, v31);
    v34 = v43;
    v35 = (void *)v50;
    ccsrp_generate_server_S(v8, v43, v50, v32, v33, v36);
    ccsrp_digest_ccn(v8, v34, v8 + 16LL * **(_QWORD **)(v8 + 8) + 40);
    v37 = ccsrp_generate_M(v8, v47, v48, v49, v33, v52);
    ccsrp_generate_H_AMK(v37, v8, v33);
    v51 = v34;
    v52 = 0LL;
    ccn_zero_multi(v41, v35, v33, v46, v45, v34, v32);
    v17 = 0;
    v19 = off_69010[0];
  }
  result = *(_QWORD *)v19;
  if ( result == v53 )
    result = (unsigned int)v17;
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004A3BE) ----------------------------------------------------
__int64 __usercall ccsrp_generate_server_pubkey@<rax>(__int64 a1@<rax>, unsigned __int64 *a2@<rdx>, __int64 a3@<rdi>, __int64 a4@<rsi>, __int64 a5@<r8>, __int64 a6@<r9>)
{
  unsigned __int64 *v6; // r12@1
  __int64 v7; // r14@1
  __int64 v8; // rdi@1
  unsigned __int64 v9; // rax@1
  char *v10; // rbx@1
  __int64 v11; // r15@1
  __int64 v12; // r8@1
  __int64 v13; // r9@1
  __int64 v15; // [sp+0h] [bp-30h]@1
  char v16; // [sp+8h] [bp-28h]@1

  v15 = a1;
  v6 = a2;
  v7 = a3;
  v15 = *(_QWORD *)off_69010[0];
  v8 = **(_QWORD **)(a3 + 8);
  v9 = (8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v10 = (char *)&v15 - v9;
  v11 = (__int64)(&v16 - v9);
  ccn_zero_multi(v8, (char *)&v15 - v9, (__int64)(&v16 - v9), 0LL, a5, a6, v16);
  bzero(v10, 8LL * **(_QWORD **)(v7 + 8));
  cczp_mul(*(signed __int64 **)(v7 + 8), (__int64)v10, a4, v6);
  cczp_power(
    *(__int64 **)(v7 + 8),
    v11,
    16LL * **(_QWORD **)(v7 + 8) + *(_QWORD *)(v7 + 8) + 24,
    v7 + 8LL * **(_QWORD **)(v7 + 8) + 40);
  cczp_add(*(__int64 **)(v7 + 8), v7 + 40, (__int64)v10, v11);
  ccn_zero_multi(**(_QWORD **)(v7 + 8), v10, v11, 0LL, v12, v13, v16);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004A4A9) ----------------------------------------------------
__int64 __fastcall ccsrp_generate_server_S(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 v6; // r15@1
  __int64 v7; // r13@1
  __int64 v8; // r14@1
  __int64 v9; // rdi@1
  unsigned __int64 v10; // rax@1
  __int64 v11; // r12@1
  __int64 v12; // rbx@1
  __int64 v13; // r8@1
  __int64 v14; // r9@1
  __int64 v16; // [sp+0h] [bp-40h]@1
  __int64 v17; // [sp+8h] [bp-38h]@1
  __int64 v18; // [sp+10h] [bp-30h]@1
  char v19; // [sp+18h] [bp-28h]@1

  v16 = a5;
  v6 = a4;
  v7 = a3;
  v17 = a2;
  v8 = a1;
  v18 = *(_QWORD *)off_69010[0];
  v9 = **(_QWORD **)(a1 + 8);
  v10 = (8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v11 = (__int64)((char *)&v16 - v10);
  v12 = (__int64)((char *)&v16 - v10);
  ccn_zero_multi(v9, (char *)&v16 - v10, (__int64)((char *)&v16 - v10), 0LL, a5, a6, v19);
  cczp_power(*(__int64 **)(v8 + 8), v11, v7, v6);
  cczp_mul(*(signed __int64 **)(v8 + 8), v12, v16, (unsigned __int64 *)v11);
  cczp_power(*(__int64 **)(v8 + 8), v17, v12, v8 + 8LL * **(_QWORD **)(v8 + 8) + 40);
  ccn_zero_multi(**(_QWORD **)(v8 + 8), (void *)v11, v12, 0LL, v13, v14, v19);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004A584) ----------------------------------------------------
__int64 __fastcall ccsrp_digest_ccn(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r13@1
  signed __int64 v4; // r14@1
  char *v5; // rbx@1
  unsigned __int64 v6; // rax@1
  size_t v7; // r15@1
  __int64 v9; // [sp+0h] [bp-40h]@1
  __int64 v10; // [sp+8h] [bp-38h]@1
  __int64 v11; // [sp+10h] [bp-30h]@1

  v10 = a3;
  v9 = a2;
  v11 = *(_QWORD *)off_69010[0];
  v3 = **(_QWORD **)(a1 + 8);
  v4 = 8 * v3;
  v5 = (char *)&v9 - ((8 * v3 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v6 = ccn_write_uint_size(**(_QWORD **)(a1 + 8), a2);
  v7 = 0LL;
  if ( 8 * v3 > v6 )
    v7 = v4 - v6;
  bzero(v5, v7);
  ccn_write_uint(v3, v9, v4 - v7, (__int64)&v5[v7]);
  ccdigest(*(_QWORD *)(a1 + 8), v5, v10, *(_QWORD *)a1, 8LL * **(_QWORD **)(a1 + 8));
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004A64B) ----------------------------------------------------
__int64 __fastcall ccsrp_generate_M(__int64 a1, char *a2, unsigned __int64 a3, const void *a4, __int64 a5, __int64 a6)
{
  unsigned __int64 v6; // rax@1
  char *v7; // r15@1
  __int64 v8; // r13@1
  char *v9; // r14@1
  __int64 v10; // r12@1
  char *v11; // rbx@1
  size_t v12; // rax@1
  unsigned __int64 v13; // rax@2
  const void *v14; // rcx@2
  __int64 v15; // r13@4
  __int64 v16; // rax@4
  unsigned __int64 v17; // r15@4
  signed __int64 v18; // rax@4
  signed __int64 v19; // rax@4
  __int64 v20; // rbx@4
  __int64 v22; // [sp+0h] [bp-80h]@1
  char *v23; // [sp+8h] [bp-78h]@1
  __int64 v24; // [sp+10h] [bp-70h]@1
  unsigned __int64 v25; // [sp+18h] [bp-68h]@1
  const void *v26; // [sp+20h] [bp-60h]@1
  __int64 v27; // [sp+28h] [bp-58h]@1
  __int64 v28; // [sp+30h] [bp-50h]@1
  const void *v29; // [sp+38h] [bp-48h]@1
  __int64 v30; // [sp+40h] [bp-40h]@1
  unsigned __int64 v31; // [sp+48h] [bp-38h]@1
  __int64 v32; // [sp+50h] [bp-30h]@1

  v28 = a6;
  v27 = a5;
  v26 = a4;
  v25 = a3;
  v23 = a2;
  v24 = a1;
  v32 = *(_QWORD *)off_69010[0];
  v30 = *(_QWORD *)a1;
  v31 = *(_QWORD *)v30;
  v6 = (v31 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v7 = (char *)&v22 - v6;
  v8 = (__int64)((char *)&v22 - v6);
  v9 = (char *)&v22 - v6;
  v29 = (char *)&v22 - v6;
  v10 = (__int64)((char *)&v22
                - ((((*(_QWORD *)(v30 + 8) + *(_QWORD *)(v30 + 16) + 19LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL));
  ccsrp_digest_ccn(a1, *(_QWORD *)(a1 + 8) + 16LL, (__int64)((char *)&v22 - v6));
  ccsrp_digest_ccn(a1, 16LL * **(_QWORD **)(a1 + 8) + *(_QWORD *)(a1 + 8) + 24, v8);
  v11 = v23;
  v12 = strlen(v23);
  ccdigest(v12, v11, (__int64)v9, v30, v12);
  if ( v31 )
  {
    v13 = v31;
    v14 = v29;
    do
    {
      *(_BYTE *)v14 = *v7++ ^ *(_BYTE *)v8++;
      v14 = (char *)v14 + 1;
      --v13;
    }
    while ( v13 );
  }
  v15 = v30;
  v16 = ccdigest_init(v30, v10);
  v17 = v31;
  LODWORD(v18) = ccdigest_update(v16, v31, v29, v15, v10);
  LODWORD(v19) = ccdigest_update(v18, v17, v9, v15, v10);
  ccdigest_update(v19, v25, v26, v15, v10);
  v20 = v24;
  ccsrp_digest_update_ccn(v24, v10, v27);
  ccsrp_digest_update_ccn(v20, v10, v28);
  ccdigest_update(
    16LL * **(_QWORD **)(v20 + 8),
    v17,
    (const void *)(v20 + 16LL * **(_QWORD **)(v20 + 8) + 40),
    v15,
    v10);
  (*(void (__fastcall **)(__int64, __int64, signed __int64))(v15 + 56))(
    v15,
    v10,
    **(_QWORD **)v20 + v20 + 16LL * **(_QWORD **)(v20 + 8) + 40);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004A824) ----------------------------------------------------
__int64 __usercall ccsrp_generate_H_AMK@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>, __int64 a3@<rsi>)
{
  __int64 v3; // rbx@1
  __int64 v4; // r13@1
  __int64 v5; // r14@1
  __int64 v6; // r15@1
  __int64 v8; // [sp+0h] [bp-30h]@1

  v8 = a1;
  v3 = a2;
  v4 = off_69010[0];
  v8 = *(_QWORD *)off_69010[0];
  v5 = *(_QWORD *)a2;
  v6 = (__int64)((char *)&v8
               - ((((*(_QWORD *)(*(_QWORD *)a2 + 8LL) + *(_QWORD *)(*(_QWORD *)a2 + 16LL) + 19LL) & 0xFFFFFFFFFFFFFFF8LL)
                 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  ccdigest_init(*(_QWORD *)a2, v6);
  ccsrp_digest_update_ccn(a2, v6, a3);
  ccdigest_update(
    *(_QWORD *)v3,
    *(_QWORD *)v5,
    (const void *)(**(_QWORD **)v3 + v3 + 16LL * **(_QWORD **)(v3 + 8) + 40),
    v5,
    v6);
  ccdigest_update(
    16LL * **(_QWORD **)(v3 + 8),
    *(_QWORD *)v5,
    (const void *)(v3 + 16LL * **(_QWORD **)(v3 + 8) + 40),
    v5,
    v6);
  (*(void (__fastcall **)(__int64, __int64, signed __int64))(v5 + 56))(
    v5,
    v6,
    v3 + 16LL * **(_QWORD **)(v3 + 8) + 40 + 2LL * **(_QWORD **)v3);
  return *(_QWORD *)v4;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004A909) ----------------------------------------------------
char __fastcall ccsrp_server_verify_session(__int64 a1, const void *a2, void *a3)
{
  void *v3; // r14@1
  int v4; // eax@1
  int v5; // ecx@1
  char v6; // al@1

  v3 = a3;
  v4 = memcmp((const void *)(**(_QWORD **)a1 + a1 + 16LL * **(_QWORD **)(a1 + 8) + 40), a2, **(_QWORD **)a1);
  v5 = v4;
  v6 = (v4 == 0) | *(_BYTE *)(a1 + 16) & 0xFE;
  *(_BYTE *)(a1 + 16) = v6;
  if ( !v5 )
  {
    memcpy(v3, (const void *)(a1 + 16LL * **(_QWORD **)(a1 + 8) + 2LL * **(_QWORD **)a1 + 40), **(_QWORD **)a1);
    v6 = *(_BYTE *)(a1 + 16);
  }
  return v6 & 1;
}

//----- (000000000004A974) ----------------------------------------------------
__int64 __fastcall ccsrp_digest_update_ccn(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r13@1
  signed __int64 v4; // r14@1
  char *v5; // rbx@1
  unsigned __int64 v6; // rax@1
  size_t v7; // r15@1
  __int64 v9; // [sp+0h] [bp-40h]@1
  __int64 v10; // [sp+8h] [bp-38h]@1
  __int64 v11; // [sp+10h] [bp-30h]@1

  v9 = a3;
  v10 = a2;
  v11 = *(_QWORD *)off_69010[0];
  v3 = **(_QWORD **)(a1 + 8);
  v4 = 8 * v3;
  v5 = (char *)&v9 - ((8 * v3 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v6 = ccn_write_uint_size(**(_QWORD **)(a1 + 8), a3);
  v7 = 0LL;
  if ( 8 * v3 > v6 )
    v7 = v4 - v6;
  bzero(v5, v7);
  ccn_write_uint(v3, v9, v4 - v7, (__int64)&v5[v7]);
  ccdigest_update(*(_QWORD *)(a1 + 8), 8LL * **(_QWORD **)(a1 + 8), v5, *(_QWORD *)a1, v10);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004AA3E) ----------------------------------------------------
__int64 __fastcall ccsrp_digest_ccn_ccn(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  const void *v5; // r13@1
  __int64 v6; // r14@1
  unsigned __int64 v7; // rax@1
  size_t v8; // rbx@1
  __int64 v9; // rbx@3
  unsigned __int64 v10; // rax@3
  size_t v11; // r14@3
  __int64 v12; // rax@5
  unsigned __int64 *v13; // rbx@5
  size_t v14; // r15@5
  void *v15; // r14@5
  __int64 v17; // [sp+0h] [bp-70h]@1
  void *v18; // [sp+8h] [bp-68h]@3
  size_t v19; // [sp+10h] [bp-60h]@1
  void *v20; // [sp+18h] [bp-58h]@1
  unsigned __int64 *v21; // [sp+20h] [bp-50h]@1
  __int64 v22; // [sp+28h] [bp-48h]@1
  __int64 v23; // [sp+30h] [bp-40h]@1
  __int64 v24; // [sp+38h] [bp-38h]@1
  __int64 v25; // [sp+40h] [bp-30h]@1

  v24 = a4;
  v22 = a2;
  v23 = a1;
  v25 = *(_QWORD *)off_69010[0];
  v21 = *(unsigned __int64 **)a1;
  v4 = **(_QWORD **)(a1 + 8);
  v20 = (char *)&v17 - ((*v21 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v19 = 16 * v4;
  v5 = &v17 - 2 * v4;
  v6 = a3;
  v7 = ccn_write_uint_size(v4, a3);
  v8 = 8 * v4 - v7;
  if ( 8 * v4 <= v7 )
    v8 = 0LL;
  bzero(&v17 - 2 * v4, v8);
  ccn_write_uint(v4, v6, 8 * v4 - v8, (__int64)((char *)v5 + v8));
  v18 = (char *)v5 + 8 * v4;
  v9 = **(_QWORD **)(v23 + 8);
  v10 = ccn_write_uint_size(v9, v24);
  v11 = 8 * v9 - v10;
  if ( 8 * v9 <= v10 )
    v11 = 0LL;
  bzero(v18, v11);
  v12 = ccn_write_uint(v9, v24, 8 * v9 - v11, (__int64)((char *)v5 + 8 * v4 + v11));
  v13 = v21;
  v14 = v19;
  v15 = v20;
  ccdigest(v12, v5, (__int64)v20, (__int64)v21, v19);
  cc_clear(v14, (void *)v5);
  ccn_read_uint(**(_QWORD **)(v23 + 8), v22, *v13, (unsigned __int64)v15);
  cc_clear(*v13, v15);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004ABB4) ----------------------------------------------------
unsigned __int64 __fastcall ccsrp_client_start_authentication(__int64 a1, __int64 a2, void *a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 v6; // r12@1
  __int64 *v7; // r13@1
  __int64 v8; // rbx@1
  __int64 v9; // r14@1
  __int64 *v10; // rax@3
  __int64 v11; // r14@3
  signed __int64 v12; // r15@3
  unsigned __int64 v13; // rax@3
  size_t v14; // rbx@3
  char v16; // [sp+0h] [bp-40h]@0
  __int64 v17; // [sp+8h] [bp-38h]@1
  void *v18; // [sp+10h] [bp-30h]@1

  v18 = a3;
  v6 = a1;
  v17 = a1 + 40;
  ccn_zero_multi(**(_QWORD **)(a1 + 8), (void *)(v6 + 8LL * **(_QWORD **)(a1 + 8) + 40), v6 + 40, 0LL, a5, a6, v16);
  ccn_random_bits(**(_QWORD **)(v6 + 8) << 6, v6 + 8LL * **(_QWORD **)(v6 + 8) + 40, a2);
  v7 = *(__int64 **)(a1 + 8);
  v8 = *v7;
  v9 = a1 + 8 * *v7 + 40;
  if ( (signed int)ccn_cmp(*v7, v9, (__int64)(v7 + 2)) >= 0 )
  {
    ccn_sub(v8, v9, v9, (__int64)(v7 + 2));
    v7 = *(__int64 **)(a1 + 8);
    v8 = *v7;
  }
  cczp_power(v7, v17, (__int64)&v7[2 * v8 + 3], a1 + 8 * v8 + 40);
  v10 = *(__int64 **)(a1 + 8);
  v11 = *v10;
  v12 = 8 * *v10;
  v13 = ccn_write_uint_size(*v10, v17);
  v14 = 0LL;
  if ( v12 > v13 )
    v14 = v12 - v13;
  bzero(v18, v14);
  return ccn_write_uint(v11, v17, v12 - v14, (__int64)((char *)v18 + v14));
}

//----- (000000000004ACBD) ----------------------------------------------------
__int64 __fastcall ccsrp_client_process_challenge(__int64 a1, void *a2, unsigned __int64 a3, const void *a4, unsigned __int64 a5, const void *a6, unsigned __int64 a7, void *a8)
{
  __int64 v8; // r15@1
  __int64 v9; // r12@1
  unsigned __int64 v10; // rax@1
  char *v11; // r13@1
  __int64 v12; // rbx@1
  char *v13; // r14@1
  signed __int64 v14; // rax@1
  unsigned __int64 v15; // rdi@2
  __int64 v16; // rbx@3
  __int64 v17; // r12@3
  __int64 v18; // rbx@3
  __int64 v19; // rax@3
  __int64 result; // rax@4
  __int64 v21; // [sp+20h] [bp-B0h]@1
  __int64 v22; // [sp+28h] [bp-A8h]@2
  __int64 v23; // [sp+30h] [bp-A0h]@2
  const void *v24; // [sp+38h] [bp-98h]@1
  unsigned __int64 v25; // [sp+40h] [bp-90h]@1
  __int64 v26; // [sp+48h] [bp-88h]@1
  __int64 v27; // [sp+50h] [bp-80h]@1
  char *v28; // [sp+58h] [bp-78h]@1
  __int64 v29; // [sp+60h] [bp-70h]@2
  char *v30; // [sp+68h] [bp-68h]@1
  unsigned __int64 v31; // [sp+70h] [bp-60h]@1
  const void *v32; // [sp+78h] [bp-58h]@1
  __int64 v33; // [sp+80h] [bp-50h]@1
  __int64 v34; // [sp+88h] [bp-48h]@3
  char *v35; // [sp+90h] [bp-40h]@1
  __int64 v36; // [sp+98h] [bp-38h]@1
  __int64 v37; // [sp+A0h] [bp-30h]@1

  v32 = a6;
  v31 = a5;
  v24 = a4;
  v25 = a3;
  v30 = (char *)a2;
  v8 = a1;
  v37 = *(_QWORD *)off_69010[0];
  v9 = **(_QWORD **)(a1 + 8);
  v10 = (8 * v9 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v11 = (char *)&v21 - v10;
  v12 = (__int64)((char *)&v21 - v10);
  v33 = (__int64)((char *)&v21 - v10);
  v26 = (__int64)((char *)&v21 - v10);
  v13 = (char *)&v21 - v10;
  v28 = (char *)&v21 - v10;
  v27 = (__int64)((char *)&v21 - v10);
  v36 = (__int64)((char *)&v21 - v10);
  v35 = (char *)&v21 - v10;
  v37 = 0LL;
  ccn_zero_multi(
    v9,
    (char *)&v21 - v10,
    (__int64)((char *)&v21 - v10),
    (__int64)((char *)&v21 - v10),
    (__int64)((char *)&v21 - v10),
    (__int64)((char *)&v21 - v10),
    (char)((char *)&v21 - v10));
  ccn_read_uint(**(_QWORD **)(v8 + 8), (__int64)v11, 8LL * **(_QWORD **)(v8 + 8), a7);
  cczp_modn(*(_QWORD *)(a1 + 8), v13, v9, (__int64)v11);
  v14 = ccn_n(v9, (__int64)v13);
  HIDWORD(v35) = -1;
  if ( v14 )
  {
    v22 = a1 + 40;
    ccsrp_digest_ccn_ccn_0(a1, v12, a1 + 40, (__int64)v11);
    v15 = *(_QWORD *)(a1 + 8);
    v29 = v12;
    v23 = v9;
    cczp_modn(v15, v13, v9, v12);
    if ( ccn_n(v9, (__int64)v13) )
    {
      v16 = v34;
      ccsrp_generate_x_0(v8, v34, v30, v31, v32, v25, v24);
      ccsrp_digest_ccn_ccn_0(
        v8,
        v36,
        *(_QWORD *)(v8 + 8) + 16LL,
        16LL * **(_QWORD **)(v8 + 8) + *(_QWORD *)(v8 + 8) + 24);
      cczp_power(*(__int64 **)(v8 + 8), v33, 16LL * **(_QWORD **)(v8 + 8) + *(_QWORD *)(v8 + 8) + 24, v16);
      v17 = v26;
      ccsrp_generate_client_S(v8, v26, v36, v16, v29, (__int64)v11);
      ccsrp_digest_ccn_0(v8, v17, v8 + 16LL * **(_QWORD **)(v8 + 8) + 40);
      v18 = v22;
      v19 = ccsrp_generate_M_0(v8, v30, v31, v32, v22, (__int64)v11);
      ccsrp_generate_H_AMK_0(v19, v8, v18);
      memcpy(a8, (const void *)(**(_QWORD **)v8 + v8 + 16LL * **(_QWORD **)(v8 + 8) + 40), **(_QWORD **)v8);
      v36 = v27;
      v35 = v28;
      v37 = 0LL;
      HIDWORD(v35) = 0;
      ccn_zero_multi(v23, v11, v29, (__int64)v13, v27, v33, (char)v13);
    }
  }
  result = *(_QWORD *)off_69010[0];
  if ( *(_QWORD *)off_69010[0] == v37 )
    result = HIDWORD(v35);
  return result;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004AFAF) ----------------------------------------------------
__int64 __fastcall ccsrp_generate_x_0(__int64 a1, __int64 a2, char *a3, unsigned __int64 a4, const void *a5, unsigned __int64 a6, const void *a7)
{
  __int64 v7; // r14@1
  __int64 v8; // rbx@1
  char *v9; // r12@1
  __int64 v10; // r13@1
  __int64 v11; // rax@1
  char *v12; // r14@2
  size_t v13; // rax@2
  const void *v14; // rcx@2
  signed __int64 v15; // rax@3
  __int64 v16; // rax@3
  signed __int64 v17; // rax@3
  __int64 v19; // [sp+0h] [bp-60h]@1
  __int64 v20; // [sp+8h] [bp-58h]@2
  char *v21; // [sp+10h] [bp-50h]@1
  unsigned __int64 v22; // [sp+18h] [bp-48h]@1
  unsigned __int64 v23; // [sp+20h] [bp-40h]@1
  const void *v24; // [sp+28h] [bp-38h]@1
  __int64 v25; // [sp+30h] [bp-30h]@1

  v22 = a6;
  v24 = a5;
  v23 = a4;
  v21 = a3;
  v7 = a2;
  v25 = *(_QWORD *)off_69010[0];
  v8 = *(_QWORD *)a1;
  v9 = (char *)&v19 - ((**(_QWORD **)a1 + 15LL) & 0xFFFFFFFFFFFFFFF0LL);
  v10 = (__int64)((char *)&v19
                - ((((*(_QWORD *)(*(_QWORD *)a1 + 8LL) + *(_QWORD *)(*(_QWORD *)a1 + 16LL) + 19LL) & 0xFFFFFFFFFFFFFFF8LL)
                  + 15) & 0xFFFFFFFFFFFFFFF0LL));
  v11 = ccdigest_init(*(_QWORD *)a1, v10);
  if ( !(*(_BYTE *)(a1 + 16) & 2) )
  {
    v20 = a2;
    v12 = v21;
    v13 = strlen(v21);
    v14 = v12;
    v7 = v20;
    LODWORD(v11) = ccdigest_update(v13, v13, v14, v8, v10);
  }
  LODWORD(v15) = ccdigest_update(v11, 1uLL, ":", v8, v10);
  ccdigest_update(v15, v22, a7, v8, v10);
  (*(void (__fastcall **)(__int64, __int64, char *))(v8 + 56))(v8, v10, v9);
  v16 = ccdigest_init(v8, v10);
  LODWORD(v17) = ccdigest_update(v16, v23, v24, v8, v10);
  ccdigest_update(v17, *(_QWORD *)v8, v9, v8, v10);
  (*(void (__fastcall **)(__int64, __int64, char *))(v8 + 56))(v8, v10, v9);
  ccn_read_uint(**(_QWORD **)(a1 + 8), v7, *(_QWORD *)v8, (unsigned __int64)v9);
  cczp_modn(*(_QWORD *)(a1 + 8), (void *)v7, **(_QWORD **)(a1 + 8), v7);
  cc_clear(*(_QWORD *)v8, v9);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004B11B) ----------------------------------------------------
__int64 __fastcall ccsrp_generate_client_S(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, __int64 a6)
{
  __int64 v6; // r12@1
  __int64 v7; // r14@1
  __int64 v8; // rdi@1
  unsigned __int64 v9; // rax@1
  __int64 v10; // rbx@1
  __int64 v11; // r13@1
  __int64 v12; // r15@1
  __int64 v13; // r9@1
  __int64 v15; // [sp+0h] [bp-50h]@1
  __int64 v16; // [sp+8h] [bp-48h]@1
  __int64 v17; // [sp+10h] [bp-40h]@1
  __int64 v18; // [sp+18h] [bp-38h]@1
  __int64 v19; // [sp+20h] [bp-30h]@1
  char v20; // [sp+28h] [bp-28h]@1

  v17 = a6;
  v15 = a5;
  v6 = a4;
  v16 = a3;
  v18 = a2;
  v7 = a1;
  v19 = *(_QWORD *)off_69010[0];
  v8 = **(_QWORD **)(a1 + 8);
  v9 = (8 * v8 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v10 = (__int64)((char *)&v15 - v9);
  v11 = (__int64)((char *)&v15 - v9);
  v12 = (__int64)((char *)&v15 - v9);
  ccn_zero_multi(v8, (char *)&v15 - v9, (__int64)((char *)&v15 - v9), (__int64)((char *)&v15 - v9), 0LL, a6, v20);
  cczp_mul(*(signed __int64 **)(v7 + 8), v10, v15, (unsigned __int64 *)v6);
  cczp_add(*(__int64 **)(v7 + 8), v11, v7 + 8LL * **(_QWORD **)(v7 + 8) + 40, v10);
  cczp_power(*(__int64 **)(v7 + 8), v10, 16LL * **(_QWORD **)(v7 + 8) + *(_QWORD *)(v7 + 8) + 24, v6);
  cczp_mul(*(signed __int64 **)(v7 + 8), v12, v16, (unsigned __int64 *)v10);
  cczp_sub(*(__int64 **)(v7 + 8), v10, v17, v12);
  cczp_power(*(__int64 **)(v7 + 8), v18, v10, v11);
  ccn_zero_multi(**(_QWORD **)(v7 + 8), (void *)v10, v11, v12, 0LL, v13, v20);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004B24D) ----------------------------------------------------
__int64 __fastcall ccsrp_digest_ccn_0(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r13@1
  signed __int64 v4; // r14@1
  char *v5; // rbx@1
  unsigned __int64 v6; // rax@1
  size_t v7; // r15@1
  __int64 v9; // [sp+0h] [bp-40h]@1
  __int64 v10; // [sp+8h] [bp-38h]@1
  __int64 v11; // [sp+10h] [bp-30h]@1

  v10 = a3;
  v9 = a2;
  v11 = *(_QWORD *)off_69010[0];
  v3 = **(_QWORD **)(a1 + 8);
  v4 = 8 * v3;
  v5 = (char *)&v9 - ((8 * v3 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v6 = ccn_write_uint_size(**(_QWORD **)(a1 + 8), a2);
  v7 = 0LL;
  if ( 8 * v3 > v6 )
    v7 = v4 - v6;
  bzero(v5, v7);
  ccn_write_uint(v3, v9, v4 - v7, (__int64)&v5[v7]);
  ccdigest(*(_QWORD *)(a1 + 8), v5, v10, *(_QWORD *)a1, 8LL * **(_QWORD **)(a1 + 8));
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004B314) ----------------------------------------------------
__int64 __fastcall ccsrp_generate_M_0(__int64 a1, char *a2, unsigned __int64 a3, const void *a4, __int64 a5, __int64 a6)
{
  unsigned __int64 v6; // rax@1
  char *v7; // r15@1
  __int64 v8; // r13@1
  char *v9; // r14@1
  __int64 v10; // r12@1
  char *v11; // rbx@1
  size_t v12; // rax@1
  unsigned __int64 v13; // rax@2
  const void *v14; // rcx@2
  __int64 v15; // r13@4
  __int64 v16; // rax@4
  unsigned __int64 v17; // r15@4
  signed __int64 v18; // rax@4
  signed __int64 v19; // rax@4
  __int64 v20; // rbx@4
  __int64 v22; // [sp+0h] [bp-80h]@1
  char *v23; // [sp+8h] [bp-78h]@1
  __int64 v24; // [sp+10h] [bp-70h]@1
  unsigned __int64 v25; // [sp+18h] [bp-68h]@1
  const void *v26; // [sp+20h] [bp-60h]@1
  __int64 v27; // [sp+28h] [bp-58h]@1
  __int64 v28; // [sp+30h] [bp-50h]@1
  const void *v29; // [sp+38h] [bp-48h]@1
  __int64 v30; // [sp+40h] [bp-40h]@1
  unsigned __int64 v31; // [sp+48h] [bp-38h]@1
  __int64 v32; // [sp+50h] [bp-30h]@1

  v28 = a6;
  v27 = a5;
  v26 = a4;
  v25 = a3;
  v23 = a2;
  v24 = a1;
  v32 = *(_QWORD *)off_69010[0];
  v30 = *(_QWORD *)a1;
  v31 = *(_QWORD *)v30;
  v6 = (v31 + 15) & 0xFFFFFFFFFFFFFFF0LL;
  v7 = (char *)&v22 - v6;
  v8 = (__int64)((char *)&v22 - v6);
  v9 = (char *)&v22 - v6;
  v29 = (char *)&v22 - v6;
  v10 = (__int64)((char *)&v22
                - ((((*(_QWORD *)(v30 + 8) + *(_QWORD *)(v30 + 16) + 19LL) & 0xFFFFFFFFFFFFFFF8LL) + 15) & 0xFFFFFFFFFFFFFFF0LL));
  ccsrp_digest_ccn_0(a1, *(_QWORD *)(a1 + 8) + 16LL, (__int64)((char *)&v22 - v6));
  ccsrp_digest_ccn_0(a1, 16LL * **(_QWORD **)(a1 + 8) + *(_QWORD *)(a1 + 8) + 24, v8);
  v11 = v23;
  v12 = strlen(v23);
  ccdigest(v12, v11, (__int64)v9, v30, v12);
  if ( v31 )
  {
    v13 = v31;
    v14 = v29;
    do
    {
      *(_BYTE *)v14 = *v7++ ^ *(_BYTE *)v8++;
      v14 = (char *)v14 + 1;
      --v13;
    }
    while ( v13 );
  }
  v15 = v30;
  v16 = ccdigest_init(v30, v10);
  v17 = v31;
  LODWORD(v18) = ccdigest_update(v16, v31, v29, v15, v10);
  LODWORD(v19) = ccdigest_update(v18, v17, v9, v15, v10);
  ccdigest_update(v19, v25, v26, v15, v10);
  v20 = v24;
  ccsrp_digest_update_ccn_0(v24, v10, v27);
  ccsrp_digest_update_ccn_0(v20, v10, v28);
  ccdigest_update(
    16LL * **(_QWORD **)(v20 + 8),
    v17,
    (const void *)(v20 + 16LL * **(_QWORD **)(v20 + 8) + 40),
    v15,
    v10);
  (*(void (__fastcall **)(__int64, __int64, signed __int64))(v15 + 56))(
    v15,
    v10,
    **(_QWORD **)v20 + v20 + 16LL * **(_QWORD **)(v20 + 8) + 40);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004B4ED) ----------------------------------------------------
__int64 __usercall ccsrp_generate_H_AMK_0@<rax>(__int64 a1@<rax>, __int64 a2@<rdi>, __int64 a3@<rsi>)
{
  __int64 v3; // rbx@1
  __int64 v4; // r13@1
  __int64 v5; // r14@1
  __int64 v6; // r15@1
  __int64 v8; // [sp+0h] [bp-30h]@1

  v8 = a1;
  v3 = a2;
  v4 = off_69010[0];
  v8 = *(_QWORD *)off_69010[0];
  v5 = *(_QWORD *)a2;
  v6 = (__int64)((char *)&v8
               - ((((*(_QWORD *)(*(_QWORD *)a2 + 8LL) + *(_QWORD *)(*(_QWORD *)a2 + 16LL) + 19LL) & 0xFFFFFFFFFFFFFFF8LL)
                 + 15) & 0xFFFFFFFFFFFFFFF0LL));
  ccdigest_init(*(_QWORD *)a2, v6);
  ccsrp_digest_update_ccn_0(a2, v6, a3);
  ccdigest_update(
    *(_QWORD *)v3,
    *(_QWORD *)v5,
    (const void *)(**(_QWORD **)v3 + v3 + 16LL * **(_QWORD **)(v3 + 8) + 40),
    v5,
    v6);
  ccdigest_update(
    16LL * **(_QWORD **)(v3 + 8),
    *(_QWORD *)v5,
    (const void *)(v3 + 16LL * **(_QWORD **)(v3 + 8) + 40),
    v5,
    v6);
  (*(void (__fastcall **)(__int64, __int64, signed __int64))(v5 + 56))(
    v5,
    v6,
    v3 + 16LL * **(_QWORD **)(v3 + 8) + 40 + 2LL * **(_QWORD **)v3);
  return *(_QWORD *)v4;
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004B5D2) ----------------------------------------------------
bool __fastcall ccsrp_client_verify_session(__int64 a1, const void *a2)
{
  bool result; // al@1

  result = memcmp((const void *)(a1 + 16LL * **(_QWORD **)(a1 + 8) + 2LL * **(_QWORD **)a1 + 40), a2, **(_QWORD **)a1) == 0;
  *(_BYTE *)(a1 + 16) = result | *(_BYTE *)(a1 + 16) & 0xFE;
  return result;
}

//----- (000000000004B610) ----------------------------------------------------
__int64 __fastcall ccsrp_digest_update_ccn_0(__int64 a1, __int64 a2, __int64 a3)
{
  __int64 v3; // r13@1
  signed __int64 v4; // r14@1
  char *v5; // rbx@1
  unsigned __int64 v6; // rax@1
  size_t v7; // r15@1
  __int64 v9; // [sp+0h] [bp-40h]@1
  __int64 v10; // [sp+8h] [bp-38h]@1
  __int64 v11; // [sp+10h] [bp-30h]@1

  v9 = a3;
  v10 = a2;
  v11 = *(_QWORD *)off_69010[0];
  v3 = **(_QWORD **)(a1 + 8);
  v4 = 8 * v3;
  v5 = (char *)&v9 - ((8 * v3 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v6 = ccn_write_uint_size(**(_QWORD **)(a1 + 8), a3);
  v7 = 0LL;
  if ( 8 * v3 > v6 )
    v7 = v4 - v6;
  bzero(v5, v7);
  ccn_write_uint(v3, v9, v4 - v7, (__int64)&v5[v7]);
  ccdigest_update(*(_QWORD *)(a1 + 8), 8LL * **(_QWORD **)(a1 + 8), v5, *(_QWORD *)a1, v10);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004B6DA) ----------------------------------------------------
__int64 __fastcall ccsrp_digest_ccn_ccn_0(__int64 a1, __int64 a2, __int64 a3, __int64 a4)
{
  __int64 v4; // r15@1
  const void *v5; // r13@1
  __int64 v6; // r14@1
  unsigned __int64 v7; // rax@1
  size_t v8; // rbx@1
  __int64 v9; // rbx@3
  unsigned __int64 v10; // rax@3
  size_t v11; // r14@3
  __int64 v12; // rax@5
  unsigned __int64 *v13; // rbx@5
  size_t v14; // r15@5
  void *v15; // r14@5
  __int64 v17; // [sp+0h] [bp-70h]@1
  void *v18; // [sp+8h] [bp-68h]@3
  size_t v19; // [sp+10h] [bp-60h]@1
  void *v20; // [sp+18h] [bp-58h]@1
  unsigned __int64 *v21; // [sp+20h] [bp-50h]@1
  __int64 v22; // [sp+28h] [bp-48h]@1
  __int64 v23; // [sp+30h] [bp-40h]@1
  __int64 v24; // [sp+38h] [bp-38h]@1
  __int64 v25; // [sp+40h] [bp-30h]@1

  v24 = a4;
  v22 = a2;
  v23 = a1;
  v25 = *(_QWORD *)off_69010[0];
  v21 = *(unsigned __int64 **)a1;
  v4 = **(_QWORD **)(a1 + 8);
  v20 = (char *)&v17 - ((*v21 + 15) & 0xFFFFFFFFFFFFFFF0LL);
  v19 = 16 * v4;
  v5 = &v17 - 2 * v4;
  v6 = a3;
  v7 = ccn_write_uint_size(v4, a3);
  v8 = 8 * v4 - v7;
  if ( 8 * v4 <= v7 )
    v8 = 0LL;
  bzero(&v17 - 2 * v4, v8);
  ccn_write_uint(v4, v6, 8 * v4 - v8, (__int64)((char *)v5 + v8));
  v18 = (char *)v5 + 8 * v4;
  v9 = **(_QWORD **)(v23 + 8);
  v10 = ccn_write_uint_size(v9, v24);
  v11 = 8 * v9 - v10;
  if ( 8 * v9 <= v10 )
    v11 = 0LL;
  bzero(v18, v11);
  v12 = ccn_write_uint(v9, v24, 8 * v9 - v11, (__int64)((char *)v5 + 8 * v4 + v11));
  v13 = v21;
  v14 = v19;
  v15 = v20;
  ccdigest(v12, v5, (__int64)v20, (__int64)v21, v19);
  cc_clear(v14, (void *)v5);
  ccn_read_uint(**(_QWORD **)(v23 + 8), v22, *v13, (unsigned __int64)v15);
  cc_clear(*v13, v15);
  return *(_QWORD *)off_69010[0];
}
// 69010: using guessed type __int64 off_69010[2];

//----- (000000000004B850) ----------------------------------------------------
int _start()
{
  int result; // eax@2

  if ( _realmain )
    result = _realmain();
  else
    result = 0;
  return result;
}
// 6B678: using guessed type int (*_realmain)(void);

//----- (000000000004B86A) ----------------------------------------------------
__int64 *OSKextGetCurrentIdentifier()
{
  return &kmod_info + 2;
}
// 6B5B0: using guessed type __int64 kmod_info;

//----- (000000000004B87B) ----------------------------------------------------
__int64 *OSKextGetCurrentVersionString()
{
  return &kmod_info + 10;
}
// 6B5B0: using guessed type __int64 kmod_info;

//----- (000000000004B88C) ----------------------------------------------------
__int64 OSKextGetCurrentLoadTag()
{
  return *((_DWORD *)&kmod_info + 3);
}
// 6B5B0: using guessed type __int64 kmod_info;

//----- (000000000004B89C) ----------------------------------------------------
int _stop()
{
  int result; // eax@2

  if ( _antimain )
    result = _antimain();
  else
    result = 0;
  return result;
}
// 6B680: using guessed type int (*_antimain)(void);

#error "There were 15 decompilation failure(s) on 594 function(s)"
